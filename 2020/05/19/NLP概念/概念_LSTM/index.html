<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <link rel="icon" type="image/png" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="description" content="desc">
  <meta name="author" content="LiuBingqian">
  <meta name="keywords" content="">
  <title>NLP概念/概念_LSTM ~ 刘秉乾的技术博客 :)</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"  >
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css"  >
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css"  >
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css"  >

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css"  >

<link rel="stylesheet" href="/css/main.css"  >


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css"  >


<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header style="height: 100vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>LiuBingqian`s Blog</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">主页</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">分类</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">所有文章</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">标签</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">关于</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>


</nav>

    <div class="view intro-2" id="background"
         style="background: url('http://bqlab-pic.test.upcdn.net/archive.jpg')no-repeat center center;
           background-size: cover;
           background-attachment: fixed;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              <br>
              
                <p class="mt-3">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>&nbsp;
                  星期二, 五月 19日 2020, 1:56 下午
                </p>
              

              <p>
                
                  
                  &nbsp;<i class="far fa-chart-bar"></i>
                  <span class="post-count">
                    4.6k 字
                  </span>&nbsp;
                

                
                  
                  &nbsp;<i class="far fa-clock"></i>
                  <span class="post-count">
                      15 分钟
                  </span>&nbsp;
                

                
                  <!-- 不蒜子统计文章PV -->
                  
                  &nbsp;<i class="far fa-eye" aria-hidden="true"></i>&nbsp;
                  <span id="busuanzi_container_page_pv">
                    <span id="busuanzi_value_page_pv"></span> 次
                  </span>&nbsp;
                
              </p>
            
          </div>

          
            <div class="scroll-down-bar">
              <i class="fas fa-angle-down scroll-down-arrow"></i>
            </div>
          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="py-5 z-depth-3" id="board">
        <div class="post-content mx-auto" id="post">
          <div class="markdown-body">
            <h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p>LSTM和GRU这两个重要的深度学习模型，几乎所有能看到的所有深度学习论文都是以这两个模型为基础。</p>
<h4 id="朴素RNN与展开"><a href="#朴素RNN与展开" class="headerlink" title="朴素RNN与展开"></a><strong>朴素RNN</strong>与展开</h4><p>是一种处理序列数据的神经网络，某单词根据上下文变化而有不同含义这种任务就适合交给RNN来做。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1429_27_198.png" alt=""></p>
<p>RNN在时序上展开</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1429_35_535.png" alt=""></p>
<h4 id="LSTM与RNN结构对比"><a href="#LSTM与RNN结构对比" class="headerlink" title="LSTM与RNN结构对比"></a><strong>LSTM与RNN结构对比</strong></h4><p>长短期记忆（Long short-term memory, LSTM）是一种特殊的RNN，主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。简单来说，就是相比普通的RNN，LSTM能够在更长的序列中有更好的表现。</p>
<p>LSTM结构和普通RNN的对比图如下，LSTM有两个传输状态，分别是cell state <strong>ct</strong> 和 hidden state <strong>ht</strong>，一般ct改变的很慢，通常是上一个ct加上一些数值。不同的时序下ht改变很快。</p>
<p>而RNN只有一个传输状态也就是<strong>ht</strong>，对应于LSTM中的ct。 </p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1453_46_649.png" alt=""></p>
<h4 id="LSTM的一胞三门两终态"><a href="#LSTM的一胞三门两终态" class="headerlink" title="LSTM的一胞三门两终态"></a><strong>LSTM的一胞三门两终态</strong></h4><ul>
<li><p><strong>输入门</strong>，基本上决定了关心当前向量的强度，或者说当前的cell或者当前输入的词能起多少作用</p>
<p>公式：</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1523_51_723.png" alt=""></p>
<p>示意图：</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1523_06_419.png" alt=""></p>
</li>
</ul>
<ul>
<li><p><strong>遗忘门</strong>，是一种分离机制，告诉cell有些时候需要忘记信息或者保留信息。这种情况看起来可能有一些不直观，通常定义遗忘门时0代表忘记过去。</p>
<p>公式：</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1527_37_613.png" alt=""></p>
<p>示意图：</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1528_00_542.png" alt=""></p>
</li>
</ul>
<ul>
<li><p><strong>输出门</strong>，用于分离那些与预测相关的内容和那些在该时点上需要保留在RNN中的内容。因此可以说，在当前时点上，这个特定的细胞单元并不重要，不过随后会变得很重要，比如现在不会输出最终的softmax但是还是希望保留这一信息，这就是另一个分离机制，会去学习何时输出。</p>
<p>公式：</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1528_52_937.png" alt=""></p>
<p>示意图：</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1529_07_262.png" alt=""></p>
</li>
</ul>
<ul>
<li><p><strong>新的记忆cell</strong>，跟GRU中的作用类似。在数学形式上和上</p>
<p>公式：</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1529_39_278.png" alt=""></p>
<p>示意图：</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1530_44_309.png" alt=""></p>
</li>
<li><p><strong>最终记忆cell</strong>，ct,分离了输入门和遗忘门。就是遗忘门乘以上一次的记忆细胞，加上输入门乘以新的记忆细胞。这里会再次决定，多大程度上希望保存或者忘记过去的内容。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1532_14_473.png" alt=""></p>
</li>
</ul>
<ul>
<li><p><strong>最终隐状态</strong>，如图是输出乘以<strong>tan(最终记忆cell）</strong></p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1532_56_409.png" alt=""></p>
</li>
</ul>
<pre><code class="python"># 遗忘门，输入门，输出门，以及新的记忆细胞都是单层神经网络。

# 其中记忆细胞z是将结果通过tanh激活函数将数值映射在-1 到 1之间，使用tanh而不是sigmod是因为其职责是输入信号而非门控信号。

# 其中输入门zi、遗忘门zf、输出门zo这三个门是由拼接向量乘以相应的权重矩阵后再通过一个sigmoid激活函数转换为0到1之间的数值。</code></pre>
<h4 id="LSTM内部的三个阶段："><a href="#LSTM内部的三个阶段：" class="headerlink" title="LSTM内部的三个阶段："></a><strong>LSTM内部的三个阶段：</strong></h4><ol>
<li><p>忘记阶段</p>
<p>这个阶段主要是对上一个节点传进来的输入进行选择性忘记，具体操作是通过计算得到的zf来作为忘记门控来控制上一个c(t-1)哪些需要记哪些要忘。</p>
</li>
<li><p>选择记忆阶段</p>
<p>对输入xt进行选择性记忆，重要的多记不重要少记。门控信号使用zi输入门控制。</p>
<p>另一个输入内容是z。也是用zi信息门控制。</p>
<p>上面两步得到的结果相加，即可得到传输给下一个状态的c0</p>
</li>
<li><p>输出阶段</p>
<p>对选择记忆阶段输出的c0通过tanh函数放缩，然后输出门z0控制哪些会被作为当前的状态输出。</p>
<p>与普通RNN类似，输出yt往往由最终状态ht得到。</p>
</li>
</ol>
<p><strong>把门控制单元部署到LSTM上的解释</strong></p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1439_41_349.png" alt=""></p>
<p>解析上图第二行，标绿的<strong>神经网络的循环单元</strong>：<br>h(t)，循环单元，等于当前的向量输入与W矩阵相乘，加上之前的状态乘以U矩阵，然后再加上一个偏差b，然后将他们放进tan函数中去，这是标准的RNN迭代。我们像常规的RNN一样去更新迭代。</p>
<p>解析上图第一行<strong>核心公式</strong>;<br>对于准确确定我们计算的函数，我们需要自适应学习需要用多少和哪些维度去更新candidate，以及我们需要截取并使用百分之多少我们之前时间段获得的内容也就是h(t-1)。我们允许使用距离远的过去时间段的内容，并且这个远距离内容并不是使用乘法进入当前时间点，而是相加进入，而核心公式的相加第二项也就是我们刚才说的在下一个时间点之前移动这个内容，我们直接从过去获取这个内容，影响现在并进一步影响决定。这是一种递增法，你可以一直这么处理，你正在决定哪些内容应该被关注，但是如果你一旦关注了他，效果会一直伴随，因为你正在添加更多的东西上去。</p>
<p><strong>遗忘门</strong></p>
<p>最后我们可能想要修剪一下过去自适应的东西，使他不要一直存在。这个需求引入了第二个门控，遗忘门控。遗忘门控给了你一个0到1之间的向量，他像一个标准的循环单元一样计算，他可以删掉在t-1时间点的不再相关的东西，因此如下图<strong>标绿</strong>所示，遗忘门控与之前的隐藏单元之间的元素做点积。这样我们就能忘掉过去的隐藏层。而我们忘掉的部分已经嵌入进更新的候选内容中了。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1046_59_208.png" alt=""></p>
<p><strong>tanh-RNN</strong></p>
<p>如下图，没有GRU的情况下，在隐藏层里，先读取寄存器h，然后执行RNN更新，然后返回。这样做要不停地更新，非常不灵活。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1249_53_627.png" alt=""></p>
<p>如下图，有了GRU，便允许灵活的自适应学习。<br>首先，重置门控，然后学习一个可读的隐藏状态层的子集r，其他部分将会丢弃，这样就有了忘记的能力。<br>然后将读取的r和h点乘，继续进行标准的RNN计算，更新，重写。<br>然后选择一个可写入的子集u，将子集按照下图第四行公式更新写入。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1258_39_454.png" alt=""></p>
<p><strong>tricky概念阐述</strong></p>
<p><strong>更新门</strong>是选择读取什么，然后更新候选值；<strong>重置门</strong>负责选择哪些部分的隐藏状态需要被覆盖。</p>
<p>问题：如何知道哪个子集依赖特定的重置门？</p>
<p>重置门基于当前的输入来计算需要读取/更新隐藏状态的哪个部分以及需要读取/更新哪个以前的状态，如下图，根据输入xt确定需要读取Wr的哪个部分。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1308_26_328.png" alt=""></p>
<p>重置门rt还在更新候选值中发挥作用，与之前的h(t-1)隐藏层点乘，决定了要在多大程度上采纳之前的隐藏状态。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1315_55_469.png" alt="">    </p>
<p><strong>根据提问对整体概念再次阐述</strong></p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1319_51_030.png" alt=""></p>
<p>我们从左往右一直携带一个隐藏状态层，并在每一个时间序列点上，我们想尝试计算一个新的隐藏状态层through乘以矩阵W。当然有的时候不需要计算整个隐藏状态层，仅仅传过来的之前的信息也是有用的。</p>
<p>如果我们仅仅是在每一步做乘法，沿着一个朴素的RNN,我们就失去了长期记忆的概念。而且实质上，我们不能记住超过7步的内容。（我的理解是每一个新的乘法都会冲淡之前的乘法对整体数值概念的程度，比如1连乘七次0.1，就会基本消失掉）</p>
<p>如下，我们想要做的是计算一个候选值更新同时保留我们已经完成的工作。（加法具有累计性，每一个新的加法都不会太大幅度地改变总体值）</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1331_23_028.png" alt=""></p>
<p>具体实现：</p>
<p>更新门ut : ut这个向量的输出值在0到1之间。如上左右切割缩放公式，若它接近1，表示用我们这个时间计算的内容覆盖当前隐藏状态层，接近0表示使用过去的那个状态保持住这个元素向量。</p>
<p>计算更新门ut的方法是使用如下常规的循环单元，他括号内第一项关注当前的输入xt，括号内第二项关心近期的历史h(t-1)。最后加上第三项偏置项。最后投入西格玛函数里，输出一个0到1的结果，然后作用于上文提到的核心公式里。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1337_02_492.png" alt=""></p>
<p>假设一个单元对输入的词敏感，我们想要这些向量的维度记录见过的词，直到一个新词的出现，在新词出现的瞬间，我们说，好了，是时候更新了，忘记过去在这五个维度中存储的内容，现在需要存储一个新词，这就是更新门所做的内容。更新门的括号内第一项能够记录输入词，比如向量的47到52维应该给一个1的值。那意味着，它们将被存储和从计算更新，并且忽略他们曾今在过去存储中的内容。但是比如更新门发现它们正存储着一个介词，他将会将更新值设置为接近0。那意味着47到52维将继续存储上次见过的词，即使它已经是前十个词了。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1524_49_078.png" alt=""></p>
<p>不仅门控会更新，候选值也会同时更新。当然候选值是使用tan函数来输出的，最后的值在-1到1之间。<br>sum up idea就是如果你正在做一个候选值的更新，你总是使用之前的隐藏状态层，和新的输入词在更新门中使用相同的方式。如果你已经在输入中检测到一个新的词汇，你应该存储进47到52维，然后你就应该忽略你过去在那里有的内容，还要一些情况你需要丢掉当前的隐藏层的就是用新的隐藏层取代它。这种取代就是重置门所要做的，重置门能观察之前的隐藏状态层在当前的重要性，并且让他选择一个0-1之间的值，如果重置门的选择的值接近0，必须扔掉之前的隐藏状态层，并且根据新的输入计算。</p>
<p>这里有一个具体语言的类比。比如你正在记录东西，上次见过的词在47到52维之间，当你见到一个新的词，要做的正确的事情是丢掉之前47到52维的历史记录，并且根据新的计算输入更新值。但是上述操作并不是总适用。比如英语中有大量的助词词组，例如make up,make out之类的。比如你想计算出make out这个词的意思，当你先看见了make，并且把它放进了47到52维之间，如果47到52维之间真的存储了主要谓语make的意思，但是后边的out又出来了，你肯定不想丢掉make，因为这是一个词组。在这种情况下，你希望重置门能有一个接近1的值，希望即保留前边的词又import进新的词。</p>
<h3 id=""><a href="#" class="headerlink" title=""></a></h3><h4 id="LSTM和GRU的对比图"><a href="#LSTM和GRU的对比图" class="headerlink" title="LSTM和GRU的对比图"></a>LSTM和GRU的对比图</h4><p>这里要注意LSTM引入了cell细胞的概念，可等价于GRU中的hidden layer。也就是C&lt;==&gt;L</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1649_02_108.png" alt=""></p>
<p>下图LSTM细胞更新公式是LSTM的核心，和GRU中的隐藏状态层更新公式有细微的差别，在LSTM中，有两个门，一个是遗忘门，一个是输入门，他们的值都在0到1之间，所以你可以保存任何之前和新计算的值，然后还能把他们加起来。这是在权衡保留多少值，这点是和GRU不一样的。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1649_57_914.png" alt=""></p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1650_44_552.png" alt=""></p>
<p>候选值更新是完全相同的：</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1654_35_751.png" alt=""></p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1654_43_736.png" alt=""></p>
<p>使用RNN风格的各个门控几乎是完全相同的，都会得到一个0到1之间的某个值，区别是LSTM加了一个新的门，输入门i</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1656_34_491.png" alt=""></p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1656_42_614.png" alt=""></p>
<p>GRU和LSTM还有个区别就是GRU有重置门的功能，在计算候选值的时候可能忽略部分过去的值，LSTM的做法稍微有些不同，LSTM在候选更新的时候总是使用当前输入，但是对于另一部分，不使用C(t-1)。而且在LSTM中也有ht，是通过ct推导过来的。形式和GRU略有不同。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1445_05_063.png" alt=""> </p>
<p>上图是LSTM的构造图，横着的三行是各种门，第四行是候选值更新，和RNN一样。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1859_02_952.png" alt=""></p>
<p>上图紫色部分的加权加法是整个LSTM网络拥有长期记忆能力的秘诀。一种可能是你仅仅复制前一个时间步的内容，如果出现这种情况，梯度是1，就可以把误差反向传播回去，对于每个时间步长都可以持续这么做。所以把这个和前一个时间步加起来，而不是做矩阵乘法，这是让LSTM拥有了长短期记忆的核心思想。这被证实是一个非常强大的想法。这个思想在深度学习做计算机视觉的领域也应用地很好。这种思想也叫做残差网络，一般简写为ResNet，大概来说，大概来说，我们希望能够设计100层的深度神经网络，希望能够成功的训练他们，这里的加号和ResNet的第一次近似的思想完全相同。随着你不断向后层计算，我们会使用常规的神经网络层进行一些非线性运算，但是提供一种选择，你可以把前面层的输入移过来，并把他们相加起来，重复一次又一次直到100层。</p>
<p>如下图片演示是从第128个时间步反向进行，可清晰看出LSTM中的信息会持续多久。</p>
<ul>
<li><p>步数128</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1940_55_049.png" alt=""></p>
</li>
<li><p>步数120：左边RNN基本丢光了</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1941_28_304.png" alt=""></p>
</li>
<li><p>步数90：LSTM还有信息记录</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1942_11_246.png" alt=""></p>
</li>
<li><p>步数60：</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1942_37_957.png" alt=""></p>
</li>
<li><p>步数30</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1942_54_429.png" alt=""></p>
</li>
</ul>
<h4 id="警惕可读性过差的信息"><a href="#警惕可读性过差的信息" class="headerlink" title="警惕可读性过差的信息"></a>警惕可读性过差的信息</h4><p>下图为LSTM原论文配图，严重缺少注释，结构表现不明显，放大缩小无度。和前文知乎作者的图片比起来表达力高下立判。</p>
<p>其实所有的模型or概念只要表达得当都很容易理解。以后再遇到类似下图这种表达力实在呵呵的东西如<strong>高级数据结构/可读性巨差的数学公式时</strong>，要明确这都是已经提出了很久的模型或者固定知识，动作要敏捷，不要在一个具体的点上耗费太多精力，科学地、分层地去看，每一层不会有很多hardcore信息。</p>
<p>某个点过于难顶一定是<strong>作者表达方法不行，或者结构设置不合理，或者自己极度缺乏基础知识</strong>。</p>
<p>遇见这种难顶的点的时候一定注意时间和passion的消耗问题，路一直都明确地在脚下，失败是因为停止行路，停止的原因不就是passion消耗殆尽吗，认识难顶概念被卡住时passion消耗最快。这时必须高度警惕，迅速调整打法，通过阅读其他大牛的技术博客或其他靠谱信息源去理解它，横看成岭侧成峰，现成的概念一定有一个角度是非常好理解的！</p>
<p>我们无法摆脱对前人后人工作的依赖和影响，单个科学家生命周期实在太短，人类科学是作为一个整体前进的。所以写代码的人有维持可读性的责任，论文、博客、图片同理。苏联式教育虽然能打下扎实基本功，但对可读性的追求不够讲究，这种结构性的缺陷要格外注意回避。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191209_1703_05_453.png" alt=""></p>
<h4 id="关于Gated-RNN的一些实践性的指导意见"><a href="#关于Gated-RNN的一些实践性的指导意见" class="headerlink" title="关于Gated RNN的一些实践性的指导意见"></a><strong>关于Gated RNN的一些实践性的指导意见</strong></h4><ol>
<li><p>如果你要建立一个大的循环神经网络RNN，不管是GRU还是LSTM,初始化真的非常非常重要，如果你的RNN不能正常工作，很多时候是你的初始化做的不好。通常来讲把前一个cell的隐藏状态层乘起来对循环矩阵很有用，通常可以让他正交，这样就有了使用线性代数的机会。</p>
</li>
<li><p>RNN中其实没有多少参数，为了能让它们学习到一些有用的东西，提供一个正交的初始化被证实确实是一种更好的方式。有了这种想法，你就能够在RNN中一直做乘法，通常你的初始值非常小，不可以大，大了就会毁掉神经网络。</p>
</li>
<li><p>通常我们把权重随机地初始化为0，不过在设置遗忘门的bias的时候是个例外，如果你把遗忘门的bias设置为一个大小合适的正数，比如1或者2，通常效果会更好。这表明，你应该对遥远的过去给予更多的关注。这种偏置用来保存长久的记忆。这鼓励你得到一个能有使用的有效的长期记忆的模型。如果过去的内容没用了，就丢掉一部分。但是遗忘门开始主要是遗忘。</p>
</li>
<li><p>这些RNN LSTM等等算法结合现在的动态学习率算法效果会更好。比如adam，adaDelta算法等。这笔SGD好多了。</p>
</li>
<li><p>对于各种RNN,在垂直方向使用dropout是很常见的。这通常能够改进性能。但是不能在水平方向使用，因为这样几乎每一个维度都会被dropout，你就不会得到信息流。</p>
</li>
<li><p>assembly，如果希望结果能提升2%，那么使用集成是行之有效的手段。如下使用投票型集成，紫色是使用集成后的提升，可看到性能提升显著。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_2041_14_615.png" alt=""></p>
</li>
</ol>

            <hr>
          </div>
          <br>
          <div>
            <p>
            
              <span>
                <i class="iconfont icon-inbox"></i>
                
                  <a class="hover-with-bg" href="/categories/nlp%E6%A6%82%E5%BF%B5">nlp概念</a>
                  &nbsp;
                
              </span>&nbsp;&nbsp;
            
            
              <span>
                <i class="iconfont icon-tag"></i>
                
                  <a class="hover-with-bg" href="/tags/LSTM">LSTM</a>
                
              </span>
            
            </p>
            
              <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" target="_blank" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
            
          </div>
        </div>
      </div>
    </div>
    <div class="d-none d-lg-block col-lg-2 toc-container">
      
  <div id="toc">
    <p class="h4"><i class="far fa-list-alt"></i>&nbsp;目录</p>
    <div id="tocbot"></div>
  </div>

    </div>
  </div>
</div>

<!-- custom -->


<!-- Comments -->
<div class="col-lg-7 mx-auto nopadding-md">
  <div class="container comments mx-auto" id="comments">
    
  </div>
</div>

    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>

    
  
    <!-- 不蒜子统计PV -->
    
    &nbsp;<span id="busuanzi_container_site_pv">总访问量 
          <span id="busuanzi_value_site_pv"></span> 次</span>&nbsp;
  
  
    <!-- 不蒜子统计UV -->
    
    &nbsp;<span id="busuanzi_container_site_uv">总访客数 
            <span id="busuanzi_value_site_uv"></span> 人</span>&nbsp;
  
  <br>



    

  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js" ></script>
<script src="/lib/popper/popper.min.js" ></script>
<script src="/lib/bootstrap/js/bootstrap.min.js" ></script>
<script src="/lib/mdbootstrap/js/mdb.min.js" ></script>
<script src="/js/main.js" ></script>




  
    <script src="/lib/tocbot/tocbot.min.js" ></script>
  
  <script src="/js/post.js" ></script>



  <script src="/lib/smooth-scroll/smooth-scroll.min.js" ></script>



  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>


<!-- Plugins -->


  

  

  

  

  




  <script src="/lib/prettify/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "NLP概念/概念_LSTM&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 120,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="/lib/anchor/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "false",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>







</body>
</html>

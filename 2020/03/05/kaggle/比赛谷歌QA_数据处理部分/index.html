<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <link rel="icon" type="image/png" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="description" content="desc">
  <meta name="author" content="LiuBingqian">
  <meta name="keywords" content="">
  <title>kaggle/比赛谷歌QA_数据处理部分 ~ 刘秉乾的技术博客 :)</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"  >
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css"  >
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css"  >
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css"  >

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css"  >

<link rel="stylesheet" href="/css/main.css"  >


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css"  >


<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header style="height: 100vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>LiuBingqian`s Blog</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">主页</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">分类</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">所有文章</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">标签</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">关于</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>


</nav>

    <div class="view intro-2" id="background"
         style="background: url('http://bqlab-pic.test.upcdn.net/archive.jpg')no-repeat center center;
           background-size: cover;
           background-attachment: fixed;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              <br>
              
                <p class="mt-3">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>&nbsp;
                  星期四, 三月 5日 2020, 10:49 晚上
                </p>
              

              <p>
                
                  
                  &nbsp;<i class="far fa-chart-bar"></i>
                  <span class="post-count">
                    4.1k 字
                  </span>&nbsp;
                

                
                  
                  &nbsp;<i class="far fa-clock"></i>
                  <span class="post-count">
                      21 分钟
                  </span>&nbsp;
                

                
                  <!-- 不蒜子统计文章PV -->
                  
                  &nbsp;<i class="far fa-eye" aria-hidden="true"></i>&nbsp;
                  <span id="busuanzi_container_page_pv">
                    <span id="busuanzi_value_page_pv"></span> 次
                  </span>&nbsp;
                
              </p>
            
          </div>

          
            <div class="scroll-down-bar">
              <i class="fas fa-angle-down scroll-down-arrow"></i>
            </div>
          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="py-5 z-depth-3" id="board">
        <div class="post-content mx-auto" id="post">
          <div class="markdown-body">
            <h1 id="数据处理部分总体代码"><a href="#数据处理部分总体代码" class="headerlink" title="数据处理部分总体代码"></a>数据处理部分总体代码</h1><p>如下，各个程序块在pycharm中分开运行，用#%%分割。</p>
<p>上来是import部分，然后是数据清洗和构建embedding矩阵等。</p>
<pre><code class="python">import numpy as np
#numpy是主要用于数组计算，线性代数，傅里叶变换等。
import pandas as pd
#pandas基于numpy，可以处理高纬数据
from sklearn.manifold import TSNE
# sklearn是机器学习中常用的第三方模块，对常见的机器学习算法进行了封装，包括回归、降维、分类、聚类,sklearn.manifold是流形学习，非
# 线性降维的手段。最简单的降维手段是随机投影，但是会导致结构丢失,manifold learning是一种类似主成分分析(PCA)的线性框架，不会错失数据结构中的非线性项  ，TSNE提供了一种画图方式，让高维的数据降低为二维画出来
import seaborn as sns
# 基于matplotlib的画图工具
import glob
# glob是查找模块。支持空格 ，问号？，方括号[]这三个通配符。空格代表0个或者多个字符，问号？代表一个字符，[]代表范围，例如0-9.glob主要是两个方法，glob方法获（取全部匹配路径）和iglob方法（逐个获取满足路径）。
from urllib.parse import urlparse
# url.parse定义了URL的标准接口，实现了对url的解析，合并，编码，解码等，#%matplotlib inline，这一句在pycharm和spyder中可以注释掉，主要是在jupter notebook中用于打印图像的
import re
# 正则表达式
from category_encoders.ordinal import OrdinalEncoder
# category_encoders.ordinal可以将非数值型数据比如语言，转换成类别变量例如独热编码，A collection sklearn transformers to encode categorical variables as numeric
import spacy
# 最快的工业级自然语言处理工具，主要功能是分词，词干化，命名实体识别，名词短语提取等
import gc
# gc是垃圾回收机制，gc.collect()是核心函数
import gensim
# genism模块中有word2vec，可以把词汇转换成向量
from nltk.tokenize import TweetTokenizer
# nltk.tokenize就是把句子令牌化，tweetTokenize就是按照空格进行分词，同时针对推文特性，去除@用户名，保留表情符号
import datetime
# datetime是python处理时间和日期的标准库
from scipy import stats
# scipy的stats是统计学的各种分布，stats.chi2是卡方分布，stats.norm是正态分布，还有t和f分布。
from scipy.sparse import hstack,csr_matrix
# vstack，hstack分别是横向合并和纵向合并，csr_matrix是压缩稀疏行格式的矩阵，sparse matrix (稀疏矩阵) ，sparse就是scipy模块中负责处理稀疏矩阵的模块
from sklearn.model_selection import train_test_split,cross_val_score,KFold
# sklearn中的model_selection用于交叉验证，三个包分别是数据集分割，交叉验证，K折交叉验证器
from wordcloud import WordCloud
# 词云
from collections import Counter
# 计数器
from nltk.corpus import stopwords
# 停止词
from nltk.util import ngrams
# ngram
from sklearn.feature_extraction.text import TfidfVectorizer
# sklearn的词频-逆向文件频率
from sklearn.preprocessing import StandardScaler
# StandardScaler是数据归一化和标准化的类。所谓归一化就是使得数据方差为1，均值为0
from sklearn.linear_model import LogisticRegression,LinearRegression,Ridge# 逻辑回归模型
from sklearn.svm import LinearSVC
# 线性分类支持向量机
from sklearn.multiclass import OneVsRestClassifier
# 多分类问题的分类器
import time
# pd.set_option(&#39;max_colwidth&#39;,400)。设置列长度400
from scipy.stats import spearmanr
# stats是统计函数库，spearmanr是斯皮尔曼等级相关系数
from keras.preprocessing.text import Tokenizer
# 令牌化
from keras.preprocessing.sequence import pad_sequences
# keras只能接受相同长度的序列输入，pad_sequences()序列填充让序列长度一致。
import scipy as sp
# scipy用于数学科学工程领域，可以处插值、积分、优化、图像处理、常微分方程、信号处理等问题。
import random
# 随机模块
from sklearn.preprocessing import OneHotEncoder
# 独热码
import os
# 不同的操作系统目录不同，os模块可以处理此问题
import torch
# pytorch框架
import torch.nn as nn
# nn是循环神经网络
import torch.optim as optim
# 实现各种优化算法的库
import torch.nn.functional as F
# torch的nn网络里边的函数库，比如距离函数、损失函数、正则化函数等
import torch.utils.data
# 如上上import，定义了torch的数据格式
import tensorflow_hub as hub
# tensorflow_hub是tensorflow的hub库，里边有很多训练好的hub模型，module = hub.Module(&lt;&lt;Module URL as string&gt;&gt;, trainable=True)
import keras.backend as K
# keras可作为tensorflow，CNTK,Theano的应用程序接口。通过keras.json传递数据
import sys
# sys就是python和解释器打交道的系统。sys.argv是系统带入的参数，sys.version查看版本信息, \n #pip install ..  / / /  &gt; del/null, \n # sys.path.insert(0,/ / /)
import transformers   # transformer模型
import pickle
# 普通数据和python数据之间互相转换，dump是普通换python，load是反过来,pickle是类似于json的一种标准数据格式
from torch.utils.data import Dataset,DataLoader
# Dataset定义torch的数据格式。loader有如下参数:(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None)
from torch.autograd import Variable
# Variable是torch的基本构件
sb = SnowballStemmer(&quot;english&quot;)
# SnowballStemmer另一种词干提取算法
from tqdm import tqdm
# 进度条
from nltk.stem import PorterStemmer
# stem是词干提取，PorterStemmer是其中一种提取算法
ps = PorterStemmer()
from nltk.stem import SnowballStemmer
from torch.optim import lr_scheduler
# lr_scheduler模块提供了根据epoch训练次数来调整学习率的方法
from torch.nn.utils.rnn import pack_padded_sequence,pad_packed_sequence
# 长度不同的句子们打包成一条线/一条线拆分成几个长度不同的句子

#%%
pd.set_option(&#39;max_rows&#39;, 500)
pd.set_option(&#39;max_columns&#39;, 500)
path = &#39;F:/Project/kag1/data&#39;
sample_submission = pd.read_csv(f&#39;{path}/sample_submission.csv&#39;)
test = pd.read_csv(f&#39;{path}/test.csv&#39;).fillna(&#39; &#39;)
train = pd.read_csv(f&#39;{path}/train.csv&#39;).fillna(&#39; &#39;)
#pandas
#%%
def sigmoid(x):
    return 1 / (1 + np.exp(-x)) #定义钟形函数

#无效字符
puncts = [&#39;,&#39;, &#39;.&#39;, &#39;&quot;&#39;, &#39;:&#39;, &#39;)&#39;, &#39;(&#39;, &#39;-&#39;, &#39;!&#39;, &#39;?&#39;, &#39;|&#39;, &#39;;&#39;, &quot;&#39;&quot;, &#39;$&#39;, &#39;&amp;&#39;, &#39;/&#39;, &#39;[&#39;, &#39;]&#39;, &#39;&gt;&#39;, &#39;%&#39;, &#39;=&#39;, &#39;#&#39;, &#39;*&#39;, &#39;+&#39;, &#39;\\&#39;, &#39;•&#39;,  &#39;~&#39;, &#39;@&#39;, &#39;£&#39;,
 &#39;·&#39;, &#39;_&#39;, &#39;{&#39;, &#39;}&#39;, &#39;©&#39;, &#39;^&#39;, &#39;®&#39;, &#39;`&#39;,  &#39;&lt;&#39;, &#39;→&#39;, &#39;°&#39;, &#39;€&#39;, &#39;™&#39;, &#39;›&#39;,  &#39;♥&#39;, &#39;←&#39;, &#39;×&#39;, &#39;§&#39;, &#39;″&#39;, &#39;′&#39;, &#39;Â&#39;, &#39;█&#39;, &#39;½&#39;, &#39;à&#39;, &#39;…&#39;, &#39;\n&#39;, &#39;\xa0&#39;, &#39;\t&#39;,
 &#39;“&#39;, &#39;★&#39;, &#39;”&#39;, &#39;–&#39;, &#39;●&#39;, &#39;â&#39;, &#39;►&#39;, &#39;−&#39;, &#39;¢&#39;, &#39;²&#39;, &#39;¬&#39;, &#39;░&#39;, &#39;¶&#39;, &#39;↑&#39;, &#39;±&#39;, &#39;¿&#39;, &#39;▾&#39;, &#39;═&#39;, &#39;¦&#39;, &#39;║&#39;, &#39;―&#39;, &#39;¥&#39;, &#39;▓&#39;, &#39;—&#39;, &#39;‹&#39;, &#39;─&#39;, &#39;\u3000&#39;, &#39;\u202f&#39;,
 &#39;▒&#39;, &#39;：&#39;, &#39;¼&#39;, &#39;⊕&#39;, &#39;▼&#39;, &#39;▪&#39;, &#39;†&#39;, &#39;■&#39;, &#39;’&#39;, &#39;▀&#39;, &#39;¨&#39;, &#39;▄&#39;, &#39;♫&#39;, &#39;☆&#39;, &#39;é&#39;, &#39;¯&#39;, &#39;♦&#39;, &#39;¤&#39;, &#39;▲&#39;, &#39;è&#39;, &#39;¸&#39;, &#39;¾&#39;, &#39;Ã&#39;, &#39;⋅&#39;, &#39;‘&#39;, &#39;∞&#39;, &#39;«&#39;,
 &#39;∙&#39;, &#39;）&#39;, &#39;↓&#39;, &#39;、&#39;, &#39;│&#39;, &#39;（&#39;, &#39;»&#39;, &#39;，&#39;, &#39;♪&#39;, &#39;╩&#39;, &#39;╚&#39;, &#39;³&#39;, &#39;・&#39;, &#39;╦&#39;, &#39;╣&#39;, &#39;╔&#39;, &#39;╗&#39;, &#39;▬&#39;, &#39;❤&#39;, &#39;ï&#39;, &#39;Ø&#39;, &#39;¹&#39;, &#39;≤&#39;, &#39;‡&#39;, &#39;√&#39;, ]

mispell_dict = {&quot;aren&#39;t&quot; : &quot;are not&quot;,
&quot;can&#39;t&quot; : &quot;cannot&quot;,
&quot;couldn&#39;t&quot; : &quot;could not&quot;,
&quot;couldnt&quot; : &quot;could not&quot;,
&quot;didn&#39;t&quot; : &quot;did not&quot;,
&quot;doesn&#39;t&quot; : &quot;does not&quot;,
&quot;doesnt&quot; : &quot;does not&quot;,
&quot;don&#39;t&quot; : &quot;do not&quot;,
&quot;hadn&#39;t&quot; : &quot;had not&quot;,
&quot;hasn&#39;t&quot; : &quot;has not&quot;,
&quot;haven&#39;t&quot; : &quot;have not&quot;,
&quot;havent&quot; : &quot;have not&quot;,
&quot;he&#39;d&quot; : &quot;he would&quot;,
&quot;he&#39;ll&quot; : &quot;he will&quot;,
&quot;he&#39;s&quot; : &quot;he is&quot;,
&quot;i&#39;d&quot; : &quot;I would&quot;,
&quot;i&#39;d&quot; : &quot;I had&quot;,
&quot;i&#39;ll&quot; : &quot;I will&quot;,
&quot;i&#39;m&quot; : &quot;I am&quot;,
&quot;isn&#39;t&quot; : &quot;is not&quot;,
&quot;it&#39;s&quot; : &quot;it is&quot;,
&quot;it&#39;ll&quot;:&quot;it will&quot;,
&quot;i&#39;ve&quot; : &quot;I have&quot;,
&quot;let&#39;s&quot; : &quot;let us&quot;,
&quot;mightn&#39;t&quot; : &quot;might not&quot;,
&quot;mustn&#39;t&quot; : &quot;must not&quot;,
&quot;shan&#39;t&quot; : &quot;shall not&quot;,
&quot;she&#39;d&quot; : &quot;she would&quot;,
&quot;she&#39;ll&quot; : &quot;she will&quot;,
&quot;she&#39;s&quot; : &quot;she is&quot;,
&quot;shouldn&#39;t&quot; : &quot;should not&quot;,
&quot;shouldnt&quot; : &quot;should not&quot;,
&quot;that&#39;s&quot; : &quot;that is&quot;,
&quot;thats&quot; : &quot;that is&quot;,
&quot;there&#39;s&quot; : &quot;there is&quot;,
&quot;theres&quot; : &quot;there is&quot;,
&quot;they&#39;d&quot; : &quot;they would&quot;,
&quot;they&#39;ll&quot; : &quot;they will&quot;,
&quot;they&#39;re&quot; : &quot;they are&quot;,
&quot;theyre&quot;:  &quot;they are&quot;,
&quot;they&#39;ve&quot; : &quot;they have&quot;,
&quot;we&#39;d&quot; : &quot;we would&quot;,
&quot;we&#39;re&quot; : &quot;we are&quot;,
&quot;weren&#39;t&quot; : &quot;were not&quot;,
&quot;we&#39;ve&quot; : &quot;we have&quot;,
&quot;what&#39;ll&quot; : &quot;what will&quot;,
&quot;what&#39;re&quot; : &quot;what are&quot;,
&quot;what&#39;s&quot; : &quot;what is&quot;,
&quot;what&#39;ve&quot; : &quot;what have&quot;,
&quot;where&#39;s&quot; : &quot;where is&quot;,
&quot;who&#39;d&quot; : &quot;who would&quot;,
&quot;who&#39;ll&quot; : &quot;who will&quot;,
&quot;who&#39;re&quot; : &quot;who are&quot;,
&quot;who&#39;s&quot; : &quot;who is&quot;,
&quot;who&#39;ve&quot; : &quot;who have&quot;,
&quot;won&#39;t&quot; : &quot;will not&quot;,
&quot;wouldn&#39;t&quot; : &quot;would not&quot;,
&quot;you&#39;d&quot; : &quot;you would&quot;,
&quot;you&#39;ll&quot; : &quot;you will&quot;,
&quot;you&#39;re&quot; : &quot;you are&quot;,
&quot;you&#39;ve&quot; : &quot;you have&quot;,
&quot;&#39;re&quot;: &quot; are&quot;,
&quot;wasn&#39;t&quot;: &quot;was not&quot;,
&quot;we&#39;ll&quot;:&quot; will&quot;,
&quot;didn&#39;t&quot;: &quot;did not&quot;,
&quot;tryin&#39;&quot;:&quot;trying&quot;}

def clean_text(x):
    x = str(x)
    for punct in puncts:
        x = x.replace(punct,f&#39;{punct}&#39;)
    return x
# replace函数就是把第一个old变量换成第二个new变量
# f格式化字符串使用举例：
# &gt;&gt;&gt; name = &#39;Eric&#39;
# &gt;&gt;&gt; f&#39;Hello, my name is {name}&#39;
# &#39;Hello, my name is Eric&#39;


def clean_numbers(x):
    x = re.sub(&#39;[0-9]{5,}&#39;, &#39;#####&#39;, x)
# 正则表达式，sub是替换函数，[0-9]是全体数字，{5，}是长度大于等于5的所有字符串
# 这一句的意思就是把所有长度大于等于5的所有数字都替换成五个井号，下面的就是替换成1-4的长度的字符
    x = re.sub(&#39;[0-9]{4}&#39;, &#39;####&#39;, x)
    x = re.sub(&#39;[0-9]{3}&#39;, &#39;###&#39;, x)
    x = re.sub(&#39;[0-9]{2}&#39;, &#39;##&#39;, x)
    return x


def _get_mispell(mispell_dict):
    mispell_re = re.compile(&#39;(%s)&#39; % &#39;|&#39;.join(mispell_dict.keys()))
    return mispell_dict, mispell_re
# 获取拼写错误的词
# re.compile函数：编译一个正则表达式模板，返回一个正则表达式对象，多个正则表达式模板用&#39;|&#39;链接
# join函数：把join前的符号依次链接到join后的各个词的间隙中，如下所示：
# str = &quot;-&quot;;
# seq = (&quot;a&quot;, &quot;b&quot;, &quot;c&quot;); # 字符串序列
# print str.join( seq );


def replace_typical_misspell(text):
    mispellings, mispellings_re = _get_mispell(mispell_dict)

    def replace(match):
        return mispellings[match.group(0)]

    return mispellings_re.sub(replace, text)

# 将拼写有误的词替换掉的函数
# .group(0) 返回母串中与子串匹配的第一个


def clean_data(df,columns:list):
    for col in columns:
        df[col] = df[col].apply(lambda x : clean_numbers(x))
        df[col] = df[col].apply(lambda x : clean_text(x.lower()))
        df[col] = df[col].apply(lambda x : replace_typical_misspell(x))

    return df

# 总的清洗函数：
# df是pandas的数据框架，就是excel表。col是数据行。
# .apply是对df的操作。lambda x基本泛指所有的数据

train = clean_data(train, [&#39;answer&#39;, &#39;question_body&#39;, &#39;question_title&#39;])
test = clean_data(test, [&#39;answer&#39;, &#39;question_body&#39;, &#39;question_title&#39;])


#%%
seed_everything()
#%%
%%time
embedding_test = get_embedding_features(train, test, [&#39;answer&#39;, &#39;question_body&#39;, &#39;question_title&#39;], only_test=True)
embedding_train = {}
embedding_train[&#39;answer_embedding&#39;] = np.load(&#39;/kaggle/input/qa-labeling-files-for-inference/embedding_train_answer_embedding.npy&#39;, allow_pickle=True)
embedding_train[&#39;question_body_embedding&#39;] = np.load(&#39;/kaggle/input/qa-labeling-files-for-inference/embedding_train_question_body_embedding.npy&#39;, allow_pickle=True)
embedding_train[&#39;question_title_embedding&#39;] = np.load(&#39;/kaggle/input/qa-labeling-files-for-inference/embedding_train_question_title_embedding.npy&#39;, allow_pickle=True)
#%%
%%time
dist_features_train, dist_features_test  = get_dist_features(embedding_train, embedding_test)
#%%
tokenizer = Tokenizer()
full_text = list(train[&#39;question_body&#39;]) + \
                       list(train[&#39;answer&#39;]) + \
                       list(train[&#39;question_title&#39;]) + \
                       list(test[&#39;question_body&#39;]) + \
                       list(test[&#39;answer&#39;]) + \
                       list(test[&#39;question_title&#39;])
tokenizer.fit_on_texts(full_text)
#%%
embed_size=300
embedding_path = &quot;/kaggle/input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl&quot;
#%%
%%time
lemma_dict, word_dict = get_word_lemma_dict(full_text)
#%%
# tk.word_index = {k: v for k, v in tk.word_index.items() if k in word_dict.keys()}
# train[&#39;host&#39;] = train[&#39;host&#39;].apply(lambda x: x.split(&#39;.&#39;)[-2])
# test[&#39;host&#39;] = test[&#39;host&#39;].apply(lambda x: x.split(&#39;.&#39;)[-2])
unique_hosts = list(set(train[&#39;host&#39;].unique().tolist() + test[&#39;host&#39;].unique().tolist()))
host_dict = {i + 1: e for i, e in enumerate(unique_hosts)}
host_dict_reverse = {v: k for k, v in host_dict.items()}

unique_categories = list(set(train[&#39;category&#39;].unique().tolist() + test[&#39;category&#39;].unique().tolist()))
category_dict = {i + 1: e for i, e in enumerate(unique_categories)}
category_dict_reverse = {v: k for k, v in category_dict.items()}
max_len = 500
max_len_title = 30
train_question_tokenized = pad_sequences(tokenizer.texts_to_sequences(train[&#39;question_body&#39;]), maxlen = max_len)
train_answer_tokenized = pad_sequences(tokenizer.texts_to_sequences(train[&#39;answer&#39;]), maxlen = max_len)
train_title_tokenized = pad_sequences(tokenizer.texts_to_sequences(train[&#39;question_title&#39;]), maxlen = max_len_title)

test_question_tokenized = pad_sequences(tokenizer.texts_to_sequences(test[&#39;question_body&#39;]), maxlen = max_len)
test_answer_tokenized = pad_sequences(tokenizer.texts_to_sequences(test[&#39;answer&#39;]), maxlen = max_len)
test_title_tokenized = pad_sequences(tokenizer.texts_to_sequences(test[&#39;question_title&#39;]), maxlen = max_len_title)

train_host = train[&#39;host&#39;].apply(lambda x: host_dict_reverse[x]).values
train_category = train[&#39;category&#39;].apply(lambda x: category_dict_reverse[x]).values

test_host = test[&#39;host&#39;].apply(lambda x: host_dict_reverse[x]).values
test_category = test[&#39;category&#39;].apply(lambda x: category_dict_reverse[x]).values
#%%
y = train[sample_submission.columns[1:]].values
#%%
num_workers = 0
bs = 16
n_cat = len(category_dict) + 1
cat_emb = min(np.ceil((len(category_dict)) / 2), 50)
n_host = len(host_dict)+1
host_emb = min(np.ceil((len(host_dict)) / 2), 50)
#%%
bs_test = 16
test_loader = DataLoader(TextDataset(test_question_tokenized, test_answer_tokenized, test_title_tokenized,
                                     test_category, test_host, embedding_test, dist_features_test, test.index),
                                     batch_size=bs_test, shuffle=False, num_workers=num_workers)
#%%

def get_coefs(word,*arr):
    return word,np.asarray(arr,dtype=&#39;float32&#39;)

def load_embeddings(path):
    with open(path,&#39;rb&#39;) as f:
        emb_arr = pickle.load(f)
    return emb_arr
# pickle.load(f)是python的标准导入数据的格式

def build_matrix_adv(embedding_path : str = &#39;&#39;,
                     embedding_path_spellcheck: str = r&#39;f:\embeddings\wiki-news-300d-1M\wiki-news-300d-1M.vec&#39;,
                     word_dict : dict None,
                     lemma_dict : dict = None,
                     max_features : int = 100000,
                     embed_size: int = 300, ):
    spell_model = gensim.models.KeyedVectors.load_word2vec_format(embedding_path_spellcheck)
    # 调用维基字典,字典的用途是训练词向量（本文件目的）
    words = spell_model.index2word
    # index转换成词
    w_rank = {}
    for i,word in enumrate(words):
        w_rank[word] = i
    WORDS = w_rank

    def P(word):
        &quot;word的概率&quot;
        # 使用逆秩作为代理
        return - WORDS.get(word,0)

    def correction(word):
        &quot;最可能的单次拼写的更正：&quot;
        return max(candidates(word),key = P)

    def candidates(word):
        &quot;为word参数 生成可能的拼写更正&quot;
        return (known([word])  or known(edits1(word))or [word])

    def known(words):
        &quot;在字典WORD中出现的words参数的子集&quot;
        # 如26行所示，WORD是一个数组，words是训练集训练出来的word2vec矩阵？
        # 不可以留疑问，这个文件写完后单步调试进去看看做的是什么事情
        return set(w for w in words if w in WORDS)

    def edits1(word):
        &quot;与word相距离一次编辑的所有编辑&quot;
        letters = &#39;abcdefghijklmnopqrstuuvwxyz&#39;
        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]
        deletes = [L + R[1:] for L, R in splits if R]
        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) &gt; 1]
        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]
        inserts = [L + c + R for L, R in splits for c in letters]
        return set(deletes + transposes + replaces + inserts)
### L 和 R这里需要单帧步进研究一下

    def edits2(word):
        &quot;与word相距离两次编辑的所有编辑&quot;
        return (e2 for e1 in edits1(word) for e2 in edits1(e1))

    def singlify(word):
        return &quot;&quot;.join([letter for i,letter in enumerate(word) if i == 0 or letter != word[i - 1]])

# embedding_index = dict(get_coefs(*o.strip().split(&quot; &quot;)) for o in open(embedding_path, encoding=&#39;utf-8&#39;))
# embedding_index = dict(get_coefs(*o.strip().split(&quot; &quot;)) for o in open(embedding_path, encoding=&#39;utf-8&#39;, errors=&#39;ignore&#39;))

    embedding_index = load_embeddings(embedding_path)
    nb_words = min(max_features,len(word_dict))
    embedding_matrix = np.zeros((nb_words + 1,embed_size))
    uknown_words = []
    for word, i in word_dict.items():
        key = word
        embedding_vector = embedding_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
            continue
        embedding_vector = embedding_index.get(word.lower())
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
            continue
        embedding_vector = embedding_index.get(word.upper())
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
            continue
        embedding_vector = embedding_index.get(word.capitalize())
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
            continue
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        word = lc.stem(key)
        embedding_vector = embedding_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        word = sb.stem(key)
        embedding_vector = embedding_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        word = lemma_dict[key]
        embedding_vector = embedding_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue
        if len(key) &gt; 1:
            word = correction(key)
            embedding_vector = embedding_index.get(word)
            if embedding_vector is not None:
                embedding_matrix[word_dict[key]] = embedding_vector
                continue
        unknown_words.append(key)

    print(f&#39;{len(unknown_words) * 100 / len(word_dict):.4f}% words are not in embeddings&#39;)
    return embedding_matrix, nb_words, unknown_words


def get_word_lemma_dict(full_text: list = None, ):
    nlp = spacy.load(&#39;en_core_web_lg&#39;, disable=[&#39;parser&#39;,&#39;ner&#39;,&#39;tagger&#39;])
    nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)
    word_dict = {}
    word_index = 1
    lemma_dict = {}
    docs = nlp.pipe(full_text, n_threads = os.cpu_count())
    for doc in docs:
        for token in doc:
            if (token.text not in word_dict) and (token.pos_ is not &quot;PUNCT&quot;):
                word_dict[token.text] = word_index
                word_index += 1
                lemma_dict[token.text] = token.lemma_

    return lemma_dict, word_dict


def build_matrix(embedding_path: str = &#39;&#39;,
                 embedding_path_spellcheck: str = r&#39;f:\embeddings\wiki-news-300d-1M\wiki-news-300d-1M.vec&#39;,
                 word_dict: dict = None, max_features: int = 100000,
                 embed_size: int= 300, ):

    # embedding_index = dict(get_coefs(*o.strip().split(&quot; &quot;)) for o in open(embedding_path, encoding=&#39;utf-8&#39;))
    embedding_index = load_embeddings(embedding_path)

    nb_words = min(max_features, len(word_dict))
    embedding_matrix = np.zeros((nb_words + 1, embed_size))
    unknown_words = []
    for word, i in word_dict.items():
        key = word
        embedding_vector = embedding_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
            continue
        embedding_vector = embedding_index.get(word.lower())
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
            continue
        embedding_vector = embedding_index.get(word.upper())
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
            continue
        embedding_vector = embedding_index.get(word.capitalize())
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
            continue
        unknown_words.append(key)

    print(f&#39;{len(unknown_words) * 100 / len(word_dict):.4f}% words are not in embeddings&#39;)
    return embedding_matrix, nb_words, unknown_words


def seed_everything(seed=1234):
    random.seed(seed)
    os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True</code></pre>

            <hr>
          </div>
          <br>
          <div>
            <p>
            
              <span>
                <i class="iconfont icon-inbox"></i>
                
                  <a class="hover-with-bg" href="/categories/kaggle">kaggle</a>
                  &nbsp;
                
              </span>&nbsp;&nbsp;
            
            
              <span>
                <i class="iconfont icon-tag"></i>
                
                  <a class="hover-with-bg" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a>
                
              </span>
            
            </p>
            
              <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" target="_blank" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
            
          </div>
        </div>
      </div>
    </div>
    <div class="d-none d-lg-block col-lg-2 toc-container">
      
  <div id="toc">
    <p class="h4"><i class="far fa-list-alt"></i>&nbsp;目录</p>
    <div id="tocbot"></div>
  </div>

    </div>
  </div>
</div>

<!-- custom -->


<!-- Comments -->
<div class="col-lg-7 mx-auto nopadding-md">
  <div class="container comments mx-auto" id="comments">
    
  </div>
</div>

    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>

    
  
    <!-- 不蒜子统计PV -->
    
    &nbsp;<span id="busuanzi_container_site_pv">总访问量 
          <span id="busuanzi_value_site_pv"></span> 次</span>&nbsp;
  
  
    <!-- 不蒜子统计UV -->
    
    &nbsp;<span id="busuanzi_container_site_uv">总访客数 
            <span id="busuanzi_value_site_uv"></span> 人</span>&nbsp;
  
  <br>



    

  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js" ></script>
<script src="/lib/popper/popper.min.js" ></script>
<script src="/lib/bootstrap/js/bootstrap.min.js" ></script>
<script src="/lib/mdbootstrap/js/mdb.min.js" ></script>
<script src="/js/main.js" ></script>




  
    <script src="/lib/tocbot/tocbot.min.js" ></script>
  
  <script src="/js/post.js" ></script>



  <script src="/lib/smooth-scroll/smooth-scroll.min.js" ></script>



  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>


<!-- Plugins -->


  

  

  

  

  




  <script src="/lib/prettify/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "kaggle/比赛谷歌QA_数据处理部分&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 120,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="/lib/anchor/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "false",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>







</body>
</html>

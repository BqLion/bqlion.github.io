<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <link rel="icon" type="image/png" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="description" content="desc">
  <meta name="author" content="LiuBingqian">
  <meta name="keywords" content="">
  <title>Speech and language processing/概念_6_EM算法 ~ 刘秉乾的技术博客 :)</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"  >
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css"  >
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css"  >
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css"  >

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css"  >

<link rel="stylesheet" href="/css/main.css"  >


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css"  >


<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header style="height: 100vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>LiuBingqian`s Blog</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">主页</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">分类</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">所有文章</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">标签</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">关于</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>


</nav>

    <div class="view intro-2" id="background"
         style="background: url('http://bqlab-pic.test.upcdn.net/archive.jpg')no-repeat center center;
           background-size: cover;
           background-attachment: fixed;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              <br>
              
                <p class="mt-3">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>&nbsp;
                  星期一, 二月 24日 2020, 12:04 中午
                </p>
              

              <p>
                
                  
                  &nbsp;<i class="far fa-chart-bar"></i>
                  <span class="post-count">
                    2.5k 字
                  </span>&nbsp;
                

                
                  
                  &nbsp;<i class="far fa-clock"></i>
                  <span class="post-count">
                      8 分钟
                  </span>&nbsp;
                

                
                  <!-- 不蒜子统计文章PV -->
                  
                  &nbsp;<i class="far fa-eye" aria-hidden="true"></i>&nbsp;
                  <span id="busuanzi_container_page_pv">
                    <span id="busuanzi_value_page_pv"></span> 次
                  </span>&nbsp;
                
              </p>
            
          </div>

          
            <div class="scroll-down-bar">
              <i class="fas fa-angle-down scroll-down-arrow"></i>
            </div>
          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="py-5 z-depth-3" id="board">
        <div class="post-content mx-auto" id="post">
          <div class="markdown-body">
            <h1 id="2-EM算法推导"><a href="#2-EM算法推导" class="headerlink" title="2. EM算法推导"></a>2. EM算法推导</h1><h2 id="2-1-基础知识"><a href="#2-1-基础知识" class="headerlink" title="2.1 基础知识"></a>2.1 基础知识</h2><h2 id="2-1-1-凸函数"><a href="#2-1-1-凸函数" class="headerlink" title="2.1.1 凸函数"></a>2.1.1 凸函数</h2><p>设是定义在实数域上的函数，如果对于任意的实数，都有：<br> <img src="https://www.zhihu.com/equation?tex=f%27%27+%5Cge0+%5C%5C" alt="[公式]"><br>那么是凸函数。若不是单个实数，而是由实数组成的向量，此时，如果函数的 Hesse 矩阵是半正定的，即<br> <img src="https://www.zhihu.com/equation?tex=H%27%27+%5Cge+0+%5C%5C" alt="[公式]"><br>是凸函数。特别地，如果 <img src="https://www.zhihu.com/equation?tex=f%27%27+%3E+0" alt="[公式]"> 或者  <img src="https://www.zhihu.com/equation?tex=H%27%27+%3E+0" alt="[公式]"> ，称为严格凸函数。</p>
<h2 id="2-1-2-Jensen不等式"><a href="#2-1-2-Jensen不等式" class="headerlink" title="2.1.2 Jensen不等式"></a>2.1.2 Jensen不等式</h2><p>如下图，如果函数 <img src="https://www.zhihu.com/equation?tex=f" alt="[公式]"> 是凸函数， <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 是随机变量，有 0.5 的概率是 a，有 0.5 的概率是 b， <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 的期望值就是 a 和 b 的中值了那么：<br> <img src="https://www.zhihu.com/equation?tex=E%5Bf%28x%29%5D+%5Cge+f%28E%28x%29%29+%5C%5C" alt="[公式]"><br>其中，<img src="https://www.zhihu.com/equation?tex=E%5Bf%28x%29%5D+%3D+0.5f%28a%29+%2B+0.5+f%28b%29%EF%BC%8Cf%28E%28x%29%29+%3D+f%280.5a+%2B+0.5b%29" alt="[公式]"> ，这里 a 和 b 的权值为 0.5,  <img src="https://www.zhihu.com/equation?tex=f%28a%29" alt="[公式]">  与 a 的权值相等，<img src="https://www.zhihu.com/equation?tex=f%28b%29" alt="[公式]"> 与 b 的权值相等。</p>
<p>特别地，如果函数 <img src="https://www.zhihu.com/equation?tex=f" alt="[公式]">  是严格凸函数，当且仅当： <img src="https://www.zhihu.com/equation?tex=p%28x+%3D+E%28x%29%29+%3D+1" alt="[公式]">  (即随机变量是常量) 时等号成立。</p>
<p><img src="https://pic1.zhimg.com/v2-22d1d68bb9db46d48c1a4c194477427c_b.jpg" alt="img"></p>
<p>注：若函数  <img src="https://www.zhihu.com/equation?tex=f" alt="[公式]">  是凹函数，Jensen不等式符号相反。</p>
<h2 id="2-1-3-期望"><a href="#2-1-3-期望" class="headerlink" title="2.1.3 期望"></a>2.1.3 期望</h2><p>对于离散型随机变量 X 的概率分布为  <img src="https://www.zhihu.com/equation?tex=p_i+%3D+p%5C%7BX%3Dx_i%5C%7D" alt="[公式]"> ，数学期望 <img src="https://www.zhihu.com/equation?tex=E%28X%29" alt="[公式]">  为：<br> <img src="https://www.zhihu.com/equation?tex=E%28X%29+%3D+%5Csum+%5Climits+_i+x_ip_i+%5C%5C" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=p_i" alt="[公式]"> 是权值，满足两个条件 <img src="https://www.zhihu.com/equation?tex=1+%5Cge+p_i+%5Cge+0%EF%BC%8C%5Csum+%5Climits+_i+p_i+%3D+1" alt="[公式]">  。</p>
<p>若连续型随机变量X的概率密度函数为 <img src="https://www.zhihu.com/equation?tex=f%28x%29" alt="[公式]"> ，则数学期望 <img src="https://www.zhihu.com/equation?tex=E%28X%29" alt="[公式]"> 为：<br> <img src="https://www.zhihu.com/equation?tex=E%28X%29+%3D+%5Cint+_+%7B-%5Cinfty%7D+%5E%7B%2B%5Cinfty%7D+xf%28x%29+dx+%5C%5C" alt="[公式]"><br>设 <img src="https://www.zhihu.com/equation?tex=Y+%3D+g%28X%29" alt="[公式]">， 若 <img src="https://www.zhihu.com/equation?tex=X" alt="[公式]"> 是离散型随机变量，则：<br> <img src="https://www.zhihu.com/equation?tex=E%28Y%29+%3D+%5Csum+%5Climits+_i+g%28x_i%29p_i+%5C%5C" alt="[公式]"><br>若  <img src="https://www.zhihu.com/equation?tex=X" alt="[公式]">  是连续型随机变量，则：<br> <img src="https://www.zhihu.com/equation?tex=E%28X%29+%3D+%5Cint+_+%7B-%5Cinfty%7D+%5E%7B%2B%5Cinfty%7D+g%28x%29f%28x%29+dx+%5C%5C" alt="[公式]"> </p>
<h2 id="2-2-EM算法的推导"><a href="#2-2-EM算法的推导" class="headerlink" title="2.2 EM算法的推导"></a>2.2 EM算法的推导</h2><p>对于 <img src="https://www.zhihu.com/equation?tex=m" alt="[公式]"> 个相互独立的样本 <img src="https://www.zhihu.com/equation?tex=x%3D%28x%5E%7B%281%29%7D%2Cx%5E%7B%282%29%7D%2C...x%5E%7B%28m%29%7D%29" alt="[公式]"> ，对应的隐含数据 <img src="https://www.zhihu.com/equation?tex=z%3D%28z%5E%7B%281%29%7D%2Cz%5E%7B%282%29%7D%2C...z%5E%7B%28m%29%7D%29" alt="[公式]"> ，此时 <img src="https://www.zhihu.com/equation?tex=%28x%2Cz%29" alt="[公式]"> 即为完全数据，样本的模型参数为 <img src="https://www.zhihu.com/equation?tex=%CE%B8" alt="[公式]"> , 则观察数据 <img src="https://www.zhihu.com/equation?tex=x%5E%7B%28i%29%7D" alt="[公式]"> 的概率为  <img src="https://www.zhihu.com/equation?tex=P%28x%5E%7B%28i%29%7D%7C%5Ctheta%29" alt="[公式]"> ，完全数据 <img src="https://www.zhihu.com/equation?tex=%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%29" alt="[公式]"> 的似然函数为 <img src="https://www.zhihu.com/equation?tex=P%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%7C%5Ctheta%29" alt="[公式]"> 。</p>
<p>假如没有隐含变量 <img src="https://www.zhihu.com/equation?tex=z" alt="[公式]">，我们仅需要找到合适的 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 极大化对数似然函数即可：<br> <img src="https://www.zhihu.com/equation?tex=%5Ctheta+%3Darg+%5Cmax+%5Climits_%7B%5Ctheta%7DL%28%5Ctheta%29+%3D+arg+%5Cmax+%5Climits_%7B%5Ctheta%7D%5Csum%5Climits_%7Bi%3D1%7D%5Em+logP%28x%5E%7B%28i%29%7D%7C%5Ctheta%29+%5C%5C" alt="[公式]"> </p>
<p>增加隐含变量 <img src="https://www.zhihu.com/equation?tex=z" alt="[公式]"> 之后，我们的目标变成了找到合适的 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=z" alt="[公式]"> 让对数似然函数极大<em>：</em><br> <img src="https://www.zhihu.com/equation?tex=%5Ctheta%2C+z+%3D+arg+%5Cmax+%5Climits_%7B%5Ctheta%2Cz%7DL%28%5Ctheta%2C+z%29+%3D+arg+%5Cmax+%5Climits_%7B%5Ctheta%2Cz%7D%5Csum%5Climits_%7Bi%3D1%7D%5Em+log%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29+%5C%5C" alt="[公式]"> </p>
<p>不就是多了一个隐变量 <img src="https://www.zhihu.com/equation?tex=z" alt="[公式]"> 吗？那我们自然而然会想到分别对未知的 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=z" alt="[公式]"> 分别求偏导，这样做可行吗？</p>
<p>理论上是可行的，然而如果对分别对未知的 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=z" alt="[公式]"> 分别求偏导，由于<img src="https://www.zhihu.com/equation?tex=+logP%28x%5E%7B%28i%29%7D%7C%5Ctheta%29" alt="[公式]"> 是 <img src="https://www.zhihu.com/equation?tex=P%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29" alt="[公式]"> 边缘概率(建议没基础的同学网上搜一下边缘概率的概念)，转化为 <img src="https://www.zhihu.com/equation?tex=+logP%28x%5E%7B%28i%29%7D%7C%5Ctheta%29" alt="[公式]"> 求导后形式会非常复杂（可以想象下 <img src="https://www.zhihu.com/equation?tex=log%28f_1%28x%29%2B+f_2%28x%29%2B%E2%80%A6" alt="[公式]">)复合函数的求导) ，所以很难求解得到 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=z" alt="[公式]"> 。那么我们想一下可不可以将加号从 log 中提取出来呢？我们对这个式子进行缩放如下：  <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%5Csum%5Climits_%7Bi%3D1%7D%5Em+log%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29+%26+%3D+%5Csum%5Climits_%7Bi%3D1%7D%5Em+log%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D+%5Ctag%7B1%7D+%5C%5C+%26+%5Cgeq+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29log%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D+%5Ctag%7B2%7D+%5Cend%7Balign%7D" alt="[公式]"> </p>
<p>上面第(1)式引入了一个未知的新的分布 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]">，满足：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Csum+%5Climits+_z+Q_i%28z%29%3D1%2C0+%5Cle+Q_i%28z%29%5Cle+1+%5C%5C" alt="[公式]"> </p>
<p>第(2)式用到了 Jensen 不等式 (对数函数是凹函数)：</p>
<p> <img src="https://www.zhihu.com/equation?tex=log%28E%28y%29%29+%5Cge+E%28log%28y%29%29+%5C%5C" alt="[公式]"><br>其中：</p>
<p><img src="https://www.zhihu.com/equation?tex=E%28y%29+%3D+%5Csum%5Climits_i%5Clambda_iy_i%2C+%5Clambda_i+%5Cgeq+0%2C+%5Csum%5Climits_i%5Clambda_i+%3D1+" alt="[公式]"> </p>
<p><img src="https://www.zhihu.com/equation?tex=y_i+%3D+%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D" alt="[公式]"> </p>
<p><img src="https://www.zhihu.com/equation?tex=%5Clambda_i+%3D+Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]"> </p>
<p>也就是说 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D" alt="[公式]"> 为第 i 个样本<em>，</em> <img src="https://www.zhihu.com/equation?tex=+Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]"> 为第 i 个样本对应的权重，那么：</p>
<p><img src="https://www.zhihu.com/equation?tex=E%28log%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%29+%3D+%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29+log%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D+%5C%5C" alt="[公式]"> </p>
<p>上式我实际上是我们构建了 <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%2C+z%29" alt="[公式]"> 的下界，我们发现实际上就是 <img src="https://www.zhihu.com/equation?tex=log%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D" alt="[公式]"> 的加权求和，由于上面讲过权值 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]"> 累积和为1，因此上式是 <img src="https://www.zhihu.com/equation?tex=log%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D" alt="[公式]"> 的加权平均，也是我们所说的期望，<strong>这就是Expectation的来历啦</strong>。下一步要做的就是寻找一个合适的 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%29" alt="[公式]"> 最优化这个下界(M步)。</p>
<p>假设 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 已经给定，那么 <img src="https://www.zhihu.com/equation?tex=logL%28%5Ctheta%29" alt="[公式]"> 的值就取决于 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%29" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=+p%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%29" alt="[公式]"> 了。我们可以通过调整这两个概率使下界逼近 <img src="https://www.zhihu.com/equation?tex=logL%28%5Ctheta%29" alt="[公式]"> 的真实值，当不等式变成等式时，说明我们调整后的下界能够等价于<img src="https://www.zhihu.com/equation?tex=logL%28%5Ctheta%29" alt="[公式]"> 了。由 Jensen 不等式可知，等式成立的条件是随机变量是常数，则有：  <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D+%3Dc+%5C%5C" alt="[公式]"><br>其中 c 为常数，对于任意 <img src="https://www.zhihu.com/equation?tex=i" alt="[公式]">，我们得到：<br> <img src="https://www.zhihu.com/equation?tex=%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D+%3Dc%7BQ_i%28z%5E%7B%28i%29%7D%29%7D+%5C%5C" alt="[公式]"><br>方程两边同时累加和：<br> <img src="https://www.zhihu.com/equation?tex=%5Csum%5Climits_%7Bz%7D+%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D+%3D+c%5Csum%5Climits_%7Bz%7D+%7BQ_i%28z%5E%7B%28i%29%7D%29%7D+%5C%5C" alt="[公式]"><br>由于 <img src="https://www.zhihu.com/equation?tex=%5Csum%5Climits_%7Bz%7DQ_i%28z%5E%7B%28i%29%7D%29+%3D1" alt="[公式]">。 从上面两式，我们可以得到：<br> <img src="https://www.zhihu.com/equation?tex=%5Csum%5Climits_%7Bz%7D+%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D+%3D+c+%5C%5C" alt="[公式]"> </p>
<p><img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29+%3D+%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7Bc%7D+%3D+%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7B%5Csum%5Climits_%7Bz%7DP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D+%3D+%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7BP%28x%5E%7B%28i%29%7D%7C%5Ctheta%29%7D+%3D+P%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%29+%5C%5C" alt="[公式]"> </p>
<p>其中：</p>
<p>边缘概率公式： <img src="https://www.zhihu.com/equation?tex=P%28x%5E%7B%28i%29%7D%7C%5Ctheta%29+%3D+%5Csum%5Climits_%7Bz%7DP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29" alt="[公式]"> </p>
<p>条件概率公式： <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7BP%28x%5E%7B%28i%29%7D%7C%5Ctheta%29%7D+%3D+P%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%29" alt="[公式]"> </p>
<p>从上式可以发现 <img src="https://www.zhihu.com/equation?tex=Q%28z%29" alt="[公式]">是已知样本和模型参数下的隐变量分布。</p>
<p>如果 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29+%3D+P%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%29%29" alt="[公式]"> , 则第 (2) 式是我们的包含隐藏数据的对数似然的一个下界。如果我们能极大化这个下界，则也在尝试极大化我们的对数似然。即我们需要极大化下式：  <img src="https://www.zhihu.com/equation?tex=arg+%5Cmax+%5Climits_%7B%5Ctheta%7D+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29log%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D+%5C%5C" alt="[公式]"> </p>
<p>至此，我们推出了在固定参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]">后分布 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]"> 的选择问题， 从而建立了 <img src="https://www.zhihu.com/equation?tex=logL%28%5Ctheta%29" alt="[公式]"> 的下界，这是 E 步，接下来的M 步骤就是固定 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]"> 后，调整 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]">)，去极大化<img src="https://www.zhihu.com/equation?tex=logL%28%5Ctheta%29" alt="[公式]">的下界。</p>
<p>去掉上式中常数的部分 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]"> ，则我们需要极大化的对数似然下界为：<br> <img src="https://www.zhihu.com/equation?tex=arg+%5Cmax+%5Climits_%7B%5Ctheta%7D+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29log%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D+%5C%5C" alt="[公式]"> </p>
<h2 id="2-3-EM算法流程"><a href="#2-3-EM算法流程" class="headerlink" title="2.3 EM算法流程"></a>2.3 EM算法流程</h2><p>现在我们总结下EM算法的流程。</p>
<p>输入：观察数据<img src="https://www.zhihu.com/equation?tex=x%3D%28x%5E%7B%281%29%7D%2Cx%5E%7B%282%29%7D%2C...x%5E%7B%28m%29%7D%29" alt="[公式]">，联合分布 <img src="https://www.zhihu.com/equation?tex=p%28x%2Cz+%7C%5Ctheta%29" alt="[公式]"> ，条件分布 <img src="https://www.zhihu.com/equation?tex=p%28z%7Cx%2C+%5Ctheta%29" alt="[公式]">， 极大迭代次数 <img src="https://www.zhihu.com/equation?tex=J" alt="[公式]"> 。</p>
<p>1) 随机初始化模型参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 的初值  <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E0" alt="[公式]"> </p>
<p>2)  <img src="https://www.zhihu.com/equation?tex=%5Ctext%7Bfor+j+from+1+to+J%7D" alt="[公式]">：</p>
<ul>
<li>E步：计算联合分布的条件概率期望：</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29+%3A%3D+P%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%29%29+%5C%5C" alt="[公式]"> </p>
<ul>
<li>M步：极大化 <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29" alt="[公式]"> ,得到 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> :</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=%5Ctheta+%3A+%3D+arg+%5Cmax+%5Climits_%7B%5Ctheta%7D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29log%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D+%5C%5C" alt="[公式]"> </p>
<ul>
<li>重复E、M步骤直到 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 收敛</li>
</ul>
<p>输出：模型参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> </p>
<h2 id="2-4-EM算法另一种理解"><a href="#2-4-EM算法另一种理解" class="headerlink" title="2.4 EM算法另一种理解"></a>2.4 EM算法另一种理解</h2><p>坐标上升法（Coordinate ascent）(<strong>类似于梯度下降法，梯度下降法的目的是最小化代价函数，坐标上升法的目的是最大化似然函数；梯度下降每一个循环仅仅更新模型参数就可以了，EM算法每一个循环既需要更新隐含参数和也需要更新模型参数，梯度下降和坐标上升的详细分析参见</strong><a href="https://zhuanlan.zhihu.com/p/36535299" target="_blank" rel="noopener">攀登传统机器学习的珠峰-SVM (下)</a>)：</p>
<p><img src="https://pic4.zhimg.com/v2-389aa0ac570f105b0e3b77ed0d3cf10b_b.jpg" alt="img"></p>
<p>图中的直线式迭代优化的路径，可以看到每一步都会向最优值前进一步，而且前进路线是平行于坐标轴的，因为每一步只优化一个变量。</p>
<p>这犹如在x-y坐标系中找一个曲线的极值，然而曲线函数不能直接求导，因此什么梯度下降方法就不适用了。但固定一个变量后，另外一个可以通过求导得到，因此可以使用坐标上升法，一次固定一个变量，对另外的求极值，最后逐步逼近极值。对应到EM上，<strong>E步：</strong>固定 θ，优化Q；<strong>M步：</strong>固定 Q，优化 θ；交替将极值推向极大。</p>
<h2 id="2-5-EM算法的收敛性思考"><a href="#2-5-EM算法的收敛性思考" class="headerlink" title="2.5 EM算法的收敛性思考"></a>2.5 EM算法的收敛性思考</h2><p>EM算法的流程并不复杂，但是还有两个问题需要我们思考：</p>
<p>1） EM算法能保证收敛吗？</p>
<p>2） EM算法如果收敛，那么能保证收敛到全局极大值吗？　</p>
<p>首先我们来看第一个问题, EM 算法的收敛性。要证明 EM 算法收敛，则我们需要证明我们的对数似然函数的值在迭代的过程中一直在增大。即：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Csum%5Climits_%7Bi%3D1%7D%5Em+logP%28x%5E%7B%28i%29%7D%7C%5Ctheta%5E%7Bj%2B1%7D%29+%5Cgeq+%5Csum%5Climits_%7Bi%3D1%7D%5Em+logP%28x%5E%7B%28i%29%7D%7C%5Ctheta%5E%7Bj%7D%29+%5C%5C" alt="[公式]"> </p>
<p>由于：</p>
<p><img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%2C+%5Ctheta%5E%7Bj%7D%29+%3D+%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DP%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%5E%7Bj%7D%29%29log%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D+%5C%5C" alt="[公式]"> </p>
<p>令：</p>
<p><img src="https://www.zhihu.com/equation?tex=H%28%5Ctheta%2C+%5Ctheta%5E%7Bj%7D%29+%3D+%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DP%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%5E%7Bj%7D%29%29log%7BP%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%29%7D+%5C%5C" alt="[公式]"> </p>
<p>上两式相减得到：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Csum%5Climits_%7Bi%3D1%7D%5Em+logP%28x%5E%7B%28i%29%7D%7C%5Ctheta%29+%3D+L%28%5Ctheta%2C+%5Ctheta%5E%7Bj%7D%29+-+H%28%5Ctheta%2C+%5Ctheta%5E%7Bj%7D%29+%5C%5C" alt="[公式]"> </p>
<p>在上式中分别取 <img src="https://www.zhihu.com/equation?tex=%CE%B8" alt="[公式]"> 为 <img src="https://www.zhihu.com/equation?tex=%CE%B8%5Ej" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%CE%B8%5E%7Bj%2B1%7D" alt="[公式]">，并相减得到：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Csum%5Climits_%7Bi%3D1%7D%5Em+logP%28x%5E%7B%28i%29%7D%7C%5Ctheta%5E%7Bj%2B1%7D%29+-+%5Csum%5Climits_%7Bi%3D1%7D%5Em+logP%28x%5E%7B%28i%29%7D%7C%5Ctheta%5E%7Bj%7D%29+%3D+%5BL%28%5Ctheta%5E%7Bj%2B1%7D%2C+%5Ctheta%5E%7Bj%7D%29+-+L%28%5Ctheta%5E%7Bj%7D%2C+%5Ctheta%5E%7Bj%7D%29+%5D+-%5BH%28%5Ctheta%5E%7Bj%2B1%7D%2C+%5Ctheta%5E%7Bj%7D%29+-+H%28%5Ctheta%5E%7Bj%7D%2C+%5Ctheta%5E%7Bj%7D%29+%5D+%5C%5C" alt="[公式]"> </p>
<p>要证明EM算法的收敛性，我们只需要证明上式的右边是非负的即可。</p>
<p>由于<img src="https://www.zhihu.com/equation?tex=%CE%B8%5E%7Bj%2B1%7D" alt="[公式]">)使得<img src="https://www.zhihu.com/equation?tex=L%28%CE%B8%2C%CE%B8%5Ej%29" alt="[公式]">极大，因此有：</p>
<p><img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%5E%7Bj%2B1%7D%2C+%5Ctheta%5E%7Bj%7D%29+-+L%28%5Ctheta%5E%7Bj%7D%2C+%5Ctheta%5E%7Bj%7D%29+%5Cgeq+0+%5C%5C" alt="[公式]"> </p>
<p>而对于第二部分，我们有：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+H%28%5Ctheta%5E%7Bj%2B1%7D%2C+%5Ctheta%5E%7Bj%7D%29+-+H%28%5Ctheta%5E%7Bj%7D%2C+%5Ctheta%5E%7Bj%7D%29+%26+%3D+%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DP%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%5E%7Bj%7D%29log%5Cfrac%7BP%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%5E%7Bj%2B1%7D%29%7D%7BP%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%5Ej%29%7D+%5Ctag%7B3%7D+%5C%5C+%26+%5Cleq+%5Csum%5Climits_%7Bi%3D1%7D%5Emlog%28%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DP%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%5E%7Bj%7D%29%5Cfrac%7BP%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%5E%7Bj%2B1%7D%29%7D%7BP%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%5Ej%29%7D%29+%5Ctag%7B4%7D+%5C%5C+%26+%3D+%5Csum%5Climits_%7Bi%3D1%7D%5Emlog%28%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DP%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%5E%7Bj%2B1%7D%29%29+%3D+0+%5Ctag%7B5%7D+%5Cend%7Balign%7D" alt="[公式]"> </p>
<p>其中第（4）式用到了Jensen不等式，只不过和第二节的使用相反而已，第（5）式用到了概率分布累积为1的性质。</p>
<p>至此，我们得到了：<img src="https://www.zhihu.com/equation?tex=%5Csum%5Climits_%7Bi%3D1%7D%5Em+logP%28x%5E%7B%28i%29%7D%7C%5Ctheta%5E%7Bj%2B1%7D%29+-+%5Csum%5Climits_%7Bi%3D1%7D%5Em+logP%28x%5E%7B%28i%29%7D%7C%5Ctheta%5E%7Bj%7D%29+%5Cgeq+0" alt="[公式]"> ，证明了EM算法的收敛性。</p>
<p>从上面的推导可以看出，EM 算法可以保证收敛到一个稳定点，但是却不能保证收敛到全局的极大值点，因此它是局部最优的算法，当然，如果我们的优化目标 <img src="https://www.zhihu.com/equation?tex=L%28%CE%B8%2C%CE%B8%5Ej%29" alt="[公式]"> 是凸的，则EM算法可以保证收敛到全局极大值，这点和梯度下降法这样的迭代算法相同。至此我们也回答了上面提到的第二个问题。</p>
<h2 id="2-6-EM算法应用"><a href="#2-6-EM算法应用" class="headerlink" title="2.6. EM算法应用"></a>2.6. EM算法应用</h2><p>如果我们从算法思想的角度来思考EM算法，我们可以发现我们的算法里已知的是观察数据，未知的是隐含数据和模型参数，在E步，我们所做的事情是固定模型参数的值，优化隐含数据的分布，而在M步，我们所做的事情是固定隐含数据分布，优化模型参数的值。EM的应用包括：</p>
<ul>
<li>支持向量机的SMO算法</li>
<li>混合高斯模型</li>
<li>K-means</li>
<li>隐马尔可夫模型</li>
</ul>
<h2 id="3-EM算法案例-两硬币模型"><a href="#3-EM算法案例-两硬币模型" class="headerlink" title="3. EM算法案例-两硬币模型"></a>3. <a href="https://link.zhihu.com/?target=http%3A//ai.stanford.edu/~chuongdo/papers/em_tutorial.pdf">EM算法案例-两硬币模型</a></h2><p>假设有两枚硬币A、B，以相同的概率随机选择一个硬币，进行如下的掷硬币实验：共做 5 次实验，每次实验独立的掷十次，结果如图中 a 所示，例如某次实验产生了H、T、T、T、H、H、T、H、T、H (H代表正面朝上)。a 是在知道每次选择的是A还是B的情况下进行，b是在不知道选择的是A还是B的情况下进行，问如何估计两个硬币正面出现的概率？</p>
<p><img src="https://pic2.zhimg.com/v2-a5b47206d802b392e0e72a23c6b7bb95_b.jpg" alt="img"></p>
<p><strong>CASE a</strong></p>
<p>已知每个实验选择的是硬币A 还是硬币 B，重点是如何计算输出的概率分布，这其实也是极大似然求导所得。<br> <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cunderset%7B%5Ctheta+%7D%7Bargmax%7DlogP%28Y%7C%5Ctheta%29+%26%3D+log%28%28%5Ctheta_B%5E5%281-%5Ctheta_B%29%5E5%29+%28%5Ctheta_A%5E9%281-%5Ctheta_A%29%29%28%5Ctheta_A%5E8%281-%5Ctheta_A%29%5E2%29+%28%5Ctheta_B%5E4%281-%5Ctheta_B%29%5E6%29+%28%5Ctheta_A%5E7%281-%5Ctheta_A%29%5E3%29+%29+%5C%5C+%26%3D+log%5B%28%5Ctheta_A%5E%7B24%7D%281-%5Ctheta_A%29%5E6%29+%28%5Ctheta_B%5E9%281-%5Ctheta_B%29%5E%7B11%7D%29+%5D+%5Cend%7Balign%2A%7D" alt="[公式]"><br>上面这个式子求导之后发现，5 次实验中A正面向上的次数再除以总次数作为即为  <img src="https://www.zhihu.com/equation?tex=%5Chat%CE%B8_A" alt="[公式]"> ，5次实验中B正面向上的次数再除以总次数作为即为 ，即:</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D_A+%3D+%5Cfrac%7B24+%7D%7B24%2B6%7D+%3D+0.80+%5C%5C" alt="[公式]"> </p>
<p><img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D_B+%3D+%5Cfrac%7B9%7D%7B+9+%2B+11%7D+%3D+0.45+%5C%5C" alt="[公式]"> </p>
<p><strong>CASE b</strong></p>
<p>由于并不知道选择的是硬币 A 还是硬币 B，因此采用EM算法。</p>
<p>E步：初始化<img src="https://www.zhihu.com/equation?tex=%5Chat+%CE%B8_A%5E%7B%280%29%7D+%3D+0.60" alt="[公式]">和 <img src="https://www.zhihu.com/equation?tex=%5Chat+%CE%B8_B%5E%7B%280%29%7D+%3D+0.50" alt="[公式]"> ，计算每个实验中选择的硬币是 A 和 B 的概率，例如第一个实验中选择 A 的概率为：</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28z%3DA%7Cy_1%2C+%5Ctheta%29+%3D+%5Cfrac+%7BP%28z%3DA%2C+y_1%7C%5Ctheta%29%7D%7BP%28z%3DA%2Cy_1%7C%5Ctheta%29+%2B+P%28z%3DB%2Cy_1%7C%5Ctheta%29%7D+%3D+%5Cfrac%7B%280.6%29%5E5%2A%280.4%29%5E5%7D%7B%280.6%29%5E5%2A%280.4%29%5E5%2B%280.5%29%5E%7B10%7D%7D+%3D+0.45+%5C%5C" alt="[公式]"> </p>
<p><img src="https://www.zhihu.com/equation?tex=P%28z%3DB%7Cy_1%2C+%5Ctheta%29+%3D+1-+P%28z%3DA%7Cy_1%2C+%5Ctheta%29+%3D+0.55+%5C%5C" alt="[公式]"> </p>
<p>计算出每个实验为硬币 A 和硬币 B 的概率，然后进行加权求和。</p>
<p><strong>M步</strong>：求出似然函数下界 <img src="https://www.zhihu.com/equation?tex=+Q%28%5Ctheta%2C+%5Ctheta%5Ei%29" alt="[公式]">， <img src="https://www.zhihu.com/equation?tex=y_j" alt="[公式]">代表第 <img src="https://www.zhihu.com/equation?tex=j" alt="[公式]"> 次实验正面朝上的个数，<img src="https://www.zhihu.com/equation?tex=%5Cmu_j" alt="[公式]"> 代表第 <img src="https://www.zhihu.com/equation?tex=j" alt="[公式]"> 次实验选择硬币 A 的概率，<img src="https://www.zhihu.com/equation?tex=1-%5Cmu_j" alt="[公式]"> 代表第 <img src="https://www.zhihu.com/equation?tex=j" alt="[公式]"> 次实验选择硬币B的概率 。</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+Q%28%5Ctheta%2C+%5Ctheta%5Ei%29+%26%3D+%5Csum_%7Bj%3D1%7D%5E5%5Csum_%7Bz%7D+P%28z%7Cy_j%2C+%5Ctheta%5Ei%29logP%28y_j%2C+z%7C%5Ctheta%29%5C%5C%26%3D%5Csum_%7Bj%3D1%7D%5E5+%5Cmu_jlog%28%5Ctheta_A%5E%7By_j%7D%281-%5Ctheta_A%29%5E%7B10-y_j%7D%29+%2B+%281-%5Cmu_j%29log%28%5Ctheta_B%5E%7By_j%7D%281-%5Ctheta_B%29%5E%7B10-y_j%7D%29+%5Cend%7Balign%2A%7D" alt="[公式]"> </p>
<p>针对L函数求导来对参数求导，例如对 <img src="https://www.zhihu.com/equation?tex=%CE%B8_A" alt="[公式]">求导：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cfrac%7B%5Cpartial+Q%7D%7B%5Cpartial+%5Ctheta_A%7D+%26%3D+%5Cmu_1%28%5Cfrac%7By_1%7D%7B%5Ctheta_A%7D-%5Cfrac%7B10-y_1%7D%7B1-%5Ctheta_A%7D%29+%2B+%5Ccdot+%5Ccdot+%5Ccdot+%2B+%5Cmu_5%28%5Cfrac%7By_5%7D%7B%5Ctheta_A%7D-%5Cfrac%7B10-y_5%7D%7B1-%5Ctheta_A%7D%29+%3D+%5Cmu_1%28%5Cfrac%7By_1+-+10%5Ctheta_A%7D+%7B%5Ctheta_A%281-%5Ctheta_A%29%7D%29+%2B+%5Ccdot+%5Ccdot+%5Ccdot+%2B%5Cmu_5%28%5Cfrac%7By_5+-+10%5Ctheta_A%7D+%7B%5Ctheta_A%281-%5Ctheta_A%29%7D%29+%5C%5C+%26%3D+%5Cfrac%7B%5Csum_%7Bj%3D1%7D%5E5+%5Cmu_jy_j+-+%5Csum_%7Bj%3D1%7D%5E510%5Cmu_j%5Ctheta_A%7D+%7B%5Ctheta_A%281-%5Ctheta_A%29%7D+%5Cend%7Balign%2A%7D+%5C%5C" alt="[公式]"> </p>
<p>求导等于 0 之后就可得到图中的第一次迭代之后的参数值:</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D_A%5E%7B%281%29%7D+%3D+0.71+%5C%5C" alt="[公式]"> </p>
<p><img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D_B%5E%7B%281%29%7D+%3D+0.58+%5C%5C" alt="[公式]"> </p>
<p>当然，基于Case a 我们也可以用一种更简单的方法求得：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D_A%5E%7B%281%29%7D+%3D+%5Cfrac%7B21.3%7D%7B21.3%2B8.6%7D+%3D+0.71+%5C%5C" alt="[公式]"> </p>
<p><img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D_B%5E%7B%281%29%7D+%3D+%5Cfrac%7B11.7%7D%7B+11.7+%2B+8.4%7D+%3D+0.58+%5C%5C" alt="[公式]"> </p>
<p><strong>第二轮迭代</strong>：基于第一轮EM计算好的 <img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D_A%5E%7B%281%29%7D%2C+%5Chat%7B%5Ctheta%7D_B%5E%7B%281%29%7D" alt="[公式]"> , 进行第二轮 EM，计算每个实验中选择的硬币是 A 和 B 的概率（E步），然后在计算M步，如此继续迭代……迭代十步之后 <img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D_A%5E%7B%2810%29%7D+%3D+0.8%2C+%5Chat%7B%5Ctheta%7D_B%5E%7B%2810%29%7D+%3D+0.52" alt="[公式]"> </p>

            <hr>
          </div>
          <br>
          <div>
            <p>
            
              <span>
                <i class="iconfont icon-inbox"></i>
                
                  <a class="hover-with-bg" href="/categories/Speech%20and%20language%20processing">Speech and language processing</a>
                  &nbsp;
                
              </span>&nbsp;&nbsp;
            
            
              <span>
                <i class="iconfont icon-tag"></i>
                
                  <a class="hover-with-bg" href="/tags/NLP%E6%A6%82%E5%BF%B5">NLP概念</a>
                
                  <a class="hover-with-bg" href="/tags/EM%E7%AE%97%E6%B3%95">EM算法</a>
                
              </span>
            
            </p>
            
              <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" target="_blank" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
            
          </div>
        </div>
      </div>
    </div>
    <div class="d-none d-lg-block col-lg-2 toc-container">
      
  <div id="toc">
    <p class="h4"><i class="far fa-list-alt"></i>&nbsp;目录</p>
    <div id="tocbot"></div>
  </div>

    </div>
  </div>
</div>

<!-- custom -->


<!-- Comments -->
<div class="col-lg-7 mx-auto nopadding-md">
  <div class="container comments mx-auto" id="comments">
    
  </div>
</div>

    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>

    
  
    <!-- 不蒜子统计PV -->
    
    &nbsp;<span id="busuanzi_container_site_pv">总访问量 
          <span id="busuanzi_value_site_pv"></span> 次</span>&nbsp;
  
  
    <!-- 不蒜子统计UV -->
    
    &nbsp;<span id="busuanzi_container_site_uv">总访客数 
            <span id="busuanzi_value_site_uv"></span> 人</span>&nbsp;
  
  <br>



    

  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js" ></script>
<script src="/lib/popper/popper.min.js" ></script>
<script src="/lib/bootstrap/js/bootstrap.min.js" ></script>
<script src="/lib/mdbootstrap/js/mdb.min.js" ></script>
<script src="/js/main.js" ></script>




  
    <script src="/lib/tocbot/tocbot.min.js" ></script>
  
  <script src="/js/post.js" ></script>



  <script src="/lib/smooth-scroll/smooth-scroll.min.js" ></script>



  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>


<!-- Plugins -->


  

  

  

  

  




  <script src="/lib/prettify/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "Speech and language processing/概念_6_EM算法&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 120,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="/lib/anchor/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "false",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>







</body>
</html>

<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <link rel="icon" type="image/png" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="description" content="desc">
  <meta name="author" content="LiuBingqian">
  <meta name="keywords" content="">
  <title>Speech and language processing/4 - N-Grams ~ 刘秉乾的技术博客 :)</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"  >
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css"  >
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css"  >
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css"  >

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css"  >

<link rel="stylesheet" href="/css/main.css"  >


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css"  >


<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header style="height: 100vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>LiuBingqian`s Blog</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">主页</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">分类</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">所有文章</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">标签</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">关于</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>


</nav>

    <div class="view intro-2" id="background"
         style="background: url('http://bqlab-pic.test.upcdn.net/archive.jpg')no-repeat center center;
           background-size: cover;
           background-attachment: fixed;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              <br>
              
                <p class="mt-3">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>&nbsp;
                  星期一, 二月 24日 2020, 12:04 中午
                </p>
              

              <p>
                
                  
                  &nbsp;<i class="far fa-chart-bar"></i>
                  <span class="post-count">
                    1k 字
                  </span>&nbsp;
                

                
                  
                  &nbsp;<i class="far fa-clock"></i>
                  <span class="post-count">
                      5 分钟
                  </span>&nbsp;
                

                
                  <!-- 不蒜子统计文章PV -->
                  
                  &nbsp;<i class="far fa-eye" aria-hidden="true"></i>&nbsp;
                  <span id="busuanzi_container_page_pv">
                    <span id="busuanzi_value_page_pv"></span> 次
                  </span>&nbsp;
                
              </p>
            
          </div>

          
            <div class="scroll-down-bar">
              <i class="fas fa-angle-down scroll-down-arrow"></i>
            </div>
          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="py-5 z-depth-3" id="board">
        <div class="post-content mx-auto" id="post">
          <div class="markdown-body">
            <h1 id="4-N-Grams"><a href="#4-N-Grams" class="headerlink" title="4 - N-Grams"></a>4 - N-Grams</h1><p>N-gram is a language model,is a N-token sequence of words.</p>
<h3 id="1-the-way-to-calculate-conditional-probability"><a href="#1-the-way-to-calculate-conditional-probability" class="headerlink" title="1.the way to calculate conditional probability."></a>1.the way to calculate conditional probability.</h3><p>For example,how can we calculate <strong>P(the|its water is so transparent that)</strong> ?</p>
<p><strong>Method1):</strong> counting 2 sentences</p>
<p>Counting the times of “its water is so transparent that the” and “its water is so transparent that” in the whole corpus.It works sometime,but language is creative,many sentences is not exist.</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191222_1112_48_111.png" alt=""> </p>
<p><strong>Method2):</strong> chain rule of probability,then use method 1.</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191222_1149_27_804.png" alt=" m"></p>
<p>notice that:</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191222_1331_13_781.png" alt=""></p>
<p>means  P(w1w2w3w4……wn)</p>
<p>But <strong>P(its|water is) * P(water|is) * P(is) *<em>still don`t work fine.The reason is same to *</em>Method1</strong>,some sentence not exist.</p>
<p><strong>Method3):</strong> transform the question to <strong>P(the | that)</strong></p>
<p>Markov assumption: We can predict the probability of future words without looking too far away.So here is the <strong>bigram</strong> model : <strong>P(the|Walden Pond`s water is so transparent that)</strong> = <strong>P(the|that)</strong>.</p>
<p>How can we caclute <strong>P(the | that)</strong>?</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191222_1342_51_243.png" alt=""></p>
<p>As above,we count the number of <strong>（”that the“ / “that”)</strong>.</p>
<p>Example for method3；</p>
<p>mini-corpus:</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191222_1355_44_172.png" alt=""></p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191222_1356_06_426.png" alt=""></p>
<p>例如上图第一个概率等式：分母是&lt;s&gt;出现的总次数，一共是3。分子是&lt;s&gt;后面跟着I 的次数，一共是2。</p>
<h3 id="2-randomly-genarate-sentences-by-n-gram"><a href="#2-randomly-genarate-sentences-by-n-gram" class="headerlink" title="2.randomly genarate sentences by n-gram"></a>2.randomly genarate sentences by n-gram</h3><p>Sentences randomly generated by using corpora of <strong>Shakespeare`s works</strong></p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191223_1438_50_162.png" alt=""></p>
<p>Sentences randomly generated by using corpora of <strong>Wall Street Journal</strong></p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191223_1441_18_717.png" alt=""></p>
<h3 id="3-How-to-evaluating-N-gram-Perplexity"><a href="#3-How-to-evaluating-N-gram-Perplexity" class="headerlink" title="3.How to evaluating N-gram : Perplexity"></a>3.How to evaluating N-gram : Perplexity</h3><p>How to evaluating a language model from good to bad?</p>
<ul>
<li><p>extrinsic evalation ：</p>
<p>Extrinsic evalation is applying language model to different specific question(machine translator,speech recognition,spelling corrector etc),and compare two LM the cost of time.Is`s intution,but usually cost too much time.</p>
</li>
<li><p>intrinsic evalation :</p>
<p>Just calculate perplexity : 当语言模型训练完后，测试集中的句子都是正常的句子，模型在测试集上验证概率越高，说明句子概率越大，困惑度越小，模型越好。</p>
</li>
</ul>
<p>The <strong>Perplexity</strong> was defined as follow:</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191223_1504_16_492.png" alt=""></p>
<p>a <strong>example</strong> of calculate <strong>perplexity</strong> in a mini corpora:</p>
<p>​    mini corpora is just 10 digits:(0,1,2,3,4,5,6,7,8,9)</p>
<p>​    <img src="http://bqlab-pic.test.upcdn.net/pic/20191223_1512_43_979.png" alt=""></p>
<p>​    <img src="http://bqlab-pic.test.upcdn.net/pic/20191223_1513_07_960.png" alt=""></p>
<p>如下，另一个在同一个语料库上生成的un / bi / tri - gram模型的困 惑值有显著不同。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191223_1529_59_667.png" alt=""></p>
<h3 id="4-Smoothing"><a href="#4-Smoothing" class="headerlink" title="4. Smoothing"></a>4. Smoothing</h3><p>为什么需要平滑（Smoothing）？</p>
<p>（平滑通过）</p>
<p>Smoothing.Smooth provides a better way of estimating the probability of N-grams than MLE.Commonly used smoothing algorithms for N-grams rely on lower-order N-gram counts through backoff or interpolation</p>
<p>3.Both backoff and interpolation require discounting such as Kneser-Ney,WittenBell,or Good-Turing discounting.</p>
<p>假设语料库由如下三个句子构成：</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1317_13_202.png" alt=""></p>
<p>按照最大似然估计的方法来计算p(BROWN READ A BOOK)的概率如下：</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1317_57_370.png" alt=""></p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1318_14_432.png" alt=""></p>
<p>结果是0.06。这时候求p（David read a book）的概率是多少呢？因为David从来没有在语料库中出现过。所以如下所示，求出来的概率为0.但这与事实不符。因为语言是creative的，但语料库是有限的，一定会有一些语料没有在语料库中出现，所以需要对这部分没有出现在语料库中的creative的语料做平滑处理（也就是劫富济贫，把出现多的语料劫一部分出来分给他们）。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1321_41_820.png" alt=""></p>
<p><strong>Laplace Smoothing</strong></p>
<p>Laplace smoothing just add 1 to any tokens.It`s not a good enough smoothing method,but it`s a good start-up baseline.</p>
<p>Before Laplace smoothing:</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1117_36_583.png" alt=""></p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1120_07_549.png" alt=""></p>
<p>After Laplace smoothing</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1117_06_667.png" alt=""></p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1121_21_187.png" alt=""></p>
<p><strong>Kneser-Ney smoothing</strong></p>
<p>Equation:</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1340_17_404.png" alt=""></p>
<p>c是词语出现的次数。N(c+1)是出现次数为（c+1）次的词语的总数，如下是两个语料库的示例：</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1342_40_890.png" alt=""></p>
<p><strong>插值与回退（Interlation and backoff）</strong></p>
<p>插值：interlation</p>
<p>在一些情况下，无3-gram example，只得用2-gram，1-gram代替之</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1513_27_145.png" alt=""></p>
<p>λ的其他构成</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1513_40_098.png" alt=""></p>
<p>回退：backoff</p>
<p>回退法恐怕是最好理解的一种平滑方法了，它的思路就是：如果一个n-gram的条件概率为0，则用(n-1)-gram的条件概率取代，如果(n-1)-gram的条件概率依然为0，继续回退，直到1-gram概率，如果1-gram依然为0，就直接忽略掉该词。用式子表示如下：</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1525_53_830.png" alt=""></p>
<p><strong>交叉熵cross-entropy</strong></p>
<p>cross entropy of m on p is defined by</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1539_52_534.png" alt=""></p>
<pre><code>Summary:
1.N-gram probability is conditioinal probability of a word given the previous N-1 words.N-gram probabilities can be computed by simply counting in a corpus and normalizing(the maximum likelihood estimate),or they can be computed by more sophisticated algorithms.

2.Smoothing.Smooth provides a better way of estimating the probability of N-grams than MLE.Commonly used smoothing algorithms for N-grams rely on lower-order N-gram counts through backoff or interpolation

3.Both backoff and interpolation require discounting such as Kneser-Ney,WittenBell,or Good-Turing discounting.

4.N-gram LM are evaluated by separating the corpus int a training set and a test set.
training the model on the training set,and evaluating on the test set.The perplexity of LM on a test set is used to compare LM performance.</code></pre>
            <hr>
          </div>
          <br>
          <div>
            <p>
            
              <span>
                <i class="iconfont icon-inbox"></i>
                
                  <a class="hover-with-bg" href="/categories/Speech%20and%20language%20processing">Speech and language processing</a>
                  &nbsp;
                
              </span>&nbsp;&nbsp;
            
            
              <span>
                <i class="iconfont icon-tag"></i>
                
                  <a class="hover-with-bg" href="/tags/NLP">NLP</a>
                
              </span>
            
            </p>
            
              <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" target="_blank" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
            
          </div>
        </div>
      </div>
    </div>
    <div class="d-none d-lg-block col-lg-2 toc-container">
      
  <div id="toc">
    <p class="h4"><i class="far fa-list-alt"></i>&nbsp;目录</p>
    <div id="tocbot"></div>
  </div>

    </div>
  </div>
</div>

<!-- custom -->


<!-- Comments -->
<div class="col-lg-7 mx-auto nopadding-md">
  <div class="container comments mx-auto" id="comments">
    
  </div>
</div>

    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>

    
  
    <!-- 不蒜子统计PV -->
    
    &nbsp;<span id="busuanzi_container_site_pv">总访问量 
          <span id="busuanzi_value_site_pv"></span> 次</span>&nbsp;
  
  
    <!-- 不蒜子统计UV -->
    
    &nbsp;<span id="busuanzi_container_site_uv">总访客数 
            <span id="busuanzi_value_site_uv"></span> 人</span>&nbsp;
  
  <br>



    

  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js" ></script>
<script src="/lib/popper/popper.min.js" ></script>
<script src="/lib/bootstrap/js/bootstrap.min.js" ></script>
<script src="/lib/mdbootstrap/js/mdb.min.js" ></script>
<script src="/js/main.js" ></script>




  
    <script src="/lib/tocbot/tocbot.min.js" ></script>
  
  <script src="/js/post.js" ></script>



  <script src="/lib/smooth-scroll/smooth-scroll.min.js" ></script>



  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>


<!-- Plugins -->


  

  

  

  

  




  <script src="/lib/prettify/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "Speech and language processing/4 - N-Grams&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 120,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="/lib/anchor/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "false",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>







</body>
</html>

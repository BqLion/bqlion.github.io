<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <link rel="icon" type="image/png" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="description" content="desc">
  <meta name="author" content="LiuBingqian">
  <meta name="keywords" content="">
  <title>NLP概念/概念_词向量_Word2vec ~ 刘秉乾的技术博客 :)</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"  >
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css"  >
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css"  >
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css"  >

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css"  >

<link rel="stylesheet" href="/css/main.css"  >


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css"  >


<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header style="height: 100vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>LiuBingqian`s Blog</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">主页</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">分类</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">所有文章</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">标签</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">关于</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>


</nav>

    <div class="view intro-2" id="background"
         style="background: url('http://bqlab-pic.test.upcdn.net/archive.jpg')no-repeat center center;
           background-size: cover;
           background-attachment: fixed;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              <br>
              
                <p class="mt-3">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>&nbsp;
                  星期一, 二月 24日 2020, 12:04 中午
                </p>
              

              <p>
                
                  
                  &nbsp;<i class="far fa-chart-bar"></i>
                  <span class="post-count">
                    3.9k 字
                  </span>&nbsp;
                

                
                  
                  &nbsp;<i class="far fa-clock"></i>
                  <span class="post-count">
                      13 分钟
                  </span>&nbsp;
                

                
                  <!-- 不蒜子统计文章PV -->
                  
                  &nbsp;<i class="far fa-eye" aria-hidden="true"></i>&nbsp;
                  <span id="busuanzi_container_page_pv">
                    <span id="busuanzi_value_page_pv"></span> 次
                  </span>&nbsp;
                
              </p>
            
          </div>

          
            <div class="scroll-down-bar">
              <i class="fas fa-angle-down scroll-down-arrow"></i>
            </div>
          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="py-5 z-depth-3" id="board">
        <div class="post-content mx-auto" id="post">
          <div class="markdown-body">
            <h1 id="Lecture2-词向量表示（上）"><a href="#Lecture2-词向量表示（上）" class="headerlink" title="Lecture2 - 词向量表示（上）"></a>Lecture2 - 词向量表示（上）</h1><p>这节课深入语言的底层，做一些向量和计算，这节课提到的数学是后边内容的基础。这节课将用很慢的速度来仔细讲解一些基础，以便大家可以使用神经网络来学习词语表征这样的简单任务。</p>
<h3 id="1-语言学和NLP对词语释义的不同做法"><a href="#1-语言学和NLP对词语释义的不同做法" class="headerlink" title="1.语言学和NLP对词语释义的不同做法"></a>1.语言学和NLP对词语释义的不同做法</h3><h4 id="语言学"><a href="#语言学" class="headerlink" title="语言学"></a>语言学</h4><p>用如下词典释义来解释“meaning”这个词的意思</p>
<ul>
<li>the idea that is represented by a word,phrase,etc</li>
<li>the idea that i person wants to express by using words,signs,etc</li>
<li>the idea that is expressed in a work of writing,art,etc</li>
</ul>
<h4 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h4><p>同义词描述词汇</p>
<p>如下图左侧，用一段代码演示nltk如何抓取wordnet中”panda“这个词的分类信息，panda是一种肉食动物，一种有胎盘的哺乳动物，再往上上溯可抽象为动物，物体，物理实体等等。</p>
<p>如下图右侧，抓取“good“的同义词，这里显示的就是wordnet的答案。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1319_14_209.png" alt=""></p>
<p>同义词描述词汇本义毕竟是离散的方法，会有如下的问题：</p>
<ul>
<li>同义词的描述资源很多，但分类关系会遗漏大量的细微差别。例如I am good at NL和I am expert of NL虽然是同义词但是还是有不少的区别。</li>
<li>语言是动态变化的，不同圈子对同样的词汇的语义指代也不同，wordnet不可能跟上这种变化。</li>
<li>难以获得某个词汇的精确含义</li>
</ul>
<h3 id="2-独热编码的问题与分布式表示的提出"><a href="#2-独热编码的问题与分布式表示的提出" class="headerlink" title="2.独热编码的问题与分布式表示的提出"></a>2.独热编码的问题与分布式表示的提出</h3><p>独热码是由稀疏向量构成的，所以是离散的。独热码的问题有两个：并不能显示近义词的<strong>相似性</strong>。会造成<strong>维灾难</strong>。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1319_22_169.png" alt=""></p>
<p>遂提出<strong>分布式表示</strong>（也就是非稀疏向量表示），两个非稀疏向量点积后能算出<strong>相似度</strong>，而且无<strong>维灾难</strong>。</p>
<p>思路是通过训练，将每个词都映射到一个较短的向量上来，到底有多短一般我们自己指定。比如”Royalty”,”Masculinity”, “Femininity”和”Age”4个维度来表示，King这个词对应的词向量可能是(0.99,0.99,0.05,0.7)。这样的非稀疏向量表示每个维度是有具体 意义的。把king这个词从的热码的高维度降到四维空间时，这个过程也称为<strong>词嵌入（word embedding）</strong>，词嵌入其实也是一种投影，甚至可以把词汇从4维进一步降低到2维。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191215_1003_25_105.png" alt=""></p>
<p>分布式表示理论基础是<strong>分布相似性</strong>的存在。一句关于分布相似性的名言是你可以根据一个词周围的词语来了解本词的意思，也就是说<strong>一个词汇他身边的词汇总是相对固定的</strong>，也就是说<strong>一个词汇可以用他身边这些相对固定的词汇作为向量的维度被表示出来</strong>。以banking为例，把语料库中所有包含banking的句子都抽出来，可以看到围绕着banking的词的词无外乎就是贸易，银行，欠债，还债，柜台等等。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20200514_1242_36_405.png" alt=""></p>
<p>​    </p>
<h3 id="3-Word2Vec的骨架"><a href="#3-Word2Vec的骨架" class="headerlink" title="3.Word2Vec的骨架"></a>3.Word2Vec的骨架</h3><p>Word2vec的引出：我们定义了一个<strong>预测模型</strong>，来根据中心词汇预测它的上下文词汇。公式表示：</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20200514_1324_33_619.png" alt=""></p>
<p>word2vec预测模型的损失函数：</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20200514_1325_38_553.png" alt=""></p>
<h4 id="两个生成词向量的算法-两个训练方法"><a href="#两个生成词向量的算法-两个训练方法" class="headerlink" title="两个生成词向量的算法 + 两个训练方法"></a>两个生成词向量的算法 + 两个训练方法</h4><p>Word2vec尝试去做的事情是利用语言的意义理论，来预测每个单词和他的上下文词汇。</p>
<p>两个生成词汇向量的算法：</p>
<ul>
<li><p><strong>Continuous Bag of Word (CBOW)</strong></p>
<p>输入特征词的上下文向量，输出这个特征词的向量。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191215_1011_39_444.png" alt=""></p>
</li>
<li><p><strong>Skip - grams</strong></p>
<p>反过来，输入一个特征词向量，输出上下文向量</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191215_1011_15_137.png" alt=""></p>
</li>
</ul>
<p>  两个训练效率尚可的方法：</p>
<ul>
<li>Negative sampling</li>
<li>Hierarchical softmax</li>
</ul>
<h3 id="为解耦，Skip-gram和词袋在“NLP概念”文件夹中另起文件描述。"><a href="#为解耦，Skip-gram和词袋在“NLP概念”文件夹中另起文件描述。" class="headerlink" title="为解耦，Skip-gram和词袋在“NLP概念”文件夹中另起文件描述。"></a><strong>为解耦，Skip-gram和词袋在“NLP概念”文件夹中另起文件描述。</strong></h3><h3 id="4-总结word2vec"><a href="#4-总结word2vec" class="headerlink" title="4.总结word2vec"></a><strong>4.总结word2vec</strong></h3><ul>
<li>会遍历语料库中的每个词</li>
<li>预测每个词的周围词</li>
<li>一次只捕获一个单词的重复出现</li>
<li>为什么不直接捕获所有词的同现计数？</li>
</ul>
<p>基本上我们会遍历语料库中的每个单词，观察窗口周围的单词， 预测周围的词，我们在努力抓取词的共同点，这个词和其他词共同出现的频率是多少？每出现一次进行一次计数，就像看到deep和learning同时出现， 就对着两个向量做一次梯度更新，然后再次浏览语料库，你很有可能发现deep和learning又共同出现了，然后对其再做一次梯度更新，你可能会觉得这样做很没有效率，为什么现在仅对整个语料库计算一次deep和learning这两个词共同出现的频率？我们能捕获整个计数而不是一个样本。实际上我们可以那样做，这也是出现word2vec之前的做法，这里有可供我们选择的不同做法。</p>
<p>最简单的类似于word2vec的方法是在每个单词周围使用了一个窗口，这基本上上相当于遍历了整个语料库，我们不对任何部分进行梯度更新，不是用SGD，首先对计数结果进行采集，一旦获得了计数结果，就对矩阵进行操作，窗口的长度可能会是2词的句法信息， 也可能是5（一些单词周围的小窗口），我们要抓取的不仅仅是语义，还有每个词的句法信息，即它是哪一种词性标签，所以动词之间会更接近，例如，动词变为名词，另一方面，观测共同出现的计数结果不仅在窗口周围，而是整个文档的，那我们不能仅仅只观测每个窗口。但是我们认为这个词和其他这些词出现在所有维基百科文章中或者整个词文档中，然后你会捕捉到更多的话题，这通常被称作潜在语义分析，这是以前流行的一个模型，and忽略掉词性和句法信息，基本上你会得到的仅仅是游泳，船，水，天气，太阳等等，他们都是在这个话题上出现的，在这个文档中一起出现。我们不会太过深入这个话题，因为这涉及到其他的下游任务，比如机器翻译等，但是掌握使用这些窗口的知识是很有用的。</p>
<h4 id="一个例子：基于窗口的共现矩阵"><a href="#一个例子：基于窗口的共现矩阵" class="headerlink" title="一个例子：基于窗口的共现矩阵"></a>一个例子：基于窗口的共现矩阵</h4><ul>
<li><p>窗口长度：1( 通常更普遍的是5 -10)</p>
</li>
<li><p>对称性：中心词汇的左右词汇一视同仁</p>
</li>
<li><p>简化的语料库由如下三句话构成</p>
<ul>
<li>I like deep learning</li>
<li>I like NLP</li>
<li>I enjoy flying</li>
</ul>
<p>上图的解读：</p>
</li>
</ul>
<p>第一行是第一个词I的周围词汇统计：like在右侧出现两次，enjoy在右侧出现一次。</p>
<p>第二行同理：I出现两次，deep出现一次，flying出现一次。</p>
<p>这个矩阵的行或者列已经是向量了，但是从几个原因来说，它并不是非常理想的向量：</p>
<ol>
<li>随着字典加入新的单词，所有向量的维度都会发生变化，因此，如果有一些下游的机器学习模型来接受这个向量的输入，他们总会需要改变而且有一些参数的缺失。</li>
<li>这个向量的维度会非常高，因此在训练机器学习模型时会存在稀疏性问题，在此之后会训练出一个不那么健壮的模型。</li>
</ol>
<p>共现矩阵如下</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191215_1507_38_708.png" alt=""></p>
<h4 id="共现矩阵的奇异值分解方法"><a href="#共现矩阵的奇异值分解方法" class="headerlink" title="共现矩阵的奇异值分解方法"></a>共现矩阵的奇异值分解方法</h4><p>解决这个问题的办法就是参考word2vec的思路，并不只是存储所有共同出现的次数、每个单独的数字，还存储重要的信息，维度的固定小数，与word2vec的相似度。这将会使向量的维度在25维到1000维之间。所以现在的问题如何进行降维?</p>
<p>在实际的设置中，会有20000*20000或者百万乘百万的数量级，降维的思路是简单的奇异值分解。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1415_25_219.png" alt=""></p>
<p>奇异值分解的具体做法见线性代数的文档</p>
<p>奇异值分解后就实现了矩阵的压缩功能，然后取出U的头两列，在二维图上画出他们。奇异值分解的降维思路就是，你要降低到几维，就取U的前几列。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1415_35_511.png" alt=""></p>
<p>分解之后可得，I和like是频率比较高的词，enjoy和learning和flying都是动词距离就比较近</p>
<p>奇异值分解是个不错的办法，妥善处理停止词能提高性能，例如the,he,has太多太频繁了，他们又没有给我们太多有用的信息，只是不停地出现。而文本量越大这些越经常出现的词就会出现地更频繁，出现频率很少的词反而带有大量的语义信息。</p>
<ul>
<li><p>一种解决的办法是遮住他们出现的次数，限定在比如一百次。</p>
</li>
<li><p>或者不对所有的词汇平等计数，比如中心词的邻居可计数为1，五步外的词汇计数为0,5</p>
</li>
</ul>
<p><strong>几种模型的对比</strong></p>
<p>虽然奇异值分解很简单，只需要一行python代码，但是从计算方面来说效果不是很好，尤其是当处理越来越巨大的矩阵时，因此需要将二次损失函数定在较小的维度上。</p>
<p>两种基于奇异值分解的共现矩阵的语义预测方法的对比</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1415_44_077.png" alt=""></p>
<ul>
<li><p>Count based</p>
<p>PCA的优点是相对来说训练速度更快，通常会非常有效地使用我们拥有的统计数据。我们只需要搜集一次统计信息，理论上只需要把语料库放一边，然后对共现计数进行一系列的操作，但它主要是捕获大多数词的相似性，没有word2vec模型抓取的其他模式。</p>
</li>
<li><p>Skip-gram模型的缺点是他会缩放语料库的尺寸，必须遍历每一个单一窗口，这非常低效，因此不能有效地利用数据集的统计信息，实际上在下游任务很多的情况下，他能获得更好的性能。但是还是不知道下游具体的任务。但是对于不同的问题，比如命名实体识别或者词性标注等，这都是在作业集中需要实现的部分，事实证明，这种任务skip - gram模型的性能会好一点。我们还能捕获很多复杂的模式，其中一些非常令人震惊。</p>
</li>
</ul>
<p>  <strong>集成以上两种模型优点的模型：GloVe</strong></p>
<p>  global vector模型，θ在这里表示所有的参数，在这个情况下，已有U和V向量，我们基本上会遍历所有可能共现的词对，P是非常巨大的共现矩阵，对于语料库中的每一对词，我们希望最小化内积距离和两个词计数的对数。代价函数如下。与之对比的skip-gram模型的代价函数的目的是要最大化预测词和中心词的条件概率总加和，这个GloVe的代价函数目的是根据UV的交叉熵来动态调整Pij这个共现矩阵来达到J(θ)的最值。</p>
<p>  <img src="http://bqlab-pic.test.upcdn.net/pic/20191215_1507_22_743.png" alt=""></p>
<p>  <strong>内部评价与外部评价</strong></p>
<p>  内在评价通常是针对特定的或者中间环节的子任务进行的，可以观察这些向量的差异性或者相似性，以及内部产品对人类相似性的判断有多大的相关性。</p>
<p>  内部评估的优点是计算很快，已有向量的情况下，可以通过这个快速相似相关的研究评估他们，然后得到数值结果，然后可以修改结果，尝试50000种的旋钮组合的快速调整。有时他们会帮助大家非常快速地理解系统的工作原理。例如什么样的超参数实际上会对这种相似性指标产生影响。</p>
<p>  最近发表的word2vec论文中，其中一个很受欢迎<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1416_00_202.png" alt=""></p>
<p>例如，男人之于女人，所对应的的国王对于什么？（王后）。</p>
<p>这是一个简单的类比。如上图所示，向量（女人 - 男人 + 国王）与哪个向量i的余弦距离最近？最近的那个向量i就是答案，也就是queen。</p>
<p> 这个计算类比词的余弦向量距离方法的效果是相当不错的，如下图所示</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1416_13_585.png" alt=""></p>
<p>在简单的欧几里得加减法中，这些余弦距离很好地捕捉到了他们。下图是另一个例子，公司名称和ceo名称之间也有类似的相同向量距离的关系。<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1416_30_754.png" alt=""></p>
<p>下图是一个更有趣的例子，多项式加减法。例如寿司 - 日本 + 德国 = 德式小香肠等。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1416_46_933.png" alt=""></p>
<ul>
<li>外在评价基本上是对习一个任务的真正评价，对于新的单词向量，取皮尔森相关性，而不是核心矩阵的原始计数。我现在要评估这个词向量是否对机器翻译有帮助。外部评价通常需要消耗大量时间，而且是各个子系统耦合在一起，最后只能得出一个总体结果。</li>
</ul>
<p>​    </p>
<h4 id="CS224N-Research-Highlight-陈丹琦的热门论文分享"><a href="#CS224N-Research-Highlight-陈丹琦的热门论文分享" class="headerlink" title="CS224N Research Highlight(陈丹琦的热门论文分享)"></a>CS224N Research Highlight(陈丹琦的热门论文分享)</h4><p>标题：A simple but Tough-to-beat Baseline for Sentence Embeddings,简单但是难以超越的基线句向量表示.</p>
<p>今天我们正在学习单词向量表示，所以我们希望这些向量可以对词汇意思进行编码，但是在nlp问题中，核心是我们如何才能定义可编码句子含义的向量表示（    how we could have the vector representations that encode the meaning of sentences）.</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1414_21_171.png" alt="">我们还可以将这个句子表示作为一个特征去处理一些句子分类任务，例如情感分析问题。我们给定一个句子，例如自然语言处理很有趣， 我们可以对向量表示运用分类器，来判断这句话的情感是不是正面的。</p>
<p> 有很多方法来做词汇向量表示，最简单的方法是使用词袋，词袋的最简单用法如下图所示，一个句子由每个单词均权重构成。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1414_28_979.png" alt=""></p>
<pre><code>小结：
1.单词有如下三种表达方式。
    指称，直接释义词是什么意思。
    独热编码，也就是稀疏，有维灾难
    词向量表示，在向量空间中，语义近似的词汇距离近。
2.本文终点就是word2vec，掌握如下两种生成词向量的方法。
    词袋：输入上下文向量，得到中心词向量。输入独热码，动态调整W矩阵，输出前过一道softmax，终点labelled          独热码在那等着，根据是否正确的结果来调整W，不断循环完成训练。
    skip-gram:输入特征值，得到上下文向量，代价函数时一个条件概率的二重累加，注意数学细节。也是动态调整                  矩阵W。
3.基于窗口的共现矩阵可以用SVD方法压缩。奇异值分解的数学方法作为另一篇文档存在线性代数文件夹里。
4.GloVe方法是继承了PCA和Skip-gram优点的方法，代价函数大同小异。
5.一个优秀的word embedding可视化网站：https://ronxin.github.io/wevi/</code></pre>
            <hr>
          </div>
          <br>
          <div>
            <p>
            
              <span>
                <i class="iconfont icon-inbox"></i>
                
                  <a class="hover-with-bg" href="/categories/CS224n">CS224n</a>
                  &nbsp;
                
              </span>&nbsp;&nbsp;
            
            
              <span>
                <i class="iconfont icon-tag"></i>
                
                  <a class="hover-with-bg" href="/tags/NLP">NLP</a>
                
              </span>
            
            </p>
            
              <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" target="_blank" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
            
          </div>
        </div>
      </div>
    </div>
    <div class="d-none d-lg-block col-lg-2 toc-container">
      
  <div id="toc">
    <p class="h4"><i class="far fa-list-alt"></i>&nbsp;目录</p>
    <div id="tocbot"></div>
  </div>

    </div>
  </div>
</div>

<!-- custom -->


<!-- Comments -->
<div class="col-lg-7 mx-auto nopadding-md">
  <div class="container comments mx-auto" id="comments">
    
  </div>
</div>

    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>

    
  
    <!-- 不蒜子统计PV -->
    
    &nbsp;<span id="busuanzi_container_site_pv">总访问量 
          <span id="busuanzi_value_site_pv"></span> 次</span>&nbsp;
  
  
    <!-- 不蒜子统计UV -->
    
    &nbsp;<span id="busuanzi_container_site_uv">总访客数 
            <span id="busuanzi_value_site_uv"></span> 人</span>&nbsp;
  
  <br>



    

  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js" ></script>
<script src="/lib/popper/popper.min.js" ></script>
<script src="/lib/bootstrap/js/bootstrap.min.js" ></script>
<script src="/lib/mdbootstrap/js/mdb.min.js" ></script>
<script src="/js/main.js" ></script>




  
    <script src="/lib/tocbot/tocbot.min.js" ></script>
  
  <script src="/js/post.js" ></script>



  <script src="/lib/smooth-scroll/smooth-scroll.min.js" ></script>



  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>


<!-- Plugins -->


  

  

  

  

  




  <script src="/lib/prettify/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "NLP概念/概念_词向量_Word2vec&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 120,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="/lib/anchor/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "false",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>







</body>
</html>

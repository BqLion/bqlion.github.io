<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <link rel="icon" type="image/png" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="description" content="desc">
  <meta name="author" content="LiuBingqian">
  <meta name="keywords" content="">
  <title>NLP概念/概念_BERT ~ 刘秉乾的技术博客 :)</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"  >
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css"  >
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css"  >
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css"  >

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css"  >

<link rel="stylesheet" href="/css/main.css"  >


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css"  >


<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header style="height: 100vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>LiuBingqian`s Blog</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">主页</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">分类</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">所有文章</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">标签</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">关于</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>


</nav>

    <div class="view intro-2" id="background"
         style="background: url('http://bqlab-pic.test.upcdn.net/archive.jpg')no-repeat center center;
           background-size: cover;
           background-attachment: fixed;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              <br>
              
                <p class="mt-3">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>&nbsp;
                  星期一, 二月 24日 2020, 12:04 中午
                </p>
              

              <p>
                
                  
                  &nbsp;<i class="far fa-chart-bar"></i>
                  <span class="post-count">
                    1.7k 字
                  </span>&nbsp;
                

                
                  
                  &nbsp;<i class="far fa-clock"></i>
                  <span class="post-count">
                      6 分钟
                  </span>&nbsp;
                

                
                  <!-- 不蒜子统计文章PV -->
                  
                  &nbsp;<i class="far fa-eye" aria-hidden="true"></i>&nbsp;
                  <span id="busuanzi_container_page_pv">
                    <span id="busuanzi_value_page_pv"></span> 次
                  </span>&nbsp;
                
              </p>
            
          </div>

          
            <div class="scroll-down-bar">
              <i class="fas fa-angle-down scroll-down-arrow"></i>
            </div>
          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="py-5 z-depth-3" id="board">
        <div class="post-content mx-auto" id="post">
          <div class="markdown-body">
            <h1 id="Bert-概念"><a href="#Bert-概念" class="headerlink" title="Bert 概念"></a>Bert 概念</h1><h4 id="1-全景介绍"><a href="#1-全景介绍" class="headerlink" title="1.全景介绍"></a>1.全景介绍</h4><p>BERT是一种深度学习模型，他是Transformer的双向编码器表示，在维基百科和Books Corpus上预训练过，运用于特定任务时只需要微调即可。</p>
<p>他的效果很好，在很多NLP任务中都有最近进展，包括问答系统（Squad）和自然语言推理（MNLI）任务。</p>
<p>BERT改变了NLP的格局。跑一个在大量未标记数据集上训练的模型，在11个单独的NLP任务中仅仅通过不同的微调，就可以分别得到11个最新的结果，这种表现只有BERT可以做到。</p>
<p>BERT还启发了TransformerXL,GPT-2,XLNet,ERNIE2.0,RoBERTa等等。</p>
<h4 id="2-什么是BERT"><a href="#2-什么是BERT" class="headerlink" title="2.什么是BERT?"></a>2.什么是BERT?</h4><p>1.基本上是堆叠在一起的一堆Transformer encoders，注意仅仅是Transformer encoders不是整个Transformer架构。<strong>双向性</strong>的概念十分重要，是BERT和其前身OpenAL GPT的关键区别，BERT是<strong>双向的</strong>因为他在<strong>自我注意层</strong>的两个方向上都执行自我注意。</p>
<p>2.要注意的是BERT在维基百科（25亿词）和Books Corpus（8亿词）上的预训练非常重要，因为模型在一个大的文本语料库中训练一个模型时，我们的模型开始对语言的工作方式有了更深入的了解。这种了解是瑞士军刀，对各种任务都是有用的。</p>
<p>3.BERT是深度双向模型，双向的意思就是，每个token在训练阶段，都从上下文的左边和右边学习信息。</p>
<p>4.BERT通过添加几个额外的输出层来微调它，从而为各种NLP任务创建最先进的模型。</p>
<h4 id="3-BERT如何工作？"><a href="#3-BERT如何工作？" class="headerlink" title="3.BERT如何工作？"></a>3.BERT如何工作？</h4><p><strong>BERT ARCH</strong></p>
<p>多层双向Transformer结构：</p>
<ul>
<li>BERT base： 12层（transformer blocks），12个attention heads，110 million 参数</li>
<li>BERT Large：24层，16个attention heads，340 million参数</li>
</ul>
<p><strong>text的预处理</strong> </p>
<p>首先，BERT的输入可以是一个句子，也可以是一对句子（一问一答）。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_Y5XO2ivPic.png" alt=""></p>
<p>如图，[CLS]符号作用于整个句子序列表示的聚合，在非分类任务中这个符号被忽略。对于单文本任务，[CLS]标记后面跟的是WordPiece标记和[SEP]分隔符标记。</p>
<p>对于句子对任务，两个句子的词条标记由一个[SEP]标记开始，另一个[SEP]标记结束。</p>
<p>BERT的开发人员对在输入模型之前已经设置了一组特定的规则来表示语言：</p>
<ul>
<li>位置嵌入Position Embedding：学习并使用位置嵌入来表示单词在句子中的位置，这样做是为了克服Transformer的限制，他不像RNN一样可以捕获序列或者顺序的信息。</li>
<li>段嵌入Segment Embedding：BERT可将句子作为对任务（问答）的输入，这就是为什么他学习第一句和第二句的独特嵌入，以帮助模型区分他们，所有标记为EA的标记，都属于句子A。</li>
<li>令牌嵌入Toekn Embedding：这些是WordPiece令牌词汇表中为特定的令牌学习的嵌入，对于给定的令牌，其输入表示是通过对相应的令牌，段和位置嵌入求和来构造的。</li>
</ul>
<p><strong>令牌化Tokenization</strong>：</p>
<p>BERT使用词块标记化，用语言中的所有单个字符初始化词汇表，然后迭代添加词汇表中现有单词的最频繁、最可能的组合。</p>
<h4 id="3-Pre-Training："><a href="#3-Pre-Training：" class="headerlink" title="3.Pre Training："></a>3.Pre Training：</h4><p>该模型同时接受了“蒙面语言学模型”和“下一句猜测”两个任务的训练。</p>
<h4 id="4-BERT的特定微调技术"><a href="#4-BERT的特定微调技术" class="headerlink" title="4.BERT的特定微调技术"></a>4.BERT的特定微调技术</h4><p>将BERT用于特定任务相对简单，BERT可以应用于各种各样的语言任务，同时只向核心模型添加一个小层。</p>
<ul>
<li><p>序列分类任务</p>
<p>[CLS]标记的最终隐状态被视为输入序列的固定维池表示，这被输入到分类层，分类层是唯一添加的新参数，其维数是K x H,其中K是分类器标签的数目，H是隐藏状态的大小，标签概率用标准的softmax计算。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_YYl72NFjdd.png" alt=""></p>
</li>
<li><p>句子对分类任务</p>
<p>与上边的序列分类任务完全类似，唯一的不同就是输入的表示是两个句子连在一起的。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_wfpkyM0vZ4.png" alt=""></p>
</li>
<li><p>QA任务</p>
<p>QA估计回答任务是一个预测任务，给定一个问题一个一段文章，该模型从最有可能回答该问题的段落预测一个开始标记和一个结束标记。</p>
<p>和句子对任务一样，QA任务中的问题也会成为输入序列的第一个和第二个句子。在微调开始和结束向量（大小与隐状态相同）的过程中，只有两个参数被学习到。Token i作为应答开始的文本标签的概率，被计算为softmax（S,K）,S是开始向量，K是令牌的最终Transformer输出。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_NPyWFd2L5e.png" alt=""></p>
</li>
<li><p>单个句子tagging任务</p>
<p>在诸如命名实体识别的单句标记任务中，必须为输入的每个单词预测一个标记。最终隐状态（transformer输出）被feed给分类层，已获得对每个token的预测。由于WordPiece标记器将一些单词分解成子单词，因此只考虑单词的第一个标记的预测。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_sUOeH4lp7v.png" alt=""></p>
</li>
<li><p>超参数调整</p>
<p>每个任务都有最适合自己的超参数，如下的超参数在各个任务中普遍表现不差。</p>
<p>dropout – 0.1<br>Batch Size – 16, 32<br>Learning Rate (Adam) – 5e-5, 3e-5, 2e-5<br>Number of epochs – 3, 4 (yeah you read it right)</p>
<p>同时，大数据集（标记样本  &gt;  100k）对超参数选择的敏感性低于小数据集。</p>
</li>
</ul>
<h4 id="5-关于QA任务的BERT-Benchmarks"><a href="#5-关于QA任务的BERT-Benchmarks" class="headerlink" title="5.关于QA任务的BERT Benchmarks"></a>5.关于QA任务的BERT Benchmarks</h4><p>斯坦福问答数据集（SQuAD）是10万个众包问答对的集合，给定一个问题和维基百科里包含答案的段落,QA问题的任务就是找到问答段落里的答案文本跨度。</p>
<p>在SQuAD中，得分最高的模型就是BERT Large。</p>
<h4 id="6-其他关键点"><a href="#6-其他关键点" class="headerlink" title="6.其他关键点"></a>6.其他关键点</h4><ul>
<li>模型大小很重要，BERT_LARGE有3.45亿个参数，是同类模型中最大的，它在小规模任务中的优势也要明显高于BERT_BASE，后者使用同样的架构，参数只有1.1亿个。</li>
<li>在有了足够的训练数据的情况下，更多的训练步骤=更高的准确性，例如，在MNLI（Multi-Genre Natural Language Inference）多题材自然语言推理任务中，128000字的批大小上训练时，训练100万步的精度要比训练50万步的精度高1%。</li>
<li>BERT的双向方法收敛速度要慢于从左向右的方法，因为每批中只有15%的单词被预测。但是经过少量的预训练步骤后，双向训练仍然优于从左向右的训练。</li>
</ul>

            <hr>
          </div>
          <br>
          <div>
            <p>
            
              <span>
                <i class="iconfont icon-inbox"></i>
                
                  <a class="hover-with-bg" href="/categories/nlp%E6%A6%82%E5%BF%B5">nlp概念</a>
                  &nbsp;
                
              </span>&nbsp;&nbsp;
            
            
              <span>
                <i class="iconfont icon-tag"></i>
                
                  <a class="hover-with-bg" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a>
                
              </span>
            
            </p>
            
              <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" target="_blank" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
            
          </div>
        </div>
      </div>
    </div>
    <div class="d-none d-lg-block col-lg-2 toc-container">
      
  <div id="toc">
    <p class="h4"><i class="far fa-list-alt"></i>&nbsp;目录</p>
    <div id="tocbot"></div>
  </div>

    </div>
  </div>
</div>

<!-- custom -->


<!-- Comments -->
<div class="col-lg-7 mx-auto nopadding-md">
  <div class="container comments mx-auto" id="comments">
    
  </div>
</div>

    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>

    
  
    <!-- 不蒜子统计PV -->
    
    &nbsp;<span id="busuanzi_container_site_pv">总访问量 
          <span id="busuanzi_value_site_pv"></span> 次</span>&nbsp;
  
  
    <!-- 不蒜子统计UV -->
    
    &nbsp;<span id="busuanzi_container_site_uv">总访客数 
            <span id="busuanzi_value_site_uv"></span> 人</span>&nbsp;
  
  <br>



    

  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js" ></script>
<script src="/lib/popper/popper.min.js" ></script>
<script src="/lib/bootstrap/js/bootstrap.min.js" ></script>
<script src="/lib/mdbootstrap/js/mdb.min.js" ></script>
<script src="/js/main.js" ></script>




  
    <script src="/lib/tocbot/tocbot.min.js" ></script>
  
  <script src="/js/post.js" ></script>



  <script src="/lib/smooth-scroll/smooth-scroll.min.js" ></script>



  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>


<!-- Plugins -->


  

  

  

  

  




  <script src="/lib/prettify/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "NLP概念/概念_BERT&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 120,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="/lib/anchor/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "false",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>







</body>
</html>

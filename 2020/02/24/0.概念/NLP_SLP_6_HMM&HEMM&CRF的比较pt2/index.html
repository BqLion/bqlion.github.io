<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <link rel="icon" type="image/png" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="description" content="desc">
  <meta name="author" content="LiuBingqian">
  <meta name="keywords" content="">
  <title>0.概念/NLP_SLP_6_HMM&amp;HEMM&amp;CRF的比较pt2 ~ 刘秉乾的技术博客 :)</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"  >
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css"  >
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css"  >
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css"  >

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css"  >

<link rel="stylesheet" href="/css/main.css"  >


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css"  >


<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header style="height: 100vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>LiuBingqian`s Blog</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">主页</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">分类</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">所有文章</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">标签</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">关于</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>


</nav>

    <div class="view intro-2" id="background"
         style="background: url('http://bqlab-pic.test.upcdn.net/archive.jpg')no-repeat center center;
           background-size: cover;
           background-attachment: fixed;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              <br>
              
                <p class="mt-3">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>&nbsp;
                  星期一, 二月 24日 2020, 12:04 中午
                </p>
              

              <p>
                
                  
                  &nbsp;<i class="far fa-chart-bar"></i>
                  <span class="post-count">
                    9.4k 字
                  </span>&nbsp;
                

                
                  
                  &nbsp;<i class="far fa-clock"></i>
                  <span class="post-count">
                      34 分钟
                  </span>&nbsp;
                

                
                  <!-- 不蒜子统计文章PV -->
                  
                  &nbsp;<i class="far fa-eye" aria-hidden="true"></i>&nbsp;
                  <span id="busuanzi_container_page_pv">
                    <span id="busuanzi_value_page_pv"></span> 次
                  </span>&nbsp;
                
              </p>
            
          </div>

          
            <div class="scroll-down-bar">
              <i class="fas fa-angle-down scroll-down-arrow"></i>
            </div>
          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="py-5 z-depth-3" id="board">
        <div class="post-content mx-auto" id="post">
          <div class="markdown-body">
            <h2 id="2-1-1-概览"><a href="#2-1-1-概览" class="headerlink" title="2.1.1 概览"></a><strong>2.1.1 概览</strong></h2><p>在统计概率图（probability graph models）中，参考宗成庆老师的书，是这样的体系结构（个人非常喜欢这种类型的图）：</p>
<p><img src="https://pic3.zhimg.com/v2-714c1843f78b6aecdb0c57cdd08e1c6a_b.jpg" alt="img"></p>
<p>在概率图模型中，数据(样本)由公式 <img src="https://www.zhihu.com/equation?tex=G%3D%28V%2CE%29" alt="[公式]"> 建模表示： </p>
<ul>
<li><img src="https://www.zhihu.com/equation?tex=V" alt="[公式]"> 表示节点，即随机变量（放在此处的，可以是一个token或者一个label），具体地，用 <img src="https://www.zhihu.com/equation?tex=Y+%3D+%28y_%7B1%7D%2C+%7B%5Ccdots%7D%2C+y_%7Bn%7D+%29+" alt="[公式]"> 为随机变量建模，注意 <img src="https://www.zhihu.com/equation?tex=Y+" alt="[公式]"> 现在是代表了一批随机变量（想象对应一条sequence，包含了很多的token）， <img src="https://www.zhihu.com/equation?tex=+P%28Y%29+" alt="[公式]"> 为这些随机变量的分布；</li>
<li><img src="https://www.zhihu.com/equation?tex=E" alt="[公式]"> 表示边，即概率依赖关系。具体咋理解，还是要在后面结合HMM或CRF的graph具体解释。</li>
</ul>
<h2 id="2-1-2-有向图-vs-无向图"><a href="#2-1-2-有向图-vs-无向图" class="headerlink" title="2.1.2 有向图 vs. 无向图"></a><strong>2.1.2 有向图 vs. 无向图</strong></h2><p>上图可以看到，贝叶斯网络（信念网络）都是有向的，马尔科夫网络无向。所以，贝叶斯网络适合为有单向依赖的数据建模，马尔科夫网络适合实体之间互相依赖的建模。具体地，他们的核心差异表现在如何求 <img src="https://www.zhihu.com/equation?tex=P%3D%28Y%29" alt="[公式]"> ，即怎么表示 <img src="https://www.zhihu.com/equation?tex=Y%3D%EF%BC%88y_%7B1%7D%2C%5Ccdots%2Cy_%7Bn%7D%EF%BC%89" alt="[公式]"> 这个的联合概率。</p>
<p><strong>1. 有向图</strong></p>
<p>对于有向图模型，这么求联合概率： <img src="https://www.zhihu.com/equation?tex=P%28x_%7B1%7D%2C+%7B%5Ccdots%7D%2C+x_%7Bn%7D+%29%3D%5Cprod_%7Bi%3D0%7DP%28x_%7Bi%7D+%7C+%5Cpi%28x_%7Bi%7D%29%29" alt="[公式]"></p>
<p>举个例子，对于下面的这个有向图的随机变量(注意，这个图我画的还是比较广义的)：</p>
<p><img src="https://pic1.zhimg.com/v2-5b3f6b4a2d905297b7f73a89e92ee618_b.jpg" alt="img"></p>
<p>应该这样表示他们的联合概率:</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28x_%7B1%7D%2C+%7B%5Ccdots%7D%2C+x_%7Bn%7D+%29%3DP%28x_%7B1%7D%29%C2%B7P%28x_%7B2%7D%7Cx_%7B1%7D+%29%C2%B7P%28x_%7B3%7D%7Cx_%7B2%7D+%29%C2%B7P%28x_%7B4%7D%7Cx_%7B2%7D+%29%C2%B7P%28x_%7B5%7D%7Cx_%7B3%7D%2Cx_%7B4%7D+%29+" alt="[公式]"></p>
<p>应该很好理解吧。</p>
<p><strong>2. 无向图</strong></p>
<p>对于无向图，我看资料一般就指马尔科夫网络(注意，这个图我画的也是比较广义的)。</p>
<p><img src="https://pic4.zhimg.com/v2-1d8faeb71d690d02e110c7cd1d39eed3_b.jpg" alt="img"></p>
<p>如果一个graph太大，可以用因子分解将 <img src="https://www.zhihu.com/equation?tex=P%3D%28Y%29" alt="[公式]"> 写为若干个联合概率的乘积。咋分解呢，将一个图分为若干个“小团”，注意每个团必须是“最大团”（就是里面任何两个点连在了一块，具体……算了不解释，就是最大连通子图），则有：</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28Y+%29%3D%5Cfrac%7B1%7D%7BZ%28x%29%7D+%5Cprod_%7Bc%7D%5Cpsi_%7Bc%7D%28Y_%7Bc%7D+%29+" alt="[公式]"></p>
<p>, 其中 <img src="https://www.zhihu.com/equation?tex=Z%28x%29+%3D+%5Csum_%7BY%7D+%5Cprod_%7Bc%7D%5Cpsi_%7Bc%7D%28Y_%7Bc%7D+%29" alt="[公式]"> ，公式应该不难理解吧，归一化是为了让结果算作概率。</p>
<p>所以像上面的无向图：</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28Y+%29%3D%5Cfrac%7B1%7D%7BZ%28x%29%7D+%28+%5Cpsi_%7B1%7D%28X_%7B1%7D%2C+X_%7B3%7D%2C+X_%7B4%7D+%29+%C2%B7+%5Cpsi_%7B2%7D%28X_%7B2%7D%2C+X_%7B3%7D%2C+X_%7B4%7D+%29+%29" alt="[公式]"></p>
<p>其中， <img src="https://www.zhihu.com/equation?tex=+%5Cpsi_%7Bc%7D%28Y_%7Bc%7D+%29" alt="[公式]"> 是一个最大团 <img src="https://www.zhihu.com/equation?tex=C" alt="[公式]"> 上随机变量们的联合概率，一般取指数函数的：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cpsi_%7Bc%7D%28Y_%7Bc%7D+%29+%3D+e%5E%7B-E%28Y_%7Bc%7D%29%7D+%3De%5E%7B%5Csum_%7Bk%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28c%2Cy%7Cc%2Cx%29%7D" alt="[公式]"></p>
<p>好了，管这个东西叫做<code>势函数</code>。注意 <img src="https://www.zhihu.com/equation?tex=e%5E%7B%5Csum_%7Bk%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28c%2Cy%7Cc%2Cx%29%7D" alt="[公式]"> 是否有看到CRF的影子。</p>
<p>那么概率无向图的联合概率分布可以在因子分解下表示为：</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28Y+%29%3D%5Cfrac%7B1%7D%7BZ%28x%29%7D+%5Cprod_%7Bc%7D%5Cpsi_%7Bc%7D%28Y_%7Bc%7D+%29+%3D+%5Cfrac%7B1%7D%7BZ%28x%29%7D+%5Cprod_%7Bc%7D+e%5E%7B%5Csum_%7Bk%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28c%2Cy%7Cc%2Cx%29%7D+%3D+%5Cfrac%7B1%7D%7BZ%28x%29%7D+e%5E%7B%5Csum_%7Bc%7D%5Csum_%7Bk%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28y_%7Bi%7D%2Cy_%7Bi-1%7D%2Cx%2Ci%29%7D" alt="[公式]"></p>
<p>注意，这里的理解还蛮重要的，注意递推过程，敲黑板，这是CRF的开端！<br>这个由<code>Hammersly-Clifford law</code>保证，具体不展开。</p>
<h2 id="2-1-3-马尔科夫假设-amp-马尔科夫性"><a href="#2-1-3-马尔科夫假设-amp-马尔科夫性" class="headerlink" title="2.1.3 马尔科夫假设&amp;马尔科夫性"></a><strong>2.1.3 马尔科夫假设&amp;马尔科夫性</strong></h2><p>这个也属于前馈知识。</p>
<p><strong>1. 马尔科夫假设</strong></p>
<p>额应该是齐次马尔科夫假设，这样假设：马尔科夫链 <img src="https://www.zhihu.com/equation?tex=%EF%BC%88x_%7B1%7D%2C%5Ccdots%2Cx_%7Bn%7D%29" alt="[公式]"> 里的 <img src="https://www.zhihu.com/equation?tex=+x_%7Bi%7D" alt="[公式]"> 总是只受 <img src="https://www.zhihu.com/equation?tex=+x_%7Bi-1%7D" alt="[公式]"> 一个人的影响。<br>马尔科夫假设这里相当于就是个1-gram。</p>
<p>马尔科夫过程呢？即，在一个过程中，每个状态的转移只依赖于前n个状态，并且只是个n阶的模型。最简单的马尔科夫过程是一阶的，即只依赖于器哪一个状态。</p>
<p><strong>2. 马尔科夫性</strong></p>
<p>马尔科夫性是是保证或者判断概率图是否为概率无向图的条件。</p>
<p>三点内容：a. 成对，b. 局部，c. 全局。</p>
<p>我觉得这个不用展开。</p>
<h2 id="2-2-判别式（discriminative）模型-vs-生成式-generative-模型"><a href="#2-2-判别式（discriminative）模型-vs-生成式-generative-模型" class="headerlink" title="2.2 判别式（discriminative）模型 vs. 生成式(generative)模型"></a><strong>2.2 判别式（discriminative）模型 vs. 生成式(generative)模型</strong></h2><p>在监督学习下，模型可以分为判别式模型与生成式模型。</p>
<p>重点来了。上面有提到，我理解了HMM、CRF模型的区别是从理解了判别式模型与生成式模型的那刻，并且瞬间对其他的模型有一个恍然大悟。我记得是一年前就开始纠结这两者的区别，但我只能说，栽在了一些烂博客上，大部分都没有自己的insightful理解，也就是一顿官话，也真是难以理解。后来在知乎上一直琢磨别人的答案，然后某日早晨终于豁然开朗，就是这种感觉。</p>
<p>好了，我要用自己的理解来转述两者的区别了below。</p>
<p>先问个问题，根据经验，A批模型（神经网络模型、SVM、perceptron、LR、DT……）与B批模型（NB、LDA……），有啥区别不？（这个问题需要一些模型使用经验）应该是这样的： </p>
<blockquote>
<p>\1. A批模型是这么工作的，他们直接将数据的Y（或者label），根据所提供的features，学习，最后画出了一个明显或者比较明显的边界（具体怎么做到的？通过复杂的函数映射，或者决策叠加等等mechanism），这一点线性LR、线性SVM应该很明显吧。 </p>
<p>\2. B批模型是这么工作的，他们先从训练样本数据中，将所有的数据的分布情况摸透，然后最终确定一个分布，来作为我的所有的输入数据的分布，并且他是一个联合分布 <img src="https://www.zhihu.com/equation?tex=P%28X%2CY%29" alt="[公式]"> (注意 <img src="https://www.zhihu.com/equation?tex=X" alt="[公式]"> 包含所有的特征 <img src="https://www.zhihu.com/equation?tex=x_%7Bi%7D" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=Y" alt="[公式]"> 包含所有的label)。然后我来了新的样本数据（inference），好，通过学习来的模型的联合分布 <img src="https://www.zhihu.com/equation?tex=P%28X%2CY%29" alt="[公式]"> ，再结合新样本给的 <img src="https://www.zhihu.com/equation?tex=X" alt="[公式]"> ，通过条件概率就能出来 <img src="https://www.zhihu.com/equation?tex=Y" alt="[公式]"> ：<br><img src="https://www.zhihu.com/equation?tex=P%28Y%7CX%29+%3D+%5Cfrac%7BP%28X%2CY%29%7D%7BP%28X%29%7D" alt="[公式]"></p>
</blockquote>
<p>好了，应该说清楚了。</p>
<p><strong>1. 判别式模型</strong></p>
<p>那么A批模型对应了判别式模型。根据上面的两句话的区别，可以知道判别模型的特征了，所以有句话说：<strong>判别模型是直接对</strong> <img src="https://www.zhihu.com/equation?tex=P%28Y%7CX%29" alt="[公式]"><strong>建模</strong>，就是说，直接根据X特征来对Y建模训练。</p>
<p>具体地，我的训练过程是确定构件 <img src="https://www.zhihu.com/equation?tex=P%28Y%7CX%29" alt="[公式]"> 模型里面“复杂映射关系”中的参数，完了再去inference一批新的sample。</p>
<p>所以判别式模型的特征总结如下：</p>
<ol>
<li>对 <img src="https://www.zhihu.com/equation?tex=P%28Y%7CX%29" alt="[公式]"> 建模</li>
<li>对所有的样本只构建一个模型，确认总体判别边界</li>
<li>观测到输入什么特征，就预测最可能的label</li>
<li>另外，判别式的优点是：对数据量要求没生成式的严格，速度也会快，小数据量下准确率也会好些。</li>
</ol>
<p><strong>2. 生成式模型</strong></p>
<p>同样，B批模型对应了生成式模型。并且需要注意的是，在模型训练中，我学习到的是X与Y的联合模型 <img src="https://www.zhihu.com/equation?tex=P%28X%2CY%29" alt="[公式]"> ，也就是说，<strong>我在训练阶段是只对</strong> <img src="https://www.zhihu.com/equation?tex=P%28X%2CY%29" alt="[公式]"><strong>建模</strong>，我需要确定维护这个联合概率分布的所有的信息参数。完了之后在inference再对新的sample计算 <img src="https://www.zhihu.com/equation?tex=P%28Y%7CX%29" alt="[公式]"> ，导出 <img src="https://www.zhihu.com/equation?tex=Y" alt="[公式]"> ,但这已经不属于建模阶段了。</p>
<p>结合NB过一遍生成式模型的工作流程。学习阶段，建模： <img src="https://www.zhihu.com/equation?tex=P%28X%2CY%29%3DP%28X%7CY%29P%28Y%29" alt="[公式]"> （当然，NB具体流程去隔壁参考）,然后 <img src="https://www.zhihu.com/equation?tex=P%28Y%7CX%29+%3D+%5Cfrac%7BP%28X%2CY%29%7D%7BP%28X%29%7D" alt="[公式]"> 。<br>另外，LDA也是这样，只是他更过分，需要确定很多个概率分布，而且建模抽样都蛮复杂的。</p>
<p>所以生成式总结下有如下特点：</p>
<ol>
<li>对 <img src="https://www.zhihu.com/equation?tex=P%28X%2CY%29" alt="[公式]"> 建模</li>
<li>这里我们主要讲分类问题，所以是要对每个label（ <img src="https://www.zhihu.com/equation?tex=y_%7Bi%7D+" alt="[公式]"> ）都需要建模，最终选择最优概率的label为结果，所以没有什么判别边界。（对于序列标注问题，那只需要构件一个model）</li>
<li>中间生成联合分布，并可生成采样数据。</li>
<li>生成式模型的优点在于，所包含的信息非常齐全，我称之为“上帝信息”，所以不仅可以用来输入label，还可以干其他的事情。生成式模型关注结果是如何产生的。但是生成式模型需要非常充足的数据量以保证采样到了数据本来的面目，所以速度相比之下，慢。</li>
</ol>
<p>这一点明白后，后面讲到的HMM与CRF的区别也会非常清晰。<br>最后identity the picture below:</p>
<p><img src="https://pic2.zhimg.com/v2-9dfdfb246a1e6922ff8835d7fdf45e05_b.jpg" alt="img"></p>
<h2 id="2-3-序列建模"><a href="#2-3-序列建模" class="headerlink" title="2.3 序列建模"></a><strong>2.3 序列建模</strong></h2><p>为了号召零门槛理解，现在解释如何为序列问题建模。</p>
<p><img src="https://pic2.zhimg.com/v2-5358abb09290b93a5642fd81702dfe41_b.jpg" alt="img"></p>
<p>序列包括时间序列以及general sequence，但两者无异。连续的序列在分析时也会先离散化处理。常见的序列有如：时序数据、本文句子、语音数据、等等。</p>
<p>广义下的序列有这些特点：</p>
<ul>
<li>节点之间有关联依赖性/无关联依赖性</li>
<li>序列的节点是随机的/确定的</li>
<li>序列是线性变化/非线性的</li>
<li>……</li>
</ul>
<p>对不同的序列有不同的问题需求，常见的序列建模方法总结有如下：</p>
<ol>
<li>拟合，预测未来节点（或走势分析）：</li>
</ol>
<p>​      a. 常规序列建模方法：AR、MA、ARMA、ARIMA</p>
<p>​      b. 回归拟合</p>
<p>​      c. Neural Networks</p>
<p> \2. 判断不同序列类别，即分类问题：HMM、CRF、General Classifier（ML models、NN models）</p>
<p> \3. 不同时序对应的状态的分析，即序列标注问题：HMM、CRF、RecurrentNNs</p>
<p>在本篇文字中，我们只关注在2. &amp; 3.类问题下的建模过程和方法。</p>
<h2 id="三、HMM"><a href="#三、HMM" class="headerlink" title="三、HMM"></a><strong>三、HMM</strong></h2><p>最早接触的是HMM。较早做过一个项目，关于声波手势识别，跟声音识别的机制一样，使用的正是HMM的一套方法。后来又用到了<em>kalman filter</em>,之后做序列标注任务接触到了CRF，所以整个概率图模型还是接触的方面还蛮多。</p>
<h2 id="3-1-理解HMM"><a href="#3-1-理解HMM" class="headerlink" title="3.1 理解HMM"></a><strong>3.1 理解HMM</strong></h2><p>在2.2、2.3中提序列的建模问题时，我们只是讨论了常规的序列数据，e.g., <img src="https://www.zhihu.com/equation?tex=%EF%BC%88X_%7B1%7D%2C%5Ccdots%2CX_%7Bn%7D%EF%BC%89" alt="[公式]"> ,像2.3的图片那样。像这种序列一般用马尔科夫模型就可以胜任。实际上我们碰到的更多的使用HMM的场景是每个节点 <img src="https://www.zhihu.com/equation?tex=+X_%7Bi%7D" alt="[公式]"> 下还附带着另一个节点 <img src="https://www.zhihu.com/equation?tex=+Y_%7Bi%7D" alt="[公式]"> ，正所谓<strong>隐含</strong>马尔科夫模型，那么除了正常的节点，还要将<strong>隐含状态节点</strong>也得建模进去。正儿八经地，将 <img src="https://www.zhihu.com/equation?tex=X_%7Bi%7D+%E3%80%81+Y_%7Bi%7D+" alt="[公式]"> 换成 <img src="https://www.zhihu.com/equation?tex=i_%7Bi%7D+%E3%80%81o_%7Bi%7D" alt="[公式]"> ,并且他们的名称变为状态节点、观测节点。状态节点正是我的隐状态。</p>
<p>HMM属于典型的生成式模型。对照2.1的讲解，应该是要从训练数据中学到数据的各种分布，那么有哪些分布呢以及是什么呢？直接正面回答的话，正是<strong>HMM的5要素</strong>，其中有3个就是整个数据的不同角度的概率分布：</p>
<ul>
<li><img src="https://www.zhihu.com/equation?tex=N" alt="[公式]"> ，隐藏状态集 <img src="https://www.zhihu.com/equation?tex=N+%3D+%5Clbrace+q_%7B1%7D%2C+%5Ccdots%2C+q_%7BN%7D+%5Crbrace" alt="[公式]"> , 我的隐藏节点不能随意取，只能限定取包含在隐藏状态集中的符号。</li>
<li><img src="https://www.zhihu.com/equation?tex=M" alt="[公式]"> ，观测集 <img src="https://www.zhihu.com/equation?tex=M+%3D+%5Clbrace+v_%7B1%7D%2C+%5Ccdots%2C+v_%7BM%7D+%5Crbrace" alt="[公式]"> , 同样我的观测节点不能随意取，只能限定取包含在观测状态集中的符号。</li>
<li><img src="https://www.zhihu.com/equation?tex=A" alt="[公式]"> ，状态转移概率矩阵，这个就是其中一个概率分布。他是个矩阵， <img src="https://www.zhihu.com/equation?tex=+A%3D+%5Ba_%7Bij%7D%5D_%7BN+%5Ctimes+N%7D+" alt="[公式]"> （N为隐藏状态集元素个数），其中 <img src="https://www.zhihu.com/equation?tex=a_%7Bij%7D+%3D+P%28i_%7Bt%2B1%7D%7Ci_%7Bt%7D%29%EF%BC%8C+i_%7Bt%7D" alt="[公式]"> 即第i个隐状态节点,即所谓的状态转移嘛。</li>
<li><img src="https://www.zhihu.com/equation?tex=B" alt="[公式]"> ，观测概率矩阵，这个就是另一个概率分布。他是个矩阵， <img src="https://www.zhihu.com/equation?tex=B+%3D+%5Bb_%7Bij%7D%5D_%7BN+%5Ctimes+M%7D" alt="[公式]"> （N为隐藏状态集元素个数，M为观测集元素个数），其中 <img src="https://www.zhihu.com/equation?tex=b_%7Bij%7D+%3D+P%28o_%7Bt%7D%7Ci_%7Bt%7D%29%EF%BC%8C+o_%7Bt%7D" alt="[公式]"> 即第i个观测节点, <img src="https://www.zhihu.com/equation?tex=+i_%7Bt%7D" alt="[公式]"> 即第i个隐状态节点,即所谓的观测概率（发射概率）嘛。</li>
<li><img src="https://www.zhihu.com/equation?tex=%CF%80" alt="[公式]"> ，在第一个隐状态节点 <img src="https://www.zhihu.com/equation?tex=i_%7Bt%7D" alt="[公式]"> ,我得人工单独赋予，我第一个隐状态节点的隐状态是 <img src="https://www.zhihu.com/equation?tex=N" alt="[公式]"> 中的每一个的概率分别是多少，然后 <img src="https://www.zhihu.com/equation?tex=%CF%80" alt="[公式]"> 就是其概率分布。</li>
</ul>
<p>所以图看起来是这样的：</p>
<p><img src="https://pic4.zhimg.com/v2-d4077c2dbd9899d8896751a28490c9c7_b.jpg" alt="img"></p>
<p>看的很清楚，我的模型先去学习要确定以上5要素，之后在inference阶段的工作流程是：首先，隐状态节点 <img src="https://www.zhihu.com/equation?tex=i_%7Bt%7D" alt="[公式]"> 是不能直接观测到的数据节点， <img src="https://www.zhihu.com/equation?tex=o_%7Bt%7D" alt="[公式]"> 才是能观测到的节点，并且注意箭头的指向表示了依赖生成条件关系， <img src="https://www.zhihu.com/equation?tex=i_%7Bt%7D" alt="[公式]"> 在A的指导下生成下一个隐状态节点 <img src="https://www.zhihu.com/equation?tex=i_%7Bt%2B1%7D" alt="[公式]"> ，并且 <img src="https://www.zhihu.com/equation?tex=i_%7Bt%7D" alt="[公式]"> 在 <img src="https://www.zhihu.com/equation?tex=B" alt="[公式]"> 的指导下生成依赖于该 <img src="https://www.zhihu.com/equation?tex=i_%7Bt%7D" alt="[公式]"> 的观测节点 <img src="https://www.zhihu.com/equation?tex=o_%7Bt%7D" alt="[公式]"> , 并且我只能观测到序列 <img src="https://www.zhihu.com/equation?tex=%EF%BC%88o_%7B1%7D%2C+%5Ccdots%2C+o_%7Bi%7D%29" alt="[公式]"> 。</p>
<p>好，举例子说明（序列标注问题，POS，标注集BES）：</p>
<blockquote>
<p>input: “学习出一个模型，然后再预测出一条指定”</p>
<p>expected output: 学/B 习/E 出/S 一/B 个/E 模/B 型/E ，/S 然/B 后/E 再/E 预/B 测/E ……</p>
<p>其中，input里面所有的char构成的字表，形成观测集 <img src="https://www.zhihu.com/equation?tex=M" alt="[公式]"> ，因为字序列在inference阶段是我所能看见的；标注集BES构成隐藏状态集 <img src="https://www.zhihu.com/equation?tex=N" alt="[公式]"> ，这是我无法直接获取的，也是我的预测任务；至于 <img src="https://www.zhihu.com/equation?tex=A%E3%80%81B%E3%80%81%CF%80" alt="[公式]"> ，这些概率分布信息（上帝信息）都是我在学习过程中所确定的参数。</p>
</blockquote>
<p>然后一般初次接触的话会疑问：为什么要这样？……好吧，就应该是这样啊，根据具有同时带着隐藏状态节点和观测节点的类型的序列，在HMM下就是这样子建模的。</p>
<p>下面来点高层次的理解：</p>
<ol>
<li>根据概率图分类，可以看到HMM属于有向图，并且是生成式模型，直接对联合概率分布建模 <img src="https://www.zhihu.com/equation?tex=P%28O%2CI%29+%3D+%5Csum_%7Bt%3D1%7D%5E%7BT%7DP%28O_%7Bt%7D+%7C+O_%7Bt-1%7D%29P%28I_%7Bt%7D+%7C+O_%7Bt%7D%29" alt="[公式]"> (注意，这个公式不在模型运行的任何阶段能体现出来，只是我们都去这么来表示HMM是个生成式模型，他的联合概率 <img src="https://www.zhihu.com/equation?tex=P%28O%2CI%29" alt="[公式]"> 就是这么计算的)。</li>
<li>并且B中 <img src="https://www.zhihu.com/equation?tex=b_%7Bij%7D+%3D+P%28o_%7Bt%7D%7Ci_%7Bt%7D%29" alt="[公式]"> ，这意味着o对i有依赖性。</li>
<li>在A中， <img src="https://www.zhihu.com/equation?tex=a_%7Bij%7D+%3D+P%28i_%7Bt%2B1%7D%7Ci_%7Bt%7D%29" alt="[公式]"> ，也就是说只遵循了一阶马尔科夫假设，1-gram。试想，如果数据的依赖超过1-gram，那肯定HMM肯定是考虑不进去的。这一点限制了HMM的性能。</li>
</ol>
<h2 id="3-2-模型运行过程"><a href="#3-2-模型运行过程" class="headerlink" title="3.2 模型运行过程"></a><strong>3.2 模型运行过程</strong></h2><p>模型的运行过程（工作流程）对应了HMM的3个问题。</p>
<h2 id="3-2-1-学习训练过程"><a href="#3-2-1-学习训练过程" class="headerlink" title="3.2.1 学习训练过程"></a><strong>3.2.1 学习训练过程</strong></h2><p>对照2.1的讲解，HMM学习训练的过程，就是找出数据的分布情况，也就是模型参数的确定。</p>
<p>主要学习算法按照训练数据除了观测状态序列 <img src="https://www.zhihu.com/equation?tex=%EF%BC%88o_%7B1%7D%2C+%5Ccdots%2C+o_%7Bi%7D%29" alt="[公式]"> 是否还有隐状态序列 <img src="https://www.zhihu.com/equation?tex=%EF%BC%88i_%7B1%7D%2C+%5Ccdots%2C+i_%7Bi%7D%29" alt="[公式]"> 分为：</p>
<ul>
<li>极大似然估计, with 隐状态序列</li>
<li>Baum-Welch(前向后向), without 隐状态序列</li>
</ul>
<p>感觉不用做很多的介绍，都是很实实在在的算法，看懂了就能理解。简要提一下。</p>
<p><strong>1. 极大似然估计</strong></p>
<p>一般做NLP的序列标注等任务，在训练阶段肯定是有隐状态序列的。所以极大似然估计法是非常常用的学习算法，我见过的很多代码里面也是这么计算的。比较简单。</p>
<ul>
<li>step1. 算A</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=%5Chat%7Ba_%7Bij%7D%7D+%3D+%5Cfrac%7BA_%7Bij%7D%7D%7B%5Csum_%7Bj%3D1%7D%5E%7BN%7DA_%7Bij%7D%7D+" alt="[公式]"></p>
<ul>
<li>step2. 算B</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=%5Chat%7Bb_%7Bj%7D%7D%28k%29+%3D+%5Cfrac%7BB_%7Bjk%7D%7D%7B%5Csum_%7Bk%3D1%7D%5E%7BM%7DB_%7Bjk%7D%7D" alt="[公式]"></p>
<ul>
<li>step3. 直接估计 <img src="https://www.zhihu.com/equation?tex=%CF%80" alt="[公式]"></li>
</ul>
<p>比如说，在代码里计算完了就是这样的： </p>
<p><img src="https://pic2.zhimg.com/v2-5343666b942f952f6c79c4fa5e2dfdd9_b.jpg" alt="img"></p>
<p><img src="https://pic1.zhimg.com/v2-0d695539c785fe16cfcfff3b9bd0c164_b.jpg" alt="img"></p>
<p><img src="https://pic2.zhimg.com/v2-443053bd4342c71bf602ac15aa7c8731_b.jpg" alt="img"></p>
<p><strong>2. Baum-Welch(前向后向)</strong></p>
<p>就是一个EM的过程，如果你对EM的工作流程有经验的话，对这个Baum-Welch一看就懂。EM的过程就是初始化一套值，然后迭代计算，根据结果再调整值，再迭代，最后收敛……好吧，这个理解是没有捷径的，去隔壁钻研EM吧。</p>
<p>这里只提一下核心。因为我们手里没有隐状态序列 <img src="https://www.zhihu.com/equation?tex=%EF%BC%88i_%7B1%7D%2C+%5Ccdots%2C+i_%7Bi%7D%29" alt="[公式]"> 信息，所以我先必须给初值 <img src="https://www.zhihu.com/equation?tex=a_%7Bij%7D%5E%7B0%7D%2C+b_%7Bj%7D%28k%29%5E%7B0%7D%2C+%5Cpi%5E%7B0%7D" alt="[公式]"> ，初步确定模型，然后再迭代计算出 <img src="https://www.zhihu.com/equation?tex=a_%7Bij%7D%5E%7Bn%7D%2C+b_%7Bj%7D%28k%29%5E%7Bn%7D%2C+%5Cpi%5E%7Bn%7D" alt="[公式]"> ,中间计算过程会用到给出的观测状态序列 <img src="https://www.zhihu.com/equation?tex=%EF%BC%88o_%7B1%7D%2C+%5Ccdots%2C+o_%7Bi%7D%29" alt="[公式]"> 。另外，收敛性由EM的XXX定理保证。</p>
<h2 id="3-2-2-序列标注（解码）过程"><a href="#3-2-2-序列标注（解码）过程" class="headerlink" title="3.2.2 序列标注（解码）过程"></a><strong>3.2.2 序列标注（解码）过程</strong></h2><p>好了，学习完了HMM的分布参数，也就确定了一个HMM模型。需要注意的是，这个HMM是对我这一批全部的数据进行训练所得到的参数。</p>
<p>序列标注问题也就是“预测过程”，通常称为解码过程。对应了序列建模问题3.。对于序列标注问题，我们只需要学习出一个HMM模型即可，后面所有的新的sample我都用这一个HMM去apply。</p>
<p>我们的目的是，在学习后已知了 <img src="https://www.zhihu.com/equation?tex=P%28Q%2CO%29" alt="[公式]"> ,现在要求出 <img src="https://www.zhihu.com/equation?tex=P%28Q%7CO%29" alt="[公式]"> ，进一步</p>
<p><img src="https://www.zhihu.com/equation?tex=Q_%7Bmax%7D+%3D+argmax_%7BallQ%7D%5Cfrac%7BP%28Q%2CO%29%7D%7BP%28O%29%7D" alt="[公式]"></p>
<p>再直白点就是，我现在要在给定的观测序列下找出一条隐状态序列，条件是这个隐状态序列的概率是最大的那个。</p>
<p>具体地，都是用Viterbi算法解码，是用DP思想减少重复的计算。Viterbi也是满大街的，不过要说的是，Viterbi不是HMM的专属，也不是任何模型的专属，他只是恰好被满足了被HMM用来使用的条件。谁知，现在大家都把Viterbi跟HMM捆绑在一起了, shame。</p>
<p>Viterbi计算有向无环图的一条最大路径，应该还好理解。如图：</p>
<p><img src="https://pic1.zhimg.com/v2-71f1ea9abbab357f7d9bad1138ee7344_b.jpg" alt="img"></p>
<p>关键是注意，每次工作热点区只涉及到t 与 t-1,这对应了DP的无后效性的条件。如果对某些同学还是很难理解，请参考<a href="https://www.zhihu.com/question/20136144" target="_blank" rel="noopener">这个答案</a>下@Kiwee的回答吧。</p>
<h2 id="3-2-3-序列概率过程"><a href="#3-2-3-序列概率过程" class="headerlink" title="3.2.3 序列概率过程"></a><strong>3.2.3 序列概率过程</strong></h2><p>我通过HMM计算出序列的概率又有什么用？针对这个点我把这个问题详细说一下。 </p>
<p>实际上，序列概率过程对应了序列建模问题2.，即序列分类。<br>在3.2.2第一句话我说，在序列标注问题中，我用一批完整的数据训练出了一支HMM模型即可。好，那在序列分类问题就不是训练一个HMM模型了。我应该这么做（结合语音分类识别例子）： </p>
<blockquote>
<p>目标：识别声音是A发出的还是B发出的。<br>HMM建模过程：<br>   \1.   训练：我将所有A说的语音数据作为dataset_A,将所有B说的语音数据作为dataset_B（当然，先要分别对dataset A ,B做预处理encode为元数据节点，形成sequences）,然后分别用dataset_A、dataset_B去训练出HMM_A/HMM_B<br>   \2.   inference：来了一条新的sample（sequence），我不知道是A的还是B的，没问题，分别用HMM_A/HMM_B计算一遍序列的概率得到 <img src="https://www.zhihu.com/equation?tex=P_%7BA%7D%EF%BC%88S%EF%BC%89%E3%80%81P_%7BB%7D%EF%BC%88S%EF%BC%89" alt="[公式]"> ，比较两者大小，哪个概率大说明哪个更合理，更大概率作为目标类别。</p>
</blockquote>
<p>所以，本小节的理解重点在于，<strong>如何对一条序列计算其整体的概率</strong>。即目标是计算出 <img src="https://www.zhihu.com/equation?tex=P%28O%7C%CE%BB%29" alt="[公式]"> 。这个问题前辈们在他们的经典中说的非常好了，比如参考李航老师整理的：</p>
<ul>
<li>直接计算法（穷举搜索）</li>
<li>前向算法</li>
<li>后向算法</li>
</ul>
<p>后面两个算法采用了DP思想，减少计算量，即每一次直接引用前一个时刻的计算结果以避免重复计算，跟Viterbi一样的技巧。</p>
<p>还是那句，因为这篇文档不是专门讲算法细节的，所以不详细展开这些。毕竟，所有的科普HMM、CRF的博客貌似都是在扯这些算法，妥妥的街货，就不搬运了。</p>
<h2 id="四、MEMM"><a href="#四、MEMM" class="headerlink" title="四、MEMM"></a><strong>四、MEMM</strong></h2><p>MEMM，即最大熵马尔科夫模型，这个是在接触了HMM、CRF之后才知道的一个模型。说到MEMM这一节时，得转换思维了，因为现在这MEMM属于判别式模型。</p>
<p>不过有一点很尴尬，MEMM貌似被使用或者讲解引用的不及HMM、CRF。</p>
<h2 id="4-1-理解MEMM"><a href="#4-1-理解MEMM" class="headerlink" title="4.1 理解MEMM"></a><strong>4.1 理解MEMM</strong></h2><p>这里还是啰嗦强调一下，MEMM正因为是判别模型，所以不废话，我上来就直接为了确定边界而去建模，比如说序列求概率（分类）问题，我直接考虑找出函数分类边界。这一点跟HMM的思维方式发生了很大的变化，如果不对这一点有意识，那么很难理解为什么MEMM、CRF要这么做。</p>
<p>HMM中，观测节点 <img src="https://www.zhihu.com/equation?tex=o_%7Bi%7D" alt="[公式]"> 依赖隐藏状态节点 <img src="https://www.zhihu.com/equation?tex=i_%7Bi%7D" alt="[公式]"> ,也就意味着我的观测节点只依赖当前时刻的隐藏状态。但在更多的实际场景下，观测序列是需要很多的特征来刻画的，比如说，我在做NER时，我的标注 <img src="https://www.zhihu.com/equation?tex=i_%7Bi%7D" alt="[公式]"> 不仅跟当前状态 <img src="https://www.zhihu.com/equation?tex=o_%7Bi%7D" alt="[公式]"> 相关，而且还跟前后标注 <img src="https://www.zhihu.com/equation?tex=o_%7Bj%7D%28j+%5Cneq+i%29" alt="[公式]"> 相关，比如字母大小写、词性等等。</p>
<p>为此，提出来的MEMM模型就是能够直接允许<strong>“定义特征”</strong>，直接学习条件概率，即 <img src="https://www.zhihu.com/equation?tex=P%28i_%7Bi%7D%7Ci_%7Bi-1%7D%2Co_%7Bi%7D%29+%28i+%3D+1%2C%5Ccdots%2Cn%29" alt="[公式]"> , 总体为：</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28I%7CO%29+%3D+%5Cprod_%7Bt%3D1%7D%5E%7Bn%7DP%28i_%7Bi%7D%7Ci_%7Bi-1%7D%2Co_%7Bi%7D%29%2C+i+%3D+1%2C%5Ccdots%2Cn" alt="[公式]"></p>
<p>并且， <img src="https://www.zhihu.com/equation?tex=P%28i%7Ci%5E%7B%27%7D%2Co%29" alt="[公式]"> 这个概率通过最大熵分类器建模（取名MEMM的原因）:</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28i%7Ci%5E%7B%27%7D%2Co%29+%3D+%5Cfrac%7B1%7D%7BZ%28o%2Ci%5E%7B%27%7D%29%7D+exp%28%5Csum_%7Ba%7D%29%5Clambda_%7Ba%7Df_%7Ba%7D%28o%2Ci%29" alt="[公式]"></p>
<p>重点来了，这是ME的内容，也是理解MEMM的关键： <img src="https://www.zhihu.com/equation?tex=Z%28o%2Ci%5E%7B%27%7D%29" alt="[公式]"> 这部分是归一化； <img src="https://www.zhihu.com/equation?tex=f_%7Ba%7D%28o%2Ci%29" alt="[公式]"> 是<strong>特征函数</strong>，具体点，这个函数是需要去定义的; <img src="https://www.zhihu.com/equation?tex=%CE%BB" alt="[公式]"> 是特征函数的权重，这是个未知参数，需要从训练阶段学习而得。</p>
<p>比如我可以这么定义特征函数：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+f_%7Ba%7D%28o%2Ci%29+%3D+%5Cbegin%7Bcases%7D+1%26+%5Ctext%7B%E6%BB%A1%E8%B6%B3%E7%89%B9%E5%AE%9A%E6%9D%A1%E4%BB%B6%7D%EF%BC%8C%5C%5C+0%26+%5Ctext%7Bother%7D+%5Cend%7Bcases%7D+%5Cend%7Bequation%7D" alt="[公式]"></p>
<p>其中，特征函数 <img src="https://www.zhihu.com/equation?tex=+f_%7Ba%7D%28o%2Ci%29" alt="[公式]"> 个数可任意制定， <img src="https://www.zhihu.com/equation?tex=%EF%BC%88a+%3D+1%2C+%5Ccdots%2C+n%EF%BC%89" alt="[公式]"></p>
<p>所以总体上，MEMM的建模公式这样：</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28I%7CO%29+%3D+%5Cprod_%7Bt%3D1%7D%5E%7Bn%7D%5Cfrac%7B+exp%28%5Csum_%7Ba%7D%29%5Clambda_%7Ba%7Df_%7Ba%7D%28o%2Ci%29+%7D%7BZ%28o%2Ci_%7Bi-1%7D%29%7D+%2C+i+%3D+1%2C%5Ccdots%2Cn" alt="[公式]"></p>
<p>是的，公式这部分之所以长成这样，是由ME模型决定的。</p>
<p>请务必注意，理解<strong>判别模型</strong>和<strong>定义特征</strong>两部分含义，这已经涉及到CRF的雏形了。</p>
<p>所以说，他是判别式模型，直接对条件概率建模。 上图： </p>
<p><img src="https://pic4.zhimg.com/v2-cb2cc25593fcaf06e682191d551ba03b_b.jpg" alt="img"></p>
<p>MEMM需要两点注意：</p>
<ol>
<li>与HMM的 <img src="https://www.zhihu.com/equation?tex=o_%7Bi%7D" alt="[公式]"> 依赖 <img src="https://www.zhihu.com/equation?tex=i_%7Bi%7D" alt="[公式]"> 不一样，MEMM当前隐藏状态 <img src="https://www.zhihu.com/equation?tex=i_%7Bi%7D" alt="[公式]"> 应该是依赖当前时刻的观测节点 <img src="https://www.zhihu.com/equation?tex=o_%7Bi%7D" alt="[公式]"> 和上一时刻的隐藏节点 <img src="https://www.zhihu.com/equation?tex=i_%7Bi-1%7D" alt="[公式]"></li>
<li>需要注意，之所以图的箭头这么画，是由MEMM的公式决定的，而公式是creator定义出来的。</li>
</ol>
<p>好了，走一遍完整流程。</p>
<blockquote>
<p>step1. 先预定义特征函数 <img src="https://www.zhihu.com/equation?tex=f_%7Ba%7D%28o%2Ci%29" alt="[公式]"> ，<br>step2. 在给定的数据上，训练模型，确定参数，即确定了MEMM模型<br>step3. 用确定的模型做序列标注问题或者序列求概率问题。</p>
</blockquote>
<h2 id="4-2-模型运行过程"><a href="#4-2-模型运行过程" class="headerlink" title="4.2 模型运行过程"></a><strong>4.2 模型运行过程</strong></h2><p>MEMM模型的工作流程也包括了学习训练问题、序列标注问题、序列求概率问题。</p>
<h2 id="4-2-1-学习训练过程"><a href="#4-2-1-学习训练过程" class="headerlink" title="4.2.1 学习训练过程"></a><strong>4.2.1 学习训练过程</strong></h2><p>一套MEMM由一套参数唯一确定，同样地，我需要通过训练数据学习这些参数。MEMM模型很自然需要学习里面的特征权重λ。</p>
<p>不过跟HMM不用的是，因为HMM是生成式模型，参数即为各种概率分布元参数，数据量足够可以用最大似然估计。而判别式模型是用函数直接判别，学习边界，MEMM即通过特征函数来界定。但同样，MEMM也有极大似然估计方法、梯度下降、牛顿迭代发、拟牛顿下降、BFGS、L-BFGS等等。各位应该对各种优化方法有所了解的。</p>
<p>嗯，具体详细求解过程貌似问题不大。</p>
<h2 id="4-2-2-序列标注过程"><a href="#4-2-2-序列标注过程" class="headerlink" title="4.2.2 序列标注过程"></a><strong>4.2.2 序列标注过程</strong></h2><p>还是跟HMM一样的，用学习好的MEMM模型，在新的sample（观测序列 <img src="https://www.zhihu.com/equation?tex=+o_%7B1%7D%2C+%5Ccdots%2C+o_%7Bi%7D" alt="[公式]"> ）上找出一条概率最大最可能的隐状态序列 <img src="https://www.zhihu.com/equation?tex=i_%7B1%7D%2C+%5Ccdots%2C+i_%7Bi%7D" alt="[公式]"> 。</p>
<p>只是现在的图中的每个隐状态节点的概率求法有一些差异而已,正确将每个节点的概率表示清楚，路径求解过程还是一样，采用viterbi算法。</p>
<h2 id="4-2-3-序列求概率过程"><a href="#4-2-3-序列求概率过程" class="headerlink" title="4.2.3 序列求概率过程"></a><strong>4.2.3 序列求概率过程</strong></h2><p>跟HMM举的例子一样的，也是分别去为每一批数据训练构建特定的MEMM，然后根据序列在每个MEMM模型的不同得分概率，选择最高分数的模型为wanted类别。</p>
<p>应该可以不用展开，吧……</p>
<h2 id="4-3-标注偏置？"><a href="#4-3-标注偏置？" class="headerlink" title="4.3 标注偏置？"></a><strong>4.3 标注偏置？</strong></h2><p>MEMM讨论的最多的是他的labeling bias 问题。</p>
<p><strong>1. 现象</strong></p>
<p>是从街货上烤过来的…… </p>
<p><img src="https://pic2.zhimg.com/v2-40f9945cdffb12cfec84bebc7b7e3be5_b.jpg" alt="img"></p>
<p>用Viterbi算法解码MEMM，状态1倾向于转换到状态2，同时状态2倾向于保留在状态2。 解码过程细节（需要会viterbi算法这个前提）：</p>
<blockquote>
<p>P(1-&gt; 1-&gt; 1-&gt; 1)= 0.4 x 0.45 x 0.5 = 0.09 ，<br>P(2-&gt;2-&gt;2-&gt;2)= 0.2 X 0.3 X 0.3 = 0.018，<br>P(1-&gt;2-&gt;1-&gt;2)= 0.6 X 0.2 X 0.5 = 0.06，<br>P(1-&gt;1-&gt;2-&gt;2)= 0.4 X 0.55 X 0.3 = 0.066 </p>
</blockquote>
<p>但是得到的最优的状态转换路径是1-&gt;1-&gt;1-&gt;1，为什么呢？因为状态2可以转换的状态比状态1要多，从而使转移概率降低,即MEMM倾向于选择拥有更少转移的状态。</p>
<p><strong>2. 解释原因</strong></p>
<p>直接看MEMM公式：</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28I%7CO%29+%3D+%5Cprod_%7Bt%3D1%7D%5E%7Bn%7D%5Cfrac%7B+exp%28%5Csum_%7Ba%7D%29%5Clambda_%7Ba%7Df_%7Ba%7D%28o%2Ci%29+%7D%7BZ%28o%2Ci_%7Bi-1%7D%29%7D+%2C+i+%3D+1%2C%5Ccdots%2Cn" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=%E2%88%91" alt="[公式]"> 求和的作用在概率中是归一化，但是这里归一化放在了指数内部，管这叫local归一化。 来了，viterbi求解过程，是用dp的状态转移公式（MEMM的没展开，请参考CRF下面的公式），因为是局部归一化，所以MEMM的viterbi的转移公式的第二部分出现了问题，导致dp无法正确的递归到全局的最优。</p>
<p><img src="https://www.zhihu.com/equation?tex=+%5Cdelta_%7Bi%2B1%7D+%3D+max_%7B1+%5Cle+j+%5Cle+m%7D%5Clbrace+%5Cdelta_%7Bi%7D%28I%29+%2B+%5Csum_%7Bi%7D%5E%7BT%7D%5Csum_%7Bk%7D%5E%7BM%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28O%2CI_%7Bi-1%7D%2CI_%7Bi%7D%2Ci%29+%5Crbrace+" alt="[公式]"></p>
<h2 id="五、CRF"><a href="#五、CRF" class="headerlink" title="五、CRF"></a><strong>五、CRF</strong></h2><p>我觉得一旦有了一个清晰的工作流程，那么按部就班地，没有什么很难理解的地方，因为整体框架已经胸有成竹了，剩下了也只有添砖加瓦小修小补了。有了上面的过程基础，CRF也是类似的，只是有方法论上的细微区别。</p>
<h2 id="5-1-理解CRF"><a href="#5-1-理解CRF" class="headerlink" title="5.1 理解CRF"></a><strong>5.1 理解CRF</strong></h2><p>请看第一张概率图模型构架图，CRF上面是马尔科夫随机场（马尔科夫网络），而条件随机场是在给定的随机变量 <img src="https://www.zhihu.com/equation?tex=X" alt="[公式]"> （具体，对应观测序列 <img src="https://www.zhihu.com/equation?tex=o_%7B1%7D%2C+%5Ccdots%2C+o_%7Bi%7D" alt="[公式]"> ）条件下，随机变量 <img src="https://www.zhihu.com/equation?tex=Y" alt="[公式]"> （具体，对应隐状态序列 <img src="https://www.zhihu.com/equation?tex=i_%7B1%7D%2C+%5Ccdots%2C+i_%7Bi%7D" alt="[公式]"> 的马尔科夫随机场。<br>广义的CRF的定义是： 满足 <img src="https://www.zhihu.com/equation?tex=P%28Y_%7Bv%7D%7CX%2CY_%7Bw%7D%2Cw+%5Cneq+v%29+%3D+P%28Y_%7Bv%7D%7CX%2CY_%7Bw%7D%2Cw+%5Csim+v%29+" alt="[公式]"> 的马尔科夫随机场叫做条件随机场（CRF）。</p>
<p>不过一般说CRF为序列建模，就专指CRF线性链（linear chain CRF）：</p>
<p><img src="https://pic3.zhimg.com/v2-c5e2e782e35f6412ed65e58cdda0964e_b.jpg" alt="img"></p>
<p>在2.1.2中有提到过，概率无向图的联合概率分布可以在因子分解下表示为：</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28Y+%7C+X%29%3D%5Cfrac%7B1%7D%7BZ%28x%29%7D+%5Cprod_%7Bc%7D%5Cpsi_%7Bc%7D%28Y_%7Bc%7D%7CX+%29+%3D+%5Cfrac%7B1%7D%7BZ%28x%29%7D+%5Cprod_%7Bc%7D+e%5E%7B%5Csum_%7Bk%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28c%2Cy%7Cc%2Cx%29%7D+%3D+%5Cfrac%7B1%7D%7BZ%28x%29%7D+e%5E%7B%5Csum_%7Bc%7D%5Csum_%7Bk%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28y_%7Bi%7D%2Cy_%7Bi-1%7D%2Cx%2Ci%29%7D" alt="[公式]"></p>
<p>而在线性链CRF示意图中，每一个（ <img src="https://www.zhihu.com/equation?tex=I_%7Bi%7D+%5Csim+O_%7Bi%7D" alt="[公式]"> ）对为一个最大团,即在上式中 <img src="https://www.zhihu.com/equation?tex=c+%3D+i" alt="[公式]"> 。并且线性链CRF满足 <img src="https://www.zhihu.com/equation?tex=P%28I_%7Bi%7D%7CO%2CI_%7B1%7D%2C%5Ccdots%2C+I_%7Bn%7D%29+%3D+P%28I_%7Bi%7D%7CO%2CI_%7Bi-1%7D%2CI_%7Bi%2B1%7D%29+" alt="[公式]"> 。</p>
<p><strong>所以CRF的建模公式如下：</strong></p>
<p><img src="https://www.zhihu.com/equation?tex=P%28I+%7C+O%29%3D%5Cfrac%7B1%7D%7BZ%28O%29%7D+%5Cprod_%7Bi%7D%5Cpsi_%7Bi%7D%28I_%7Bi%7D%7CO+%29+%3D+%5Cfrac%7B1%7D%7BZ%28O%29%7D+%5Cprod_%7Bi%7D+e%5E%7B%5Csum_%7Bk%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28O%2CI_%7Bi-1%7D%2CI_%7Bi%7D%2Ci%29%7D+%3D+%5Cfrac%7B1%7D%7BZ%28O%29%7D+e%5E%7B%5Csum_%7Bi%7D%5Csum_%7Bk%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28O%2CI_%7Bi-1%7D%2CI_%7Bi%7D%2Ci%29%7D" alt="[公式]"></p>
<p>我要敲黑板了，这个公式是非常非常关键的，注意递推过程啊，我是怎么从 <img src="https://www.zhihu.com/equation?tex=%E2%88%8F" alt="[公式]"> 跳到 <img src="https://www.zhihu.com/equation?tex=e%5E%7B%5Csum%7D" alt="[公式]"> 的。</p>
<p>不过还是要多啰嗦一句，想要理解CRF，必须判别式模型的概念要深入你心。正因为是判别模型，所以不废话，我上来就直接为了确定边界而去建模，因为我创造出来就是为了这个分边界的目的的。比如说序列求概率（分类）问题，我直接考虑找出函数分类边界。所以才为什么会有这个公式。所以再看到这个公式也别懵逼了，he was born for discriminating the given data from different classes. 就这样。不过待会还会具体介绍特征函数部分的东西。</p>
<p>除了建模总公式，关键的CRF重点概念在MEMM中已强调过：<strong>判别式模型</strong>、<strong>特征函数</strong>。</p>
<p><strong>1. 特征函数</strong></p>
<p>上面给出了CRF的建模公式：</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28I+%7C+O%29%3D%5Cfrac%7B1%7D%7BZ%28O%29%7D+e%5E%7B%5Csum_%7Bi%7D%5E%7BT%7D%5Csum_%7Bk%7D%5E%7BM%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28O%2CI_%7Bi-1%7D%2CI_%7Bi%7D%2Ci%29%7D" alt="[公式]"></p>
<ul>
<li>下标<em>i</em>表示我当前所在的节点（token）位置。</li>
<li>下标<em>k</em>表示我这是第几个特征函数，并且每个特征函数都附属一个权重 <img src="https://www.zhihu.com/equation?tex=%5Clambda_%7Bk%7D" alt="[公式]"> ，也就是这么回事，每个团里面，我将为 <img src="https://www.zhihu.com/equation?tex=token_%7Bi%7D" alt="[公式]"> 构造M个特征，每个特征执行一定的限定作用，然后建模时我再为每个特征函数加权求和。</li>
<li><img src="https://www.zhihu.com/equation?tex=Z%28O%29" alt="[公式]"> 是用来归一化的，为什么？想想LR以及softmax为何有归一化呢，一样的嘛，形成概率值。</li>
<li>再来个重要的理解。 <img src="https://www.zhihu.com/equation?tex=P%28I%7CO%29" alt="[公式]"> 这个表示什么？具体地，表示了在给定的一条观测序列 <img src="https://www.zhihu.com/equation?tex=O%3D%28o_%7B1%7D%2C%5Ccdots%2C+o_%7Bi%7D%29" alt="[公式]"> 条件下，我用CRF所求出来的隐状态序列 <img src="https://www.zhihu.com/equation?tex=I%3D%28i_%7B1%7D%2C%5Ccdots%2C+i_%7Bi%7D%29" alt="[公式]"> 的概率，注意，这里的<em>I</em>是一条序列，有多个元素（一组随机变量），而至于观测序列 <img src="https://www.zhihu.com/equation?tex=O%3D%28o_%7B1%7D%2C%5Ccdots%2C+o_%7Bi%7D%29" alt="[公式]"> ，它可以是一整个训练语料的所有的观测序列；也可以是在inference阶段的一句sample，比如说对于序列标注问题，我对一条sample进行预测，可能能得到 <img src="https://www.zhihu.com/equation?tex=P_%7Bj%7D%28I+%7C+O%29%EF%BC%88j%3D1%2C%E2%80%A6%2CJ%EF%BC%89" alt="[公式]"><em>J</em>条隐状态<em>I</em>，但我肯定最终选的是最优概率的那条（by viterbi）。这一点希望你能理解。</li>
</ul>
<p>对于CRF，可以为他定义两款特征函数：转移特征&amp;状态特征。 我们将建模总公式展开：</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28I+%7C+O%29%3D%5Cfrac%7B1%7D%7BZ%28O%29%7D+e%5E%7B%5Csum_%7Bi%7D%5E%7BT%7D%5Csum_%7Bk%7D%5E%7BM%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28O%2CI_%7Bi-1%7D%2CI_%7Bi%7D%2Ci%29%7D%3D%5Cfrac%7B1%7D%7BZ%28O%29%7D+e%5E%7B+%5B+%5Csum_%7Bi%7D%5E%7BT%7D%5Csum_%7Bj%7D%5E%7BJ%7D%5Clambda_%7Bj%7Dt_%7Bj%7D%28O%2CI_%7Bi-1%7D%2CI_%7Bi%7D%2Ci%29+%2B+%5Csum_%7Bi%7D%5E%7BT%7D%5Csum_%7Bl%7D%5E%7BL%7D%5Cmu_%7Bl%7Ds_%7Bl%7D%28O%2CI_%7Bi%7D%2Ci%29+%5D+%7D" alt="[公式]"></p>
<p>其中：</p>
<ul>
<li><img src="https://www.zhihu.com/equation?tex=t_%7Bj%7D" alt="[公式]"> 为i处的转移特征，对应权重 <img src="https://www.zhihu.com/equation?tex=%5Clambda_%7Bj%7D" alt="[公式]"> ,每个 <img src="https://www.zhihu.com/equation?tex=token_%7Bi%7D" alt="[公式]"> 都有J个特征,转移特征针对的是前后token之间的限定。</li>
<li><ul>
<li>举个例子：</li>
</ul>
</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+t_%7Bk%3D1%7D%28o%2Ci%29+%3D+%5Cbegin%7Bcases%7D+1%26+%5Ctext%7B%E6%BB%A1%E8%B6%B3%E7%89%B9%E5%AE%9A%E8%BD%AC%E7%A7%BB%E6%9D%A1%E4%BB%B6%EF%BC%8C%E6%AF%94%E5%A6%82%E5%89%8D%E4%B8%80%E4%B8%AAtoken%E6%98%AF%E2%80%98I%E2%80%99%7D%EF%BC%8C%5C%5C+0%26+%5Ctext%7Bother%7D+%5Cend%7Bcases%7D+%5Cend%7Bequation%7D" alt="[公式]"></p>
<ul>
<li>sl为i处的状态特征，对应权重μl,每个tokeni都有L个特征</li>
<li><ul>
<li>举个例子：</li>
</ul>
</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+s_%7Bl%3D1%7D%28o%2Ci%29+%3D+%5Cbegin%7Bcases%7D+1%26+%5Ctext%7B%E6%BB%A1%E8%B6%B3%E7%89%B9%E5%AE%9A%E7%8A%B6%E6%80%81%E6%9D%A1%E4%BB%B6%EF%BC%8C%E6%AF%94%E5%A6%82%E5%BD%93%E5%89%8Dtoken%E7%9A%84POS%E6%98%AF%E2%80%98V%E2%80%99%7D%EF%BC%8C%5C%5C+0%26+%5Ctext%7Bother%7D+%5Cend%7Bcases%7D+%5Cend%7Bequation%7D" alt="[公式]"></p>
<p>不过一般情况下，我们不把两种特征区别的那么开，合在一起：</p>
<p><img src="https://www.zhihu.com/equation?tex=P%28I+%7C+O%29%3D%5Cfrac%7B1%7D%7BZ%28O%29%7D+e%5E%7B%5Csum_%7Bi%7D%5E%7BT%7D%5Csum_%7Bk%7D%5E%7BM%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28O%2CI_%7Bi-1%7D%2CI_%7Bi%7D%2Ci%29%7D" alt="[公式]"></p>
<p>满足特征条件就取值为1，否则没贡献，甚至你还可以让他打负分，充分惩罚。</p>
<p>再进一步理解的话，我们需要把特征函数部分抠出来：</p>
<p><img src="https://www.zhihu.com/equation?tex=Score+%3D+%5Csum_%7Bi%7D%5E%7BT%7D%5Csum_%7Bk%7D%5E%7BM%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28O%2CI_%7Bi-1%7D%2CI_%7Bi%7D%2Ci%29+" alt="[公式]"></p>
<p>是的，我们为 <img src="https://www.zhihu.com/equation?tex=token_%7Bi%7D" alt="[公式]"> 打分，满足条件的就有所贡献。最后将所得的分数进行log线性表示，求和后归一化，即可得到概率值……完了又扯到了log线性模型。现在稍作解释：</p>
<blockquote>
<p>log-linear models take the following form:<br><img src="https://www.zhihu.com/equation?tex=P%28y%7Cx%3B%5Comega%29+%3D+%5Cfrac%7B+exp%28%5Comega%C2%B7%5Cphi%28x%2Cy%29%29+%7D%7B+%5Csum_%7By%5E%7B%27%7D%5Cin+Y+%7Dexp%28%5Comega%C2%B7%5Cphi%28x%2Cy%5E%7B%E2%80%98%7D%29%29+%7D" alt="[公式]"></p>
</blockquote>
<p>我觉得对LR或者sotfmax熟悉的对这个应该秒懂。然后CRF完美地满足这个形式，所以又可以归入到了log-linear models之中。</p>
<h2 id="5-2-模型运行过程"><a href="#5-2-模型运行过程" class="headerlink" title="5.2 模型运行过程"></a><strong>5.2 模型运行过程</strong></h2><p>模型的工作流程，跟MEMM是一样的：</p>
<ul>
<li>step1. 先预定义特征函数 <img src="https://www.zhihu.com/equation?tex=+f_%7Ba%7D%28o%2Ci%29" alt="[公式]"> ，</li>
<li>step2. 在给定的数据上，训练模型，确定参数 <img src="https://www.zhihu.com/equation?tex=%5Clambda_%7Bk%7D" alt="[公式]"></li>
<li>step3. 用确定的模型做<code>序列标注问题</code>或者<code>序列求概率问题</code>。</li>
</ul>
<p>可能还是没做到100%懂，结合例子说明：</p>
<blockquote>
<p>……</p>
</blockquote>
<h2 id="5-2-1-学习训练过程"><a href="#5-2-1-学习训练过程" class="headerlink" title="5.2.1 学习训练过程"></a><strong>5.2.1 学习训练过程</strong></h2><p>一套CRF由一套参数λ唯一确定（先定义好各种特征函数）。</p>
<p>同样，CRF用极大似然估计方法、梯度下降、牛顿迭代、拟牛顿下降、IIS、BFGS、L-BFGS等等。各位应该对各种优化方法有所了解的。其实能用在log-linear models上的求参方法都可以用过来。</p>
<p>嗯，具体详细求解过程貌似问题不大。</p>
<h2 id="5-2-2-序列标注过程"><a href="#5-2-2-序列标注过程" class="headerlink" title="5.2.2 序列标注过程"></a><strong>5.2.2 序列标注过程</strong></h2><p>还是跟HMM一样的，用学习好的CRF模型，在新的sample（观测序列 <img src="https://www.zhihu.com/equation?tex=o_%7B1%7D%2C+%5Ccdots%2C+o_%7Bi%7D" alt="[公式]"> ）上找出一条概率最大最可能的隐状态序列 <img src="https://www.zhihu.com/equation?tex=i_%7B1%7D%2C+%5Ccdots%2C+i_%7Bi%7D" alt="[公式]"> 。</p>
<p>只是现在的图中的每个隐状态节点的概率求法有一些差异而已,正确将每个节点的概率表示清楚，路径求解过程还是一样，采用viterbi算法。</p>
<p>啰嗦一下，我们就定义i处的局部状态为 <img src="https://www.zhihu.com/equation?tex=%5Cdelta_%7Bi%7D%28I%29" alt="[公式]"> ,表示在位置i处的隐状态的各种取值可能为<em>I</em>，然后递推位置i+1处的隐状态，写出来的DP转移公式为：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cdelta_%7Bi%2B1%7D+%3D+max_%7B1+%5Cle+j+%5Cle+m%7D%5Clbrace+%5Cdelta_%7Bi%7D%28I%29+%2B+%5Csum_%7Bi%7D%5E%7BT%7D%5Csum_%7Bk%7D%5E%7BM%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28O%2CI_%7Bi-1%7D%2CI_%7Bi%7D%2Ci%29+%5Crbrace" alt="[公式]"></p>
<p>这里没写规范因子 <img src="https://www.zhihu.com/equation?tex=Z%28O%29" alt="[公式]"> 是因为不规范化不会影响取最大值后的比较。</p>
<p>具体还是不展开为好。</p>
<h2 id="5-2-3-序列求概率过程"><a href="#5-2-3-序列求概率过程" class="headerlink" title="5.2.3 序列求概率过程"></a><strong>5.2.3 序列求概率过程</strong></h2><p>跟HMM举的例子一样的，也是分别去为每一批数据训练构建特定的CRF，然后根据序列在每个MEMM模型的不同得分概率，选择最高分数的模型为wanted类别。只是貌似很少看到拿CRF或者MEMM来做分类的，直接用网络模型不就完了不……</p>
<p>应该可以不用展开，吧……</p>
<h2 id="5-3-CRF-分析"><a href="#5-3-CRF-分析" class="headerlink" title="5.3 CRF++分析"></a><strong>5.3 CRF++分析</strong></h2><p>本来做task用CRF++跑过baseline,后来在对CRF做调研时，非常想透析CRF++的工作原理，以identify以及verify做的各种假设猜想。当然，也看过其他的CRF实现源码。</p>
<p>所以干脆写到这里来，结合CRF++实例讲解过程。</p>
<p>有一批语料数据，并且已经tokenized好了：</p>
<blockquote>
<p>Nuclear<br>theory<br>devoted<br>major<br>efforts<br>…… </p>
</blockquote>
<p>并且我先确定了13个标注元素：</p>
<blockquote>
<p>B_MAT<br>B_PRO<br>B_TAS<br>E_MAT<br>E_PRO<br>E_TAS<br>I_MAT<br>I_PRO<br>I_TAS<br>O<br>S_MAT<br>S_PRO<br>S_TAS </p>
</blockquote>
<p><strong>1. 定义模板</strong></p>
<p>按道理应该是定义特征函数才对吧？好的，在CRF++下，应该是先定义特征模板，然后用模板自动批量产生大量的特征函数。我之前也蛮confused的，用完CRF++还以为模板就是特征，后面就搞清楚了：每一条模板将在每一个token处生产若干个特征函数。</p>
<p>CRF++的模板（template）有U系列（unigram）、B系列(bigram)，不过我至今搞不清楚B系列的作用，因为U模板都可以完成2-gram的作用。</p>
<blockquote>
<p>U00:%x[-2,0]<br>U01:%x[-1,0]<br>U02:%x[0,0]<br>U03:%x[1,0]<br>U04:%x[2,0] </p>
<p>U05:%x[-2,0]/%x[-1,0]/%x[0,0]<br>U06:%x[-1,0]/%x[0,0]/%x[1,0]<br>U07:%x[0,0]/%x[1,0]/%x[2,0]<br>U08:%x[-1,0]/%x[0,0]<br>U09:%x[0,0]/%x[1,0] </p>
<p>B </p>
</blockquote>
<p>所以，U00 - U09 我定义了10个模板。</p>
<p><strong>2. 产生特征函数</strong></p>
<p>是的，会产生大量的特征。 U00 - U04的模板产生的是状态特征函数；U05 - U09的模板产生的是转移特征函数。</p>
<p>在CRF++中，每个特征都会try每个标注label（这里有13个），总共将生成 <img src="https://www.zhihu.com/equation?tex=N+%2A+L+%3D+i+%2A+k%5E%7B%27%7D+%2A+L" alt="[公式]"> 个特征函数以及对应的权重出来。N表示每一套特征函数 <img src="https://www.zhihu.com/equation?tex=N%3D+i+%2A+k%5E%7B%27%7D" alt="[公式]"> ，L表示标注集元素个数。</p>
<p>比如训练好的CRF模型的部分特征函数是这样存储的：</p>
<blockquote>
<p>22607 B<br>790309 U00:%<br>3453892 U00:%)<br>2717325 U00:&amp;<br>2128269 U00:’t<br>2826239 U00:(0.3534<br>2525055 U00:(0.593–1.118<br>197093 U00:(1)<br>2079519 U00:(1)L=14w2−12w−FμνaFaμν<br>2458547 U00:(1)δn=∫−∞En+1ρ˜(E)dE−n<br>1766024 U00:(1.0g<br>2679261 U00:(1.1wt%)<br>1622517 U00:(100)<br>727701 U00:(1000–5000A)<br>2626520 U00:(10a)<br>2626689 U00:(10b)<br>……<br>2842814 U07:layer/thicknesses/Using<br>2847533 U07:layer/thicknesses/are<br>2848651 U07:layer/thicknesses/in<br>331539 U07:layer/to/the<br>1885871 U07:layer/was/deposited<br>……（数量非常庞大） </p>
</blockquote>
<p>其实也就是对应了这样些个特征函数：</p>
<blockquote>
<p>func1 = if (output = B and feature=”U02:一”) return 1 else return 0<br>func2 = if (output = M and feature=”U02:一”) return 1 else return 0<br>func3 = if (output = E and feature=”U02:一”) return 1 else return 0<br>func4 = if (output = S and feature=”U02:一”) return 1 else return 0 </p>
</blockquote>
<p>比如模板U06会从语料中one by one逐句抽出这些各个特征：</p>
<blockquote>
<p>一/个/人/……<br>个/人/走/……</p>
</blockquote>
<p><strong>3. 求参</strong></p>
<p>对上述的各个特征以及初始权重进行迭代参数学习。</p>
<p>在CRF++ 训练好的模型里，权重是这样的：</p>
<blockquote>
<p>0.3972716048310705<br>0.5078838237171732<br>0.6715316559507898<br>-0.4198827647512405<br>-0.4233310655891150<br>-0.4176580083832543<br>-0.4860489836004728<br>-0.6156475863742051<br>-0.6997919485753300<br>0.8309956709647820<br>0.3749695682658566<br>0.2627347894057647<br>0.0169732441379157<br>0.3972716048310705<br>0.5078838237171732<br>0.6715316559507898<br>……（数量非常庞大，与每个label的特征函数对应，我这有300W个）</p>
</blockquote>
<p><strong>4. 预测解码</strong></p>
<p>结果是这样的：</p>
<blockquote>
<p>Nuclear B<em>TAStheory E</em>TAS<br>devoted O<br>major O<br>efforts O<br>…… </p>
</blockquote>
<h2 id="六、总结"><a href="#六、总结" class="headerlink" title="六、总结"></a><strong>六、总结</strong></h2><p><strong>1. 总体对比</strong></p>
<p>应该看到了熟悉的图了，现在看这个图的话，应该可以很清楚地get到他所表达的含义了。这张图的内容正是按照生成式&amp;判别式来区分的，NB在sequence建模下拓展到了HMM；LR在sequence建模下拓展到了CRF。</p>
<p><img src="https://pic1.zhimg.com/v2-376fd85a490e161978130ddd759244d4_b.jpg" alt="img"></p>
<p><strong>2. HMM vs. MEMM vs. CRF</strong></p>
<p>将三者放在一块做一个总结：</p>
<ol>
<li>HMM -&gt; MEMM： HMM模型中存在两个假设：一是输出观察值之间严格独立，二是状态的转移过程中当前状态只与前一状态有关。但实际上序列标注问题不仅和单个词相关，而且和观察序列的长度，单词的上下文，等等相关。MEMM解决了HMM输出独立性假设的问题。因为HMM只限定在了观测与状态之间的依赖，而MEMM引入自定义特征函数，不仅可以表达观测之间的依赖，还可表示当前观测与前后多个状态之间的复杂依赖。</li>
<li>MEMM -&gt; CRF:</li>
</ol>
<ul>
<li>CRF不仅解决了HMM输出独立性假设的问题，还解决了MEMM的标注偏置问题，MEMM容易陷入局部最优是因为只在局部做归一化，而CRF统计了全局概率，在做归一化时考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。使得序列标注的解码变得最优解。</li>
<li>HMM、MEMM属于有向图，所以考虑了x与y的影响，但没讲x当做整体考虑进去（这点问题应该只有HMM）。CRF属于无向图，没有这种依赖性，克服此问题。</li>
</ul>
<p><strong>3. Machine Learning models vs. Sequential models</strong></p>
<p>为了一次将概率图模型理解的深刻到位，我们需要再串一串，更深度与原有的知识体系融合起来。</p>
<p>机器学习模型，按照学习的范式或方法，以及加上自己的理解，给常见的部分的他们整理分了分类（主流上，都喜欢从训练样本的歧义型分，当然也可以从其他角度来）：</p>
<pre><code class="text">一、监督：{

1.1 分类算法(线性和非线性)：{

    感知机

    KNN

    概率{
        朴素贝叶斯（NB）
        Logistic Regression（LR）
        最大熵MEM（与LR同属于对数线性分类模型）
    }

    支持向量机(SVM)

    决策树(ID3、CART、C4.5)

    assembly learning{
        Boosting{
            Gradient Boosting{
                GBDT
                xgboost（传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）；xgboost是Gradient Boosting的一种高效系统实现，并不是一种单一算法。）
            }
            AdaBoost
        }   
        Bagging{
            随机森林
        }
        Stacking
    }

    ……
}

1.2 概率图模型：{
    HMM
    MEMM（最大熵马尔科夫）
    CRF
    ……
}


1.3 回归预测：{
    线性回归
    树回归
    Ridge岭回归
    Lasso回归
    ……
}

……  
}


二、非监督：{
2.1 聚类：{
    1. 基础聚类
        K—mean
        二分k-mean
        K中值聚类
        GMM聚类
    2. 层次聚类
    3. 密度聚类
    4. 谱聚类()
}

2.2 主题模型:{
    pLSA
    LDA隐含狄利克雷分析
}

2.3 关联分析：{
    Apriori算法
    FP-growth算法
}

2.4 降维：{
    PCA算法
    SVD算法
    LDA线性判别分析
    LLE局部线性嵌入
}

2.5 异常检测：
……
}

三、半监督学习

四、迁移学习</code></pre>
<p>（注意到，没有把神经网络体系加进来。因为NNs的范式很灵活，不太适用这套分法，largely, off this framework）</p>
<p>Generally speaking，机器学习模型，尤其是有监督学习，一般是为一条sample预测出一个label，作为预测结果。 但与典型常见的机器学习模型不太一样，序列模型（概率图模型）是试图为一条sample里面的每个基本元数据分别预测出一个label。这一点，往往是beginner伊始难以理解的。</p>
<p>具体的实现手段差异，就是：ML models通过直接预测得出label；Sequential models是给每个token预测得出label还没完，还得将他们每个token对应的labels进行组合，具体的话，用viterbi来挑选最好的那个组合。</p>
<h2 id="over"><a href="#over" class="headerlink" title="over"></a><strong>over</strong></h2>
            <hr>
          </div>
          <br>
          <div>
            <p>
            
              <span>
                <i class="iconfont icon-inbox"></i>
                
                  <a class="hover-with-bg" href="/categories/%E6%A6%82%E5%BF%B5">概念</a>
                  &nbsp;
                
              </span>&nbsp;&nbsp;
            
            
              <span>
                <i class="iconfont icon-tag"></i>
                
                  <a class="hover-with-bg" href="/tags/HMM">HMM</a>
                
              </span>
            
            </p>
            
              <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" target="_blank" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
            
          </div>
        </div>
      </div>
    </div>
    <div class="d-none d-lg-block col-lg-2 toc-container">
      
  <div id="toc">
    <p class="h4"><i class="far fa-list-alt"></i>&nbsp;目录</p>
    <div id="tocbot"></div>
  </div>

    </div>
  </div>
</div>

<!-- custom -->


<!-- Comments -->
<div class="col-lg-7 mx-auto nopadding-md">
  <div class="container comments mx-auto" id="comments">
    
  </div>
</div>

    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>

    
  
    <!-- 不蒜子统计PV -->
    
    &nbsp;<span id="busuanzi_container_site_pv">总访问量 
          <span id="busuanzi_value_site_pv"></span> 次</span>&nbsp;
  
  
    <!-- 不蒜子统计UV -->
    
    &nbsp;<span id="busuanzi_container_site_uv">总访客数 
            <span id="busuanzi_value_site_uv"></span> 人</span>&nbsp;
  
  <br>



    

  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js" ></script>
<script src="/lib/popper/popper.min.js" ></script>
<script src="/lib/bootstrap/js/bootstrap.min.js" ></script>
<script src="/lib/mdbootstrap/js/mdb.min.js" ></script>
<script src="/js/main.js" ></script>




  
    <script src="/lib/tocbot/tocbot.min.js" ></script>
  
  <script src="/js/post.js" ></script>



  <script src="/lib/smooth-scroll/smooth-scroll.min.js" ></script>



  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>


<!-- Plugins -->


  

  

  

  

  




  <script src="/lib/prettify/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "0.概念/NLP_SLP_6_HMM&HEMM&CRF的比较pt2&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 120,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="/lib/anchor/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "false",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>







</body>
</html>

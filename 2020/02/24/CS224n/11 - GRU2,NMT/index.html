<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <link rel="icon" type="image/png" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="description" content="desc">
  <meta name="author" content="LiuBingqian">
  <meta name="keywords" content="">
  <title>CS224n/11 - GRU2,NMT ~ 刘秉乾的技术博客 :)</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"  >
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css"  >
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css"  >
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css"  >

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css"  >

<link rel="stylesheet" href="/css/main.css"  >


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css"  >


<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header style="height: 100vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>LiuBingqian`s Blog</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">主页</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">分类</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">所有文章</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">标签</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">关于</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>


</nav>

    <div class="view intro-2" id="background"
         style="background: url('http://bqlab-pic.test.upcdn.net/archive.jpg')no-repeat center center;
           background-size: cover;
           background-attachment: fixed;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              <br>
              
                <p class="mt-3">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>&nbsp;
                  星期一, 二月 24日 2020, 12:04 中午
                </p>
              

              <p>
                
                  
                  &nbsp;<i class="far fa-chart-bar"></i>
                  <span class="post-count">
                    4.3k 字
                  </span>&nbsp;
                

                
                  
                  &nbsp;<i class="far fa-clock"></i>
                  <span class="post-count">
                      14 分钟
                  </span>&nbsp;
                

                
                  <!-- 不蒜子统计文章PV -->
                  
                  &nbsp;<i class="far fa-eye" aria-hidden="true"></i>&nbsp;
                  <span id="busuanzi_container_page_pv">
                    <span id="busuanzi_value_page_pv"></span> 次
                  </span>&nbsp;
                
              </p>
            
          </div>

          
            <div class="scroll-down-bar">
              <i class="fas fa-angle-down scroll-down-arrow"></i>
            </div>
          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="py-5 z-depth-3" id="board">
        <div class="post-content mx-auto" id="post">
          <div class="markdown-body">
            <h1 id="Lecture11-GRU-NMT"><a href="#Lecture11-GRU-NMT" class="headerlink" title="Lecture11 - GRU,NMT"></a>Lecture11 - GRU,NMT</h1><p>这部分集中讲如何使用循环模型和和seq2seq架构。</p>
<h3 id="1-对RNN的补充阐述"><a href="#1-对RNN的补充阐述" class="headerlink" title="1.对RNN的补充阐述"></a>1.对RNN的补充阐述</h3><p><strong>RNN到底在做什么？</strong></p>
<p>在时间t时有一些随着时间变化的东西（图的左边开始端），然后到了时间t+n的时候它发生了变化（图的右边结束端）。我们要研究的就是在时间t时看到的东西是如何影响到t+n时刻的。我们要测量比如现在看到的单词是如何影响到接下里的6个或者8个单词的。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1415_39_199.png" alt=""></p>
<p>我们要如何实现RNN？我们要做的是，在基本RNN的每个时间点，我们已经获得了一些隐藏的状态，我们用矩阵去乘他们，然后我们增加一些东西去处理输入，然后我们进入下一个时间，我们将隐藏状态再次乘以相同的矩阵，然后多次重复这个过程就会遇到梯度消失的问题。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1419_07_794.png" alt=""></p>
<p>这个问题其实是由如下的朴素的transition函数导致的。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1421_20_904.png" alt=""></p>
<p><strong>门控制单元GRU的阐述</strong></p>
<p>在朴素的翻译函数中，我们在序列中执行矩阵乘法，我们每一步乘以一个矩阵U，这样代表着我们正在学习这其中对我们的决策会起到多大的影响。我们通过在整个序列的中间节点做反向传播，这也就是说，整个GRU单元的思路中，我们希望能够获得更多直接的证据证明前期步骤对后续步骤有影响，但是不用做长序列的矩阵乘法。我们同时希望能够在短距离链接中如下图下边的示意，能够有反向传播的跨度为2的反馈，这样我们就能学到这些长期的依赖关系。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1433_29_452.png" alt=""></p>
<p><strong>把门控制单元部署到LSTM上的解释</strong></p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1533_28_787.png" alt=""></p>
<p>解析上图第二行，标绿的<strong>神经网络的循环单元</strong>：<br>h(t)，循环单元，等于当前的向量输入与W矩阵相乘，加上之前的状态乘以U矩阵，然后再加上一个偏差b，然后将他们放进tan函数中去，这是标准的RNN迭代。我们像常规的RNN一样去更新迭代。</p>
<p>解析上图第一行<strong>核心公式</strong>;<br>对于准确确定我们计算的函数，我们需要自适应学习需要用多少和哪些维度去更新candidate，以及我们需要截取并使用百分之多少我们之前时间段获得的内容也就是h(t-1)。我们允许使用距离远的过去时间段的内容，并且这个远距离内容并不是使用乘法进入当前时间点，而是相加进入，而核心公式的相加第二项也就是我们刚才说的在下一个时间点之前移动这个内容，我们直接从过去获取这个内容，影响现在并进一步影响决定。这是一种递增法，你可以一直这么处理，你正在决定哪些内容应该被关注，但是如果你一旦关注了他，效果会一直伴随，因为你正在添加更多的东西上去。</p>
<p><strong>遗忘门</strong></p>
<p>最后我们可能想要修剪一下过去自适应的东西，使他不要一直存在。这个需求引入了第二个门控，遗忘门控。遗忘门控给了你一个0到1之间的向量，他像一个标准的循环单元一样计算，他可以删掉在t-1时间点的不再相关的东西，因此如下图<strong>标绿</strong>所示，遗忘门控与之前的隐藏单元之间的元素做点积。这样我们就能忘掉过去的隐藏层。而我们忘掉的部分已经嵌入进更新的候选内容中了。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1046_59_208.png" alt=""></p>
<p><strong>tanh-RNN</strong></p>
<p>如下图，没有GRU的情况下，在隐藏层里，先读取寄存器h，然后执行RNN更新，然后返回。这样做要不停地更新，非常不灵活。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1249_53_627.png" alt=""></p>
<p>如下图，有了GRU，便允许灵活的自适应学习。<br>首先，重置门控，然后学习一个可读的隐藏状态层的子集r，其他部分将会丢弃，这样就有了忘记的能力。<br>然后将读取的r和h点乘，继续进行标准的RNN计算，更新，重写。<br>然后选择一个可写入的子集u，将子集按照下图第四行公式更新写入。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1258_39_454.png" alt=""></p>
<p><strong>tricky概念阐述</strong></p>
<p><strong>更新门</strong>是选择读取什么，然后更新候选值；<strong>重置门</strong>负责选择哪些部分的隐藏状态需要被覆盖。</p>
<p>问题：如何知道哪个子集依赖特定的重置门？</p>
<p>重置门基于当前的输入来计算需要读取/更新隐藏状态的哪个部分以及需要读取/更新哪个以前的状态，如下图，根据输入xt确定需要读取Wr的哪个部分。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1308_26_328.png" alt=""></p>
<p>重置门rt还在更新候选值中发挥作用，与之前的h(t-1)隐藏层点乘，决定了要在多大程度上采纳之前的隐藏状态。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1315_55_469.png" alt="">    </p>
<p><strong>根据提问对整体概念再次阐述</strong></p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1319_51_030.png" alt=""></p>
<p>我们从左往右一直携带一个隐藏状态层，并在每一个时间序列点上，我们想尝试计算一个新的隐藏状态层through乘以矩阵W。当然有的时候不需要计算整个隐藏状态层，仅仅传过来的之前的信息也是有用的。</p>
<p>如果我们仅仅是在每一步做乘法，沿着一个朴素的RNN,我们就失去了长期记忆的概念。而且实质上，我们不能记住超过7步的内容。（我的理解是每一个新的乘法都会冲淡之前的乘法对整体数值概念的程度，比如1连乘七次0.1，就会基本消失掉）</p>
<p>如下，我们想要做的是计算一个候选值更新同时保留我们已经完成的工作。（加法具有累计性，每一个新的加法都不会太大幅度地改变总体值）</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1331_23_028.png" alt=""></p>
<p>具体实现：</p>
<p>更新门ut : ut这个向量的输出值在0到1之间。如上左右切割缩放公式，若它接近1，表示用我们这个时间计算的内容覆盖当前隐藏状态层，接近0表示使用过去的那个状态保持住这个元素向量。</p>
<p>计算更新门ut的方法是使用如下常规的循环单元，他括号内第一项关注当前的输入xt，括号内第二项关心近期的历史h(t-1)。最后加上第三项偏置项。最后投入西格玛函数里，输出一个0到1的结果，然后作用于上文提到的核心公式里。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1337_02_492.png" alt=""></p>
<p>假设一个单元对输入的词敏感，我们想要这些向量的维度记录见过的词，直到一个新词的出现，在新词出现的瞬间，我们说，好了，是时候更新了，忘记过去在这五个维度中存储的内容，现在需要存储一个新词，这就是更新门所做的内容。更新门的括号内第一项能够记录输入词，比如向量的47到52维应该给一个1的值。那意味着，它们将被存储和从计算更新，并且忽略他们曾今在过去存储中的内容。但是比如更新门发现它们正存储着一个介词，他将会将更新值设置为接近0。那意味着47到52维将继续存储上次见过的词，即使它已经是前十个词了。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1524_49_078.png" alt=""></p>
<p>不仅门控会更新，候选值也会同时更新。当然候选值是使用tan函数来输出的，最后的值在-1到1之间。<br>sum up idea就是如果你正在做一个候选值的更新，你总是使用之前的隐藏状态层，和新的输入词在更新门中使用相同的方式。如果你已经在输入中检测到一个新的词汇，你应该存储进47到52维，然后你就应该忽略你过去在那里有的内容，还要一些情况你需要丢掉当前的隐藏层的就是用新的隐藏层取代它。这种取代就是重置门所要做的，重置门能观察之前的隐藏状态层在当前的重要性，并且让他选择一个0-1之间的值，如果重置门的选择的值接近0，必须扔掉之前的隐藏状态层，并且根据新的输入计算。</p>
<p>这里有一个具体语言的类比。比如你正在记录东西，上次见过的词在47到52维之间，当你见到一个新的词，要做的正确的事情是丢掉之前47到52维的历史记录，并且根据新的计算输入更新值。但是上述操作并不是总适用。比如英语中有大量的助词词组，例如make up,make out之类的。比如你想计算出make out这个词的意思，当你先看见了make，并且把它放进了47到52维之间，如果47到52维之间真的存储了主要谓语make的意思，但是后边的out又出来了，你肯定不想丢掉make，因为这是一个词组。在这种情况下，你希望重置门能有一个接近1的值，希望即保留前边的词又import进新的词。</p>
<p><strong>GRU不会遭受梯度消失的阐述</strong></p>
<p>不会梯度消失的秘密在下图这个加号这里。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1525_59_925.png" alt=""></p>
<h3 id="注意老师在这里也提到了门控单元对设计实用系统的重要性，电路设计-系统设计又是一个深坑"><a href="#注意老师在这里也提到了门控单元对设计实用系统的重要性，电路设计-系统设计又是一个深坑" class="headerlink" title="注意老师在这里也提到了门控单元对设计实用系统的重要性，电路设计/系统设计又是一个深坑"></a><strong>注意老师在这里也提到了门控单元对设计实用系统的重要性，电路设计/</strong>系统设计又是一个深坑</h3><h3 id="2-LSTM"><a href="#2-LSTM" class="headerlink" title="2.LSTM"></a>2.LSTM</h3><p>如图是LSTM和GRU的对比图。这里要注意LSTM引入了cell细胞的概念，可等价于GRU中的hidden layer。也就是C&lt;==&gt;L</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1649_02_108.png" alt=""></p>
<p>下图LSTM细胞更新公式是LSTM的核心，和GRU中的隐藏状态层更新公式有细微的差别，在LSTM中，有两个门，一个是遗忘门，一个是输入门，他们的值都在0到1之间，所以你可以保存任何之前和新计算的值，然后还能把他们加起来。这是在权衡保留多少值，这点是和GRU不一样的。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1649_57_914.png" alt=""></p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1650_44_552.png" alt=""></p>
<p>候选值更新是完全相同的：</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1654_35_751.png" alt=""></p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1654_43_736.png" alt=""></p>
<p>使用RNN风格的各个门控几乎是完全相同的，都会得到一个0到1之间的某个值，区别是LSTM加了一个新的门，输入门i</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1656_34_491.png" alt=""></p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1656_42_614.png" alt=""></p>
<p>GRU和LSTM还有个区别就是GRU有重置门的功能，在计算候选值的时候可能忽略部分过去的值，LSTM的做法稍微有些不同，LSTM在候选更新的时候总是使用当前输入，但是对于另一部分，不使用C(t-1)。而且在LSTM中也有ht，是通过ct推导过来的。形式和GRU略有不同。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1854_21_034.png" alt=""> </p>
<p>上图是LSTM的构造图，横着的三行是各种门，第四行是候选值更新，和RNN一样。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1859_02_952.png" alt=""></p>
<p>上图紫色部分的加权加法是整个LSTM网络拥有长期记忆能力的秘诀。一种可能是你仅仅复制前一个时间步的内容，如果出现这种情况，梯度是1，就可以把误差反向传播回去，对于每个时间步长都可以持续这么做。所以把这个和前一个时间步加起来，而不是做矩阵乘法，这是让LSTM拥有了长短期记忆的核心思想。这被证实是一个非常强大的想法。这个思想在深度学习做计算机视觉的领域也应用地很好。这种思想也叫做残差网络，一般简写为ResNet，大概来说，大概来说，我们希望能够设计100层的深度神经网络，希望能够成功的训练他们，这里的加号和ResNet的第一次近似的思想完全相同。随着你不断向后层计算，我们会使用常规的神经网络层进行一些非线性运算，但是提供一种选择，你可以把前面层的输入移过来，并把他们相加起来，重复一次又一次直到100层。</p>
<p>如下图片演示是从第128个时间步反向进行，可清晰看出LSTM中的信息会持续多久。</p>
<ul>
<li><p>步数128</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1940_55_049.png" alt=""></p>
</li>
<li><p>步数120：左边RNN基本丢光了</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1941_28_304.png" alt=""></p>
</li>
<li><p>步数90：LSTM还有信息记录</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1942_11_246.png" alt=""></p>
</li>
<li><p>步数60：</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1942_37_957.png" alt=""></p>
</li>
<li><p>步数30</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1942_54_429.png" alt=""></p>
</li>
</ul>
<p><strong>一些实践性的指导意见</strong></p>
<ol>
<li><p>如果你要建立一个大的循环神经网络RNN，不管是GRU还是LSTM,初始化真的非常非常重要，如果你的RNN不能正常工作，很多时候是你的初始化做的不好。通常来讲把前一个cell的隐藏状态层乘起来对循环矩阵很有用，通常可以让他正交，这样就有了使用线性代数的机会。</p>
</li>
<li><p>RNN中其实没有多少参数，为了能让它们学习到一些有用的东西，提供一个正交的初始化被证实确实是一种更好的方式。有了这种想法，你就能够在RNN中一直做乘法，通常你的初始值非常小，不可以大，大了就会毁掉神经网络。</p>
</li>
<li><p>通常我们把权重随机地初始化为0，不过在设置遗忘门的bias的时候是个例外，如果你把遗忘门的bias设置为一个大小合适的正数，比如1或者2，通常效果会更好。这表明，你应该对遥远的过去给予更多的关注。这种偏置用来保存长久的记忆。这鼓励你得到一个能有使用的有效的长期记忆的模型。如果过去的内容没用了，就丢掉一部分。但是遗忘门开始主要是遗忘。</p>
</li>
<li><p>这些RNN LSTM等等算法结合现在的动态学习率算法效果会更好。比如adam，adaDelta算法等。这笔SGD好多了。</p>
</li>
<li><p>对于各种RNN,在垂直方向使用dropout是很常见的。这通常能够改进性能。但是不能在水平方向使用，因为这样几乎每一个维度都会被dropout，你就不会得到信息流。</p>
</li>
<li><p>assembly，如果希望结果能提升2%，那么使用集成是行之有效的手段。如下使用投票型集成，紫色是使用集成后的提升，可看到性能提升显著。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_2041_14_615.png" alt=""></p>
</li>
</ol>
<h3 id="3-MT机器翻译的评价标准"><a href="#3-MT机器翻译的评价标准" class="headerlink" title="3.MT机器翻译的评价标准"></a>3.MT机器翻译的评价标准</h3><p>面对同一个句子，十个翻译有十种不同答案，有的偏重原语义，有的偏重转述。所以翻译没有标准答案，句法的选择也多，用机器学习的术语来描述就是翻译者可以做“优化”。</p>
<p>谷歌发明了一种廉价的评价翻译效果的办法，called BLEU,他们的思想是生成参考翻译并且让机翻与之对照打分。通常，人工翻译和机器翻译对比时，n-gram的表现一般不错，也就是图中打下划线的词组们。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_2137_22_650.png" alt=""></p>
<p>如果你构建的单词生成系统中有过多的单词（语言有很大的词汇表），所以从隐状态开始，我们乘以这个softmax参数矩阵，矩阵大小是词汇表的大小乘以隐状态的大小。然后在输进softmax函数压缩。这给了我们生成不同单词的可能性。但是如果词汇表太大，过多算力都会消耗在计算softmax上，这也会是一个问题。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_2357_43_477.png" alt=""></p>
<p>一种解决办法是使用较小的词汇表，比如早起机翻系统里只有50000个单词，但是这样的话句子里有趣的词都不会出现在你的生成记结果里了，如下图</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_2359_15_079.png" alt=""></p>
<p>还有一种降低计算负荷的方法是将词汇表划分为树片段，分到树的各个分支，这样我们也能做更少的计算量。当然一旦划分成树的分支就不能很好的使用GPU了。</p>
<p>另一种思路是在训练和测试模型的时候都使用词汇表的“常用词子集”。毕竟总体上来说，有40%的词汇在一个自然语言的使用场景下，只会出现一次。我们在划分这个常用词子集的时候可以聪明一点，比如把关于篮球的文章放在一个文件里，把关于宠物的文章放在一个文件里等等。</p>

            <hr>
          </div>
          <br>
          <div>
            <p>
            
              <span>
                <i class="iconfont icon-inbox"></i>
                
                  <a class="hover-with-bg" href="/categories/CS224n">CS224n</a>
                  &nbsp;
                
              </span>&nbsp;&nbsp;
            
            
              <span>
                <i class="iconfont icon-tag"></i>
                
                  <a class="hover-with-bg" href="/tags/NLP">NLP</a>
                
              </span>
            
            </p>
            
              <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" target="_blank" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
            
          </div>
        </div>
      </div>
    </div>
    <div class="d-none d-lg-block col-lg-2 toc-container">
      
  <div id="toc">
    <p class="h4"><i class="far fa-list-alt"></i>&nbsp;目录</p>
    <div id="tocbot"></div>
  </div>

    </div>
  </div>
</div>

<!-- custom -->


<!-- Comments -->
<div class="col-lg-7 mx-auto nopadding-md">
  <div class="container comments mx-auto" id="comments">
    
  </div>
</div>

    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>

    
  
    <!-- 不蒜子统计PV -->
    
    &nbsp;<span id="busuanzi_container_site_pv">总访问量 
          <span id="busuanzi_value_site_pv"></span> 次</span>&nbsp;
  
  
    <!-- 不蒜子统计UV -->
    
    &nbsp;<span id="busuanzi_container_site_uv">总访客数 
            <span id="busuanzi_value_site_uv"></span> 人</span>&nbsp;
  
  <br>



    

  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js" ></script>
<script src="/lib/popper/popper.min.js" ></script>
<script src="/lib/bootstrap/js/bootstrap.min.js" ></script>
<script src="/lib/mdbootstrap/js/mdb.min.js" ></script>
<script src="/js/main.js" ></script>




  
    <script src="/lib/tocbot/tocbot.min.js" ></script>
  
  <script src="/js/post.js" ></script>



  <script src="/lib/smooth-scroll/smooth-scroll.min.js" ></script>



  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>


<!-- Plugins -->


  

  

  

  

  




  <script src="/lib/prettify/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "CS224n/11 - GRU2,NMT&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 120,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="/lib/anchor/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "false",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>







</body>
</html>

<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <link rel="icon" type="image/png" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="description" content="desc">
  <meta name="author" content="LiuBingqian">
  <meta name="keywords" content="">
  <title>CS224n/2 - 词向量表示 ~ 刘秉乾的技术博客 :)</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"  >
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css"  >
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css"  >
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css"  >

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css"  >

<link rel="stylesheet" href="/css/main.css"  >


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css"  >


<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header style="height: 100vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>LiuBingqian`s Blog</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">主页</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">分类</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">所有文章</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">标签</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">关于</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>


</nav>

    <div class="view intro-2" id="background"
         style="background: url('http://bqlab-pic.test.upcdn.net/archive.jpg')no-repeat center center;
           background-size: cover;
           background-attachment: fixed;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              <br>
              
                <p class="mt-3">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>&nbsp;
                  星期一, 二月 24日 2020, 12:04 中午
                </p>
              

              <p>
                
                  
                  &nbsp;<i class="far fa-chart-bar"></i>
                  <span class="post-count">
                    6.3k 字
                  </span>&nbsp;
                

                
                  
                  &nbsp;<i class="far fa-clock"></i>
                  <span class="post-count">
                      22 分钟
                  </span>&nbsp;
                

                
                  <!-- 不蒜子统计文章PV -->
                  
                  &nbsp;<i class="far fa-eye" aria-hidden="true"></i>&nbsp;
                  <span id="busuanzi_container_page_pv">
                    <span id="busuanzi_value_page_pv"></span> 次
                  </span>&nbsp;
                
              </p>
            
          </div>

          
            <div class="scroll-down-bar">
              <i class="fas fa-angle-down scroll-down-arrow"></i>
            </div>
          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="py-5 z-depth-3" id="board">
        <div class="post-content mx-auto" id="post">
          <div class="markdown-body">
            <h1 id="Lecture2-词向量表示（上）"><a href="#Lecture2-词向量表示（上）" class="headerlink" title="Lecture2 - 词向量表示（上）"></a>Lecture2 - 词向量表示（上）</h1><p>这节课我们将深入语言的底层，做一些向量和计算，这节课提到的数学是后边内容的基础。这节课将用很慢的速度来仔细讲解一些基础，以便大家可以使用神经网络来学习词语表征这样的简单任务。下周将会继续从基础开始讲解很多的数学。</p>
<h3 id="1-语言学和NLP对词语释义的不同做法"><a href="#1-语言学和NLP对词语释义的不同做法" class="headerlink" title="1.语言学和NLP对词语释义的不同做法"></a>1.语言学和NLP对词语释义的不同做法</h3><p>语言学用如下词典释义来解释“meaning”这个词的意思</p>
<ul>
<li>the idea that is represented by a word,phrase,etc</li>
<li>the idea that i person wants to express by using words,signs,etc</li>
<li>the idea that is expressed in a work of writing,art,etc</li>
</ul>
<p>NLP用多用分类资源处理词义。</p>
<p>比较著名的分类资源是WordNet，它提供很多词汇相关的分类信息。</p>
<p>如下图左侧，用一段代码演示nltk如何抓取wordnet中”panda“这个词的分类信息，panda是一种肉食动物，一种有胎盘的哺乳动物，再往上上溯可抽象为动物，物体，物理实体等等。</p>
<p>如下图右侧，抓取“good“的同义词，这里显示的就是wordnet的答案。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1319_14_209.png" alt=""></p>
<p>同义词描述词汇本义毕竟是离散的方法，会有如下的问题：</p>
<ul>
<li>同义词的描述资源很多，但分类关系会遗漏大量的细微差别。例如I am good at NL和I am expert of NL虽然是同义词但是还是有不少的区别。</li>
<li>语言是动态变化的，不同圈子对同样的词汇的语义指代也不同，wordnet不可能跟上这种变化。</li>
<li>难以获得某个词汇的精确含义</li>
</ul>
<h3 id="2-独热编码存在的问题与分布式表示的提出"><a href="#2-独热编码存在的问题与分布式表示的提出" class="headerlink" title="2.独热编码存在的问题与分布式表示的提出"></a>2.独热编码存在的问题与分布式表示的提出</h3><p>独热编码是由稀疏向量构成的，开发独热编码的系统是基于符号表示的（阿拉伯数字），所以必然是离散的。所以基于向量的表示的独热码在表示两个词义相近的词汇时，并不能显示出他们的相似性。而且独热编码会造成维数灾难。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1319_22_169.png" alt=""></p>
<p>于是出现了<strong>分布式表示</strong>（也就是向量表示）来改善独热码存在的问题，他的思路是通过训练，将每个词都映射到一个较短的向量上来，到底有多短一般我们自己指定。比如”Royalty”,”Masculinity”, “Femininity”和”Age”4个维度来表示，King这个词对应的词向量可能是(0.99,0.99,0.05,0.7)(0.99,0.99,0.05,0.7)，而King这个词可能用独热码表示维度会很高。把King这个词从非常高的维度降到四维空间时，这个过程也称为词嵌入（word embedding），词嵌入其实也是一种投影，甚至可以把词汇从4维进一步降低到2维。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191215_1003_25_105.png" alt=""></p>
<h3 id="3-Word2Vec"><a href="#3-Word2Vec" class="headerlink" title="3.Word2Vec"></a>3.Word2Vec</h3><p>这个模型其实是一个简化的神经网络，如下图，输入是独热编码，中间隐藏层没有激活函数，是线性的单元，输出时用的是softmax回归。这个模型有两种子模型，分别是<strong>词袋模型</strong>和<strong>skip-gram模型</strong>。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191215_1006_22_817.png" alt=""></p>
<ul>
<li><p>词袋模型：输入特征词的上下文向量，输出这个特征词的向量。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191215_1011_39_444.png" alt=""></p>
</li>
</ul>
<ul>
<li><p>Skip-gram模型：反过来，输入一个特征词向量，输出上下文向量</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191215_1011_15_137.png" alt=""></p>
</li>
</ul>
<h3 id="4-词袋模型详解"><a href="#4-词袋模型详解" class="headerlink" title="4.词袋模型详解"></a>4.词袋模型详解</h3><p>词袋的训练模型如下。</p>
<ol>
<li><p>输入层：上下文的独热码，假设向量空间V维，上下文单词个数是C。</p>
</li>
<li><p>所有onehot分别乘以共享的W（V*N）初始化权重矩阵，V是向量维度，N自己设置。</p>
</li>
<li><p>因为是onehot向量，只有一个值为1，其他都是0，所以乘以权重矩阵后仍然是向量，然后将所有这些向量相加，得到隐藏层向量hi。</p>
<p>x1–&gt;h1,x2–&gt;h2,x3–&gt;h3，…………，xn–&gt;hn.</p>
<p>sum{h1 + h2 + h3 + …… + hn} = hi</p>
</li>
<li><p>hi向量乘以W`(N<em>V)输出权重矩阵，得到向量（1\</em>V）,激活函数比如softmax处理后得到V维概率分布，其中最大的index所指示的单词就是预测的中间词，然后与训练集中打上了标签独热码作比较，误差越小越好，根据误差动态更新权重矩阵。</p>
</li>
</ol>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191215_1015_20_195.png" alt=""></p>
<p><strong>例子</strong></p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191215_1038_40_778.png" alt=""></p>
<p>coffee前后的词是输入对象，coffee作为训练集的正确label项，在结果处等待与训练结果对比，以动态调整W。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191215_1040_13_293.png" alt=""></p>
<p>达到迭代次数后，训练出来的look up table应该就是W矩阵，任何一个单词的独热码乘以它都应该得到自己的word embedding。</p>
<h3 id="5-skip-gram模型详解"><a href="#5-skip-gram模型详解" class="headerlink" title="5.skip-gram模型详解"></a>5.skip-gram模型详解</h3><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1410_21_598.png" alt=""></p>
<p>skip-gram就是给出input词汇wt，预测周围距离m个身位内的词汇w(t+j), -m&lt;j&lt;+m 的条件概率。</p>
<h5 id="代价函数J-θ-详解"><a href="#代价函数J-θ-详解" class="headerlink" title="代价函数J(θ)详解"></a>代价函数J(θ)详解</h5><pre><code>J(θ)是负的对数似然的求和，J`是连乘形式。代价函数表示的是我们拿到一串很长的文本，遍历文本中的所有位置，每个单词都当一次输入词汇wt，每个输入词汇wt，我们都会定义一个围绕输入词汇的大小为2m的窗口， 窗口包括前后各m个单词，把每个词汇和周围的窗口的p(o|c)都累加起来，目的就是让所有词汇和所有词汇周围2m内的p(o|c)最大化,（负的它最小化）。</code></pre><p>这样就得到一个概率分布，可以根据中心词汇给出其上下文的词汇出现的概率。然后我们就要设置模型的参数，让上下文中所有词汇出现的概率最大。θ就是词汇的向量表示，也是每个词汇的向量表示的唯一参数。我们要做最大化操作，其实就是解决对数分布问题。加上log，将所有的求连乘积就会变成求和（计算机在计算连乘时容易变成0，log转化为连加后就不存在这样的问题）。 </p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1410_34_807.png" alt=""></p>
<h5 id="P-o-c-剖析"><a href="#P-o-c-剖析" class="headerlink" title="P(o|c)剖析"></a>P(o|c)剖析</h5><p>如下图公式，每种单词类型都有一个对应的向量。uo是周围预测的词，vc是中心词。</p>
<p>softmax的分子是预测的那个词与中心词的交叉熵的自然指数</p>
<p>softmax的分母是window内所有预测词语中心的交叉熵的自然指数加和</p>
<p>代价函数想让这个p(o|c)最大化，也就是想让“预测周围的单词的信心”最大化。uo是通过W矩阵生成的，也就是通过SVD动态调整W矩阵。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1411_03_305.png" alt=""></p>
<p><strong>模型训练</strong></p>
<p>如下图，我们有一个中心词汇，他是一个独热向量wt。还有一个所有中心词汇的<strong>词向量</strong>组成的矩阵W，将独热向量乘以矩阵，提取了矩阵的那一列，并实现了词向量从稀疏的独热码变成了稠密的词向量vc。</p>
<p>然后构造第二个矩阵，用于存储上下文词汇的表示。对于上下文的每个位置（这里只列举了三个），我们挑出中心词汇vc和上下文表示uo的点积（图右第一列），然后用softmax方法将他们转换成概率分布（第二列）即大者更大。然后与第三列上下文真正的独热码对比，看是否预测正确。根据结果动态调整参数，实现模型的训练。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1411_41_142.png" alt=""></p>
<p> 我们接下来要做的事情是学习参数，我们会将模型中所有的参数放进一个大向量θ里。这个向量里有V个单词，每个单词是d维的向量，每个单词按照中心词汇和上下文词汇分别出现一次。所以向量的总长度是2dV。</p>
<p>我们得到了目标函数，想要最小化它的负对数似然。</p>
<p><strong>目标函数是</strong></p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1411_57_921.png" alt="">     </p>
<p>这个公式上文提过，我们的任务是最小化J（θ）,如下是推演过程。</p>
<p>左边项</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1412_10_040.png" alt=""></p>
<p>常数对向量求导，得到的是符合求导分母形式的向量。</p>
<p>右边项</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1413_49_452.png" alt=""> </p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1414_02_546.png" alt=""></p>
<p>左右边项是对数除法转减法实现的。右边项连续两个链式法则求导（连加符号不受求导影响。链式求导时外层直接跳出，求导分母往里层切入），最终与左边项一样，常数对向量求导时结果与向量形式相同。</p>
<p>最终结果如下</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1414_13_943.png" alt=""></p>
<p>U0是实际输出的上下文词汇，Ux是期望向量。</p>
<h3 id="6-Skip-gram模型的优化"><a href="#6-Skip-gram模型的优化" class="headerlink" title="6.Skip-gram模型的优化"></a>6.Skip-gram模型的优化</h3><p>采用随机梯度下降法（SGD）优化，在语料库中移动窗口时， 我们会在每个窗口更新损失函数，损失函数中的分母不用遍历所有的语料库，随机抽取一部分遍历就好，节省了大量的资源，效果也不差。。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1414_44_200.png" alt=""></p>
<p>算法的计算过程：</p>
<p>分子比较容易计算，只是两个百维向量相乘。分母则比较困难，因为在每个窗口，都要遍历一次完整的语料库，uw是要遍历整个物料库的向量，也许词汇表中有20000个词汇，也就是说分母要做20000次内积运算，这非常低效。</p>
<p>但实际上不需要教模型这么多，在每个窗口中，deep learning和learning和zebra并不同时出现，也不和aardvark一起出现，不和其他20000个单词同时出现.所以使用SGD的skip-gram模型主要思想其实是一个小技巧，我们只对可以配对的单词pair训练一些二元逻辑回归，因此我们保留了想要优化和最大化中心词和外围词的内积的想法。 相对于遍历所有单词，实际上支取一些随机单词并指明， 这些从语料库其余部分取出的随机词是不同时出现的，这就得到了使用SGD的skip-gram算法的原始目标函数，通常这被称为一个软件包Word2Vec，初次讲解该方法的论文标题就叫做单词和短语的分布式表示以及他们的语义合成性。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1414_53_123.png" alt=""></p>
<p>基本上需要再遍历每个窗口，因此这里的T需要遍历语料库中的每个窗口。加号左边第一项只是中心词和外围词同时出现的概率的对数（两词内积又被钟形函数压缩，再被求对数），第二项是负的，表明我们将从语料库中随机抽取几个单词，对每个单词，我们会尽量减少他们共同出现的概率（这里又引出关于目标函数的意义：所谓目标函数，就是要以本函数值最大化为目标，正数项要尽可能大，负数项要尽可能小）。</p>
<p>so，我们对语料库中随机抽取的单词做重采样，而不是遍历所有不同的单词，然后认为aardvark没有和learning一起出现等等。我们仅取出五个十个或者其他数目的随机词，然后最小化他们的概率，并且通常我们把它当做一个超参数，一个需要评估重要性的参数，我们将为目标函数的第二部分，为每个窗口，取k个负样本 ，然后最小化这些随机词出现在中心词附近的概率，接着从简单的均匀分布或者一元模型分布（unigram distribution）中采样获取，观测这些词大致出现的频率，然后基于频率去采样，但是我们也取其指数为3/4的值，这是一种简单的术语，如果你使用这个模型有一段时间了，也许你觉得它应该经常采用一些稀有的词，不然他将会经常采样THE和A以及其他stop word，在我们的语料库中对aardvark和zebra进行采样，因此取这个指数为3/4的值。</p>
<p>通常在遍历整个语料库时，可以对每个窗口做一次更新，但也可以是先走过五个窗口，收集好梯度更新，然后再对随机梯度下降的小批量数据进行进一步的操作。</p>
<p>Jt(θ),在模型中θ通常作为一个用于所有变量的参数，因此在skip-gram模型的例子中，它基本上就是所有的u向量和v向量。然后当我们调用时，我们就调用theta，它可能有神经网络，层等其他参数，J是我们的损失函数，T是遍历语料库的第T个时间步或者T个窗口，最后我们优化的整体目标函数，实际是这些目标函数的总和，再一次声明，我们不想对整个语料库做一次大的梯度更新，我们不想经过所有窗口，收集梯度更新然后做一次大的梯度更新，因为这样通常效果不怎么好。通常我们想通过中心词周围的词的加和来预测中心词。</p>
<p>word2vec将语义近似的词语放在向量空间中的近处。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1415_03_036.png" alt=""></p>
<p>训练词向量时，我们会优化目标函数，并且求梯度。词向量们会聚类，图上图中左侧，“周二周三”和“工作日周末日”聚集在一起，图中左上“数字项”聚集在一起，图中右上“名字项”聚集在一起等等。</p>
<h3 id="7-总结word2vec"><a href="#7-总结word2vec" class="headerlink" title="7.总结word2vec"></a><strong>7.总结word2vec</strong></h3><ul>
<li>会遍历语料库中的每个词</li>
<li>预测每个词的周围词</li>
<li>一次只捕获一个单词的重复出现</li>
<li>为什么不直接捕获所有词的同现计数？</li>
</ul>
<p>基本上我们会遍历语料库中的每个单词，观察窗口周围的单词， 预测周围的词，我们在努力抓取词的共同点，这个词和其他词共同出现的频率是多少？每出现一次进行一次计数，就像看到deep和learning同时出现， 就对着两个向量做一次梯度更新，然后再次浏览语料库，你很有可能发现deep和learning又共同出现了，然后对其再做一次梯度更新，你可能会觉得这样做很没有效率，为什么现在仅对整个语料库计算一次deep和learning这两个词共同出现的频率？我们能捕获整个计数而不是一个样本。实际上我们可以那样做，这也是出现word2vec之前的做法，这里有可供我们选择的不同做法。</p>
<p>最简单的类似于word2vec的方法是在每个单词周围使用了一个窗口，这基本上上相当于遍历了整个语料库，我们不对任何部分进行梯度更新，不是用SGD，首先对计数结果进行采集，一旦获得了计数结果，就对矩阵进行操作，窗口的长度可能会是2词的句法信息， 也可能是5（一些单词周围的小窗口），我们要抓取的不仅仅是语义，还有每个词的句法信息，即它是哪一种词性标签，所以动词之间会更接近，例如，动词变为名词，另一方面，观测共同出现的计数结果不仅在窗口周围，而是整个文档的，那我们不能仅仅只观测每个窗口。但是我们认为这个词和其他这些词出现在所有维基百科文章中或者整个词文档中，然后你会捕捉到更多的话题，这通常被称作潜在语义分析，这是以前流行的一个模型，and忽略掉词性和句法信息，基本上你会得到的仅仅是游泳，船，水，天气，太阳等等，他们都是在这个话题上出现的，在这个文档中一起出现。我们不会太过深入这个话题，因为这涉及到其他的下游任务，比如机器翻译等，但是掌握使用这些窗口的知识是很有用的。</p>
<h4 id="一个例子：基于窗口的共现矩阵"><a href="#一个例子：基于窗口的共现矩阵" class="headerlink" title="一个例子：基于窗口的共现矩阵"></a>一个例子：基于窗口的共现矩阵</h4><ul>
<li><p>窗口长度：1( 通常更普遍的是5 -10)</p>
</li>
<li><p>对称性：中心词汇的左右词汇一视同仁</p>
</li>
<li><p>简化的语料库由如下三句话构成</p>
<ul>
<li>I like deep learning</li>
<li>I like NLP</li>
<li>I enjoy flying</li>
</ul>
<p>上图的解读：</p>
</li>
</ul>
<p>第一行是第一个词I的周围词汇统计：like在右侧出现两次，enjoy在右侧出现一次。</p>
<p>第二行同理：I出现两次，deep出现一次，flying出现一次。</p>
<p>这个矩阵的行或者列已经是向量了，但是从几个原因来说，它并不是非常理想的向量：</p>
<ol>
<li>随着字典加入新的单词，所有向量的维度都会发生变化，因此，如果有一些下游的机器学习模型来接受这个向量的输入，他们总会需要改变而且有一些参数的缺失。</li>
<li>这个向量的维度会非常高，因此在训练机器学习模型时会存在稀疏性问题，在此之后会训练出一个不那么健壮的模型。</li>
</ol>
<p>共现矩阵如下</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191215_1507_38_708.png" alt=""></p>
<h4 id="共现矩阵的奇异值分解方法"><a href="#共现矩阵的奇异值分解方法" class="headerlink" title="共现矩阵的奇异值分解方法"></a>共现矩阵的奇异值分解方法</h4><p>解决这个问题的办法就是参考word2vec的思路，并不只是存储所有共同出现的次数、每个单独的数字，还存储重要的信息，维度的固定小数，与word2vec的相似度。这将会使向量的维度在25维到1000维之间。所以现在的问题如何进行降维?</p>
<p>在实际的设置中，会有20000*20000或者百万乘百万的数量级，降维的思路是简单的奇异值分解。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1415_25_219.png" alt=""></p>
<p>奇异值分解的具体做法见线性代数的文档</p>
<p>奇异值分解后就实现了矩阵的压缩功能，然后取出U的头两列，在二维图上画出他们。奇异值分解的降维思路就是，你要降低到几维，就取U的前几列。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1415_35_511.png" alt=""></p>
<p>分解之后可得，I和like是频率比较高的词，enjoy和learning和flying都是动词距离就比较近</p>
<p>奇异值分解是个不错的办法，妥善处理停止词能提高性能，例如the,he,has太多太频繁了，他们又没有给我们太多有用的信息，只是不停地出现。而文本量越大这些越经常出现的词就会出现地更频繁，出现频率很少的词反而带有大量的语义信息。</p>
<ul>
<li><p>一种解决的办法是遮住他们出现的次数，限定在比如一百次。</p>
</li>
<li><p>或者不对所有的词汇平等计数，比如中心词的邻居可计数为1，五步外的词汇计数为0,5</p>
</li>
</ul>
<p><strong>几种模型的对比</strong></p>
<p>虽然奇异值分解很简单，只需要一行python代码，但是从计算方面来说效果不是很好，尤其是当处理越来越巨大的矩阵时，因此需要将二次损失函数定在较小的维度上。</p>
<p>两种基于奇异值分解的共现矩阵的语义预测方法的对比</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1415_44_077.png" alt=""></p>
<ul>
<li><p>Count based</p>
<p>PCA的优点是相对来说训练速度更快，通常会非常有效地使用我们拥有的统计数据。我们只需要搜集一次统计信息，理论上只需要把语料库放一边，然后对共现计数进行一系列的操作，但它主要是捕获大多数词的相似性，没有word2vec模型抓取的其他模式。</p>
</li>
<li><p>Skip-gram模型的缺点是他会缩放语料库的尺寸，必须遍历每一个单一窗口，这非常低效，因此不能有效地利用数据集的统计信息，实际上在下游任务很多的情况下，他能获得更好的性能。但是还是不知道下游具体的任务。但是对于不同的问题，比如命名实体识别或者词性标注等，这都是在作业集中需要实现的部分，事实证明，这种任务skip - gram模型的性能会好一点。我们还能捕获很多复杂的模式，其中一些非常令人震惊。</p>
</li>
</ul>
<p>  <strong>集成以上两种模型优点的模型：GloVe</strong></p>
<p>  global vector模型，θ在这里表示所有的参数，在这个情况下，已有U和V向量，我们基本上会遍历所有可能共现的词对，P是非常巨大的共现矩阵，对于语料库中的每一对词，我们希望最小化内积距离和两个词计数的对数。代价函数如下。与之对比的skip-gram模型的代价函数的目的是要最大化预测词和中心词的条件概率总加和，这个GloVe的代价函数目的是根据UV的交叉熵来动态调整Pij这个共现矩阵来达到J(θ)的最值。</p>
<p>  <img src="http://bqlab-pic.test.upcdn.net/pic/20191215_1507_22_743.png" alt=""></p>
<p>  <strong>内部评价与外部评价</strong></p>
<p>  内在评价通常是针对特定的或者中间环节的子任务进行的，可以观察这些向量的差异性或者相似性，以及内部产品对人类相似性的判断有多大的相关性。</p>
<p>  内部评估的优点是计算很快，已有向量的情况下，可以通过这个快速相似相关的研究评估他们，然后得到数值结果，然后可以修改结果，尝试50000种的旋钮组合的快速调整。有时他们会帮助大家非常快速地理解系统的工作原理。例如什么样的超参数实际上会对这种相似性指标产生影响。</p>
<p>  最近发表的word2vec论文中，其中一个很受欢迎<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1416_00_202.png" alt=""></p>
<p>例如，男人之于女人，所对应的的国王对于什么？（王后）。</p>
<p>这是一个简单的类比。如上图所示，向量（女人 - 男人 + 国王）与哪个向量i的余弦距离最近？最近的那个向量i就是答案，也就是queen。</p>
<p> 这个计算类比词的余弦向量距离方法的效果是相当不错的，如下图所示</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1416_13_585.png" alt=""></p>
<p>在简单的欧几里得加减法中，这些余弦距离很好地捕捉到了他们。下图是另一个例子，公司名称和ceo名称之间也有类似的相同向量距离的关系。<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1416_30_754.png" alt=""></p>
<p>下图是一个更有趣的例子，多项式加减法。例如寿司 - 日本 + 德国 = 德式小香肠等。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1416_46_933.png" alt=""></p>
<ul>
<li>外在评价基本上是对习一个任务的真正评价，对于新的单词向量，取皮尔森相关性，而不是核心矩阵的原始计数。我现在要评估这个词向量是否对机器翻译有帮助。外部评价通常需要消耗大量时间，而且是各个子系统耦合在一起，最后只能得出一个总体结果。</li>
</ul>
<p>​    </p>
<h4 id="CS224N-Research-Highlight-陈丹琦的热门论文分享"><a href="#CS224N-Research-Highlight-陈丹琦的热门论文分享" class="headerlink" title="CS224N Research Highlight(陈丹琦的热门论文分享)"></a>CS224N Research Highlight(陈丹琦的热门论文分享)</h4><p>标题：A simple but Tough-to-beat Baseline for Sentence Embeddings,简单但是难以超越的基线句向量表示.</p>
<p>今天我们正在学习单词向量表示，所以我们希望这些向量可以对词汇意思进行编码，但是在nlp问题中，核心是我们如何才能定义可编码句子含义的向量表示（    how we could have the vector representations that encode the meaning of sentences）.</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1414_21_171.png" alt="">我们还可以将这个句子表示作为一个特征去处理一些句子分类任务，例如情感分析问题。我们给定一个句子，例如自然语言处理很有趣， 我们可以对向量表示运用分类器，来判断这句话的情感是不是正面的。</p>
<p> 有很多方法来做词汇向量表示，最简单的方法是使用词袋，词袋的最简单用法如下图所示，一个句子由每个单词均权重构成。</p>
<p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1414_28_979.png" alt=""></p>
<pre><code>小结：
1.单词有如下三种表达方式。
    指称，直接释义词是什么意思。
    独热编码，也就是稀疏，有维灾难
    词向量表示，在向量空间中，语义近似的词汇距离近。
2.本文终点就是word2vec，掌握如下两种生成词向量的方法。
    词袋：输入上下文向量，得到中心词向量。输入独热码，动态调整W矩阵，输出前过一道softmax，终点labelled          独热码在那等着，根据是否正确的结果来调整W，不断循环完成训练。
    skip-gram:输入特征值，得到上下文向量，代价函数时一个条件概率的二重累加，注意数学细节。也是动态调整                  矩阵W。
3.基于窗口的共现矩阵可以用SVD方法压缩。奇异值分解的数学方法作为另一篇文档存在线性代数文件夹里。
4.GloVe方法是继承了PCA和Skip-gram优点的方法，代价函数大同小异。</code></pre>
            <hr>
          </div>
          <br>
          <div>
            <p>
            
              <span>
                <i class="iconfont icon-inbox"></i>
                
                  <a class="hover-with-bg" href="/categories/CS224n">CS224n</a>
                  &nbsp;
                
              </span>&nbsp;&nbsp;
            
            
              <span>
                <i class="iconfont icon-tag"></i>
                
                  <a class="hover-with-bg" href="/tags/NLP">NLP</a>
                
              </span>
            
            </p>
            
              <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" target="_blank" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
            
          </div>
        </div>
      </div>
    </div>
    <div class="d-none d-lg-block col-lg-2 toc-container">
      
  <div id="toc">
    <p class="h4"><i class="far fa-list-alt"></i>&nbsp;目录</p>
    <div id="tocbot"></div>
  </div>

    </div>
  </div>
</div>

<!-- custom -->


<!-- Comments -->
<div class="col-lg-7 mx-auto nopadding-md">
  <div class="container comments mx-auto" id="comments">
    
  </div>
</div>

    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>

    
  
    <!-- 不蒜子统计PV -->
    
    &nbsp;<span id="busuanzi_container_site_pv">总访问量 
          <span id="busuanzi_value_site_pv"></span> 次</span>&nbsp;
  
  
    <!-- 不蒜子统计UV -->
    
    &nbsp;<span id="busuanzi_container_site_uv">总访客数 
            <span id="busuanzi_value_site_uv"></span> 人</span>&nbsp;
  
  <br>



    

  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js" ></script>
<script src="/lib/popper/popper.min.js" ></script>
<script src="/lib/bootstrap/js/bootstrap.min.js" ></script>
<script src="/lib/mdbootstrap/js/mdb.min.js" ></script>
<script src="/js/main.js" ></script>




  
    <script src="/lib/tocbot/tocbot.min.js" ></script>
  
  <script src="/js/post.js" ></script>



  <script src="/lib/smooth-scroll/smooth-scroll.min.js" ></script>



  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>


<!-- Plugins -->


  

  

  

  

  




  <script src="/lib/prettify/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "CS224n/2 - 词向量表示&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 120,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="/lib/anchor/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "false",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>







</body>
</html>

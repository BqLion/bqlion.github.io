<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <link rel="icon" type="image/png" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="description" content="desc">
  <meta name="author" content="LiuBingqian">
  <meta name="keywords" content="">
  <title>1.科研/1 - A学科调研资料库 ~ 刘秉乾的技术博客 :)</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"  >
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css"  >
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css"  >
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css"  >

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css"  >

<link rel="stylesheet" href="/css/main.css"  >


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css"  >


<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header style="height: 100vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>LiuBingqian`s Blog</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">主页</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">分类</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">所有文章</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">标签</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">关于</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>


</nav>

    <div class="view intro-2" id="background"
         style="background: url('http://bqlab-pic.test.upcdn.net/archive.jpg')no-repeat center center;
           background-size: cover;
           background-attachment: fixed;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              <br>
              
                <p class="mt-3">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>&nbsp;
                  星期一, 二月 24日 2020, 12:04 中午
                </p>
              

              <p>
                
                  
                  &nbsp;<i class="far fa-chart-bar"></i>
                  <span class="post-count">
                    3.5k 字
                  </span>&nbsp;
                

                
                  
                  &nbsp;<i class="far fa-clock"></i>
                  <span class="post-count">
                      12 分钟
                  </span>&nbsp;
                

                
                  <!-- 不蒜子统计文章PV -->
                  
                  &nbsp;<i class="far fa-eye" aria-hidden="true"></i>&nbsp;
                  <span id="busuanzi_container_page_pv">
                    <span id="busuanzi_value_page_pv"></span> 次
                  </span>&nbsp;
                
              </p>
            
          </div>

          
            <div class="scroll-down-bar">
              <i class="fas fa-angle-down scroll-down-arrow"></i>
            </div>
          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="py-5 z-depth-3" id="board">
        <div class="post-content mx-auto" id="post">
          <div class="markdown-body">
            <h1 id="1-阅读列表-文献库"><a href="#1-阅读列表-文献库" class="headerlink" title="1.阅读列表+文献库"></a>1.阅读列表+文献库</h1><p>想当GrandMaster，<strong>以下提到的所有材料都绕不过去</strong>。在还没入行时间紧迫的情况下，须有所取舍。具体取舍规则办法见2 - 计划.md</p>
<h2 id="阅读列表"><a href="#阅读列表" class="headerlink" title="阅读列表"></a>阅读列表</h2><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>CS224N - 自然语言处理</td>
<td></td>
</tr>
<tr>
<td>CS229 - 机器学习（核心课）</td>
<td></td>
</tr>
<tr>
<td>CS229A - 机器学习应用课，数学少，应用多</td>
<td></td>
</tr>
<tr>
<td>CS231N - 计算机视觉</td>
<td></td>
</tr>
<tr>
<td>CS230 - 专注深度学习，只包含一点点机器学习（which最难的那一部分）</td>
<td></td>
</tr>
<tr>
<td>CS221 - AI导论</td>
<td></td>
</tr>
<tr>
<td>CS228 - 概率图模型</td>
<td></td>
</tr>
<tr>
<td><a href="https://www.zhihu.com/org/ji-qi-zhi-xin-65/activities" target="_blank" rel="noopener">机器之心</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://zhuanlan.zhihu.com/paperweekly" target="_blank" rel="noopener">PaperWeekly</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://mp.weixin.qq.com/s/B2ocJ1Y2evLwVkG5FrUz5A" target="_blank" rel="noopener">NeurIPS 2019公布获奖论文</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://mp.weixin.qq.com/s/SxOKHRbTJdzlBNfHos-TOA" target="_blank" rel="noopener">深度学习所需数学知识</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://mp.weixin.qq.com/s/RlgX2GErySe5rAUAvveLdQ" target="_blank" rel="noopener">EE转CS成功案例</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://mp.weixin.qq.com/s/F4zQHesEGWLcBLGwYrmg3w" target="_blank" rel="noopener">Jeff Dean谈2020机器学习趋势</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://mp.weixin.qq.com/s/P0MXhMBjt41WflAMGtFr-g" target="_blank" rel="noopener">从Word2Vec到BERT </a>   done</td>
<td></td>
</tr>
<tr>
<td><a href="https://mp.weixin.qq.com/s/uaBRgRp0Yue4MtC2B-8VJA" target="_blank" rel="noopener">kaggle竞赛宝典</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://mp.weixin.qq.com/s/bSsmaOzQYzJ_7RjnzS804g" target="_blank" rel="noopener">NIPS2019</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://mp.weixin.qq.com/s/-rrj3jusoFAqt6gjuP9y6w" target="_blank" rel="noopener">斯坦福2019全球AI报告</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://mp.weixin.qq.com/s/XMcX1FQ3yDIAgvwsfEJ3yQ" target="_blank" rel="noopener">2020学术会议list</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://mp.weixin.qq.com/s/E04x_tqWPaQ4CSWfGVbnTw" target="_blank" rel="noopener">因果推演</a></td>
<td></td>
</tr>
<tr>
<td><a href="https://zhuanlan.zhihu.com/p/77357304" target="_blank" rel="noopener">ACL2019知识图谱总结</a></td>
<td></td>
</tr>
<tr>
<td>概率论与数理统计</td>
<td></td>
</tr>
<tr>
<td>线性代数</td>
<td></td>
</tr>
<tr>
<td>高等数学</td>
<td></td>
</tr>
<tr>
<td><strong>优化理论</strong></td>
<td></td>
</tr>
<tr>
<td><strong>Bubeck：《Convex Optimization: Algorithms and Complexity》</strong></td>
<td></td>
</tr>
<tr>
<td>Bottou、Curtis和Nocedal：《Optimization Methods for Large-Scale Machine Learning》</td>
<td></td>
</tr>
<tr>
<td>知识图谱</td>
<td></td>
</tr>
<tr>
<td>Ian Goodfellow&amp;Yoshua Bengio&amp;Aaron Courville：DeepLearning</td>
<td></td>
</tr>
<tr>
<td><strong>nlp基石之书：speech+and+language+processing</strong></td>
<td></td>
</tr>
<tr>
<td>Manning：Introduction to Information Retrieval</td>
<td></td>
</tr>
<tr>
<td>Neural Network Methods for Natural Language Processing</td>
<td></td>
</tr>
<tr>
<td><strong>PRML：模式识别与机器学习</strong></td>
<td></td>
</tr>
<tr>
<td>周志华：《机器学习》</td>
<td></td>
</tr>
<tr>
<td><strong>李航：《统计学习方法》</strong></td>
<td></td>
</tr>
<tr>
<td>人工智能：一种现代的方法</td>
<td></td>
</tr>
<tr>
<td>数学之美</td>
<td></td>
</tr>
<tr>
<td>网络、群体与市场（中文版）</td>
<td></td>
</tr>
<tr>
<td>EMNLP2018_如何写NLP代码</td>
<td></td>
</tr>
<tr>
<td>刘洋_写论文的技术细节</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td>理想国</td>
<td></td>
</tr>
<tr>
<td>苏格拉底自辩篇</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<h2 id="对阅读列表的阐述-刘洋"><a href="#对阅读列表的阐述-刘洋" class="headerlink" title="对阅读列表的阐述 - 刘洋"></a>对阅读列表的阐述 - 刘洋</h2><p><strong>1、了解NLP的最基本知识</strong>：Jurafsky和Martin的Speech and Language Processing是领域内的经典教材，里面包含了NLP的基础知识、语言学扫盲知识、基本任务以及解决思路。阅读此书会接触到很多NLP的最基本任务和知识，比如tagging, 各种parsing，coreference, semantic role labeling等等等等。这对于全局地了解NLP领域有着极其重要的意义。书里面的知识并不需要烂熟于心，但是刷上一两遍，起码对于NLP任务有基本认识，下次遇到了知道去哪里找还是非常有意义的。另外 Chris Manning 的 introduction to information retrieval 也是一本可以扫一下盲的书，当然我认为依然不需要记住所有细节，但轮廓需要了解。IR里面的很多基本算法跟NLP有不少的重合。说说我自己曾经走过的弯路。Stanford NLP的qualification考试的一部分就是选一些jurafsky 和 manning书里面的一些chapter来读，然后老师来问相关问题。开始我一直对里面的东西懒得看，所以qualification考试一拖再拖。但博士最后一年没办法拖的时候，才发现如果早知道这些东西，博士早年可以少走很多弯路。</p>
<p>为什么了解NLP基础知识的重要，我给大家举几个例子。<br>最近跟同学一起做语言模型 language modeling相关的事情，很多同学用LSTM或者transformers做language model随手就能实现，但是实现一个 bigram 或者 trigram的language model（LM）却因为里面的OOV的平滑问题卡了大半天（熟悉的同学可能知道，需要拉普拉斯平滑或者更sophisticated的Kneser-Ney平滑）。为什么bigram 或者 trigram的LM很重要呢？去做一个语言模型的问题，实现深度模型之前，第一步其实就要去写一个 bigram 或者 trigram的LM。为什么呢？ 因为这些N-gram模型实现简单，并且robust。通过这样简单的实现，可以告诉你这个数据集的LM模型的下限。这样我们心里会有数，神经网络模型至少不应该比这个模型差的。神经网络模型因为其超参数、梯度爆炸等问题，有时候我们不太容易决定是真的模型不行、参数没调好还是代码有bug。那么通过N-gram LM的给出的下限，我们就可以直观地知道神经网络是有bug还是没调好参数。</p>
<p>第二个例子就是涉及发文章了，不知道有没有同学想过，BERT里面训练LM的随机替换为什么就使结果变好，随机替换是什么鬼，怎么结果就好了。其实在BERT之前，斯坦福的吴恩达组的Ziang Xie的 Data Noising as Smoothing in Neural Network Language Models ICLR2017（<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1703.02573.pdf">https://arxiv.org/pdf/1703.02573.pdf</a>） 就首次提出了此方法，而且给出了理论解释。这种random替换其实本质上属于language modeling里面基于interpolation的平滑方式， 而基于interpolation的LM平滑，就躺在jurafsky那本书的第3.4.3节。</p>
<p>\2. <strong>了解早年经典的NLP模型以及论文</strong>：相比简单粗暴的神经网络模型，早年的NLP算法确实比较繁琐复杂，但里面确实有很多早年学者在硬件条件艰苦情况下的智慧结晶。熟悉了这些模型，可以在现在神经网络里面融会贯通。去年在人民大学做seminar。Seminar有大概30-40位同学参加。Seminar中，我问了一个问题，有谁知道机器翻译中的IBM模型大概是干嘛的，举手的同学大概有五分之一。我再问，谁能来手写（或者大概手写）一下IBM model1，一个人都没有。仅仅从基于IBM模型的Hierarchical Phrase-based MT, 近几年就有很多篇引用量很高的文章是基于里面的思想的。例子数不胜数： </p>
<p>1) chris dyer 组的<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1601.01085">Incorporating structural alignment biases into an attentional neural translation model</a> (NAACL16) 提出用双向attention做neural机器翻译的约束项，意思是如果在英语翻译法语生成的target中的一个法语词attend到了一个source中的英语词，那么反过来，法语翻译英文 target中相同这个英语词应该也attend到source中的这个英语词。其实这个思想就是完完全全相似 Percy Liang 曾经的成名作之一，早在NAACL06年 Alignment by Agreement，大家通过题目的意思就可以猜到文章的内容，正向翻译与反向翻译中的 对齐(alignment) 要 一致(agree)。如今做neural MT的同学，有多少同学读过Percy的这篇大作呢 （大家知道Percy最多的应该是Squad吧）。</p>
<p>2) 处理对话系统的无聊回复，用反向概率p(source|target)做reranking现在应该已经是标配。再比如Rico Sennrich的成名作之一将Monolingual data 跟seq2seq 模型结合。其实这连个思想在phrase-base MT 里面早就被广发的使用。Neural之前的MT，需要对一个大的N-best list用MERT做 reranking， 反向概率 p(source|target) 以及语言模型概率 p(target)是reranking中feature的标配。</p>
<p>3) Harvard NLP组, Sam Wiseman 和Alex 发表的EMNLP16 best paper runner-up, Sequence-to-Sequence Learning as Beam-Search Optimization, 基本上传承了Daume´ III and Daniel Marcu 2005年的 LaSO模型，将其思想adapt到neural里面。</p>
<p>如果再准本溯源，诞生于neural MT的attention，不就是IBM模型的神经网络版本嘛。</p>
<p>\3. <strong>了解机器学习的基本模型：</strong>神经网络的简单暴力并且有效。但是从科研的角度讲，熟悉基本的机器学习算法是必修课。比如吴恩达的 machine learning就是必要之选。记得前段时间我面试一个小伙子，一看就是很聪明的同学，而且很短的时间就有fvg一篇NAACL在投。我就问小伙子，EM算法是什么，小伙子说没有听说过EM，而且自己的科研也用不到EM。我认为这其实是一个挺大的误区。当我想起我自己，曾经就吃过很多类似的亏。因为早期数学基础偏弱，也没有决心恶补一下数学，所以早年每次看到跟variational inference相关的算法就头大，这种偏科持续了很久，限制了科研的广度。相比粗暴的神经网络，CRF等模型的inference确实相对复杂（当年我自己也看了很多次才彻底搞明白）。但搞懂这些，是一个NLP researcher的基本素养。Pattern Recognition and Machine Learning那本书，尤其是某些小节确实比较难（又暴露了数学基础差的事实），即便是只是为了过一遍，也需要很强的耐力才能看完，更不用说完全看懂了。我自己也曾经半途而废很多次，如今依然有很多章节是不太懂的。但是其中的很多基础chapter，我认为还是很值得一读的。其实可以组成那种两三个人的学习小组，不需要有太雄伟的目标，用个一年哪怕两年的时间，把几个重要的chapter 过一遍。</p>
<p>NLP相对是应用科学，并不是特别的数学。但是我们天天用的算法的基本数学逻辑我认为还是需要搞懂，比如dropout, 比如天天用到的优化(SGD, momentum, adaboost, adagrad)，比如各种 batch, layer normalization。这样其实可以省去很多浪费的时间，磨刀不误砍柴工。这些年来，在帮同学调bug的过程中，我至少遇见过3-5个同学 training 的时候开dropout, test 的时候没有对每个cell用 (1-dropout)去 scale （大家不要笑，这是真的）。然后画出dropout曲线就是 dropout 值越大，结果越差。在讨论的时候，同学一脸茫然并且不清楚test时候需要scale。其实本质就是并不了解dropout背后的数学原理。</p>
<p><strong>4. 多看NLP其他子领域的论文</strong>：NLP有很多子领域，MT，信息抽取，parsing，tagging，情感分析，MRC等等。多多熟悉其他子领域的进展是必要的。其实不同子领域所运用的模型不会相差太大。但是最开始看不熟悉领域的问题可能会有一点难，原因是对问题的formalization不是很了解。这可能就需要多花一些时间，多找懂的同学去问。其实了解不同问题的formalization也是对领域知识最好的扩充。</p>
<p><strong>5. 了解 CV和data mining领域的基本重大进展</strong>：当熟悉了上面所说的点之后（当然可能至少也需要一年的时间）。熟悉CV领域的基本任务、基本算法我认为对于打开科研视野很重要。但是不可否认，因为领域不用，写作风格、术语表达相差很大，又因为缺乏背景知识（文章中会省略一些基础知识，默认大家都懂。但是跨领域的人可能不懂），第一次想读懂跨领域的文章其实并不容易。我就出现过竟然在讨论班上直接把faster-RCNN讲错了的情况，以为自己看懂了，然后就讲错了（至今昱先天天还在因为这个事情调侃我）。不过重要的是，NLP领域里面一些重要的文章其实或多或少借鉴了CV里面的思想，当然也同样出现CV借鉴NLP的情况。NLP神经网络可视化、可解释性的研究，时间上还是落后于CV里面对CNN的可视化。所以很多工作大量借鉴了CV里面的类似工作。NLP运用GAN其实也是借鉴CV的。其实两个领域很多是很相通的。比如，如果不考虑question query, vision里面detection中的 region proposal（在一个大的图片背景下找一个特定区域）, 大家想是不是跟MRC里面的 span extraction （在一大堆文字里面找一个span）有异曲同工之妙。更不用说image caption generation与sequence-to-sequence模型了，本质上几乎没什么太大的区别。强化学习在生成领域generation，发完了MT(Ranzato et al., ICLR2016)再发 image caption generation, 再回到summarization. Actor-critic 模型也是类似的，还是很多做generation diversity的文章。因为跨领域不好懂，所以第一次推荐看tutorial, 如果有 sudo code 的tutorial那就更好了。另外看看扫盲课的视频，比如Stanford CS231n也是个好办法。另外，一个NLP组里面有一个很懂CV的人也很重要（拜谢昱先）， and vise versa。<br>graph embedding近两年崛起于data mining领域。目测会在（或者已经在）NLP的不少任务得到广泛应用。想到几年前，deep walk借鉴了word2vec, 开始在data mining领域发迹，然后似乎又要轮转回NLP了。</p>
<p><strong>6.如何快速了解某个领域研究进展</strong></p>
<p>最后简单说一下快速了解某领域研究进展的经验。你会发现，搜索引擎是查阅文献的重要工具，尤其是谷歌提供的Google Scholar，由于其庞大的索引量，将是我们披荆斩棘的利器。</p>
<p>当需要了解某个领域，如果能找到一篇该领域的最新研究综述，就省劲多了。最方便的方法还是在Google Scholar中搜索“领域名称 + survey / review / tutorial / 综述”来查找。也有一些出版社专门出版各领域的综述文章，例如NOW Publisher出版的Foundations and Trends系列，Morgan &amp; Claypool Publisher出版的Synthesis Lectures on Human Language Technologies系列等。它们发表了很多热门方向的综述，如文档摘要、情感分析和意见挖掘、学习排序、语言模型等。</p>
<p>如果方向太新还没有相关综述，一般还可以查找该方向发表的最新论文，阅读它们的“相关工作”章节，顺着列出的参考文献，就基本能够了解相关研究脉络了。当然，还有很多其他办法，例如去<a href="https://link.zhihu.com/?target=http%3A//videolectures.net">http://videolectures.net</a>上看著名学者在各大学术会议或暑期学校上做的tutorial报告，去直接咨询这个领域的研究者，等等。</p>
<h2 id="文献库在Endnote中构建完成"><a href="#文献库在Endnote中构建完成" class="headerlink" title="文献库在Endnote中构建完成"></a>文献库在Endnote中构建完成</h2>
            <hr>
          </div>
          <br>
          <div>
            <p>
            
              <span>
                <i class="iconfont icon-inbox"></i>
                
                  <a class="hover-with-bg" href="/categories/1.%E7%A7%91%E7%A0%94">1.科研</a>
                  &nbsp;
                
              </span>&nbsp;&nbsp;
            
            
              <span>
                <i class="iconfont icon-tag"></i>
                
                  <a class="hover-with-bg" href="/tags/%E8%AE%BA%E6%96%87%E5%86%99%E6%B3%95">论文写法</a>
                
              </span>
            
            </p>
            
              <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" target="_blank" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
            
          </div>
        </div>
      </div>
    </div>
    <div class="d-none d-lg-block col-lg-2 toc-container">
      
  <div id="toc">
    <p class="h4"><i class="far fa-list-alt"></i>&nbsp;目录</p>
    <div id="tocbot"></div>
  </div>

    </div>
  </div>
</div>

<!-- custom -->


<!-- Comments -->
<div class="col-lg-7 mx-auto nopadding-md">
  <div class="container comments mx-auto" id="comments">
    
  </div>
</div>

    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>

    
  
    <!-- 不蒜子统计PV -->
    
    &nbsp;<span id="busuanzi_container_site_pv">总访问量 
          <span id="busuanzi_value_site_pv"></span> 次</span>&nbsp;
  
  
    <!-- 不蒜子统计UV -->
    
    &nbsp;<span id="busuanzi_container_site_uv">总访客数 
            <span id="busuanzi_value_site_uv"></span> 人</span>&nbsp;
  
  <br>



    

  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js" ></script>
<script src="/lib/popper/popper.min.js" ></script>
<script src="/lib/bootstrap/js/bootstrap.min.js" ></script>
<script src="/lib/mdbootstrap/js/mdb.min.js" ></script>
<script src="/js/main.js" ></script>




  
    <script src="/lib/tocbot/tocbot.min.js" ></script>
  
  <script src="/js/post.js" ></script>



  <script src="/lib/smooth-scroll/smooth-scroll.min.js" ></script>



  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>


<!-- Plugins -->


  

  

  

  

  




  <script src="/lib/prettify/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "1.科研/1 - A学科调研资料库&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 120,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="/lib/anchor/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "false",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>







</body>
</html>

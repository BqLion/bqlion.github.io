<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <link rel="icon" type="image/png" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="description" content="desc">
  <meta name="author" content="LiuBingqian">
  <meta name="keywords" content="">
  <title>NLP发文/深度学习论文集[Done] ~ 刘秉乾的技术博客 :)</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"  >
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css"  >
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css"  >
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css"  >

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css"  >

<link rel="stylesheet" href="/css/main.css"  >


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css"  >


<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header style="height: 100vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>LiuBingqian`s Blog</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">主页</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">分类</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">所有文章</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">标签</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">关于</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>


</nav>

    <div class="view intro-2" id="background"
         style="background: url('http://bqlab-pic.test.upcdn.net/archive.jpg')no-repeat center center;
           background-size: cover;
           background-attachment: fixed;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              <br>
              
                <p class="mt-3">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>&nbsp;
                  星期一, 二月 24日 2020, 12:04 中午
                </p>
              

              <p>
                
                  
                  &nbsp;<i class="far fa-chart-bar"></i>
                  <span class="post-count">
                    4.9k 字
                  </span>&nbsp;
                

                
                  
                  &nbsp;<i class="far fa-clock"></i>
                  <span class="post-count">
                      27 分钟
                  </span>&nbsp;
                

                
                  <!-- 不蒜子统计文章PV -->
                  
                  &nbsp;<i class="far fa-eye" aria-hidden="true"></i>&nbsp;
                  <span id="busuanzi_container_page_pv">
                    <span id="busuanzi_value_page_pv"></span> 次
                  </span>&nbsp;
                
              </p>
            
          </div>

          
            <div class="scroll-down-bar">
              <i class="fas fa-angle-down scroll-down-arrow"></i>
            </div>
          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="py-5 z-depth-3" id="board">
        <div class="post-content mx-auto" id="post">
          <div class="markdown-body">
            <h2 id="1-深度学习历史和基础"><a href="#1-深度学习历史和基础" class="headerlink" title="1 深度学习历史和基础"></a>1 深度学习历史和基础</h2><h3 id="1-0-书籍"><a href="#1-0-书籍" class="headerlink" title="1.0 书籍"></a>1.0 书籍</h3><p>█[0] Bengio, Yoshua, Ian J. Goodfellow, and Aaron Courville. <strong>“Deep learning</strong>.” An MIT Press book. (2015). [pdf] (Ian Goodfellow 等大牛所著的教科书，乃深度学习圣经。你可以同时研习这本书以及以下论文) ★★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=https%3A//github.com/HFTrader/DeepLearningBook/raw/master/DeepLearningBook.pdf">https://github.com/HFTrader/DeepLearningBook/raw/master/DeepLearningBook.pdf</a></p>
<h3 id="1-1-调查"><a href="#1-1-调查" class="headerlink" title="1.1 调查"></a>1.1 调查</h3><p>█[1] LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. “<strong>Deep learning</strong>.” Nature 521.7553 (2015): 436-444. [pdf] (三巨头做的调查)  ★★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%7Ehinton/absps/NatureDeepReview.pdf">http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf</a></p>
<h3 id="1-2-深度置信网络-DBN，深度学习前夜的里程碑"><a href="#1-2-深度置信网络-DBN，深度学习前夜的里程碑" class="headerlink" title="1.2 深度置信网络 (DBN，深度学习前夜的里程碑)"></a>1.2 深度置信网络 (DBN，深度学习前夜的里程碑)</h3><p>█[2] Hinton, Geoffrey E., Simon Osindero, and Yee-Whye Teh. “<strong>A fast learning algorithm for deep belief nets</strong>.” Neural computation 18.7 (2006): 1527-1554. [pdf] (深度学习前夜) ★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%7Ehinton/absps/ncfast.pdf">http://www.cs.toronto.edu/~hinton/absps/ncfast.pdf</a></p>
<p>█[3] Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. “<strong>Reducing the dimensionality of data with neural networks.</strong>“ Science 313.5786 (2006): 504-507. [pdf] (里程碑，展示了深度学习的前景) ★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%7Ehinton/science.pdf">http://www.cs.toronto.edu/~hinton/science.pdf</a></p>
<h3 id="1-3-ImageNet-的进化（深度学习从此萌发）"><a href="#1-3-ImageNet-的进化（深度学习从此萌发）" class="headerlink" title="1.3 ImageNet 的进化（深度学习从此萌发）"></a>1.3 ImageNet 的进化（深度学习从此萌发）</h3><p>█[4] Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. “<strong>Imagenet classification with deep convolutional neural networks.</strong>“ Advances in neural information processing systems. 2012. [pdf] (AlexNet, 深度学习突破)  ★★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-    networks.pdf</a></p>
<p>█[5] Simonyan, Karen, and Andrew Zisserman. “<strong>Very deep convolutional networks for large-scale image recognition.</strong>“ arXiv preprint arXiv:1409.1556 (2014). [pdf] (VGGNet，神经网络变得很深层) ★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1409.1556.pdf">https://arxiv.org/pdf/1409.1556.pdf</a></p>
<p>█[6] Szegedy, Christian, et al. “<strong>Going deeper with convolutions.</strong>“ Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015. [pdf] (GoogLeNet) ★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf">http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf</a></p>
<p>█[7] He, Kaiming, et al. “<strong>Deep residual learning for image recognition.</strong>“ arXiv preprint arXiv:1512.03385 (2015). [pdf](ResNet，特别深的神经网络, CVPR 最佳论文)  ★★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1512.03385.pdf">https://arxiv.org/pdf/1512.03385.pdf</a></p>
<h3 id="1-4-语音识别的进化"><a href="#1-4-语音识别的进化" class="headerlink" title="1.4 语音识别的进化"></a>1.4 语音识别的进化</h3><p>█[8] Hinton, Geoffrey, et al. “<strong>Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups.</strong>“ IEEE Signal Processing Magazine 29.6 (2012): 82-97. [pdf] (语音识别的突破) ★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//cs224d.stanford.edu/papers/maas_paper.pdf">http://cs224d.stanford.edu/papers/maas_paper.pdf</a></p>
<p>█[9] Graves, Alex, Abdel-rahman Mohamed, and Geoffrey Hinton. “<strong>Speech recognition with deep recurrent neural networks.</strong>“ 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013. [pdf] (RNN) ★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1303.5778.pdf">http://arxiv.org/pdf/1303.5778.pdf</a></p>
<p>█[10] Graves, Alex, and Navdeep Jaitly. “<strong>Towards End-To-End Speech Recognition with Recurrent Neural Networks.</strong>“ ICML. Vol. 14. 2014. [pdf] ★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.jmlr.org/proceedings/papers/v32/graves14.pdf">http://www.jmlr.org/proceedings/papers/v32/graves14.pdf</a></p>
<p>█[11] Sak, Haşim, et al. “<strong>Fast and accurate recurrent neural network acoustic models for speech recognition.</strong>“ arXiv preprint arXiv:1507.06947 (2015). [pdf] (谷歌语音识别系统) ★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1507.06947">http://arxiv.org/pdf/1507.06947</a></p>
<p>█[12] Amodei, Dario, et al. “<strong>Deep speech 2: End-to-end speech recognition in english and mandarin</strong>.” arXiv preprint arXiv:1512.02595 (2015). [pdf] (百度语音识别系统) ★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1512.02595.pdf">https://arxiv.org/pdf/1512.02595.pdf</a></p>
<p>█[13] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, G. Zweig “<strong>Achieving Human Parity in Conversational Speech Recognition.</strong>“ arXiv preprint arXiv:1610.05256 (2016). [pdf] (最前沿的语音识别, 微软) ★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1610.05256v1">https://arxiv.org/pdf/1610.05256v1</a></p>
<p>研读以上论文之后，你将对深度学习历史、模型的基本架构（包括 CNN, RNN, LSTM）有一个基础的了解，并理解深度学习如何应用于图像和语音识别问题。接下来的论文，将带你深入探索深度学习方法、在不同领域的应用和前沿尖端技术。我建议，你可以根据兴趣和工作/研究方向进行选择性的阅读。</p>
<h2 id="2-深度学习方法"><a href="#2-深度学习方法" class="headerlink" title="2 深度学习方法"></a>2 深度学习方法</h2><h3 id="2-1-模型"><a href="#2-1-模型" class="headerlink" title="2.1 模型"></a>2.1 模型</h3><p>█[14] Hinton, Geoffrey E., et al. “<strong>Improving neural networks by preventing co-adaptation of feature detectors.</strong>“ arXiv preprint arXiv:1207.0580 (2012). [pdf] (Dropout) ★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1207.0580.pdf">https://arxiv.org/pdf/1207.0580.pdf</a></p>
<p>█[15] Srivastava, Nitish, et al. “<strong>Dropout: a simple way to prevent neural networks from overfitting.</strong>“ Journal of Machine Learning Research 15.1 (2014): 1929-1958. [pdf] ★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf">http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf</a></p>
<p>█[16] Ioffe, Sergey, and Christian Szegedy. “<strong>Batch normalization: Accelerating deep network training by reducing internal covariate shift.</strong>“ arXiv preprint arXiv:1502.03167 (2015). [pdf] (2015 年的杰出研究) ★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1502.03167">http://arxiv.org/pdf/1502.03167</a></p>
<p>█[17] Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. “<strong>Layer normalization.</strong>“ arXiv preprint arXiv:1607.06450 (2016). [pdf] (Batch Normalization 的更新) ★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1607.06450.pdf%3Futm_source%3Dsciontist.com%26utm_medium%3Drefer%26utm_campaign%3Dpromote">https://arxiv.org/pdf/1607.06450.pdf?utm_source=sciontist.com&amp;utm_medium=refer&amp;utm_campaign=promote</a></p>
<p>█[18] Courbariaux, Matthieu, et al. “<strong>Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to+ 1 or−1.</strong>“ [pdf] (新模型，快) ★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=https%3A//pdfs.semanticscholar.org/f832/b16cb367802609d91d400085eb87d630212a.pdf">https://pdfs.semanticscholar.org/f832/b16cb367802609d91d400085eb87d630212a.pdf</a></p>
<p>█[19] Jaderberg, Max, et al. “<strong>Decoupled neural interfaces using synthetic gradients.</strong>“ arXiv preprint arXiv:1608.05343 (2016). [pdf] (训练方法的创新，研究相当不错) ★★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1608.05343">https://arxiv.org/pdf/1608.05343</a></p>
<p>█[20] Chen, Tianqi, Ian Goodfellow, and Jonathon Shlens. “<strong>Net2net: Accelerating learning via knowledge transfer.</strong>“ arXiv preprint arXiv:1511.05641 (2015). [pdf] (改进此前的训练网络，来缩短训练周期) ★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1511.05641">Accelerating Learning via Knowledge Transfer</a></p>
<p>█[21] Wei, Tao, et al. “<strong>Network Morphism.</strong>“ arXiv preprint arXiv:1603.01670 (2016). [pdf] (改进此前的训练网络，来缩短训练周期) ★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1603.01670">[1603.01670] Network Morphism</a></p>
<h3 id="2-2-优化-Optimization"><a href="#2-2-优化-Optimization" class="headerlink" title="2.2 优化 Optimization"></a>2.2 优化 Optimization</h3><p>█[22] Sutskever, Ilya, et al. “<strong>On the importance of initialization and momentum in deep learning.</strong>“ ICML (3) 28 (2013): 1139-1147. [pdf] (Momentum optimizer) ★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.jmlr.org/proceedings/papers/v28/sutskever13.pdf">http://www.jmlr.org/proceedings/papers/v28/sutskever13.pdf</a></p>
<p>█[23] Kingma, Diederik, and Jimmy Ba. “<strong>Adam: A method for stochastic optimization.</strong>“ arXiv preprint arXiv:1412.6980 (2014). [pdf] (Maybe used most often currently) ★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1412.6980">http://arxiv.org/pdf/1412.6980</a></p>
<p>█[24] Andrychowicz, Marcin, et al. “<strong>Learning to learn by gradient descent by gradient descent.”</strong> arXiv preprint arXiv:1606.04474 (2016). [pdf] (Neural Optimizer,Amazing Work) ★★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1606.04474">https://arxiv.org/pdf/1606.04474</a></p>
<p>█[25] Han, Song, Huizi Mao, and William J. Dally. <strong>“Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding.</strong>“ CoRR, abs/1510.00149 2 (2015). [pdf] (ICLR best paper, new direction to make NN running fast,DeePhi Tech Startup) ★★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=https%3A//pdfs.semanticscholar.org/5b6c/9dda1d88095fa4aac1507348e498a1f2e863.pdf">https://pdfs.semanticscholar.org/5b6c/9dda1d88095fa4aac1507348e498a1f2e863.pdf</a></p>
<p>█[26] Iandola, Forrest N., et al. “<strong>SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&lt; 1MB model size.</strong>“ arXiv preprint arXiv:1602.07360 (2016). [pdf] (Also a new direction to optimize NN,DeePhi Tech Startup) ★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1602.07360">http://arxiv.org/pdf/1602.07360</a></p>
<h3 id="2-3-无监督学习-深度生成模型"><a href="#2-3-无监督学习-深度生成模型" class="headerlink" title="2.3 无监督学习/深度生成模型"></a>2.3 无监督学习/深度生成模型</h3><p>█[27] Le, Quoc V. “<strong>Building high-level features using large scale unsupervised learning.</strong>“ 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013. [pdf] (里程碑, 吴恩达, 谷歌大脑, Cat) ★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1112.6209.pdf%26embed">http://arxiv.org/pdf/1112.6209.pdf&amp;embed</a></p>
<p>█[28] Kingma, Diederik P., and Max Welling. “<strong>Auto-encoding variational bayes.</strong>“ arXiv preprint arXiv:1312.6114 (2013). <a href="VAE">pdf</a> ★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1312.6114">http://arxiv.org/pdf/1312.6114</a></p>
<p>█[29] Goodfellow, Ian, et al. “<strong>Generative adversarial nets.</strong>“ Advances in Neural Information Processing Systems. 2014. <a href="GAN，很酷的想法">pdf</a> ★★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5423-generative-adversarial-nets.pdf">http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf</a></p>
<p>█[30] Radford, Alec, Luke Metz, and Soumith Chintala. “<strong>Unsupervised representation learning with deep convolutional generative adversarial networks.</strong>“ arXiv preprint arXiv:1511.06434 (2015). [pdf] (DCGAN) ★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1511.06434">http://arxiv.org/pdf/1511.06434</a></p>
<p>█[31] Gregor, Karol, et al. “<strong>DRAW: A recurrent neural network for image generation.</strong>“ arXiv preprint arXiv:1502.04623 (2015). [pdf] (VAE with attention, 很出色的研究) ★★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v37/gregor15.pdf">http://jmlr.org/proceedings/papers/v37/gregor15.pdf</a></p>
<p>█[32] Oord, Aaron van den, Nal Kalchbrenner, and Koray Kavukcuoglu. “<strong>Pixel recurrent neural networks.</strong>“ arXiv preprint arXiv:1601.06759 (2016). [pdf] (PixelRNN) ★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1601.06759">http://arxiv.org/pdf/1601.06759</a></p>
<p>█[33] Oord, Aaron van den, et al. <strong>“Conditional image generation with PixelCNN decoders.</strong>“ arXiv preprint arXiv:1606.05328 (2016). [pdf] (PixelCNN) ★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1606.05328">https://arxiv.org/pdf/1606.05328</a></p>
<h3 id="2-4-递归神经网络（RNN）-Sequence-to-Sequence-Model"><a href="#2-4-递归神经网络（RNN）-Sequence-to-Sequence-Model" class="headerlink" title="2.4 递归神经网络（RNN） / Sequence-to-Sequence Model"></a>2.4 递归神经网络（RNN） / Sequence-to-Sequence Model</h3><p>█[34] Graves, Alex. “<strong>Generating sequences with recurrent neural networks.</strong>“ arXiv preprint arXiv:1308.0850 (2013). [pdf] (LSTM, 效果很好，展示了 RNN 的性能) ★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1308.0850">http://arxiv.org/pdf/1308.0850</a></p>
<p>█[35] Cho, Kyunghyun, et al. “<strong>Learning phrase representations using RNN encoder-decoder for statistical machine translation.</strong>“ arXiv preprint arXiv:1406.1078 (2014). [pdf] (第一篇 Sequence-to-Sequence 的论文) ★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1406.1078">http://arxiv.org/pdf/1406.1078</a></p>
<p>█[36] Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. “<strong>Sequence to sequence learning with neural networks.</strong>“ Advances in neural information processing systems. 2014. [pdf] (杰出研究) ★★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5346-information-based-learning-by-agents-in-unbounded-state-spaces.pdf">http://papers.nips.cc/paper/5346-information-based-learning-by-agents-in-unbounded-state-spaces.pdf</a></p>
<p>█[37] Bahdanau, Dzmitry, KyungHyun Cho, and Yoshua Bengio. “<strong>Neural Machine Translation by Jointly Learning to Align and Translate.</strong>“ arXiv preprint arXiv:1409.0473 (2014). [pdf] ★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1409.0473v7.pdf">https://arxiv.org/pdf/1409.0473v7.pdf</a></p>
<p>█[38] Vinyals, Oriol, and Quoc Le. “<strong>A neural conversational model.</strong>“ arXiv preprint arXiv:1506.05869 (2015). [pdf] (Seq-to-Seq 聊天机器人) ★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1506.05869.pdf%2520%28http%3A//arxiv.org/pdf/1506.05869.pdf%29">http://arxiv.org/pdf/1506.05869.pdf%20(http://arxiv.org/pdf/1506.05869.pdf)</a></p>
<h3 id="2-5-神经网络图灵机"><a href="#2-5-神经网络图灵机" class="headerlink" title="2.5 神经网络图灵机"></a>2.5 神经网络图灵机</h3><p>█[39] Graves, Alex, Greg Wayne, and Ivo Danihelka. “<strong>Neural turing machines.</strong>“ arXiv preprint arXiv:1410.5401 (2014). [pdf] (未来计算机的基础原型机) ★★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1410.5401.pdf">http://arxiv.org/pdf/1410.5401.pdf</a></p>
<p>█[40] Zaremba, Wojciech, and Ilya Sutskever. “<strong>Reinforcement learning neural Turing machines.</strong>“ arXiv preprint arXiv:1505.00521 362 (2015). [pdf] ★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=https%3A//pdfs.semanticscholar.org/f10e/071292d593fef939e6ef4a59baf0bb3a6c2b.pdf">https://pdfs.semanticscholar.org/f10e/071292d593fef939e6ef4a59baf0bb3a6c2b.pdf</a></p>
<p>█[41] Weston, Jason, Sumit Chopra, and Antoine Bordes. “<strong>Memory networks.</strong>“ arXiv preprint arXiv:1410.3916 (2014). [pdf] ★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1410.3916">http://arxiv.org/pdf/1410.3916</a></p>
<p>█[42] Sukhbaatar, Sainbayar, Jason Weston, and Rob Fergus. “<strong>End-to-end memory networks.</strong>“ Advances in neural information processing systems. 2015. [pdf] ★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf">http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf</a></p>
<p>█[43] Vinyals, Oriol, Meire Fortunato, and Navdeep Jaitly. “<strong>Pointer networks.</strong>“ Advances in Neural Information Processing Systems. 2015. [pdf] ★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5866-pointer-networks.pdf">http://papers.nips.cc/paper/5866-pointer-networks.pdf</a></p>
<p>█[44] Graves, Alex, et al. “<strong>Hybrid computing using a neural network with dynamic external memory.</strong>“ Nature (2016). [pdf] (里程碑，把以上论文的想法整合了起来) ★★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=https%3A//www.dropbox.com/s/0a40xi702grx3dq/2016-graves.pdf">https://www.dropbox.com/s/0a40xi702grx3dq/2016-graves.pdf</a></p>
<h3 id="2-6-深度强化学习"><a href="#2-6-深度强化学习" class="headerlink" title="2.6 深度强化学习"></a>2.6 深度强化学习</h3><p>█[45] Mnih, Volodymyr, et al. “<strong>Playing atari with deep reinforcement learning</strong>.” arXiv preprint arXiv:1312.5602 (2013). [pdf]) (第一个以深度强化学习为题的论文)  ★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1312.5602.pdf">http://arxiv.org/pdf/1312.5602.pdf</a></p>
<p>█[46] Mnih, Volodymyr, et al. “<strong>Human-level control through deep reinforcement learning</strong>.” Nature 518.7540 (2015): 529-533. [pdf] (里程碑) ★★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=https%3A//storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf">https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf</a></p>
<p>█[47] Wang, Ziyu, Nando de Freitas, and Marc Lanctot. “<strong>Dueling network architectures for deep reinforcement learning.</strong>“ arXiv preprint arXiv:1511.06581 (2015). [pdf] (ICLR 最佳论文，很棒的想法)  ★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1511.06581">http://arxiv.org/pdf/1511.06581</a></p>
<p>█[48] Mnih, Volodymyr, et al. “<strong>Asynchronous methods for deep reinforcement learning.</strong>“ arXiv preprint arXiv:1602.01783 (2016). [pdf] (前沿方法) ★★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1602.01783">http://arxiv.org/pdf/1602.01783</a></p>
<p>█[49] Lillicrap, Timothy P., et al. “<strong>Continuous control with deep reinforcement learning.</strong>“ arXiv preprint arXiv:1509.02971 (2015). [pdf] (DDPG)  ★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1509.02971">http://arxiv.org/pdf/1509.02971</a></p>
<p>█[50] Gu, Shixiang, et al. “<strong>Continuous Deep Q-Learning with Model-based Acceleration.</strong>“ arXiv preprint arXiv:1603.00748 (2016). [pdf] (NAF)  ★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1603.00748">http://arxiv.org/pdf/1603.00748</a></p>
<p>█[51] Schulman, John, et al. “<strong>Trust region policy optimization.</strong>“ CoRR, abs/1502.05477 (2015). [pdf] (TRPO)  ★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.jmlr.org/proceedings/papers/v37/schulman15.pdf">http://www.jmlr.org/proceedings/papers/v37/schulman15.pdf</a></p>
<p>█[52] Silver, David, et al. “<strong>Mastering the game of Go with deep neural networks and tree search.</strong>“ Nature 529.7587 (2016): 484-489. [pdf] (AlphaGo) ★★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//willamette.edu/%7Elevenick/cs448/goNature.pdf">http://willamette.edu/~levenick/cs448/goNature.pdf</a></p>
<h3 id="2-7-深度迁移学习-终生学习-强化学习"><a href="#2-7-深度迁移学习-终生学习-强化学习" class="headerlink" title="2.7 深度迁移学习 /终生学习 / 强化学习"></a>2.7 深度迁移学习 /终生学习 / 强化学习</h3><p>█[53] Bengio, Yoshua. “<strong>Deep Learning of Representations for Unsupervised and Transfer Learning</strong>.” ICML Unsupervised and Transfer Learning 27 (2012): 17-36. [pdf] (这是一个教程) ★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.jmlr.org/proceedings/papers/v27/bengio12a/bengio12a.pdf">http://www.jmlr.org/proceedings/papers/v27/bengio12a/bengio12a.pdf</a></p>
<p>█[54] Silver, Daniel L., Qiang Yang, and Lianghao Li. “<strong>Lifelong Machine Learning Systems: Beyond Learning Algorithms.</strong>“ AAAI Spring Symposium: Lifelong Machine Learning. 2013. [pdf] (对终生学习的简单讨论) ★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//citeseerx.ist.psu.edu/viewdoc/download%3Fdoi%3D10.1.1.696.7800%26rep%3Drep1%26type%3Dpdf">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.696.7800&amp;rep=rep1&amp;type=pdf</a></p>
<p>█[55] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. “<strong>Distilling the knowledge in a neural network.</strong>“ arXiv preprint arXiv:1503.02531 (2015). [pdf] (大神们的研究)  ★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1503.02531">http://arxiv.org/pdf/1503.02531</a></p>
<p>█[56] Rusu, Andrei A., et al. “<strong>Policy distillation.</strong>“ arXiv preprint arXiv:1511.06295 (2015). [pdf] (RL 领域) ★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1511.06295">http://arxiv.org/pdf/1511.06295</a></p>
<p>█[57] Parisotto, Emilio, Jimmy Lei Ba, and Ruslan Salakhu★★★tdinov. “<strong>Actor-mimic: Deep multitask and transfer reinforcement learning.</strong>“ arXiv preprint arXiv:1511.06342 (2015). [pdf] (RL 领域) ★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1511.06342">http://arxiv.org/pdf/1511.06342</a></p>
<p>█[58] Rusu, Andrei A., et al. “<strong>Progressive neural networks.</strong>“ arXiv preprint arXiv:1606.04671 (2016). [pdf] (杰出研究, 很新奇的想法) ★★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1606.04671">https://arxiv.org/pdf/1606.04671</a></p>
<h3 id="2-8-One-Shot-深度学习"><a href="#2-8-One-Shot-深度学习" class="headerlink" title="2.8 One Shot 深度学习"></a>2.8 One Shot 深度学习</h3><p>█[59] Lake, Brenden M., Ruslan Salakhutdinov, and Joshua B. Tenenbaum. “<strong>Human-level concept learning through probabilistic program induction.</strong>“ Science 350.6266 (2015): 1332-1338. [pdf] (不含深度学习但值得一读) ★★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//clm.utexas.edu/compjclub/wp-content/uploads/2016/02/lake2015.pdf">http://clm.utexas.edu/compjclub/wp-content/uploads/2016/02/lake2015.pdf</a></p>
<p>█[60] Koch, Gregory, Richard Zemel, and Ruslan Salakhutdinov. “<strong>Siamese Neural Networks for One-shot Image Recognition.</strong>“(2015) [pdf] ★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.cs.utoronto.ca/%7Egkoch/files/msc-thesis.pdf">http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf</a></p>
<p>█[61] Santoro, Adam, et al. “<strong>One-shot Learning with Memory-Augmented Neural Networks.</strong>“ arXiv preprint arXiv:1605.06065 (2016). [pdf] (one shot 学习的基础一步) ★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1605.06065">http://arxiv.org/pdf/1605.06065</a></p>
<p>█[62] Vinyals, Oriol, et al. “<strong>Matching Networks for One Shot Learning.</strong>“ arXiv preprint arXiv:1606.04080 (2016). [pdf] ★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1606.04080">https://arxiv.org/pdf/1606.04080</a></p>
<p>█[63] Hariharan, Bharath, and Ross Girshick. “<strong>Low-shot visual object recognition.</strong>“ arXiv preprint arXiv:1606.02819 (2016). [pdf] (通向更大规模数据的一步) ★★★★</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1606.02819">http://arxiv.org/pdf/1606.02819</a></p>
<h2 id="3-应用"><a href="#3-应用" class="headerlink" title="3 应用"></a>3 应用</h2><h3 id="3-1-自然语言处理-NLP"><a href="#3-1-自然语言处理-NLP" class="headerlink" title="3.1 自然语言处理 (NLP)"></a>3.1 自然语言处理 (NLP)</h3><p>█[1] Antoine Bordes, et al. “<strong>Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing.</strong>“ AISTATS(2012) [pdf] ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//www.hds.utc.fr/~bordesan/dokuwiki/lib/exe/fetch.php%3Fid%3Den%253Apubli%26cache%3Dcache%26media%3Den%3Abordes12aistats.pdf">https://www.hds.utc.fr/~bordesan/dokuwiki/lib/exe/fetch.php?id=en%3Apubli&amp;cache=cache&amp;media=en:bordes12aistats.pdf</a></p>
<p>█[2] Mikolov, et al. “<strong>Distributed representations of words and phrases and their compositionality.</strong>“ ANIPS(2013): 3111-3119 [pdf] (word2vec) ★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf</a></p>
<p>█[3] Sutskever, et al. ““<strong>Sequence to sequence learning with neural networks.</strong>“ ANIPS(2014) [pdf] ★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf</a></p>
<p>█[4] Ankit Kumar, et al. ““<strong>Ask Me Anything: Dynamic Memory Networks for Natural Language Processing.</strong>“ arXiv preprint arXiv:1506.07285(2015) [pdf] ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1506.07285">https://arxiv.org/abs/1506.07285</a></p>
<p>█[5] Yoon Kim, et al. “<strong>Character-Aware Neural Language Models.</strong>“ NIPS(2015) arXiv preprint arXiv:1508.06615(2015) [pdf] ★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1508.06615">[1508.06615] Character-Aware Neural Language Models</a></p>
<p>█[6] Jason Weston, et al. “<strong>Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks.</strong>“ arXiv preprint arXiv:1502.05698(2015) [pdf] (bAbI tasks) ★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1502.05698">A Set of Prerequisite Toy Tasks</a></p>
<p>█[7] Karl Moritz Hermann, et al. “<strong>Teaching Machines to Read and Comprehend.</strong>“ arXiv preprint arXiv:1506.03340(2015) <a href="CNN/每日邮报完形填空风格的问题">pdf</a> ★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1506.03340">[1506.03340] Teaching Machines to Read and Comprehend</a></p>
<p>█[8] Alexis Conneau, et al. “<strong>Very Deep Convolutional Networks for Natural Language Processing.</strong>“ arXiv preprint arXiv:1606.01781(2016) [pdf] (文本分类的前沿技术) ★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1606.01781">[1606.01781] Very Deep Convolutional Networks for Text Classification</a></p>
<p>█[9] Armand Joulin, et al. “<strong>Bag of Tricks for Efficient Text Classification.</strong>“ arXiv preprint arXiv:1607.01759(2016) [pdf] (比前沿技术稍落后, 但快很多) ★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1607.01759">[1607.01759] Bag of Tricks for Efficient Text Classification</a></p>
<h3 id="3-2-物体检测"><a href="#3-2-物体检测" class="headerlink" title="3.2 物体检测"></a>3.2 物体检测</h3><p>█[1] Szegedy, Christian, Alexander Toshev, and Dumitru Erhan. “<strong>Deep neural networks for object detection</strong>.” Advances in Neural Information Processing Systems. 2013. [pdf] ★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf">http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf</a></p>
<p>█[2] Girshick, Ross, et al. “<strong>Rich feature hierarchies for accurate object detection and semantic segmentation.</strong>“ Proceedings of the IEEE conference on computer vision and pattern recognition. 2014. [pdf] (RCNN) ★★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=http%3A//www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf">http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf</a></p>
<p>█[3] He, Kaiming, et al. “<strong>Spatial pyramid pooling in deep convolutional networks for visual recognition.</strong>“ European Conference on Computer Vision. Springer International Publishing, 2014. [pdf] (SPPNet) ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1406.4729">http://arxiv.org/pdf/1406.4729</a></p>
<p>█[4] Girshick, Ross. “<strong>Fast r-cnn.</strong>“ Proceedings of the IEEE International Conference on Computer Vision. 2015. [pdf] ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//pdfs.semanticscholar.org/8f67/64a59f0d17081f2a2a9d06f4ed1cdea1a0ad.pdf">https://pdfs.semanticscholar.org/8f67/64a59f0d17081f2a2a9d06f4ed1cdea1a0ad.pdf</a></p>
<p>█[5] Ren, Shaoqing, et al. “<strong>Faster R-CNN: Towards real-time object detection with region proposal networks.</strong>“ Advances in neural information processing systems. 2015. [pdf] ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5638-analysis-of-variational-bayesian-latent-dirichlet-allocation-weaker-sparsity-than-map.pdf">http://papers.nips.cc/paper/5638-analysis-of-variational-bayesian-latent-dirichlet-allocation-weaker-sparsity-than-map.pdf</a></p>
<p>█[6] Redmon, Joseph, et al. “<strong>You only look once: Unified, real-time object detection.</strong>“ arXiv preprint arXiv:1506.02640 (2015). [pdf] (YOLO，杰出研究，非常具有使用价值） ★★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=http%3A//homes.cs.washington.edu/~ali/papers/YOLO.pdf">http://homes.cs.washington.edu/~ali/papers/YOLO.pdf</a></p>
<p>█[7] Liu, Wei, et al. “<strong>SSD: Single Shot MultiBox Detector.</strong>“ arXiv preprint arXiv:1512.02325 (2015). [pdf] ★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1512.02325">http://arxiv.org/pdf/1512.02325</a></p>
<p>█[8] Dai, Jifeng, et al. “<strong>R-FCN: Object Detection via Region-based Fully Convolutional Networks.</strong>“ arXiv preprint arXiv:1605.06409 (2016). [pdf] ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1605.06409">Object Detection via Region-based Fully Convolutional Networks</a></p>
<h3 id="3-3-视觉追踪"><a href="#3-3-视觉追踪" class="headerlink" title="3.3 视觉追踪"></a>3.3 视觉追踪</h3><p>█[1] Wang, Naiyan, and Dit-Yan Yeung. “<strong>Learning a deep compact image representation for visual tracking.</strong>“ Advances in neural information processing systems. 2013. [pdf] (第一篇使用深度学习做视觉追踪的论文，DLT Tracker) ★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5192-learning-a-deep-compact-image-representation-for-visual-tracking.pdf">http://papers.nips.cc/paper/5192-learning-a-deep-compact-image-representation-for-visual-tracking.pdf</a></p>
<p>█[2] Wang, Naiyan, et al. “<strong>Transferring rich feature hierarchies for robust visual tracking.</strong>“ arXiv preprint arXiv:1501.04587 (2015). [pdf] (SO-DLT) ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1501.04587">http://arxiv.org/pdf/1501.04587</a></p>
<p>█[3] Wang, Lijun, et al. “<strong>Visual tracking with fully convolutional networks.</strong>“ Proceedings of the IEEE International Conference on Computer Vision. 2015. [pdf] (FCNT) ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=http%3A//www.cv-foundation.org/openaccess/content_iccv_2015/papers/Wang_Visual_Tracking_With_ICCV_2015_paper.pdf">http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Wang_Visual_Tracking_With_ICCV_2015_paper.pdf</a></p>
<p>█[4] Held, David, Sebastian Thrun, and Silvio Savarese. “<strong>Learning to Track at 100 FPS with Deep Regression Networks.</strong>“ arXiv preprint arXiv:1604.01802 (2016). [pdf] (GOTURN，在深度学习方法里算是非常快的，但仍比非深度学习方法慢很多) ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1604.01802">http://arxiv.org/pdf/1604.01802</a></p>
<p>█[5] Bertinetto, Luca, et al. “<strong>Fully-Convolutional Siamese Networks for Object Tracking.</strong>“ arXiv preprint arXiv:1606.09549 (2016). [pdf] (SiameseFC，实时物体追踪领域的最新前沿技术) ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1606.09549">https://arxiv.org/pdf/1606.09549</a></p>
<p>█[6] Martin Danelljan, Andreas Robinson, Fahad Khan, Michael Felsberg. “<strong>Beyond Correlation Filters: Learning Continuous Convolution Operators for Visual Tracking.</strong>“ ECCV (2016) [pdf] (C-COT) ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=http%3A//www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/C-COT_ECCV16.pdf">http://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/C-COT_ECCV16.pdf</a></p>
<p>█[7] Nam, Hyeonseob, Mooyeol Baek, and Bohyung Han. “<strong>Modeling and Propagating CNNs in a Tree Structure for Visual Tracking.</strong>“ arXiv preprint arXiv:1608.07242 (2016). [pdf] (VOT2016 获奖论文,TCNN) ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1608.07242">https://arxiv.org/pdf/1608.07242</a></p>
<h3 id="3-4-图像标注"><a href="#3-4-图像标注" class="headerlink" title="3.4 图像标注"></a>3.4 图像标注</h3><p>█[1] Farhadi,Ali,etal. “<strong>Every picture tells a story: Generating sentences from images</strong>“. In Computer VisionECCV 2010. Springer Berlin Heidelberg:15-29, 2010. [pdf] ★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//www.cs.cmu.edu/~afarhadi/papers/sentence.pdf">https://www.cs.cmu.edu/~afarhadi/papers/sentence.pdf</a></p>
<p>█[2] Kulkarni, Girish, et al. “<strong>Baby talk: Understanding and generating image descriptions</strong>“. In Proceedings of the 24th CVPR, 2011. [pdf] ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=http%3A//tamaraberg.com/papers/generation_cvpr11.pdf">http://tamaraberg.com/papers/generation_cvpr11.pdf</a></p>
<p>█[3] Vinyals, Oriol, et al. “<strong>Show and tell: A neural image caption generator</strong>“. In arXiv preprint arXiv:1411.4555, 2014. [pdf] ★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1411.4555.pdf">https://arxiv.org/pdf/1411.4555.pdf</a></p>
<p>█[4] Donahue, Jeff, et al. “<strong>Long-term recurrent convolutional networks for visual recognition and description</strong>“. In arXiv preprint arXiv:1411.4389 ,2014. [pdf]</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1411.4389.pdf">https://arxiv.org/pdf/1411.4389.pdf</a></p>
<p>█[5] Karpathy, Andrej, and Li Fei-Fei. “<strong>Deep visual-semantic alignments for generating image descriptions</strong>“. In arXiv preprint arXiv:1412.2306, 2014. [pdf] ★★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//cs.stanford.edu/people/karpathy/cvpr2015.pdf">https://cs.stanford.edu/people/karpathy/cvpr2015.pdf</a></p>
<p>█[6] Karpathy, Andrej, Armand Joulin, and Fei Fei F. Li. “<strong>D**</strong>eep fragment embeddings for bidirectional image sentence mapping**”. In Advances in neural information processing systems, 2014. [pdf] ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1406.5679v1.pdf">https://arxiv.org/pdf/1406.5679v1.pdf</a></p>
<p>█[7] Fang, Hao, et al. “<strong>From captions to visual concepts and back</strong>“. In arXiv preprint arXiv:1411.4952, 2014. [pdf] ★★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1411.4952v3.pdf">https://arxiv.org/pdf/1411.4952v3.pdf</a></p>
<p>█[8] Chen, Xinlei, and C. Lawrence Zitnick. “<strong>Learning a recurrent visual representation for image caption generation</strong>“. In arXiv preprint arXiv:1411.5654, 2014. [pdf] ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1411.5654v1.pdf">https://arxiv.org/pdf/1411.5654v1.pdf</a></p>
<p>█[9] Mao, Junhua, et al. “<strong>Deep captioning with multimodal recurrent neural networks (m-rnn)</strong>“. In arXiv preprint arXiv:1412.6632, 2014. [pdf] ★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1412.6632v5.pdf">https://arxiv.org/pdf/1412.6632v5.pdf</a></p>
<p>█[10] Xu, Kelvin, et al. “<strong>Show, attend and tell: Neural image caption generation with visual attention</strong>“. In arXiv preprint arXiv:1502.03044, 2015. [pdf] ★★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1502.03044v3.pdf">https://arxiv.org/pdf/1502.03044v3.pdf</a></p>
<h3 id="3-5-机器翻译"><a href="#3-5-机器翻译" class="headerlink" title="3.5 机器翻译"></a>3.5 机器翻译</h3><p>部分里程碑研究被列入 RNN / Seq-to-Seq 版块。</p>
<p>█[1] Luong, Minh-Thang, et al. “<strong>Addressing the rare word problem in neural machine translation.</strong>“ arXiv preprint arXiv:1410.8206 (2014). [pdf] ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1410.8206">http://arxiv.org/pdf/1410.8206</a></p>
<p>█[2] Sennrich, et al. <strong>“Neural Machine Translation of Rare Words with Subword Units</strong>“. In arXiv preprint arXiv:1508.07909, 2015. [pdf] ★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1508.07909.pdf">https://arxiv.org/pdf/1508.07909.pdf</a></p>
<p>█[3] Luong, Minh-Thang, Hieu Pham, and Christopher D. Manning. “<strong>Effective approaches to attention-based neural machine translation</strong>.” arXiv preprint arXiv:1508.04025 (2015). [pdf] ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1508.04025">http://arxiv.org/pdf/1508.04025</a></p>
<p><strong>█</strong>[4] Chung, et al. “<strong>A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation</strong>“. In arXiv preprint arXiv:1603.06147, 2016. [pdf] ★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1603.06147.pdf">https://arxiv.org/pdf/1603.06147.pdf</a></p>
<p>█[5] Lee, et al. “<strong>Fully Character-Level Neural Machine Translation without Explicit Segmentation</strong>“. In arXiv preprint arXiv:1610.03017, 2016. [pdf] ★★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1610.03017.pdf">https://arxiv.org/pdf/1610.03017.pdf</a></p>
<p>█[6] Wu, Schuster, Chen, Le, et al. “<strong>Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</strong>“. In arXiv preprint arXiv:1609.08144v2, 2016. [pdf] (Milestone) ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1609.08144v2.pdf">https://arxiv.org/pdf/1609.08144v2.pdf</a></p>
<h3 id="3-6-机器人"><a href="#3-6-机器人" class="headerlink" title="3.6 机器人"></a>3.6 机器人</h3><p>█[1] Koutník, Jan, et al. “<strong>Evolving large-scale neural networks for vision-based reinforcement learning.</strong>“ Proceedings of the 15th annual conference on Genetic and evolutionary computation. ACM, 2013. [pdf] ★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=http%3A//repository.supsi.ch/4550/1/koutnik2013gecco.pdf">http://repository.supsi.ch/4550/1/koutnik2013gecco.pdf</a></p>
<p>█[2] Levine, Sergey, et al. “<strong>End-to-end training of deep visuomotor policies.</strong>“ Journal of Machine Learning Research 17.39 (2016): 1-40. [pdf] ★★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=http%3A//www.jmlr.org/papers/volume17/15-522/15-522.pdf">http://www.jmlr.org/papers/volume17/15-522/15-522.pdf</a></p>
<p>█[3] Pinto, Lerrel, and Abhinav Gupta. “<strong>Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours.</strong>“ arXiv preprint arXiv:1509.06825 (2015). [pdf] ★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1509.06825">http://arxiv.org/pdf/1509.06825</a></p>
<p>█[4] Levine, Sergey, et al. “<strong>Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection</strong>.” arXiv preprint arXiv:1603.02199 (2016). [pdf] ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1603.02199">http://arxiv.org/pdf/1603.02199</a></p>
<p>█[5] Zhu, Yuke, et al. “<strong>Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning.</strong>“ arXiv preprint arXiv:1609.05143 (2016). [pdf] ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1609.05143">https://arxiv.org/pdf/1609.05143</a></p>
<p>█[6] Yahya, Ali, et al. “<strong>Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search.</strong>“ arXiv preprint arXiv:1610.00673 (2016). [pdf] ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1610.00673">https://arxiv.org/pdf/1610.00673</a></p>
<p>█[7] Gu, Shixiang, et al. “<strong>Deep Reinforcement Learning for Robotic Manipulation.</strong>“ arXiv preprint arXiv:1610.00633 (2016). [pdf] ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1610.00633">https://arxiv.org/pdf/1610.00633</a></p>
<p>█[8] A Rusu, M Vecerik, Thomas Rothörl, N Heess, R Pascanu, R Hadsell.”<strong>Sim-to-Real Robot Learning from Pixels with Progressive Nets.</strong>“ arXiv preprint arXiv:1610.04286 (2016). [pdf] ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1610.04286.pdf">https://arxiv.org/pdf/1610.04286.pdf</a></p>
<p>█[9] Mirowski, Piotr, et al. “<strong>Learning to navigate in complex environments.</strong>“ arXiv preprint arXiv:1611.03673 (2016). [pdf] ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1611.03673">https://arxiv.org/pdf/1611.03673</a></p>
<h3 id="3-7-艺术"><a href="#3-7-艺术" class="headerlink" title="3.7 艺术"></a>3.7 艺术</h3><p>█[1] Mordvintsev, Alexander; Olah, Christopher; Tyka, Mike (2015). “<strong>Inceptionism: Going Deeper into Neural Networks</strong>“. Google Research. [html] (Deep Dream) ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html">https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html</a></p>
<p>█[2] Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. “<strong>A neural algorithm of artistic style.</strong>“ arXiv preprint arXiv:1508.06576 (2015). [pdf] (杰出研究，迄今最成功的方法) ★★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1508.06576">http://arxiv.org/pdf/1508.06576</a></p>
<p>█[3] Zhu, Jun-Yan, et al. “<strong>Generative Visual Manipulation on the Natural Image Manifold.</strong>“ European Conference on Computer Vision. Springer International Publishing, 2016. [pdf] (iGAN) ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1609.03552">https://arxiv.org/pdf/1609.03552</a></p>
<p>█[4] Champandard, Alex J. “<strong>Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks.</strong>“ arXiv preprint arXiv:1603.01768 (2016). [pdf] (Neural Doodle) ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1603.01768">http://arxiv.org/pdf/1603.01768</a></p>
<p>█[5] Zhang, Richard, Phillip Isola, and Alexei A. Efros. “<strong>Colorful Image Colorization</strong>.” arXiv preprint arXiv:1603.08511 (2016). [pdf] ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1603.08511">http://arxiv.org/pdf/1603.08511</a></p>
<p>█[6] Johnson, Justin, Alexandre Alahi, and Li Fei-Fei. “<strong>Perceptual losses for real-time style transfer and super-resolution</strong>.” arXiv preprint arXiv:1603.08155 (2016). [pdf] ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1603.08155.pdf">https://arxiv.org/pdf/1603.08155.pdf</a></p>
<p>█[7] Vincent Dumoulin, Jonathon Shlens and Manjunath Kudlur. “<strong>A learned representation for artistic style.</strong>“ arXiv preprint arXiv:1610.07629 (2016). [pdf] ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1610.00633">https://arxiv.org/pdf/1610.00633</a></p>
<p>█[8] Gatys, Leon and Ecker, et al.”<strong>Controlling Perceptual Factors in Neural Style Transfer.</strong>“ arXiv preprint arXiv:1611.07865 (2016). [pdf] (control style transfer over spatial location,colour information and across spatial scale) ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1610.04286.pdf">https://arxiv.org/pdf/1610.04286.pdf</a></p>
<p>█[9] Ulyanov, Dmitry and Lebedev, Vadim, et al. “<strong>Texture Networks: Feed-forward Synthesis of Textures and Stylized Images.</strong>“ arXiv preprint arXiv:1603.03417(2016). [pdf] (纹理生成和风格变化) ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1611.03673">https://arxiv.org/pdf/1611.03673</a></p>
<h3 id="3-8-目标分割-Object-Segmentation"><a href="#3-8-目标分割-Object-Segmentation" class="headerlink" title="3.8 目标分割 Object Segmentation"></a>3.8 目标分割 Object Segmentation</h3><p>█[1] J. Long, E. Shelhamer, and T. Darrell, “<strong>Fully convolutional networks for semantic segmentation.</strong>” in CVPR, 2015. [pdf] ★★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1411.4038v2.pdf">https://arxiv.org/pdf/1411.4038v2.pdf</a></p>
<p>█[2] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. “<strong>Semantic image segmentation with deep convolutional nets and fully connected crfs.</strong>“ In ICLR, 2015. [pdf] ★★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1606.00915v1.pdf">https://arxiv.org/pdf/1606.00915v1.pdf</a></p>
<p>█[3] Pinheiro, P.O., Collobert, R., Dollar, P. “<strong>Learning to segment object candidates.</strong>“ In: NIPS. 2015. [pdf] ★★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1506.06204v2.pdf">https://arxiv.org/pdf/1506.06204v2.pdf</a></p>
<p>█[4] Dai, J., He, K., Sun, J. <strong>“Instance-aware semantic segmentation via multi-task network cascades.</strong>“ in CVPR. 2016 [pdf] ★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1512.04412v1.pdf">https://arxiv.org/pdf/1512.04412v1.pdf</a></p>
<p>█[5] Dai, J., He, K., Sun, J. “<strong>Instance-sensitive Fully Convolutional Networks.</strong>“ arXiv preprint arXiv:1603.08678 (2016). [pdf] ★★★</p>
<p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1603.08678v1.pdf">https://arxiv.org/pdf/1603.08678v1.pdf</a></p>

            <hr>
          </div>
          <br>
          <div>
            <p>
            
              <span>
                <i class="iconfont icon-inbox"></i>
                
                  <a class="hover-with-bg" href="/categories/NLP%E5%8F%91%E6%96%87">NLP发文</a>
                  &nbsp;
                
              </span>&nbsp;&nbsp;
            
            
              <span>
                <i class="iconfont icon-tag"></i>
                
                  <a class="hover-with-bg" href="/tags/%E8%AE%BA%E6%96%87%E5%86%99%E6%B3%95">论文写法</a>
                
              </span>
            
            </p>
            
              <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" target="_blank" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
            
          </div>
        </div>
      </div>
    </div>
    <div class="d-none d-lg-block col-lg-2 toc-container">
      
  <div id="toc">
    <p class="h4"><i class="far fa-list-alt"></i>&nbsp;目录</p>
    <div id="tocbot"></div>
  </div>

    </div>
  </div>
</div>

<!-- custom -->


<!-- Comments -->
<div class="col-lg-7 mx-auto nopadding-md">
  <div class="container comments mx-auto" id="comments">
    
  </div>
</div>

    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>

    
  
    <!-- 不蒜子统计PV -->
    
    &nbsp;<span id="busuanzi_container_site_pv">总访问量 
          <span id="busuanzi_value_site_pv"></span> 次</span>&nbsp;
  
  
    <!-- 不蒜子统计UV -->
    
    &nbsp;<span id="busuanzi_container_site_uv">总访客数 
            <span id="busuanzi_value_site_uv"></span> 人</span>&nbsp;
  
  <br>



    

  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js" ></script>
<script src="/lib/popper/popper.min.js" ></script>
<script src="/lib/bootstrap/js/bootstrap.min.js" ></script>
<script src="/lib/mdbootstrap/js/mdb.min.js" ></script>
<script src="/js/main.js" ></script>




  
    <script src="/lib/tocbot/tocbot.min.js" ></script>
  
  <script src="/js/post.js" ></script>



  <script src="/lib/smooth-scroll/smooth-scroll.min.js" ></script>



  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>


<!-- Plugins -->


  

  

  

  

  




  <script src="/lib/prettify/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "NLP发文/深度学习论文集[Done]&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 120,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="/lib/anchor/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "false",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>







</body>
</html>

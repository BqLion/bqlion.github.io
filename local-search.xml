<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>3.课程/正则表达式</title>
    <link href="undefined2020/05/25/3.%E8%AF%BE%E7%A8%8B/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    <url>2020/05/25/3.%E8%AF%BE%E7%A8%8B/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>正则表达式</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2.比赛/Toxic_Abhi_Bert_Senti</title>
    <link href="undefined2020/05/25/2.%E6%AF%94%E8%B5%9B/Toxic_Abhi_Bert_Senti/"/>
    <url>2020/05/25/2.%E6%AF%94%E8%B5%9B/Toxic_Abhi_Bert_Senti/</url>
    
    <content type="html"><![CDATA[<h1 id="MultiLinguToxic-Abhishek-Thakur-s-Guide"><a href="#MultiLinguToxic-Abhishek-Thakur-s-Guide" class="headerlink" title="MultiLinguToxic : Abhishek Thakur`s Guide"></a>MultiLinguToxic : Abhishek Thakur`s Guide</h1><h3 id="数据探索"><a href="#数据探索" class="headerlink" title="数据探索"></a>数据探索</h3><h4 id="基本文件"><a href="#基本文件" class="headerlink" title="基本文件"></a>基本文件</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200331_1144_50_792.png" alt=""></p><p>如上是本比赛需要的数据，标橙的是主要数据。其中第一行和第二行的两个train数据来自之前jigsaw举办的比赛<br>两个128长度的seq for BERT也许不是我们需要的东西。</p><h4 id="数据列分析"><a href="#数据列分析" class="headerlink" title="数据列分析"></a>数据列分析</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200331_1148_30_471.png" alt=""></p><p>如上是数据不同的列，分别表示了id，真实评论，语言类型，是否是toxic的标签。</p><h4 id="对train-csv的一瞥"><a href="#对train-csv的一瞥" class="headerlink" title="对train.csv的一瞥"></a>对train.csv的一瞥</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200331_1151_18_648.png" alt=""></p><p>可以看到，数据的分列情况就是id+评论+是否toxic+是否严重toxic+是否淫秽。</p><h4 id="bert-sentiment"><a href="#bert-sentiment" class="headerlink" title="bert-sentiment"></a>bert-sentiment</h4><p>bert-sentiment用的是IMDb数据集，在github上star,Abhi出了三集视频专门介绍这个project。本次比赛可以直接调用。</p>]]></content>
    
    
    <categories>
      
      <category>2.比赛</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Toxic</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2.比赛/指导_特征提取与特征选择</title>
    <link href="undefined2020/05/25/2.%E6%AF%94%E8%B5%9B/%E6%8C%87%E5%AF%BC_%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B8%8E%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
    <url>2020/05/25/2.%E6%AF%94%E8%B5%9B/%E6%8C%87%E5%AF%BC_%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B8%8E%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/</url>
    
    <content type="html"><![CDATA[<h1 id="特征提取与特征选择"><a href="#特征提取与特征选择" class="headerlink" title="特征提取与特征选择"></a>特征提取与特征选择</h1><p>这两者的效果是一样的，都是试图去减少特征数据集中的属性（特征）的数目，但是两者采用的方法不同，</p><p>特征提取的方法是通过属性之间的关系，通过组合不同的属性得到新的属性，改变了特征空间</p><p>特征选择的方法是选择出特征集的子集，是一种包含的关系，没有改变特征空间</p><h4 id="1-2特征提取的主要方法"><a href="#1-2特征提取的主要方法" class="headerlink" title="1.2特征提取的主要方法"></a>1.2特征提取的主要方法</h4><p>PCA,LDA,SVD等</p><h4 id="1-3特征选择的主要方法"><a href="#1-3特征选择的主要方法" class="headerlink" title="1.3特征选择的主要方法"></a>1.3特征选择的主要方法</h4><p>1.fliter方法</p><p>对每个特征打分，即给出每一维的特征赋予权重，该权重代表了特征的重要性，然后根据权重排序。</p><p>主要的方法：卡方，信息增益，相关系数</p><p>2.Wrapper方法</p><p>主要思想：将子集的选择看成一个搜索寻优问题，生成不同的组合，对组合进行评论</p><p>可看做一个优化问题，有很多优化算法可供使用</p><p>尤其是启发式算法GA，PSO,DE,ABC等等</p><p>主要方法：递归特征消除方法</p><p>3.Embedded方法</p><p>主要思想：在模型既定的情况下，学习出对提高模型准确性最好的属性，即在确定模型的过程中，挑选出哪些对模型训练有重要意义的属性</p><p>主要方法：正则化，如岭回归就是在回归的基础上加上了正则化项</p><h4 id="1-4总结"><a href="#1-4总结" class="headerlink" title="1.4总结"></a>1.4总结</h4><p>1.特征提取是从杂乱无章的世界中，去到更高的世界俯瞰原有世界，这时会发现很多杂乱无章的物理现象背后暗含的道理是相通的，这时会想用一个更普世的观点和理论去解释原有的世界</p><p>2.特征选择是待在原有的世界里，只想对现有的维度取其精华去其糟粕，这就是特征选择，只是对现有进行筛选</p><p>3.特征提取和特征选择统称为降维</p><h1 id="2-文本提取特征"><a href="#2-文本提取特征" class="headerlink" title="2.文本提取特征"></a>2.文本提取特征</h1><h4 id="2-1-TF-IDF"><a href="#2-1-TF-IDF" class="headerlink" title="2.1    TF-IDF"></a>2.1    TF-IDF</h4><p>算法介绍：</p><p>词频-逆向文件频率，可以体现一个词在语料库中重要的程度</p><p>词频TF表示词语t在文档d中出现的次数，文件频率DF是语料库中包含词语的文档个数</p><p>如果只使用词频来衡量重要性，很容易放大“is”“a”之类词语的重要性</p><p>所以需要加上文件频率，文件频率越低，说明本词语携带的特殊信息越多越有标识度</p><p>公式</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1440_30_231.png" alt=""></p><p>TF-IDF实施步骤</p><p>1.获取原始文本内容信息</p><p>2.转换成小写，按空格将文章分成独立的词语组成的list</p><p>3.去除噪音，如： \ =  , : ;  : \n等等</p><p>4.去除stop-word</p><p>5.提取词干 ： goes,go,went,going 都换成go</p><p>6.wordcount，统计每个词出现的次数，去掉出现次数过少的词语，比如一百篇文档出现了1-2次的词语一般无用</p><p>7.训练idf模型</p><p>8.对输入的每篇测试文章计算tf-idf向量，利用tfidf向量求文章之间的相似度（欧拉距离，余弦相似度，jaccard系数等）</p><h3 id="2-2词袋模型"><a href="#2-2词袋模型" class="headerlink" title="2.2词袋模型"></a>2.2词袋模型</h3><ul><li>词袋模型的问题是丢失了语序</li><li>对于词频率特别高的词，会频繁出现但是意义不大</li></ul><h3 id="2-3词向量wordToVector"><a href="#2-3词向量wordToVector" class="headerlink" title="2.3词向量wordToVector"></a>2.3词向量wordToVector</h3><p>将词语映射到高维空间中，意思相近的词语在空间中的距离也近</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1441_00_995.png" alt=""></p><h3 id="3-kaggle项目实战"><a href="#3-kaggle项目实战" class="headerlink" title="3.kaggle项目实战"></a>3.kaggle项目实战</h3><h4 id="–sentiment-analysis-on-movie-reviews"><a href="#–sentiment-analysis-on-movie-reviews" class="headerlink" title="–sentiment analysis on movie reviews"></a>–sentiment analysis on movie reviews</h4><h4 id="3-1明确任务，数据探索"><a href="#3-1明确任务，数据探索" class="headerlink" title="3.1明确任务，数据探索"></a>3.1明确任务，数据探索</h4><p>任务：给出了电影评论短语，已按照情感程度打分（0-4），我需要对未打分的短语打分</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1441_11_885.png" alt=""></p><pre><code class="python">data_train = pd.read_csv(&#39;./train.csv&#39;,sep = &#39;\t&#39;)data_test = pd.read_csv(&#39;./test.csv&#39;,sep = &#39;\t&#39;)</code></pre><p>首先读入数据，sep=’\t’是指数据按照制表符分割</p><pre><code class="python">data_train.head()</code></pre><p>显示前五行数据</p><pre><code class="python">data_train.shape</code></pre><p>显示数据有多少行，多少列</p><h4 id="3-2构建语料库"><a href="#3-2构建语料库" class="headerlink" title="3.2构建语料库"></a>3.2构建语料库</h4><ul><li>将文本转换成计算机看得懂的向量，这一过程叫文本的特征工程</li><li>有很多将词转换成向量的方法，下面用的是word2vec，还有词袋模型和tf-idf模型</li><li>不管采用什么模型，都要把训练集和测试集所有文本内容组合在一起，构建一个语料库</li></ul><ul><li><p>首先将train和test数据集中的文字列合并</p><pre><code class="python">train_sentences = data_train[&#39;Phrase&#39;]test_sentences = data_test[&#39;Phrase&#39;]sentences = pd.concat([train_sentences,test_sentences])sentences.shape</code></pre></li><li><p>然后提出打分那一列</p><pre><code class="python">label = data_train[&#39;Sentiment&#39;]label.shape</code></pre></li><li><p>把stopwords删除</p><pre><code class="python">stop_words = open(&#39;/home/bq/IdeaProjects/python/SentimentOnMovie/data/stop_words.txt&#39;,encoding = &#39;utf-8&#39;).read().splitlines()stop_words</code></pre></li><li><p>用词袋模型构建语料库</p><pre><code class="python">co = CountVectorizer(     analyzer=&#39;word&#39;,     ngram_range = (1,4),     stop_words = stop_words,     max_features = 150000)co.fit(sentences)</code></pre></li><li><p>2比1的分割训练集，分割为训练集和验证集，交叉验证</p><pre><code class="python">x_train,x_test,y_train,y_test = train_test_split(train_sentences,label,random_state=1234)x_train = co.transform(x_train)x_test = co.transform(x_test)x_train[1]</code></pre></li><li><p>逻辑回归</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1441_26_989.png" alt=""></p></li><li><p>忽略版本问题出现的警告</p><pre><code class="python">import warningswarnings.filterwarnings(&#39;ignore&#39;)</code></pre></li><li><p>分析结果，做预测</p><pre><code class="python">from sklearn.linear_model import LogisticRegressionlg1 = LogisticRegression()lg1.fit(x_train,y_train)print(&#39;log regression from sklearn,the prescion is:&#39;,lg1.score(x_test,y_test))</code></pre></li></ul><ul><li><p>把词袋模型换成tf-idf模型,用逻辑回归做分类器</p><pre><code class="python">from sklearn.feature_extraction.text import TfidfVectorizertf = TfidfVectorizer(    analyzer=&#39;word&#39;,    ngram_range=(1,4),    max_features = 150000)tf.fit(sentences)lg2  = LogisticRegression()lg2.fit(x_train,y_train)print(&#39;using TDIDF model to do feature engnieer,using classifer of logi regresiion&#39;,lg2.score(x_test,y_test))</code></pre><p>逻辑回归加上正则化项，结果变好</p><pre><code class="python">######tf-idf to do feature engineer,using the normalization of log#####lg3 = LogisticRegression(C = 3,dual = True)lg3.fit(x_train,y_train)print(&#39;TF-IDF doing feature engineer,using normalizaion&#39;,lg3.score(x_test,y_test))</code></pre><p>sklearn有网格化参数搜索功能，可以在C从1到10，dual从正确到错误上遍历搜索</p><pre><code class="python">from sklearn.model_selection import GridSearchCVparam_grid = {&#39;C&#39;:range(1,10),              &#39;dual&#39;:[True,False]              }lgGS = LogisticRegression()grid = GridSearchCV(lgGS,param_grid = param_grid,cv = 3,n_jobs=-1)grid.fit(x_train,y_train)</code></pre></li></ul><p>输出上文计算的最优参数</p><pre><code class="python">  grid.best_params_  lg_final = grid.best_estimator_  print(&#39;after sklearn-search,using best params,the result is :&#39;,lg_final.score(x_test,y_test))</code></pre><p>最终输出结果</p><pre><code class="python">data_test.head()text_X = tf.transform(data_test[&#39;Phrase&#39;])predictions = lg_final.predict(text_X)predictionspredictions.shape data_test.loc[:,&#39;Sentiment&#39;] = predictionsdata_test.head()final_data = data_test.loc[:,[&#39;PhraseId&#39;,&#39;Sentiment&#39;]]final_data.head()final_data.to_csv(&#39;final_data.csv&#39;,index = None)</code></pre><p>竞赛在复赛一般要考虑模型 融合，前期可能不需要，但是后期不融合可能走不远</p><p>投票法，stacking，blending等</p><p>特征提取有四大部分</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1438_37_938.png" alt=""></p><p>​    </p><ul><li><p>Counting Features –词数的统计</p></li><li><p>DIstance Features –数据集query,desc,counting之间的的距离  :  </p></li><li><p>TF - IDF Features –  词频–逆文本频率指数(一个词在本文频率越高,在语料库中出现在文件中频率越低,说明越有标识度)</p><ul><li><p>例如”母牛”在本文出现三次,本文一百个单词,则在本文词频0.03</p></li><li><p>“母牛”在语料库中出现在1000份文件中,语料库共有1000,0000文件,逆向文件频率就是</p><p>lg(1000,0000 / 1000)=4</p></li><li><p>“母牛的”TF-IDF = 0.03*4 = 0.12    </p></li></ul></li><li><p>Query ID  –  给primary key换成独热编码</p></li></ul><h3 id="1-Query-ID特征"><a href="#1-Query-ID特征" class="headerlink" title="1.Query ID特征:"></a>1.Query ID特征:</h3><p>把primary key转换为one hot encoding</p><h3 id="2-Counting特征"><a href="#2-Counting特征" class="headerlink" title="2.Counting特征"></a>2.Counting特征</h3><p>-基本Counting特征</p><ul><li><p>Count of n-gram</p><ul><li>count of ngram(query,n),ngram(title,n),and ngram(description,n)</li><li>Count &amp; Ratio of Unique n-gram</li><li>Description Missing Indicator</li></ul></li><li><p>Count &amp; Ratio of a<code>s n-gram in b</code>s n-gram </p></li><li><p>Statisticcs of position of a<code>s n-gram in b</code>s n-gram</p></li><li><p>Statisics of normalized position of a<code>s n-gram in b</code>s n-gram</p></li></ul><h3 id="3-距离特征"><a href="#3-距离特征" class="headerlink" title="3.距离特征"></a>3.距离特征</h3><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1438_54_504.png" alt=""></p><ul><li><p>数据集的距离</p><ul><li><p>Jaccard coefficient:雅克比距离</p><p>计算法则:AB两个数据集相交的数据个数 除以  AB两个数据集相并的个数</p><p>例如A = [1,2,3,4,] B = [1,2,5,6]</p><p>那么雅克比距离 =  2/6  =三分之一</p></li><li><p>Dice 距离:</p><p>计算法则:两倍的交集除以两数据集元素个数</p><p>例如例如A = [1,2,3,4,] B = [1,2,5,6]</p><p>dice距离为4除以8 = 二分之一</p></li></ul></li></ul><ul><li>D(ngram(query,n),ngram(title,n))            查询与产品名称的距离</li></ul><ul><li>D(ngram(query,n),ngram(description,n))           查询与产品描述的距离</li></ul><ul><li>D(ngram(title,n),ngram(description,n))            产品描述与产品名称的距离</li></ul><h3 id="4-tf-idf特征"><a href="#4-tf-idf特征" class="headerlink" title="4.tf-idf特征"></a>4.tf-idf特征</h3><p>基本TF-IDF特征</p><p>-TF-IDF 特征</p><p>-Basic cosine 相似度</p><p>-statistical cosine 相似度</p><p>-SVD reduce相似度</p><p>-Basic cosine similarity based on SVD reduce features</p><p>-Statistical cosine similarity based on SVD reduce features</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1439_05_660.png" alt=""></p><p>​    </p>]]></content>
    
    
    <categories>
      
      <category>2.比赛</category>
      
    </categories>
    
    
    <tags>
      
      <tag>特征提取</tag>
      
      <tag>特征选择</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/线性代数_正交归一性</title>
    <link href="undefined2020/05/17/3.%E8%AF%BE%E7%A8%8B/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0_%E6%AD%A3%E4%BA%A4%E5%BD%92%E4%B8%80%E6%80%A7/"/>
    <url>2020/05/17/3.%E8%AF%BE%E7%A8%8B/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0_%E6%AD%A3%E4%BA%A4%E5%BD%92%E4%B8%80%E6%80%A7/</url>
    
    <content type="html"><![CDATA[<h1 id="正交归一性"><a href="#正交归一性" class="headerlink" title="正交归一性"></a>正交归一性</h1><p>如果内积空间的两个向量是互相正交的，并且两个向量的范数都是1，则称这两个向量互相具有正交归一性/正交规范性。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>线代</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/线性代数_归一化</title>
    <link href="undefined2020/05/17/3.%E8%AF%BE%E7%A8%8B/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0_%E5%BD%92%E4%B8%80%E5%8C%96/"/>
    <url>2020/05/17/3.%E8%AF%BE%E7%A8%8B/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0_%E5%BD%92%E4%B8%80%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h1><p>归一化消除了量纲，是处理数据的一种手段。通过归一化最优解的寻找过程会变得平缓，更容易正确的收敛到最优解。</p><p>下图的例子是房价预测模型，横坐标是房间数量（0 - 10），纵坐标是面积大小（0 - 1000），预测结果是等高线上的没画出的第三维。</p><p>图一未归一化，图像是长椭圆，图二归一化后是正圆（拍扁了数据），可见归一化后收敛过程更平缓。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200517_1514_26_365.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200517_1514_45_488.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>线代</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2.比赛/Toxic_思路 </title>
    <link href="undefined2020/05/07/2.%E6%AF%94%E8%B5%9B/Toxic_%E6%80%9D%E8%B7%AF%20/"/>
    <url>2020/05/07/2.%E6%AF%94%E8%B5%9B/Toxic_%E6%80%9D%E8%B7%AF%20/</url>
    
    <content type="html"><![CDATA[<h1 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h1><h4 id="ToDoList"><a href="#ToDoList" class="headerlink" title="ToDoList"></a>ToDoList</h4><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>bpe</td><td>跨语言编码</td></tr><tr><td>XLM-R + ERNIE</td><td>kaggle最热跨语言模型 + 注入特定知识toxic</td></tr><tr><td>transformer</td><td><strong>特征抽取器(模型)要向匹配问题领域的特点去修改</strong></td></tr><tr><td>lstm</td><td>transformer组件</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h4 id="选择正确的特征抽取器"><a href="#选择正确的特征抽取器" class="headerlink" title="选择正确的特征抽取器"></a>选择正确的特征抽取器</h4><p>解决情感分类问题，从模型角度讲，最重要的是特征抽取器的能力。以前是研发人员设计抽取哪些特征，现在都是端到端的抽取，也就是<strong>特征抽取器Transformer自动抽取</strong>。根据今日阅读知乎和github上的观点，不要使用RNN和CNN，应该采用更加先进的Transformer。</p>]]></content>
    
    
    <categories>
      
      <category>2.比赛</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Toxic</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224n_9 - 机翻</title>
    <link href="undefined2020/05/07/3.%E8%AF%BE%E7%A8%8B/CS224n_9%20-%20%E6%9C%BA%E7%BF%BB/"/>
    <url>2020/05/07/3.%E8%AF%BE%E7%A8%8B/CS224n_9%20-%20%E6%9C%BA%E7%BF%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="机翻"><a href="#机翻" class="headerlink" title="机翻"></a>机翻</h1><p>对于机器翻译这种任务，你可以抛弃那些考虑规则的想法。情感分析类任务也许可以建立词汇的正面或者负面表格，然后根据这些词汇的词性进一步延伸出其他规则或系统，但是翻译类问题baseline一般都是基于统计的，因为没有人可以遍历一门语言翻译为另一门语言时所有的规则和异常语言。</p><p>我们总是倾向于使用相当庞大的语料库，我们通常称为平行语料库。在平行语料库中，很多相同的段落平行地以多种语言表达。最经典的平行语料库应该就是圣经了。</p><p><strong>基于统计的机翻模型介绍</strong></p><p>源语言是法语，目标语言是英语，利用贝叶斯准则的概率公式,也就是后验概率 = 显先验概率*似然，然后除以边缘概率。这里的边缘概率可以是源语言。其中p(f|e)是翻译模型，p(e)是语言模型，语言模型也就是我们试图计算更长序列概率的那个模型。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191209_1407_40_006.png" alt=""></p><p>如下是机器翻译模型的工作流程：首先翻译模型会把一个法语句子基于统计列出一系列可能的英语选项。然后使用语言模型，在这些选项里找一个简单流畅的句子。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191209_1413_18_341.png" alt=""></p><p>其中的一个模块，翻译模型中，第一件要做的事情是匹配两种语言的单词，匹配所面临的困难使用法语英语互译来展示。其实英法是相似度很高的语言，换成其他相似度低的语言则难度会进一步上升。</p><p><strong>选词</strong></p><p>首先是有些单词在法语里有，英语里不存在，翻译的时候要把这种词汇取出掉，比如le japon在英语里对应的就是japan，没有冠词，如图一。然后还有多对一或者一对多的情况，如图二。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191209_1418_10_267.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191209_1419_07_809.png" alt=""></p><p><strong>语序对齐</strong>：A语言的正序是B语言的倒装。如下是德语和英语的例子。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191209_1421_48_691.png" alt=""></p><p><strong>搜索最优options</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191209_1422_49_424.png" alt=""></p><p>老师对深度学习的阐述：</p><p>深度学习他最喜欢的一个属性是，不仅仅是机翻问题，甚至上升到NLP或者AI，深度学习模型试图构建一个端到端的可训练模型，你只需要关心最终的目标函数即可。所有的东西都在一个模型里共同学习，从某种角度看，我们现在说的机器学习模型和这个比起来是相反的，你有匹配模型，你需要先优化它，然后可能需要重新排序模型，然后会有分布在不同系统的不同模型，你不能将所有的模型得到一致的训练。</p><p>神经网络机翻通常是指，构造一个大型神经网络，我们可以在上边以端到端的方式对这个网络进行训练和优化。如下图所示，我们使用一个大型神经网络，他接受输入文本并将其编码为神经网络内部传递的向量，然后通过解码器解码，输出是文本。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1059_01_286.png" alt=""></p><p>早期机翻模型如下，结构相对简单，由认知学家和心理学家们设计</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1101_06_914.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1101_45_119.png" alt=""></p><p>这些模型的设计应该遵循一定的规则，特别是昨天学的LSTM内部的各种门和电路设计感觉是同一种思路，找到这门系统设计的学科并从头到尾过一遍，</p><p><strong>神经网络曾经是一个非常沉寂的边缘细分学科，以前都是心理学家和认知科学家们在这里耕耘，直到算力和网络技术发展成熟，大量CS科学家和资本入场，这里才变得热闹起来。做科研也需要赶时髦，因为如果当下一个科研方向很热门，说明在这个历史时期科学正在往这个方向发展。本身科学家做研究就需要互相引用他人的成果，使用他们的公式和仪器，站在巨人肩膀上做事，scientist,as a part of science,必须顺应时代的潮流，甚至要能预判时代的走向打一个提前量，这样对自己的职业发展最有利，也对推动世界进步最有利。</strong></p><p><strong>深度学习这一块提升性能最突飞猛进的几年已经过去了，国内对AI和DL这一块关注度也在下降。找一个benchmark评估下各细分方向/行业的繁荣度指标。学完NLP读完survey后好好考考虑下是继续NLP还是转到因果推理上去。毕竟因果推理已经不止第一次听说了，就像我在大三的时候知道了kaggle和python和CS229却没有抓住现在他妈的还在补课一样。有了机会和idea就应该好好分析尽量抓住。</strong></p><h3 id="1-现代机翻模型"><a href="#1-现代机翻模型" class="headerlink" title="1.现代机翻模型"></a>1.现代机翻模型</h3><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1126_02_497.png" alt=""></p><p>如上示意图，一个循环神经网络作为编码器，另一个循环神经网络作为解码器。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1128_15_920.png" alt=""></p><p>如上，使用了多层神经网络的机翻模型。 </p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224u_Sentiment_Analysis_有监督的情感分析.md</title>
    <link href="undefined2020/04/28/3.%E8%AF%BE%E7%A8%8B/CS224u_Sentiment_Analysis_%E6%9C%89%E7%9B%91%E7%9D%A3%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90.md/"/>
    <url>2020/04/28/3.%E8%AF%BE%E7%A8%8B/CS224u_Sentiment_Analysis_%E6%9C%89%E7%9B%91%E7%9D%A3%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90.md/</url>
    
    <content type="html"><![CDATA[<h1 id="Sentiment-Analysis-有监督的情感分析-md"><a href="#Sentiment-Analysis-有监督的情感分析-md" class="headerlink" title="Sentiment_Analysis_有监督的情感分析.md"></a>Sentiment_Analysis_有监督的情感分析.md</h1><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200428_2253_57_336.png" alt=""></p><p>如上所示，phi函数就是上篇介绍的feature function，提供的功能就是把tree转换成字典。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200428_2254_14_407.png" alt=""></p><p>如上所示，fit model函数提供将数据x和y fit进指定模型（逻辑回归）中的功能。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200428_2254_30_253.png" alt=""></p><p>然后使用上节介绍的sst.experiment函数（三个参数）分别是路径，feature函数和fit函数。</p><p>这样设计的好处就是实现了类的分离，如果需要改写feature函数或者fit函数，直接在本体里边修改就好。</p><h4 id="超参数的搜索与确定"><a href="#超参数的搜索与确定" class="headerlink" title="超参数的搜索与确定"></a>超参数的搜索与确定</h4><p><strong>超参数的基本原理</strong>：<br>超参数是模型的优化过程之外的“settings”。有如下几个例子：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200428_2254_40_886.png" alt=""></p><p><strong>超参数搜索</strong>实例</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200428_2254_50_180.png" alt=""></p><p>如上，In[4] 的 fit_softmax_with_crossvalidation函数，就是搜索超参数的函数。basemod是逻辑回归，给他加上了搜索空间：c,罚函数，等，然后调用系统工具箱里的utils.fit_classifier_with_crossvalidation就可以得到搜索结果。当然这个搜索的过程耗时较长。</p><p>然后把得到的函数放进experiment函数中开始实验。因为得到的函数是在超参数空间中的最优解，所以效果肯定比默认函数要好。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/Chrome OS初始化操作</title>
    <link href="undefined2020/04/28/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/Chrome%20OS%E5%88%9D%E5%A7%8B%E5%8C%96%E6%93%8D%E4%BD%9C/"/>
    <url>2020/04/28/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/Chrome%20OS%E5%88%9D%E5%A7%8B%E5%8C%96%E6%93%8D%E4%BD%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="Chrome-OS初始化操作"><a href="#Chrome-OS初始化操作" class="headerlink" title="Chrome OS初始化操作"></a>Chrome OS初始化操作</h1><p>0.进入开发者模式，esc + f３ + 电源，then ctrl + D </p><p>1.参见“共享代理文章”，用手机和电脑开启代理共享，chromebook连上wifi，登陆谷歌账号，不断重试，连接google play（这一步非常耗时）。</p><p>2.安装包管理工具crew<br>  Ctrl + Alt +t<br>  curl -Ls <a href="https://raw.github.com/skycocker/chromebrew/master/install.sh" target="_blank" rel="noopener">https://raw.github.com/skycocker/chromebrew/master/install.sh</a> | bash</p><p>3.安装ssr<br>  在主流的apk网站上下载并安装ssr，登陆amytelecom，将信息复制到ssr中，删除无关的免费节点。</p><p>4.安装马克飞象，印象笔记</p><p>5.setting中安装linux，打开linux命令行输入sudo apt install vim-gtk3安装gvim</p><p>6.用gvim打开rsa_pub，在linux中连接github，将post项目拉下来</p>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>chromebook</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/chromebook代理共享</title>
    <link href="undefined2020/04/23/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/chromebook%E4%BB%A3%E7%90%86%E5%85%B1%E4%BA%AB/"/>
    <url>2020/04/23/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/chromebook%E4%BB%A3%E7%90%86%E5%85%B1%E4%BA%AB/</url>
    
    <content type="html"><![CDATA[<p>1.下载ssr</p><p>2.google免费ssr节点，复制粘贴。连接ssr，右键ssr图标-选项设置-允许来自局域网的连接。</p><p>3.电脑连接手机热点，电脑打                          开热点共享，chromebook连接电脑发射的热点</p><p>4.电脑win+R,cmd,ipconfig,得到如下所示ip信息</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200423_1941_35_146.png" alt=""></p><p>5.chromebook点击所连接wifi热点，点击热点详情-连接类型-手动配置代理-输入上图ip，端口号是1080（详见第2条中允许来自局域网连接的按钮周围的端口信息）。</p><p>6.登录，done。</p>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>chromebook</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224u_Sentiment_Analysis_SST</title>
    <link href="undefined2020/04/21/3.%E8%AF%BE%E7%A8%8B/CS224u_Sentiment_Analysis_SST/"/>
    <url>2020/04/21/3.%E8%AF%BE%E7%A8%8B/CS224u_Sentiment_Analysis_SST/</url>
    
    <content type="html"><![CDATA[<h1 id="Sentiment-Analysis-SST"><a href="#Sentiment-Analysis-SST" class="headerlink" title="Sentiment_Analysis SST"></a>Sentiment_Analysis SST</h1><h4 id="SST项目基本信息"><a href="#SST项目基本信息" class="headerlink" title="SST项目基本信息"></a>SST项目基本信息</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1111_40_329.png" alt=""></p><p><a href="https://nlp.stanford.edu/sentiment/" target="_blank" rel="noopener">SST斯坦福链接</a></p><h4 id="SST工作展示"><a href="#SST工作展示" class="headerlink" title="SST工作展示"></a>SST工作展示</h4><p>蓝色是正面情绪，黄色是负面，灰色是中性情绪。叶子节点的merge原理暂时未知。<br>They said it would be great这句话下边都是正面情绪，直到跟节点的they才恢复为中性情绪，因为they said<br>其实不能反映出作者的 bias。</p><p>而后边接上的they were wrong明显是负面，然后merge后总体就是负面情绪了。这个总体的判断要比直接把一句话拆开按照各个单词的正负性简单sum要靠谱很多。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1123_19_945.png" alt=""></p><p>verse vesa</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1130_52_522.png" alt=""></p><h4 id="Readers讲解"><a href="#Readers讲解" class="headerlink" title="Readers讲解"></a>Readers讲解</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1145_53_021.png" alt=""></p><p>In [5]、In [6]中的<strong>train_reader</strong>参数详解</p><table><thead><tr><th></th><th>meaning</th></tr></thead><tbody><tr><td>class_func=sst.binary_class_func</td><td>二分类问题</td></tr><tr><td>class_func=sst.ternary_class_func</td><td>三分类问题</td></tr><tr><td>class_func=None</td><td>五分类问题</td></tr></tbody></table><h4 id="核心类-：-nltk-tree-Tree"><a href="#核心类-：-nltk-tree-Tree" class="headerlink" title="核心类 ： nltk.tree.Tree"></a>核心类 ： nltk.tree.Tree</h4><p>输入一个tree：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1155_58_935.png" alt=""></p><p>循环打印各个subtree：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1156_26_139.png" alt=""></p><p>输出tree的lable和subtree：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1156_42_630.png" alt=""></p><h4 id="feature-function示例（词袋）"><a href="#feature-function示例（词袋）" class="headerlink" title="feature function示例（词袋）"></a>feature function示例（词袋）</h4><p>feature function只要输入的是tree，输出的是词典即可。</p><h4 id=""><a href="#" class="headerlink" title=""></a><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1203_50_804.png" alt=""></h4><p>unigrams_phi函数输入的是一个tree，输出的是dictionary。</p><p>tree.leaves()输出了tree里的各个元素（并没有利用tree结构）。</p><p>Counter清点了各个元素的数量，输出dictionary。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1202_20_877.png" alt=""></p><p>手动构建tree</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1204_52_532.png" alt=""></p><p>输入tree，输出的是词典。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1206_41_521.png" alt=""></p><h4 id="Model-wrappers详解"><a href="#Model-wrappers详解" class="headerlink" title="Model wrappers详解"></a>Model wrappers详解</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1212_14_383.png" alt=""></p><p>Model wrappers做什么：</p><p>输入一些supvised training data,输出的是训练好的模型（fitted model）</p><p>也许model wrapper做的事情（仅仅是fit model）比较乏味，但是使用wrapper后就可以在这个预处理的过程中做很多事情而不用改变interface。</p><h4 id="sst-experiment"><a href="#sst-experiment" class="headerlink" title="sst.experiment"></a>sst.experiment</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1509_41_075.png" alt=""></p><p>运行试验的时候，就会发现experiment类就像瑞士军刀一样。</p><p>你只需要设置如下三个参数，SSH_HOME,feature function，model_wrapper就好了。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1615_51_211.png" alt=""></p><p>然后experiment就会运行，给出的结果如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1617_23_029.png" alt=""></p><p>SST的一个完整示例，在逻辑回归模型上测试词袋模型。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1621_24_339.png" alt=""></p><p><strong>SST under the hood</strong> ： sklearn.feature_extraction.DictVectorizer</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1624_05_691.png" alt=""></p><p>sst框架实际上用的是sklearn.feature_extraction.DictVectorizer来<strong>combine data with feature function</strong></p><p>例如我的train feature是如下两个词典，数字是count。也就是说，如果你写了一个feature function用来将一个tree mapping到一个dictionary上，上面那段代码做的事情就是applying the feature function to all examples,creating a list like it。</p><p>但是sklearn这个框架和各种ML框架一样，处理的是矩阵，而不是词典，所以需要转换格式。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1637_01_166.png" alt=""></p><p>如下，就是对DictVectorizer初始化以及把字典格式的train_features转换为矩阵格式，也就是X_train</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_2310_08_461.png" alt=""></p><p>输出转换成功的矩阵，这里矩阵的列就是各个元素，行是字典的顺序编号</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_2311_04_629.png" alt=""></p><h4 id="Homework"><a href="#Homework" class="headerlink" title="Homework:"></a>Homework:</h4><p>on jupyter notebook</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1633_49_395.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224u_Sentiment_Analysis_材料和预处理</title>
    <link href="undefined2020/04/18/3.%E8%AF%BE%E7%A8%8B/CS224u_Sentiment_Analysis_%E6%9D%90%E6%96%99%E5%92%8C%E9%A2%84%E5%A4%84%E7%90%86/"/>
    <url>2020/04/18/3.%E8%AF%BE%E7%A8%8B/CS224u_Sentiment_Analysis_%E6%9D%90%E6%96%99%E5%92%8C%E9%A2%84%E5%A4%84%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h1 id="Sentiment-Analysis"><a href="#Sentiment-Analysis" class="headerlink" title="Sentiment Analysis"></a>Sentiment Analysis</h1><h3 id="阅读材料和数据预处理"><a href="#阅读材料和数据预处理" class="headerlink" title="阅读材料和数据预处理"></a>阅读材料和数据预处理</h3><h4 id="核心阅读材料"><a href="#核心阅读材料" class="headerlink" title="核心阅读材料"></a>核心阅读材料</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/3dGRneJbr5.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200420_2220_31_292.png" alt=""></p><p>情感分析各子领域合适的切入点。他们入选的原因是<strong>位置合适</strong>以及<strong>有公开数据集</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200420_2255_57_400.png" alt=""></p><p>常用情感分析<strong>数据库</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200420_2257_46_870.png" alt=""></p><p>lexica</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/nhHRwoK9ZU.png" alt=""></p><h3 id="preprocessing"><a href="#preprocessing" class="headerlink" title="preprocessing"></a>preprocessing</h3><p><strong>preprocessing</strong> ： tokenizing</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/8QE9Dv0qwL.png" alt=""></p><p>preprocessing : 词干化stemming</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/YUIDg86RTY.png" alt=""></p><p>porter stemming演示</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/pT41Ong7Ie.png" alt=""></p><p>兰开斯特stemming演示</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/BmfmhXeeg7.png" alt=""></p><p>wordNet stemming演示</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/BxFjaM2dhQ.png" alt=""></p><p><strong>preprocessing</strong>:  POS tagging</p><p>有些英文单词的sentiment只能通过pos来确定。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200420_2304_40_721.png" alt=""></p><p>需要注意的是即使有些单词的pos tagging一模一样仍然会有相反的sentiment</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200420_2307_36_210.png" alt=""></p><p>preprocessing : <strong>简单情感标记</strong>和推荐阅读</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1037_27_849.png" alt=""></p><p>简单负面情绪标记示例</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1036_52_478.png" alt=""></p><p>推荐阅读</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1037_37_712.png" alt=""></p><p>负面情绪标记带来的性能提升</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1044_06_021.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/U盘安装Ubuntu</title>
    <link href="undefined2020/03/31/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/U%E7%9B%98%E5%AE%89%E8%A3%85Ubuntu/"/>
    <url>2020/03/31/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/U%E7%9B%98%E5%AE%89%E8%A3%85Ubuntu/</url>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h3 id="U盘安装Ubuntu"><a href="#U盘安装Ubuntu" class="headerlink" title="U盘安装Ubuntu"></a>U盘安装Ubuntu</h3><p>windows下 - 磁盘管理 - 压缩卷，然后删除卷。</p><p>找一个大于4G的u盘</p><p><a href="http://mirrors.ustc.edu.cn/ubuntu-releases/16.04/" target="_blank" rel="noopener">http://mirrors.ustc.edu.cn/ubuntu-releases/16.04/</a> 下载ubuntu-16.04.6-desktop-amd64.iso    </p><p><a href="https://cn.ultraiso.net/xiazai.html" target="_blank" rel="noopener">https://cn.ultraiso.net/xiazai.html</a> 下载ultralSo，安装，运行</p><p>左上角 文件 - 打开iso文件；左下角本地目录 选中u盘，启动 - 写入硬盘映像 - 写入</p><p>插入U盘，重启，F1,选择直接进入U盘。</p><p>选择安装Ubuntu - 其他选项（根据需求调整分区）-  点击左下方的加号添加分区，分区方案参考如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200331_1923_31_301.png" alt=""></p><p>现在安装 - 等待  - 拔U盘 - 重启 - 进</p>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>5.杂谈/悉达多的王子</title>
    <link href="undefined2020/03/27/5.%E6%9D%82%E8%B0%88/%E6%82%89%E8%BE%BE%E5%A4%9A%E7%9A%84%E7%8E%8B%E5%AD%90/"/>
    <url>2020/03/27/5.%E6%9D%82%E8%B0%88/%E6%82%89%E8%BE%BE%E5%A4%9A%E7%9A%84%E7%8E%8B%E5%AD%90/</url>
    
    <content type="html"><![CDATA[<h1 id="悉达多"><a href="#悉达多" class="headerlink" title="悉达多"></a>悉达多</h1><p>1.爱与知识并不能来带peace</p><p>悉达多感到，父母朋友之<strong>爱</strong>，长者传授的<strong>知识</strong>不能让他感到满足，不能使他的内心安宁。悉达多开始怀疑，<strong>沐浴、祭祀</strong>等仪式是否有必要？</p><p>爱是大脑的内分泌短暂失调+欲求不满的表现，知识的传授其实可以在极短的时间内完成（剩余时间是磨洋工），沐浴祭祀等繁文缛节是keep everybody busy的手段，换成现代概念几乎可以等同于各种繁复的人事缓冲池——若不是要严控失业率，从基层到决策层之间90%的传递信息者都可被优化掉。</p><p>2.众生皆苦，逃避无用</p><p>悉达多说，冥想，对肉体的决弃，斋戒和调息都是在逃避自我，是对自我所受的苦难的短暂的逃避，这种逃避和牧牛人在酒馆里喝几碗米酒是同样的。在这种短暂的麻醉下，他们不再感受到自我，不再感觉到生命的苦难。那几碗米酒让牧牛人浑然入睡，他同样找到了悉达多和乔文达在长时间的修行中逃离肉体并宅于非我之境所找到的感觉。</p><p>悉达多说的这种逃避痛苦的方式，暗示了痛苦的根源就在与“我”，感受到我的存在，便能感受到痛苦。喝了酒，进入了禅定，做爱，入睡了，在别的国家旅游用英语讲话逃离中文世界，weed等都可短暂的<strong>忘我</strong>。但是在短暂的忘我结束后，一切一如从前，自己并没有变的明智，从未得到更高的知识，并没有进入更高的境界。</p><p>3.听从inner voice，不要逃避自我</p><p>教师和教义能教你许多，但是有些事情是无法传授的。那就是“自我”。过去总想着去逃避自我征服自我，然后我从未能征服自我，只是在欺骗，逃避。的确，世间没有任何东西能像自我那样占据自己的全部思绪。<br>害怕自我，逃避自我，从而去追寻梵天，去追寻阿特曼。我欲求摧毁自我，摆脱自我，以便在自我未知的最深层发现万相的核心，也就是阿特曼，生命，神灵等绝对终极之物。正因为如此，我却一路丢了自我。<br>不要企图逃离自己，不要去思索宇宙的奥义和世间的苦难。不要去摧毁自我并试图在自我的废墟中寻找秘密。要以自我为师，从自我找到自我的秘密。</p><p>不要逃避自我，要去好好倾听inner voice。追求科学突破，追求荣誉金钱，疏远朋友，放任傲慢，这是我的初心，还是我在向俗世的评价标准出卖自己？若过的不开心，虚无感爆棚，为何不调整？为何允许自己陷入如此境地？这是不知道如何与自己相处、长期忽略inner voice的恶果。<br>以前有人强迫着自己去学习，不许恋爱，不让玩耍，或者工作职责强迫自己加班劳动疲惫不堪，现在我终于自由，可做任何想做之事，不可以逃避，倾听inner voice找到内心之宁静其实不难。其实很多事情开始前便有答案。</p><p>4.觉醒的阵痛</p><p>世间无人如他一般孤独，以前他还是父亲的儿子，是高贵的婆罗门，是宗教信仰者，但是现在他只是觉醒的悉达多而已。贵族可以属于某个上流阶层，工匠可以在行业协会中安身立命，苦修者可以进行沙门的修行。婆罗门可以和婆罗门一起生活，甚至林中最与世隔绝的隐士也并非孤独薏苡仁，仍然属于某一群体。僧人也有千万的僧侣是他的兄弟。但是悉达多应该归属何方？加入谁的生活？使用什么语言？</p><p>他应该坚定的成为他自己，这是觉醒欧虎的颤栗，是新生之后的阵痛。随后他立刻重新上路，不再朝回家的方向，不再希望回到父亲的身边，不再犹豫和回顾。</p><p>5.心中的圣地</p><p>悉达多对加摩拉说，你很像我，你与众不同，你的心中有一处宁静的圣地，你可以随时退避并在哪里成为你自己，我也会这样做，极少数人具备这种能力。</p><p>我曾经也有这种能能力，我的心中的圣地是一片密林，长期的应试教育摧毁了我的内心世界。</p>]]></content>
    
    
    <categories>
      
      <category>5.杂谈</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>2.比赛/指导_Google_QA_数据处理部分</title>
    <link href="undefined2020/03/05/2.%E6%AF%94%E8%B5%9B/%E6%8C%87%E5%AF%BC_Google_QA_%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E9%83%A8%E5%88%86/"/>
    <url>2020/03/05/2.%E6%AF%94%E8%B5%9B/%E6%8C%87%E5%AF%BC_Google_QA_%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E9%83%A8%E5%88%86/</url>
    
    <content type="html"><![CDATA[<h1 id="数据处理部分总体代码"><a href="#数据处理部分总体代码" class="headerlink" title="数据处理部分总体代码"></a>数据处理部分总体代码</h1><p>如下，各个程序块在pycharm中分开运行，用#%%分割。</p><p>上来是import部分，然后是数据清洗和构建embedding矩阵等。</p><pre><code class="python">import numpy as np#numpy是主要用于数组计算，线性代数，傅里叶变换等。import pandas as pd#pandas基于numpy，可以处理高纬数据from sklearn.manifold import TSNE# sklearn是机器学习中常用的第三方模块，对常见的机器学习算法进行了封装，包括回归、降维、分类、聚类,sklearn.manifold是流形学习，非# 线性降维的手段。最简单的降维手段是随机投影，但是会导致结构丢失,manifold learning是一种类似主成分分析(PCA)的线性框架，不会错失数据结构中的非线性项  ，TSNE提供了一种画图方式，让高维的数据降低为二维画出来import seaborn as sns# 基于matplotlib的画图工具import glob# glob是查找模块。支持空格 ，问号？，方括号[]这三个通配符。空格代表0个或者多个字符，问号？代表一个字符，[]代表范围，例如0-9.glob主要是两个方法，glob方法获（取全部匹配路径）和iglob方法（逐个获取满足路径）。from urllib.parse import urlparse# url.parse定义了URL的标准接口，实现了对url的解析，合并，编码，解码等，#%matplotlib inline，这一句在pycharm和spyder中可以注释掉，主要是在jupter notebook中用于打印图像的import re# 正则表达式from category_encoders.ordinal import OrdinalEncoder# category_encoders.ordinal可以将非数值型数据比如语言，转换成类别变量例如独热编码，A collection sklearn transformers to encode categorical variables as numericimport spacy# 最快的工业级自然语言处理工具，主要功能是分词，词干化，命名实体识别，名词短语提取等import gc# gc是垃圾回收机制，gc.collect()是核心函数import gensim# genism模块中有word2vec，可以把词汇转换成向量from nltk.tokenize import TweetTokenizer# nltk.tokenize就是把句子令牌化，tweetTokenize就是按照空格进行分词，同时针对推文特性，去除@用户名，保留表情符号import datetime# datetime是python处理时间和日期的标准库from scipy import stats# scipy的stats是统计学的各种分布，stats.chi2是卡方分布，stats.norm是正态分布，还有t和f分布。from scipy.sparse import hstack,csr_matrix# vstack，hstack分别是横向合并和纵向合并，csr_matrix是压缩稀疏行格式的矩阵，sparse matrix (稀疏矩阵) ，sparse就是scipy模块中负责处理稀疏矩阵的模块from sklearn.model_selection import train_test_split,cross_val_score,KFold# sklearn中的model_selection用于交叉验证，三个包分别是数据集分割，交叉验证，K折交叉验证器from wordcloud import WordCloud# 词云from collections import Counter# 计数器from nltk.corpus import stopwords# 停止词from nltk.util import ngrams# ngramfrom sklearn.feature_extraction.text import TfidfVectorizer# sklearn的词频-逆向文件频率from sklearn.preprocessing import StandardScaler# StandardScaler是数据归一化和标准化的类。所谓归一化就是使得数据方差为1，均值为0from sklearn.linear_model import LogisticRegression,LinearRegression,Ridge# 逻辑回归模型from sklearn.svm import LinearSVC# 线性分类支持向量机from sklearn.multiclass import OneVsRestClassifier# 多分类问题的分类器import time# pd.set_option(&#39;max_colwidth&#39;,400)。设置列长度400from scipy.stats import spearmanr# stats是统计函数库，spearmanr是斯皮尔曼等级相关系数from keras.preprocessing.text import Tokenizer# 令牌化from keras.preprocessing.sequence import pad_sequences# keras只能接受相同长度的序列输入，pad_sequences()序列填充让序列长度一致。import scipy as sp# scipy用于数学科学工程领域，可以处插值、积分、优化、图像处理、常微分方程、信号处理等问题。import random# 随机模块from sklearn.preprocessing import OneHotEncoder# 独热码import os# 不同的操作系统目录不同，os模块可以处理此问题import torch# pytorch框架import torch.nn as nn# nn是循环神经网络import torch.optim as optim# 实现各种优化算法的库import torch.nn.functional as F# torch的nn网络里边的函数库，比如距离函数、损失函数、正则化函数等import torch.utils.data# 如上上import，定义了torch的数据格式import tensorflow_hub as hub# tensorflow_hub是tensorflow的hub库，里边有很多训练好的hub模型，module = hub.Module(&lt;&lt;Module URL as string&gt;&gt;, trainable=True)import keras.backend as K# keras可作为tensorflow，CNTK,Theano的应用程序接口。通过keras.json传递数据import sys# sys就是python和解释器打交道的系统。sys.argv是系统带入的参数，sys.version查看版本信息, \n #pip install ..  / / /  &gt; del/null, \n # sys.path.insert(0,/ / /)import transformers   # transformer模型import pickle# 普通数据和python数据之间互相转换，dump是普通换python，load是反过来,pickle是类似于json的一种标准数据格式from torch.utils.data import Dataset,DataLoader# Dataset定义torch的数据格式。loader有如下参数:(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None)from torch.autograd import Variable# Variable是torch的基本构件sb = SnowballStemmer(&quot;english&quot;)# SnowballStemmer另一种词干提取算法from tqdm import tqdm# 进度条from nltk.stem import PorterStemmer# stem是词干提取，PorterStemmer是其中一种提取算法ps = PorterStemmer()from nltk.stem import SnowballStemmerfrom torch.optim import lr_scheduler# lr_scheduler模块提供了根据epoch训练次数来调整学习率的方法from torch.nn.utils.rnn import pack_padded_sequence,pad_packed_sequence# 长度不同的句子们打包成一条线/一条线拆分成几个长度不同的句子#%%pd.set_option(&#39;max_rows&#39;, 500)pd.set_option(&#39;max_columns&#39;, 500)path = &#39;F:/Project/kag1/data&#39;sample_submission = pd.read_csv(f&#39;{path}/sample_submission.csv&#39;)test = pd.read_csv(f&#39;{path}/test.csv&#39;).fillna(&#39; &#39;)train = pd.read_csv(f&#39;{path}/train.csv&#39;).fillna(&#39; &#39;)#pandas#%%def sigmoid(x):    return 1 / (1 + np.exp(-x)) #定义钟形函数#无效字符puncts = [&#39;,&#39;, &#39;.&#39;, &#39;&quot;&#39;, &#39;:&#39;, &#39;)&#39;, &#39;(&#39;, &#39;-&#39;, &#39;!&#39;, &#39;?&#39;, &#39;|&#39;, &#39;;&#39;, &quot;&#39;&quot;, &#39;$&#39;, &#39;&amp;&#39;, &#39;/&#39;, &#39;[&#39;, &#39;]&#39;, &#39;&gt;&#39;, &#39;%&#39;, &#39;=&#39;, &#39;#&#39;, &#39;*&#39;, &#39;+&#39;, &#39;\\&#39;, &#39;•&#39;,  &#39;~&#39;, &#39;@&#39;, &#39;£&#39;, &#39;·&#39;, &#39;_&#39;, &#39;{&#39;, &#39;}&#39;, &#39;©&#39;, &#39;^&#39;, &#39;®&#39;, &#39;`&#39;,  &#39;&lt;&#39;, &#39;→&#39;, &#39;°&#39;, &#39;€&#39;, &#39;™&#39;, &#39;›&#39;,  &#39;♥&#39;, &#39;←&#39;, &#39;×&#39;, &#39;§&#39;, &#39;″&#39;, &#39;′&#39;, &#39;Â&#39;, &#39;█&#39;, &#39;½&#39;, &#39;à&#39;, &#39;…&#39;, &#39;\n&#39;, &#39;\xa0&#39;, &#39;\t&#39;, &#39;“&#39;, &#39;★&#39;, &#39;”&#39;, &#39;–&#39;, &#39;●&#39;, &#39;â&#39;, &#39;►&#39;, &#39;−&#39;, &#39;¢&#39;, &#39;²&#39;, &#39;¬&#39;, &#39;░&#39;, &#39;¶&#39;, &#39;↑&#39;, &#39;±&#39;, &#39;¿&#39;, &#39;▾&#39;, &#39;═&#39;, &#39;¦&#39;, &#39;║&#39;, &#39;―&#39;, &#39;¥&#39;, &#39;▓&#39;, &#39;—&#39;, &#39;‹&#39;, &#39;─&#39;, &#39;\u3000&#39;, &#39;\u202f&#39;, &#39;▒&#39;, &#39;：&#39;, &#39;¼&#39;, &#39;⊕&#39;, &#39;▼&#39;, &#39;▪&#39;, &#39;†&#39;, &#39;■&#39;, &#39;’&#39;, &#39;▀&#39;, &#39;¨&#39;, &#39;▄&#39;, &#39;♫&#39;, &#39;☆&#39;, &#39;é&#39;, &#39;¯&#39;, &#39;♦&#39;, &#39;¤&#39;, &#39;▲&#39;, &#39;è&#39;, &#39;¸&#39;, &#39;¾&#39;, &#39;Ã&#39;, &#39;⋅&#39;, &#39;‘&#39;, &#39;∞&#39;, &#39;«&#39;, &#39;∙&#39;, &#39;）&#39;, &#39;↓&#39;, &#39;、&#39;, &#39;│&#39;, &#39;（&#39;, &#39;»&#39;, &#39;，&#39;, &#39;♪&#39;, &#39;╩&#39;, &#39;╚&#39;, &#39;³&#39;, &#39;・&#39;, &#39;╦&#39;, &#39;╣&#39;, &#39;╔&#39;, &#39;╗&#39;, &#39;▬&#39;, &#39;❤&#39;, &#39;ï&#39;, &#39;Ø&#39;, &#39;¹&#39;, &#39;≤&#39;, &#39;‡&#39;, &#39;√&#39;, ]mispell_dict = {&quot;aren&#39;t&quot; : &quot;are not&quot;,&quot;can&#39;t&quot; : &quot;cannot&quot;,&quot;couldn&#39;t&quot; : &quot;could not&quot;,&quot;couldnt&quot; : &quot;could not&quot;,&quot;didn&#39;t&quot; : &quot;did not&quot;,&quot;doesn&#39;t&quot; : &quot;does not&quot;,&quot;doesnt&quot; : &quot;does not&quot;,&quot;don&#39;t&quot; : &quot;do not&quot;,&quot;hadn&#39;t&quot; : &quot;had not&quot;,&quot;hasn&#39;t&quot; : &quot;has not&quot;,&quot;haven&#39;t&quot; : &quot;have not&quot;,&quot;havent&quot; : &quot;have not&quot;,&quot;he&#39;d&quot; : &quot;he would&quot;,&quot;he&#39;ll&quot; : &quot;he will&quot;,&quot;he&#39;s&quot; : &quot;he is&quot;,&quot;i&#39;d&quot; : &quot;I would&quot;,&quot;i&#39;d&quot; : &quot;I had&quot;,&quot;i&#39;ll&quot; : &quot;I will&quot;,&quot;i&#39;m&quot; : &quot;I am&quot;,&quot;isn&#39;t&quot; : &quot;is not&quot;,&quot;it&#39;s&quot; : &quot;it is&quot;,&quot;it&#39;ll&quot;:&quot;it will&quot;,&quot;i&#39;ve&quot; : &quot;I have&quot;,&quot;let&#39;s&quot; : &quot;let us&quot;,&quot;mightn&#39;t&quot; : &quot;might not&quot;,&quot;mustn&#39;t&quot; : &quot;must not&quot;,&quot;shan&#39;t&quot; : &quot;shall not&quot;,&quot;she&#39;d&quot; : &quot;she would&quot;,&quot;she&#39;ll&quot; : &quot;she will&quot;,&quot;she&#39;s&quot; : &quot;she is&quot;,&quot;shouldn&#39;t&quot; : &quot;should not&quot;,&quot;shouldnt&quot; : &quot;should not&quot;,&quot;that&#39;s&quot; : &quot;that is&quot;,&quot;thats&quot; : &quot;that is&quot;,&quot;there&#39;s&quot; : &quot;there is&quot;,&quot;theres&quot; : &quot;there is&quot;,&quot;they&#39;d&quot; : &quot;they would&quot;,&quot;they&#39;ll&quot; : &quot;they will&quot;,&quot;they&#39;re&quot; : &quot;they are&quot;,&quot;theyre&quot;:  &quot;they are&quot;,&quot;they&#39;ve&quot; : &quot;they have&quot;,&quot;we&#39;d&quot; : &quot;we would&quot;,&quot;we&#39;re&quot; : &quot;we are&quot;,&quot;weren&#39;t&quot; : &quot;were not&quot;,&quot;we&#39;ve&quot; : &quot;we have&quot;,&quot;what&#39;ll&quot; : &quot;what will&quot;,&quot;what&#39;re&quot; : &quot;what are&quot;,&quot;what&#39;s&quot; : &quot;what is&quot;,&quot;what&#39;ve&quot; : &quot;what have&quot;,&quot;where&#39;s&quot; : &quot;where is&quot;,&quot;who&#39;d&quot; : &quot;who would&quot;,&quot;who&#39;ll&quot; : &quot;who will&quot;,&quot;who&#39;re&quot; : &quot;who are&quot;,&quot;who&#39;s&quot; : &quot;who is&quot;,&quot;who&#39;ve&quot; : &quot;who have&quot;,&quot;won&#39;t&quot; : &quot;will not&quot;,&quot;wouldn&#39;t&quot; : &quot;would not&quot;,&quot;you&#39;d&quot; : &quot;you would&quot;,&quot;you&#39;ll&quot; : &quot;you will&quot;,&quot;you&#39;re&quot; : &quot;you are&quot;,&quot;you&#39;ve&quot; : &quot;you have&quot;,&quot;&#39;re&quot;: &quot; are&quot;,&quot;wasn&#39;t&quot;: &quot;was not&quot;,&quot;we&#39;ll&quot;:&quot; will&quot;,&quot;didn&#39;t&quot;: &quot;did not&quot;,&quot;tryin&#39;&quot;:&quot;trying&quot;}def clean_text(x):    x = str(x)    for punct in puncts:        x = x.replace(punct,f&#39;{punct}&#39;)    return x# replace函数就是把第一个old变量换成第二个new变量# f格式化字符串使用举例：# &gt;&gt;&gt; name = &#39;Eric&#39;# &gt;&gt;&gt; f&#39;Hello, my name is {name}&#39;# &#39;Hello, my name is Eric&#39;def clean_numbers(x):    x = re.sub(&#39;[0-9]{5,}&#39;, &#39;#####&#39;, x)# 正则表达式，sub是替换函数，[0-9]是全体数字，{5，}是长度大于等于5的所有字符串# 这一句的意思就是把所有长度大于等于5的所有数字都替换成五个井号，下面的就是替换成1-4的长度的字符    x = re.sub(&#39;[0-9]{4}&#39;, &#39;####&#39;, x)    x = re.sub(&#39;[0-9]{3}&#39;, &#39;###&#39;, x)    x = re.sub(&#39;[0-9]{2}&#39;, &#39;##&#39;, x)    return xdef _get_mispell(mispell_dict):    mispell_re = re.compile(&#39;(%s)&#39; % &#39;|&#39;.join(mispell_dict.keys()))    return mispell_dict, mispell_re# 获取拼写错误的词# re.compile函数：编译一个正则表达式模板，返回一个正则表达式对象，多个正则表达式模板用&#39;|&#39;链接# join函数：把join前的符号依次链接到join后的各个词的间隙中，如下所示：# str = &quot;-&quot;;# seq = (&quot;a&quot;, &quot;b&quot;, &quot;c&quot;); # 字符串序列# print str.join( seq );def replace_typical_misspell(text):    mispellings, mispellings_re = _get_mispell(mispell_dict)    def replace(match):        return mispellings[match.group(0)]    return mispellings_re.sub(replace, text)# 将拼写有误的词替换掉的函数# .group(0) 返回母串中与子串匹配的第一个def clean_data(df,columns:list):    for col in columns:        df[col] = df[col].apply(lambda x : clean_numbers(x))        df[col] = df[col].apply(lambda x : clean_text(x.lower()))        df[col] = df[col].apply(lambda x : replace_typical_misspell(x))    return df# 总的清洗函数：# df是pandas的数据框架，就是excel表。col是数据行。# .apply是对df的操作。lambda x基本泛指所有的数据train = clean_data(train, [&#39;answer&#39;, &#39;question_body&#39;, &#39;question_title&#39;])test = clean_data(test, [&#39;answer&#39;, &#39;question_body&#39;, &#39;question_title&#39;])#%%seed_everything()#%%%%timeembedding_test = get_embedding_features(train, test, [&#39;answer&#39;, &#39;question_body&#39;, &#39;question_title&#39;], only_test=True)embedding_train = {}embedding_train[&#39;answer_embedding&#39;] = np.load(&#39;/kaggle/input/qa-labeling-files-for-inference/embedding_train_answer_embedding.npy&#39;, allow_pickle=True)embedding_train[&#39;question_body_embedding&#39;] = np.load(&#39;/kaggle/input/qa-labeling-files-for-inference/embedding_train_question_body_embedding.npy&#39;, allow_pickle=True)embedding_train[&#39;question_title_embedding&#39;] = np.load(&#39;/kaggle/input/qa-labeling-files-for-inference/embedding_train_question_title_embedding.npy&#39;, allow_pickle=True)#%%%%timedist_features_train, dist_features_test  = get_dist_features(embedding_train, embedding_test)#%%tokenizer = Tokenizer()full_text = list(train[&#39;question_body&#39;]) + \                       list(train[&#39;answer&#39;]) + \                       list(train[&#39;question_title&#39;]) + \                       list(test[&#39;question_body&#39;]) + \                       list(test[&#39;answer&#39;]) + \                       list(test[&#39;question_title&#39;])tokenizer.fit_on_texts(full_text)#%%embed_size=300embedding_path = &quot;/kaggle/input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl&quot;#%%%%timelemma_dict, word_dict = get_word_lemma_dict(full_text)#%%# tk.word_index = {k: v for k, v in tk.word_index.items() if k in word_dict.keys()}# train[&#39;host&#39;] = train[&#39;host&#39;].apply(lambda x: x.split(&#39;.&#39;)[-2])# test[&#39;host&#39;] = test[&#39;host&#39;].apply(lambda x: x.split(&#39;.&#39;)[-2])unique_hosts = list(set(train[&#39;host&#39;].unique().tolist() + test[&#39;host&#39;].unique().tolist()))host_dict = {i + 1: e for i, e in enumerate(unique_hosts)}host_dict_reverse = {v: k for k, v in host_dict.items()}unique_categories = list(set(train[&#39;category&#39;].unique().tolist() + test[&#39;category&#39;].unique().tolist()))category_dict = {i + 1: e for i, e in enumerate(unique_categories)}category_dict_reverse = {v: k for k, v in category_dict.items()}max_len = 500max_len_title = 30train_question_tokenized = pad_sequences(tokenizer.texts_to_sequences(train[&#39;question_body&#39;]), maxlen = max_len)train_answer_tokenized = pad_sequences(tokenizer.texts_to_sequences(train[&#39;answer&#39;]), maxlen = max_len)train_title_tokenized = pad_sequences(tokenizer.texts_to_sequences(train[&#39;question_title&#39;]), maxlen = max_len_title)test_question_tokenized = pad_sequences(tokenizer.texts_to_sequences(test[&#39;question_body&#39;]), maxlen = max_len)test_answer_tokenized = pad_sequences(tokenizer.texts_to_sequences(test[&#39;answer&#39;]), maxlen = max_len)test_title_tokenized = pad_sequences(tokenizer.texts_to_sequences(test[&#39;question_title&#39;]), maxlen = max_len_title)train_host = train[&#39;host&#39;].apply(lambda x: host_dict_reverse[x]).valuestrain_category = train[&#39;category&#39;].apply(lambda x: category_dict_reverse[x]).valuestest_host = test[&#39;host&#39;].apply(lambda x: host_dict_reverse[x]).valuestest_category = test[&#39;category&#39;].apply(lambda x: category_dict_reverse[x]).values#%%y = train[sample_submission.columns[1:]].values#%%num_workers = 0bs = 16n_cat = len(category_dict) + 1cat_emb = min(np.ceil((len(category_dict)) / 2), 50)n_host = len(host_dict)+1host_emb = min(np.ceil((len(host_dict)) / 2), 50)#%%bs_test = 16test_loader = DataLoader(TextDataset(test_question_tokenized, test_answer_tokenized, test_title_tokenized,                                     test_category, test_host, embedding_test, dist_features_test, test.index),                                     batch_size=bs_test, shuffle=False, num_workers=num_workers)#%%def get_coefs(word,*arr):    return word,np.asarray(arr,dtype=&#39;float32&#39;)def load_embeddings(path):    with open(path,&#39;rb&#39;) as f:        emb_arr = pickle.load(f)    return emb_arr# pickle.load(f)是python的标准导入数据的格式def build_matrix_adv(embedding_path : str = &#39;&#39;,                     embedding_path_spellcheck: str = r&#39;f:\embeddings\wiki-news-300d-1M\wiki-news-300d-1M.vec&#39;,                     word_dict : dict None,                     lemma_dict : dict = None,                     max_features : int = 100000,                     embed_size: int = 300, ):    spell_model = gensim.models.KeyedVectors.load_word2vec_format(embedding_path_spellcheck)    # 调用维基字典,字典的用途是训练词向量（本文件目的）    words = spell_model.index2word    # index转换成词    w_rank = {}    for i,word in enumrate(words):        w_rank[word] = i    WORDS = w_rank    def P(word):        &quot;word的概率&quot;        # 使用逆秩作为代理        return - WORDS.get(word,0)    def correction(word):        &quot;最可能的单次拼写的更正：&quot;        return max(candidates(word),key = P)    def candidates(word):        &quot;为word参数 生成可能的拼写更正&quot;        return (known([word])  or known(edits1(word))or [word])    def known(words):        &quot;在字典WORD中出现的words参数的子集&quot;        # 如26行所示，WORD是一个数组，words是训练集训练出来的word2vec矩阵？        # 不可以留疑问，这个文件写完后单步调试进去看看做的是什么事情        return set(w for w in words if w in WORDS)    def edits1(word):        &quot;与word相距离一次编辑的所有编辑&quot;        letters = &#39;abcdefghijklmnopqrstuuvwxyz&#39;        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]        deletes = [L + R[1:] for L, R in splits if R]        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) &gt; 1]        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]        inserts = [L + c + R for L, R in splits for c in letters]        return set(deletes + transposes + replaces + inserts)### L 和 R这里需要单帧步进研究一下    def edits2(word):        &quot;与word相距离两次编辑的所有编辑&quot;        return (e2 for e1 in edits1(word) for e2 in edits1(e1))    def singlify(word):        return &quot;&quot;.join([letter for i,letter in enumerate(word) if i == 0 or letter != word[i - 1]])# embedding_index = dict(get_coefs(*o.strip().split(&quot; &quot;)) for o in open(embedding_path, encoding=&#39;utf-8&#39;))# embedding_index = dict(get_coefs(*o.strip().split(&quot; &quot;)) for o in open(embedding_path, encoding=&#39;utf-8&#39;, errors=&#39;ignore&#39;))    embedding_index = load_embeddings(embedding_path)    nb_words = min(max_features,len(word_dict))    embedding_matrix = np.zeros((nb_words + 1,embed_size))    uknown_words = []    for word, i in word_dict.items():        key = word        embedding_vector = embedding_index.get(word)        if embedding_vector is not None:            embedding_matrix[i] = embedding_vector            continue        embedding_vector = embedding_index.get(word.lower())        if embedding_vector is not None:            embedding_matrix[i] = embedding_vector            continue        embedding_vector = embedding_index.get(word.upper())        if embedding_vector is not None:            embedding_matrix[i] = embedding_vector            continue        embedding_vector = embedding_index.get(word.capitalize())        if embedding_vector is not None:            embedding_matrix[i] = embedding_vector            continue        if embedding_vector is not None:            embedding_matrix[word_dict[key]] = embedding_vector            continue        word = lc.stem(key)        embedding_vector = embedding_index.get(word)        if embedding_vector is not None:            embedding_matrix[word_dict[key]] = embedding_vector            continue        word = sb.stem(key)        embedding_vector = embedding_index.get(word)        if embedding_vector is not None:            embedding_matrix[word_dict[key]] = embedding_vector            continue        word = lemma_dict[key]        embedding_vector = embedding_index.get(word)        if embedding_vector is not None:            embedding_matrix[word_dict[key]] = embedding_vector            continue        if len(key) &gt; 1:            word = correction(key)            embedding_vector = embedding_index.get(word)            if embedding_vector is not None:                embedding_matrix[word_dict[key]] = embedding_vector                continue        unknown_words.append(key)    print(f&#39;{len(unknown_words) * 100 / len(word_dict):.4f}% words are not in embeddings&#39;)    return embedding_matrix, nb_words, unknown_wordsdef get_word_lemma_dict(full_text: list = None, ):    nlp = spacy.load(&#39;en_core_web_lg&#39;, disable=[&#39;parser&#39;,&#39;ner&#39;,&#39;tagger&#39;])    nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)    word_dict = {}    word_index = 1    lemma_dict = {}    docs = nlp.pipe(full_text, n_threads = os.cpu_count())    for doc in docs:        for token in doc:            if (token.text not in word_dict) and (token.pos_ is not &quot;PUNCT&quot;):                word_dict[token.text] = word_index                word_index += 1                lemma_dict[token.text] = token.lemma_    return lemma_dict, word_dictdef build_matrix(embedding_path: str = &#39;&#39;,                 embedding_path_spellcheck: str = r&#39;f:\embeddings\wiki-news-300d-1M\wiki-news-300d-1M.vec&#39;,                 word_dict: dict = None, max_features: int = 100000,                 embed_size: int= 300, ):    # embedding_index = dict(get_coefs(*o.strip().split(&quot; &quot;)) for o in open(embedding_path, encoding=&#39;utf-8&#39;))    embedding_index = load_embeddings(embedding_path)    nb_words = min(max_features, len(word_dict))    embedding_matrix = np.zeros((nb_words + 1, embed_size))    unknown_words = []    for word, i in word_dict.items():        key = word        embedding_vector = embedding_index.get(word)        if embedding_vector is not None:            embedding_matrix[i] = embedding_vector            continue        embedding_vector = embedding_index.get(word.lower())        if embedding_vector is not None:            embedding_matrix[i] = embedding_vector            continue        embedding_vector = embedding_index.get(word.upper())        if embedding_vector is not None:            embedding_matrix[i] = embedding_vector            continue        embedding_vector = embedding_index.get(word.capitalize())        if embedding_vector is not None:            embedding_matrix[i] = embedding_vector            continue        unknown_words.append(key)    print(f&#39;{len(unknown_words) * 100 / len(word_dict):.4f}% words are not in embeddings&#39;)    return embedding_matrix, nb_words, unknown_wordsdef seed_everything(seed=1234):    random.seed(seed)    os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed)    np.random.seed(seed)    torch.manual_seed(seed)    torch.cuda.manual_seed(seed)    torch.backends.cudnn.deterministic = True</code></pre>]]></content>
    
    
    <categories>
      
      <category>2.比赛</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据处理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/批量修改文件后缀名的方式</title>
    <link href="undefined2020/03/04/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E6%89%B9%E9%87%8F%E4%BF%AE%E6%94%B9%E6%96%87%E4%BB%B6%E5%90%8E%E7%BC%80%E5%90%8D%E7%9A%84%E6%96%B9%E5%BC%8F/"/>
    <url>2020/03/04/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E6%89%B9%E9%87%8F%E4%BF%AE%E6%94%B9%E6%96%87%E4%BB%B6%E5%90%8E%E7%BC%80%E5%90%8D%E7%9A%84%E6%96%B9%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<h4 id=""><a href="#" class="headerlink" title=""></a></h4><p>新建一个.txt文档，输入：</p><p>ren *.java *.md</p><p>保存</p><p>将文件后缀名改成.bat,双击运行</p>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/修改pip源</title>
    <link href="undefined2020/03/02/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E4%BF%AE%E6%94%B9pip%E6%BA%90/"/>
    <url>2020/03/02/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E4%BF%AE%E6%94%B9pip%E6%BA%90/</url>
    
    <content type="html"><![CDATA[<h4 id=""><a href="#" class="headerlink" title=""></a></h4><ol><li><h4 id="将pip切换回国内源"><a href="#将pip切换回国内源" class="headerlink" title="将pip切换回国内源"></a>将pip切换回国内源</h4><p><strong>pip国内的一些镜像</strong></p><p>  阿里云 <a href="http://mirrors.aliyun.com/pypi/simple/" target="_blank" rel="noopener">http://mirrors.aliyun.com/pypi/simple/</a><br>  中国科技大学 <a href="https://pypi.mirrors.ustc.edu.cn/simple/" target="_blank" rel="noopener">https://pypi.mirrors.ustc.edu.cn/simple/</a><br>  豆瓣(douban) <a href="http://pypi.douban.com/simple/" target="_blank" rel="noopener">http://pypi.douban.com/simple/</a><br>  清华大学 <a href="https://pypi.tuna.tsinghua.edu.cn/simple/" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple/</a><br>  中国科学技术大学 <a href="http://pypi.mirrors.ustc.edu.cn/simple/" target="_blank" rel="noopener">http://pypi.mirrors.ustc.edu.cn/simple/</a></p><p><strong>修改源方法：</strong></p><p><strong>临时使用：</strong><br>可以在使用pip的时候在后面加上-i参数，指定pip源<br>eg: pip install scrapy -i <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple</a>   –trusted-host  <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">pypi.tuna.tsinghua.edu.cn</a></p><p><strong>永久修改：</strong><br><strong>linux:</strong><br>修改 ~/.pip/pip.conf (没有就创建一个)， 内容如下：</p><pre><code>[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple</code></pre></li></ol><p>   <strong>windows:</strong><br>   直接在user目录中创建一个pip目录，如：C:\Users\xx\pip，新建文件pip.ini，内容如下</p><pre><code>   [global]   index-url = https://pypi.tuna.tsinghua.edu.cn/simple</code></pre><ol start="2"><li><h4 id="-1"><a href="#-1" class="headerlink" title=""></a></h4></li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/网络_1-概述</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BD%91%E7%BB%9C_1-%E6%A6%82%E8%BF%B0/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BD%91%E7%BB%9C_1-%E6%A6%82%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="1-计算机网络概述"><a href="#1-计算机网络概述" class="headerlink" title="1.计算机网络概述"></a>1.计算机网络概述</h1><h4 id="1-1局域网"><a href="#1-1局域网" class="headerlink" title="1.1局域网"></a>1.1局域网</h4><p>​        覆盖范围小（１００ｍ）,自己花钱买设备,贷款固定</p><p>​        接入层的交换机们星形连接到汇聚层交换机,这样分散IO</p><p>​        接入层交换机们不该串联,不然最后一台交换机那里IO压力太大</p><h4 id="1-2Internet和广域网"><a href="#1-2Internet和广域网" class="headerlink" title="1.2Internet和广域网"></a>1.2Internet和广域网</h4><p>​    Internet ISP :有自己的机房,对网民提供internet访问</p><p>　广域网：距离较远，花钱租带宽，</p><h4 id="1-3-数据包和数据帧"><a href="#1-3-数据包和数据帧" class="headerlink" title="1.3 数据包和数据帧"></a>1.3 数据包和数据帧</h4><p>在一个计算机想把数据发给另一个计算机时,需要在发送信息里携带ip地址和mac地址,其中ip地址是最终目地,mac地址是下一跳的目的地,ip地址不变,mac地址每次经过路由器都刷新.下一跳的目的地由迪杰斯特拉或其他寻路算法确定.</p><h4 id="1-4-OSI参考模型"><a href="#1-4-OSI参考模型" class="headerlink" title="1.4 OSI参考模型"></a>1.4 OSI参考模型</h4><p> 应用层:所有能产生网络流量的程序</p><p>表示层:在传输之前是否进行加密或压缩处理</p><p>会话层:查木马,netstat -n</p><p>传输层:可靠传输,流量控制,不可靠传输</p><p>网络层:负责选择最佳路径,规划ip地址</p><p>数据链路层:帧的开始和结束,透明传输,差错校验</p><p>物理层:接口标准,电器标准,如何在物理链路层上传输的更快</p><ul><li><p>osi参考模型对网络排错指导</p><ul><li>物理层故障：查看链接状态，发送和接收数据包</li><li>数据链路层故障：MAC冲突，ADSL欠费</li><li>网络层故障：IP地址，子网掩码，网关配置错误，路由器没有配置达到目标网络的路由</li><li>应用层故障：应用程序配置错误</li></ul></li><li><p>OSI参考模型和网络安全</p><ul><li>物理层安全</li><li>数据链路层安全：ADSL账号密码，vlan，交换机端口绑定mac地址</li><li>网络层安全：路由器上设置访问控制列表（ACL），设置某些计算机不能访问Internet，</li><li>应用层安全：开发的程序没有漏洞</li></ul></li><li><p>TCP/IP协议和OSI参考模型</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1241_10_572.png" alt=""></p></li></ul>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机网络</tag>
      
      <tag>OSI参考模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/网络_2-物理层</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BD%91%E7%BB%9C_2-%E7%89%A9%E7%90%86%E5%B1%82/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BD%91%E7%BB%9C_2-%E7%89%A9%E7%90%86%E5%B1%82/</url>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h1 id="2-物理层"><a href="#2-物理层" class="headerlink" title="2.物理层"></a>2.物理层</h1><h3 id="2-1物理层的基本概念"><a href="#2-1物理层的基本概念" class="headerlink" title="2.1物理层的基本概念"></a>2.1物理层的基本概念</h3><p>物理层解决如何在链接各种计算机的传输媒体上传输数据比特流。</p><p>物理层的主要任务描述：确定传输媒体的接口的一些特性。例如：</p><ul><li><p>机械特性：接口形状，大小，引线数目</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1241_35_978.png" alt=""></p></li><li><p>电器特性：电压范围</p></li><li><p>功能特性，过程特性等</p></li></ul><h3 id="2-2数据通信的基本模型"><a href="#2-2数据通信的基本模型" class="headerlink" title="2.2数据通信的基本模型"></a>2.2数据通信的基本模型</h3><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1241_43_905.png" alt=""></p><p>相关术语：</p><ul><li>数据：运送消息的实体</li><li>信号：数据的电器或电磁表现<ul><li>模拟信号：消息的参数取值是连续的</li><li>数字信号：消息的参数取值是离散的</li></ul></li></ul><h4 id="信道"><a href="#信道" class="headerlink" title="信道"></a>信道</h4><p>信道一般表示向一个方向传送信息的媒体，我们常说的信道往往包含了一条发送信息的信道和一条接受信息的信道</p><p>单向信道：信息只能单向流动</p><p>双向交替信道：信息可双向流动，但不能同时流动</p><p>双向同时信道：信息科双向，同时流动</p><h4 id="基带信号和带通信号"><a href="#基带信号和带通信号" class="headerlink" title="基带信号和带通信号"></a>基带信号和带通信号</h4><ul><li><p>基带信号：来自信源的信号，像计算机输出的各种代表文字和图像的信号都属于基带信号，我们说话的声波也是基带信号</p><ul><li><p>基带数字信号的几种调制方法</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1242_08_655.png" alt=""></p></li></ul></li><li><p>带通信号：把基带信号进行载波调制后，把信号的频率范围搬到较高的频段以便在信道中传输</p><ul><li>在传输范围较大的时候，计算机网络必须通过带通信号传输</li></ul></li></ul><p>曼彻斯特编码：由低到高是0，高到低是1</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1242_37_192.png" alt=""></p><p>差分曼彻斯特编码</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1242_47_991.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1242_56_962.png" alt=""></p><h3 id="2-3奈氏准则"><a href="#2-3奈氏准则" class="headerlink" title="2.3奈氏准则"></a>2.3奈氏准则</h3><p>给出了在理想条件下，为了避免码间串扰，码元的传输速率的上限值</p><p>在任何信道中，码元的传输速率需有限制，否则会出现码间串扰的问题，使接收端对码元的判决（识别）成为不可能</p><p>如果信道的带宽很宽，也就能通过的信号的高频分量越多，那就可以用更高的速率传送码元不出现串扰</p><h4 id="信噪比"><a href="#信噪比" class="headerlink" title="信噪比"></a>信噪比</h4><p>香农的信息论推导了带宽受限，有高斯白噪声干扰的信道的极限，无差错的信息传输速率</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1243_14_228.png" alt=""></p><p>如果频带带宽已经确定了的信道，如果信噪比和带宽都不能提高了，还有一种突破方式就是用编码的方式让一个码元携带更多比特量的信息</p><h4 id="2-4物理层的导向传输媒体"><a href="#2-4物理层的导向传输媒体" class="headerlink" title="2.4物理层的导向传输媒体"></a>2.4物理层的导向传输媒体</h4><ul><li><p>双绞线</p><ul><li>屏蔽双绞线</li><li>无屏蔽双绞线</li></ul></li><li><p>同轴电缆</p></li><li><p>光缆</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1243_29_412.png" alt=""></p></li></ul><h4 id="2-5物理层的非导向传输媒体"><a href="#2-5物理层的非导向传输媒体" class="headerlink" title="2.5物理层的非导向传输媒体"></a>2.5物理层的非导向传输媒体</h4><p>无线短波：靠电离层的反射，通信质量较差</p><p>微波：在空间中主要是直线传播（地面微波接力通信，卫星通信）</p><h4 id="2-6信道复用技术"><a href="#2-6信道复用技术" class="headerlink" title="2.6信道复用技术"></a>2.6信道复用技术</h4><p>复用是通信技术的基本概念</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1243_41_204.png" alt=""></p><ul><li>频分复用</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1243_57_759.png" alt=""></p><ul><li><p>时分复用</p><p>将时间划分为一段段等长的时分复用帧（TDM帧），每一个时分复用的用户在每一个TDM帧里占用固定序号的时隙</p><p>每一个用户占用的时隙是周期性出现的，周期就是TDM帧的长度</p><p>TDM信号也称为等时信号</p><p>时分复用的所有用户在不同的时间占用同样的带宽频率</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1244_30_908.png" alt=""></p></li></ul><p>时分复用可能造成资源浪费，优化方案是采用统计时分复用</p><ul><li>波分复用</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1245_07_550.png" alt=""></p><ul><li>码分复用</li></ul><p>各用户采取特殊挑选的不同码型，因此彼此不会造成干扰，这种系统发出的信号有很强的抗干扰能力</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1245_36_782.png" alt=""></p><h3 id="2-8数字传输系统"><a href="#2-8数字传输系统" class="headerlink" title="2.8数字传输系统"></a>2.8数字传输系统</h3><ul><li><p>脉码调制PCM体制</p><ul><li><p>脉码调制PCM体制最初是为了在电话局之间的中继线上传送多路的电话</p></li><li><p>由于历史原因PCM有两个不兼容的国际标准，北美的24路PCM和欧洲的30路PCM，我国采用欧洲的30路PCM</p></li><li><p>同步光纤网SONET和同步数字系列SDH</p><ul><li>旧的数字传输系统存在许多缺点，其中主要的是以下两个方面：<ul><li>速率标准不统一，如果速率无法统一，国际范围的高速数据传输就很难实现</li></ul></li><li>不是同步传输<ul><li>过去相当长时间内，为了节约经费，各国的数字网主要是采用准同步方式</li></ul></li></ul></li><li><p>同步光纤网详细介绍</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1245_47_582.png" alt=""></p><p>SONET标准定义了四个光接口层</p></li></ul></li><li><p>光子层</p><ul><li>处理跨越光缆的比特传送</li></ul></li><li><p>段层</p><ul><li>在光缆上传送STS-N帧</li></ul></li><li><p>线路层</p><ul><li>负责路径层的同步和复用</li></ul></li><li><p>路径层</p><ul><li>处理路径端接设备PTE（path terminating element）之间的业务传输</li></ul></li></ul><h4 id="2-9宽带介入技术"><a href="#2-9宽带介入技术" class="headerlink" title="2.9宽带介入技术"></a>2.9宽带介入技术</h4><ul><li>xDSL技术就是用数字技术对现有的模拟电话用户线进行改造，使他能够承载宽带业务</li><li>虽然标准模拟电话的频带被限制在300-3400khz的范围内，但用户线本身实际可通过的信号频率仍然超过1mhz</li><li>xDSL技术就是把0-4khz低端频谱留给传统电话业务，把原来设备中没有利用的高端频谱留给用户上网使用</li><li>DSL就是数字用户线的缩写，DSL的前缀x则表示在数字用户线上实现的不同带宽方案</li></ul><ul><li><p>ADSL的极限传输距离</p><ul><li>ADSL的极限传输距离与数据率以及用户线的路径有很大关系，用户线越细，信号传输时候的衰减就越大，而所能得到的最高数据传输速录与实际的用户线上的信噪比密切相关</li><li>例如0.5毫米的用户线，在网速为2.0Mb/s的时候可以传输5.5公里，传输速率提高到6.1Mb/s时，传输距离缩短为3.7公里</li><li>如果把用户线直径减少到0.4毫米，传输距离就缩小为2.7公里</li></ul></li><li><p>ADSL的特点</p><ul><li>上行和下行带宽做成不对称的</li><li>上行是用户到网络服务商，下行是反过来</li><li>ADSL在用户线（铜线）两端各安装一个ADSL调制解调器，目前我国采用的多为DMT调制技术</li></ul></li><li><p>ADSL的数据率</p><ul><li><p>当ADSL启动时，用户线的两端ADSL调制解调器就测试可用的频率，各子信道受到的干扰情况，以及在每一个频率上测试信号的传输质量</p></li><li><p>ADSL不能保证固定的数据率，对于质量比较差的用户甚至无法开通ADSL</p></li><li><p>通常下行速率在32kb到6.4mb之间，而上行速度一般在32kb到640kb之间</p></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机网络</tag>
      
      <tag>物理层</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/网络_3-数据链路层</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BD%91%E7%BB%9C_3-%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BD%91%E7%BB%9C_3-%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/</url>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h1 id="3-数据链路层"><a href="#3-数据链路层" class="headerlink" title="3.数据链路层"></a>3.数据链路层</h1><h4 id="3-0概述"><a href="#3-0概述" class="headerlink" title="3.0概述"></a>3.0概述</h4><p>数据链路层使用的信道主要有以下两类</p><ul><li>点对点信道<ul><li>这种信道使用一对一的点对点通信方式</li></ul></li><li>广播信道<ul><li>这种信道使用一对多的广播通信方式，因此过程较为复杂，广播信道上的链接主机很多，因此必须使用专门的共享信道协议来协调这些数据的转发</li></ul></li></ul><h4 id="3-1-数据链路和帧"><a href="#3-1-数据链路和帧" class="headerlink" title="3.1 数据链路和帧"></a>3.1 数据链路和帧</h4><p>链路是一条无源的点到点的物理线路，中间没有任何的其他交换节点。数据链路是除了物理线路之外，还得有通信协议来控制这些数据的传输，若把实现了这些协议的软硬件加到链路上，就构成了数据链路。</p><p>现在最常用的办法是使用适配器（网卡）来实现这些协议的硬件和软件，一般的适配器都包括了数据链路层和物理层这两层的功能。</p><p>数据链路层传输的是<strong>帧</strong>。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1248_20_314.png" alt=""></p><p>三个基本问题</p><ul><li>封装成帧</li><li>透明传输</li><li>差错控制</li></ul><ul><li>封装成帧：</li></ul><p>在一段数据的前后分别打上首部和尾部，首部尾部可以确定帧的界限。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1248_33_763.png" alt=""></p><ul><li>透明传输问题：</li></ul><p>类似于我想在markdown文件中打出[]()字符缺被误以为是超连接一样，需要在[]()字符之前加上转义字符\。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1249_02_535.png" alt=""></p><ul><li><p>CRC差错检测</p><p>传输过程中可能出现比特差错，1变成0或者0变成1。一段时间内传输错误的比特与总比特比值称谓误码率。为保证数据传输的可靠性，必须采用各种差错检测机制。</p></li></ul><p>  CRC差错检验算法就是让一串字符串加上几位0，然后将其除以某个固定的字符串，最后得出余数。然后根据二进制的数学特征，将余数取代之前几个0的位置，然后将这个字符串+余数的串发送出去。接收端收到后将这个大串除以某个固定的字符串，最后要是得到的余数为0就说明传输正确，如果不得0就错误要丢弃。<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1249_31_389.png" alt=""></p><h4 id="3-2点对点协议PPP"><a href="#3-2点对点协议PPP" class="headerlink" title="3.2点对点协议PPP"></a>3.2点对点协议PPP</h4><p>全世界用的最多的数据链路层协议是PPP（Point to Point）</p><p>PPP协议满足的要求：简单，封装成帧，透明性，多种网络层协议，差错检测，网络地址协商等等</p><p>PPP协议不需要的功能：纠错，流量控制，序号，多点线路，半双工或单工链路</p><p>PPP协议的三个组成部分</p><ul><li>一个将IP数据报封装到串行链路的方法</li><li>链路控制协议LCP</li><li>网络控制协议NCP</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1249_51_947.png" alt=""></p><p>​    </p><p>PPP协议的工作状态</p><p>当用户对ISP拨号上网时，路由器的调制解调器对拨号进行确认，并建立一条物理连接。</p><p>PC机向路由器发送一系列LCP分组（封装成多个PPP帧），这些分组和响应选择一些PPP参数，和进行网络层配置，NCP给新接入的的PC机分配一个临时的IP地址，通信完毕后，NCP收回临时IP，释放网络层连接，LCP释放数据链路层和物理层的连接。</p><h4 id="3-3局域网的数据链路层"><a href="#3-3局域网的数据链路层" class="headerlink" title="3.3局域网的数据链路层"></a>3.3局域网的数据链路层</h4><p>局域网最主要特点是，网络为一个单位所有，且地理范围和站点数目有限。</p><p>局域网的优点：</p><p>广播，一个站点可以方便地访问全网，局域网的主机可以共享连接在局域网上的各种硬件和资源。</p><p>可扩展性良好。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1250_02_184.png" alt=""></p><h4 id="3-4适配器的作用"><a href="#3-4适配器的作用" class="headerlink" title="3.4适配器的作用"></a>3.4适配器的作用</h4><p>网络接口板又称为通信适配器，或网络接口卡（网卡），网卡的重要功能是</p><ul><li><p>进行串行/并行转换</p></li><li><p>对数据进行存储</p></li><li><p>在计算机的操作系统安装设备驱动程序</p></li><li><p>实现以太网协议</p></li></ul><p>计算机通过适配器(网卡与局域网通信)</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1250_11_401.png" alt=""></p><h4 id="3-5数据链路层的mac子层"><a href="#3-5数据链路层的mac子层" class="headerlink" title="3.5数据链路层的mac子层"></a>3.5数据链路层的mac子层</h4><p>mac层就是媒体接入控制层（medium access control）。所有与传输媒体有关的内容都放在mac层。</p><p>局域网中，硬件地址又称为物理地址，或者mac地址。适配器从网络接收到mac帧后就首先使用硬件检查mac帧中的mac地址。</p><p>如果是发往本站的帧就收下，否则丢弃。</p><p>mac帧格式</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1251_26_001.png" alt=""></p><h4 id="3-5-1数据链路层的llc子层"><a href="#3-5-1数据链路层的llc子层" class="headerlink" title="3.5.1数据链路层的llc子层"></a>3.5.1数据链路层的llc子层</h4><p>llc层与传输媒体无关，不管采用何种协议的局域网对llc层来说都是透明的。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1252_33_876.png" alt=""></p><h4 id="3-6扩展局域网"><a href="#3-6扩展局域网" class="headerlink" title="3.6扩展局域网"></a>3.6扩展局域网</h4><ul><li>物理层扩展</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1252_43_362.png" alt=""></p><p>使用集线器扩展局域网的优劣势：</p><p>优势：扩大了通信范围</p><p>劣势：碰撞域变大，总吞吐量未变。</p><ul><li>数据链路层扩展</li></ul><p>数据链路层扩展局域网的思路是使用网桥。</p><p>网桥工作在数据链路层，它根据mac帧的目的地址对收到的帧进行转发。</p><p>网桥能过滤帧，当收到帧后，并不向所有的接口转发此帧，而是先检查此帧的目的mac地址，然后确定将帧转发到哪个接口。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1253_13_860.png" alt=""></p><p>使用网桥的好处：</p><p>过滤了通信量，扩大了物理范围，提高了可靠性，可互联不同物理层，不同mac子层和不同速率的局域网</p><p>网桥的劣势：</p><p>存储转发增加了时延，在mac子层并没有流量控制功能。网桥只适用于用户数量不太多的局域网，否则会因为过多的广播造成网络拥塞。</p><h4 id="3-7网桥"><a href="#3-7网桥" class="headerlink" title="3.7网桥"></a>3.7网桥</h4><p>网桥按照如下步骤处理收到的帧和建立转发表</p><ul><li>若从A发出的帧从接口X进入某网桥，那么这个接口出发反向一定可以把一个帧传回A</li><li>网桥每收到一个帧，就记录下其源地址和进入网桥的接口，作为转发表的一个项目</li><li>在建立转发表时把帧首部中的源地址写在地址这一栏的下方</li><li>转发帧时吗，根据帧的首部中的目的地地址来转发。</li></ul><p>网桥转发表的建立图示</p><p><img src="http://q0u9fsub0.bkt.clouddn.com///20191119/2EsnQOcgjqNl.png?imageslim" alt="mark">)<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1253_25_956.png" alt=""></p><p>透明网桥使用了 生成树算法</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1253_42_576.png" alt=""></p><p>互联在一起的网桥在彼此通信时吗，能够找出原来网络拓扑的一个子集，在这个子集里，整个网络中不存在回路，在任何两个网桥之间只有一条路径。</p><p>这样避免了某个转发的帧不停在回路中兜圈子。</p><h4 id="3-8交换机"><a href="#3-8交换机" class="headerlink" title="3.8交换机"></a>3.8交换机</h4><p>多接口网桥-交换机，以太交换机通常有十几个接口，以太网交换机其实就是一个多接口的网桥。交换机工作在链路层。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1255_19_111.png" alt=""></p><p>小结：</p><p>数据链路层的三大问题是解决将信息压缩为帧，透明传输（用转移符防止提前截断），误差控制（CRC检测算法）。</p><p>注意帧的格式分报头报尾和内容。</p><p>通过数据链路层扩展局域网的思路是在网与网之间增加网桥，网桥可以根据帧里的地址对各个主机进行转发操作。每个网桥都要维持一个自己的转发表，里边写的是目的地主机的地址和端口号。使用网桥时要注意maintain图的最小生成树，避免转发帧的时候掉进途中的回路不停兜圈子。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机网络</tag>
      
      <tag>数据链路层</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/统计学习方法_1_概论</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95_1_%E6%A6%82%E8%AE%BA/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95_1_%E6%A6%82%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="1-概论"><a href="#1-概论" class="headerlink" title="1.概论"></a>1.概论</h1><p>介绍基本概念，是对全书的概括。</p><p>首先叙述统计学习的定义、研究对象和方法，然后叙述监督学习。</p><p>然后提出统计学习方法的三要素：模型，策略和算法。</p><p>介绍模型选择，包括正则化、交叉验证和学习的泛化能力。</p><p>介绍生成模型和判别模型</p><p>介绍监督学习方法的应用： 分类问题，标注问题，回归问题。</p><h4 id="1-1统计学习"><a href="#1-1统计学习" class="headerlink" title="1.1统计学习"></a>1.1统计学习</h4><p><strong>定义</strong></p><p>统计学习是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测和分析的学科。</p><p>统计学习是概率论，统计学，信息论，计算理论，最优化理论，计算机科学等领域的交叉学科。在发展中逐步有了自己的理论体系和方法论。</p><p><strong>目的</strong></p><p>统计学习用于对未知数据预测和分析。</p><p>对数据的预测可以使计算机更加智能化/计算机的性能提升。</p><p>对数据的分析可以使人们获取新的知识。</p><p><strong>方法</strong></p><p>统计学习的方法是基于数据构建统计模型进而对数据进行预测和分析，统计学习由监督学习，半监督学习，无监督学习和强化学习等组成。</p><p>本书主要讨论的是监督学习，这种情况统计学习的方法可以概括如下：</p><p>从给定的有限的训练数据集出发（training set）,假设数据是独立同分布产生的；</p><p>假设要学习的模型属于某个函数的集合，称为假设空间。</p><p>把模型应用于某个评价标准，从假设空间中选取一个最优的模型，使得它对已知训练数据和未知测试数据(test set)在给定的评价准则下有最优的预测。</p><p>最优模型的选取由算法实现（调参？）。</p><p>统计学习的三要素：模型，策略，方法。(model,strategy,algorithm)</p><p>实现统计学习方法的步骤如下：</p><ol><li><p>得到有限的训练数据集合</p></li><li><p>确定包含所有可能的模型的假设空间，即学习模型的集合</p></li><li><p>模型选择的准则，即学习的策略</p></li><li><p>实现求解最优模型的算法，即学习的算法</p></li><li><p>通过学习方法选择最优模型</p></li><li><p>利用最优模型最新数据进行预测和分析</p></li></ol><p> 本书介绍统计学习方法为主，特别是监督学习方法，主要包括用于分类、标记、回归问题的方法。这些方法在自然语言处理、信息检索、文本数据挖掘等领域有着极为广泛的应用。</p><h4 id="1-3统计学习三要素"><a href="#1-3统计学习三要素" class="headerlink" title="1.3统计学习三要素"></a>1.3统计学习三要素</h4><p>方法 = 模型 + 策略 + 算法</p><p><strong>模型</strong></p><p>统计学习的首要问题是学习什么样的模型。</p><p>模型的假设空间包含了所有可能的模型，例如模型是线性函数，假设空间就是包含这些线性函数的函数族。</p><p><strong>策略</strong></p><p>有了模型的假设空间，统计学习接下来要考虑的是按什么样的准则学习最优模型。即明确评价标准。</p><p>引入损失函数和风险函数的概念。<strong>损失函数</strong>度量模型一次预测的好坏，<strong>风险函数</strong>度量平均意义下模型预测的好坏。</p><p>常用的损失函数比如0-1<strong>损失函数</strong>：预测对了得0分，错了得1分。得分越小说明模型越准。对数损失、平方损失同理。</p><p>学习的目标就是<strong>选择期望风险最小的模型</strong>。一方面根据期望风险最小学习模型要用到联合分布，另一方面联合分布式不可知的，所以监督学习是一个病态问题。</p><p>期望风险是模型关于联合分布的期望损失，经验风险是模型关于训练样本集的平均损失。根据大数定理，样本容量趋于N时，经验风险逼近期望风险。所以会试图使用经验风险预估期望风险。</p><p>但是效果通常不理想因为样本数不够。</p><p>所以会对经验风险进行校正。这就关系到统计学习的基本策略：<strong>经验风险最小化</strong>和<strong>结构风险最小化</strong>。</p><ul><li><p>在假设空间、损失函数、训练数据确定的情况下，<strong>经验风险</strong>函数式可以确定。可认为经验风险最小的模型就是最优模型。</p><p>按照这个思路，按经验风险最小化求解最优模型就是个最优化问题。</p><p>样本容量足够大的时候经验风险最小化有不错的效果，例如极大似然估计就是经验风险最小化的一个例子。</p><p>但是数据量小的时候，经验风险最小化就有过拟合的问题。</p></li><li><p>为了防止过拟合，有了<strong>结构风险</strong>最小化的提法。</p><p>就是在经验模型上加上了模型复杂度的罚项（正则化项），模型越复杂惩罚度越高。</p></li></ul><p><strong>算法</strong></p><p>算法是学习模型的具体计算方法，统计学习基于训练数据集，根据学习策略，从假设空间中按照算法求解最优模型。</p><p>这时统计学习问题归结为最优化问题，统计学习的算法成为求解最优化问题的算法。如果最优化问题存在显式的解析解，这个最优化问题就比较简单。</p><p>但通常解析解不存在，这需要用数值计算的方法求解。统计学习可以利用已有的最优化算法，有时也需要开发独立的最优化算法。</p><p>统计学习方法间的不同，主要来自于<strong>模型、策略、算法</strong>的不同。这就是统计学习的三要素。</p><h4 id="1-4模型评估与模型选择"><a href="#1-4模型评估与模型选择" class="headerlink" title="1.4模型评估与模型选择"></a>1.4模型评估与模型选择</h4><p><strong>训练误差和测试误差</strong></p><p>即是training error和test error。训练误差小说说明问题容易被学习，测试误差小说明模型的泛化能力做的不错。</p><p><strong>过拟合问题</strong></p><p>在预测空间里，可以存在1次到多次的模型选择。一次模型欠拟合，9次模型过于复杂过拟合。如何尽量逼近“真模型”？可使用正则化和交叉验证的方法。</p><p><strong>正则化和交叉验证</strong></p><ul><li><p>正则化是结构风险最小化的实施，在经验风险上加上一个正则化项。正则化项一般是关于模型复杂度的单调增函数。模型越复杂正则化项越大。</p><p>正则化项可以采取不同的形式，回归问题中，损失函数是平方损失，正则化项可以使参数向量的二范数/一范数。</p><p>正则化项的目的是选择经验风险和模型复杂度同时较小的模型。</p><p>正则化项符合奥卡姆剃刀原理：在所有可选择的模型中，能很好解释已知数据且简单的模型才是好模型。</p><p>从贝叶斯估计的角度讲，正则化项对应模型的先验概率。复杂的模型有较大的先验概率，简单的模型有较小的先验概率。</p></li><li><p>交叉验证</p><p>比如将训练集73分成，然后循环验证。</p></li></ul><h4 id="1-6泛化能力"><a href="#1-6泛化能力" class="headerlink" title="1.6泛化能力"></a>1.6泛化能力</h4><p>一般通过测试误差来评价学习的泛化能力，但是这种评价是依赖于数据集的，数据集有限，评价可能不准。</p><p>统计学习试图从理论上对泛化能力进行分析。</p><p>什么是泛化误差？如果学习到的模型是f，模型对未知数据的误差即为泛化误差。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/wps_ueli4LT9Sd.png" alt=""></p><h4 id="1-7生成模型和判别模型"><a href="#1-7生成模型和判别模型" class="headerlink" title="1.7生成模型和判别模型"></a>1.7生成模型和判别模型</h4><ul><li>生成模型由数据学习联合概率分布，然后求出条件概率分布作为预测模型。</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/wps_8hp1GOvI07.png" alt=""></p><p>​    典型的生成模型是隐形马尔科夫和朴素贝叶斯。</p><ul><li><p>判别模型直接学习决策函数f(x)，或者条件概率分布P(x|y)。这就是判别模型。</p><p>典型的判别式模型有 k临近，感知机，决策树，逻辑斯谛克模型，最大熵，SVM，提升方法，CRF等。</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>统计学习方法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/统计学习方法_2_感知机</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95_2_%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95_2_%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="2-感知机"><a href="#2-感知机" class="headerlink" title="2.感知机"></a>2.感知机</h1><p>感知机是二分类的线性分类模型，输入是实例的特征向量，输出是分类结果（-1,1）。</p><p>感知机对应输入空间（特征空间）中将实例划为正副两类的分离超平面。</p><p>工作原理是导入基于误分类的损失函数，利用梯度下降法岁损失函数极小化。</p><p>感知机1957年提出，是神经网络和SVM的基础。</p><p>本章先介绍感知机的模型，然后叙述学习策略，然后是算法（原始和对偶形式）。</p><h4 id="2-1感知机模型"><a href="#2-1感知机模型" class="headerlink" title="2.1感知机模型"></a>2.1感知机模型</h4><p>感知机模型是f(x) = sign(w.x + b).</p><p>其中sign是如下的函数</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/wps_Y0h9RgRT0Z.png" alt=""></p><p>也就是一个一元一次方程（直线）作为超平面的分类函数。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/wps_nfw9GjBBjJ.png" alt=""></p><h4 id="2-2感知机学习策略"><a href="#2-2感知机学习策略" class="headerlink" title="2.2感知机学习策略"></a>2.2感知机学习策略</h4><p>假设训练集是可分的，感知机的学习目的就是确定一个超平面把训练集分开。</p><p>为了找出这个超平面，就要确定感知机模型的参数w和b。</p><p>损失函数怎么定？定为错分类点到超平面的总距离w。（如上图）</p><p>某个点到超平面的距离怎么求？如下图，||w||是w的范数。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/wps_NC1ume3B2c.png" alt=""></p><p>损失函数的正式定义；</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/wps_xU9URkynlO.png" alt=""></p><h4 id="2-3感知机的学习算法"><a href="#2-3感知机的学习算法" class="headerlink" title="2.3感知机的学习算法"></a>2.3感知机的学习算法</h4><p>随机梯度下降法</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>统计学习方法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/线性代数_正交矩阵</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0_%E6%AD%A3%E4%BA%A4%E7%9F%A9%E9%98%B5/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0_%E6%AD%A3%E4%BA%A4%E7%9F%A9%E9%98%B5/</url>
    
    <content type="html"><![CDATA[<h1 id="正交矩阵"><a href="#正交矩阵" class="headerlink" title="正交矩阵"></a>正交矩阵</h1><p>如果一个矩阵乘以它自己的转置矩阵等于E（单位阵），则称为正交矩阵。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200517_1505_26_233.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>线代</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/线性代数_特征值&amp;特征向量计算</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0_%E7%89%B9%E5%BE%81%E5%80%BC&amp;%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F%E8%AE%A1%E7%AE%97/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0_%E7%89%B9%E5%BE%81%E5%80%BC&amp;%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F%E8%AE%A1%E7%AE%97/</url>
    
    <content type="html"><![CDATA[<h3 id="特征值特-amp-特征向量"><a href="#特征值特-amp-特征向量" class="headerlink" title="特征值特 &amp; 特征向量"></a>特征值特 &amp; 特征向量</h3><p>省略理论部分，一个例子讲清：<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1504_19_270.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1504_36_652.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>线代</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/线性代数_奇异值分解</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0_%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0_%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h1 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h1><h4 id="1-应用：影像压缩"><a href="#1-应用：影像压缩" class="headerlink" title="1.应用：影像压缩"></a>1.应用：影像压缩<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1501_07_483.png" alt=""></h4><h4 id="2-应用：过滤噪声"><a href="#2-应用：过滤噪声" class="headerlink" title="2.应用：过滤噪声"></a>2.应用：过滤噪声</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1501_44_400.png" alt=""></p><p>左图是原始图片。右图是经过SVD分解处理的图片，丢失了一些精确度，但是呈现效果更好，很可能丢失的精确度恰好是噪声。</p><p>具体做法是我们对15*25的矩阵做奇异值分解（SVD），然后得到如下15个特征值，其中前三个特征是比较重要，为了压缩数据和过滤噪声，我们只取前三个特征值，后边的全部丢弃。<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1501_57_277.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1502_14_915.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1502_27_306.png" alt=""></p><h4 id="5-计算例子"><a href="#5-计算例子" class="headerlink" title="5.计算例子"></a>5.计算例子<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1502_41_523.png" alt=""></h4><p>​    </p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1502_56_775.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1503_09_630.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1503_18_511.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>线代</tag>
      
      <tag>奇异值分解</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>5.杂谈/王威廉对博士生的要求</title>
    <link href="undefined2020/02/24/5.%E6%9D%82%E8%B0%88/%E7%8E%8B%E5%A8%81%E5%BB%89%E5%AF%B9%E5%8D%9A%E5%A3%AB%E7%94%9F%E7%9A%84%E8%A6%81%E6%B1%82/"/>
    <url>2020/02/24/5.%E6%9D%82%E8%B0%88/%E7%8E%8B%E5%A8%81%E5%BB%89%E5%AF%B9%E5%8D%9A%E5%A3%AB%E7%94%9F%E7%9A%84%E8%A6%81%E6%B1%82/</url>
    
    <content type="html"><![CDATA[<h1 id="Hiring-PhD-students"><a href="#Hiring-PhD-students" class="headerlink" title="Hiring PhD students"></a>Hiring PhD students</h1><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1314_54_995.png" alt=""></p><p>Q: What’s your research interest? </p><p>A: My research interests are in the broad areas of Artificial Intelligence. I am particular interested in Machine Learning and Natural Language Processing. More specifically, I am passionate about statistical relational learning, learning to reason, information extraction, multimodality, social media, and spoken language processing. Currently, I’m interested in deep learning methods for natural language processing, multimodal computing, and knowledge graph reasoning. You may want to read my research statement to get a better idea (though it might be slightly dated). </p><p>Q: What are the basic requirements for working with you? A: I have three basic requirements for my PhD students. First of all, students need to be </p><ul><li><p>Mathematically mature</p><p> (i.e., proficient and comfortable with probability theory, statistics, linear algebra, optimization, and calculus). One way to test yourself is to carefully go through some of the best ML papers published at recent ICML/NIPS/AAAI/IJCAI conferences, and see if you can fully understand the detailed Mathematical presentations from the main equations in those papers. </p></li><li><p>Secondly, I require my students to be proficient in programming</p><p>they should at least know one or more serious programming languages for relatively large-scale software engineering purposes (e.g., C, C++, Python, Java etc), and meanwhile, they have to master one of the command-line scripting languages such as Perl, AWK, Bash, Sed etc. The ability of prototyping ideas quickly in Matlab, Octave, R is often a big plus. Third, and probably most important: </p></li><li><p>hard-working</p><p>No matter how smart you are and what fields you are in, if you don’t spend enough time on your work, there is no way you can be an expert. An observation from the most successful PhD students is that they all work extremely hard. So, if you are a hard-working person who is passionate about advancing AI, and you are solid in Math and programming, please consider applying to UCSB. </p></li></ul><p>Q: What makes me stand out from the application pool? </p><p>A: To make you stand out, prior research experience is often a big plus: if you have prior publications at reputable locations, we know that you are on the right track. If you have published papers at leading conferences or journal, feel free to leave me a note and send me a copy. Alternatively, if you are a system builder / hacker, showing your contributions to major ML/NLP/AI projects on Github is also encouraged. Additionally, if you have very strong support letters from researchers that I know, it helps me to evaluate your potential. For undergraduate applicants without research experience, GPA and grades from standardized tests are often important.  </p><p>Q: How long does it take to do a PhD? </p><p>A: It depends. I’ve seen some very productive PhD students finishing in only 4 years. So the assumption is, if you are productive and you have created enough impacts, it is definitely possible to finish a PhD in a relatively short amount of time. </p><h4 id="总结（数学，编程，文章）"><a href="#总结（数学，编程，文章）" class="headerlink" title="总结（数学，编程，文章）"></a>总结（数学，编程，文章）</h4><p>三个要求和三个加分项：</p><p>要求一，数学上的成熟，精通概率论，线代，微积分，优化，统计。测试你自己的一种方法是仔细阅读最近在ICML/NIPS/AAAI/IJCAI会议上发表的一些最好的ML论文，看看你是否能从这些论文中的主要方程完全理解详细的数学演示。</p><p>要求二，精通编程，掌握如下语言至少一种：C、C++、Python、Java等，同时，必须掌握命令行脚本语言中的一个，如Perl、AWK、BASH、SED等。</p><p>要求三，勤奋，不管你多聪明，如果花在工作中的时间不够，那肯定成不了专家。</p><p>加分项一，著名刊物/会议发文</p><p>加分项二，在github上展示你对主要的ML/NLP/AL项目的贡献</p><p>加分项三，老师认识的研究人员的有力的推荐信。</p><p>tips，对于没有研究经验的本科申请者来说，GPA和标准化考试成绩非常重要。</p>]]></content>
    
    
    <categories>
      
      <category>5.杂谈</category>
      
    </categories>
    
    
    <tags>
      
      <tag>phd申请</tag>
      
      <tag>UCSB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>5.杂谈/王阿-申请phd思路</title>
    <link href="undefined2020/02/24/5.%E6%9D%82%E8%B0%88/%E7%8E%8B%E9%98%BF-%E7%94%B3%E8%AF%B7phd%E6%80%9D%E8%B7%AF/"/>
    <url>2020/02/24/5.%E6%9D%82%E8%B0%88/%E7%8E%8B%E9%98%BF-%E7%94%B3%E8%AF%B7phd%E6%80%9D%E8%B7%AF/</url>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="我是如何申请到phd的"><a href="#我是如何申请到phd的" class="headerlink" title="我是如何申请到phd的"></a>我是如何申请到phd的</h2><p>转载自知乎–王阿的阿</p><p><strong>谢谢你们。</strong></p><p>在这篇经验贴里，我会告诉你我当时申请时面临的处境、我的决策过程，以及申请当中各个环节最精华的经验和技巧，我都毫无保留地贡献在这里了。</p><p>读完这篇文章，除了我介绍的各种申请技巧以外，我希望能让你认识到申请过程中决策和心态的重要意义：</p><ul><li>想成功申请美国的PhD，除了自己软硬件要出色，还有很重要的一点是对自己的定位，包括认识到自己的目标、兴趣以及自己的长处和缺陷。<strong>不论你是申请哪个学科的PhD，请重点关注我做决策时的思考内容和思考方向，这也许会对你有所启发。</strong></li><li>心理韧性、抗打击能力也是申请成功的一个重要因素。如果你知道GradCafe这个北美grad school申请论坛，你会发现上面有很多人诉说自己第二次、第三次才申请成功的经历。我就是其中一员。</li></ul><p>我的故事是这样婶儿的……</p><p>2017一整年是我这辈子最难熬的一年，这一年我第一次（也只申请了这一次）申请美国的临床心理学PhD（clinical psychology），申了13所学校。2017年的头四个月陆续收到13封拒信，无一录取，甚至连一个面试都没有。当时的我：</p><ul><li>已经在纽约市生活了三年；</li><li>拿到了美国的心理咨询硕士学位；</li><li>从大三下学期开始参与心理学实验室科研算起，满打满算我有了三年科研经验（我在国内读的本科，心理学专业在全国名列前茅）；</li><li>虽然水但也在纽约州和American Psychological Association大会上以第一作者身份做过poster presentation；</li><li>在美国有过一年半的与专业相关的实习经验；</li><li>一位认识八年关系很好的本科中国教授（同时是心理与行为研究院院长）和两位认识两年以上的美国教授为我写推荐信，其中一位教过我课，另一位在她的实验室里做过科研；</li><li>硕士GPA 3.8/4.0；</li><li>GRE verbal 156，quant 164，writing 3.5</li><li>Psych GRE (GRE subject test）740 （88th percentile）</li><li>因为有了美国硕士学位我所申请的这些学校没有要求我再考TOEFL</li></ul><p><img src="https://pic2.zhimg.com/50/v2-8d623aae677c9d6e5d2d773d29f6715c_hd.jpg" alt="img"></p><p>当我看到GradCafe上很多人报offer或是interview invite时我的心情是这样的</p><p>那一年申请，我彻底颗粒无收，即便是我认为的“保底”学校也没有录取我。</p><p>以下是我第一次申请临床心理学时申请的学校，你们可以看出来，我申请的这些学校没有几所是大众认知的“名校”：</p><p>Fairleigh Dickinson University</p><p>Sam Houston State University</p><p>University of Hawaii</p><p>University of Wisconsin-Madison</p><p>University of North Texas</p><p>CUNY Graduate Center</p><p>San Diego State University</p><p>Simon Fraser University</p><p>Louisiana State University</p><p>University of Missouri</p><p>University of Maryland, College Park</p><p>University of Nebraska Lincoln</p><p>University of Texas, El Paso（这个不是clinical psych，是legal psychology）</p><p>我无法判断我是在哪一轮被刷下来的，我自己猜测有以下几点原因：</p><ul><li>当时的推荐信不是很强（在下面我会讲什么是强推）</li><li>没有跟未来的导师（以下称之为professor of interest，或POI）直接相关的研究经验和研究成果</li><li>不会套磁也没有认真套磁（根本原因还是第二点，没有相关的研究经验和成果）</li></ul><p>我差点被击垮。当时大学挚友准备国内考研的面试，我每天早上帮他准备英语口语。每天能跟他说一个来小时话是帮我挺过那段艰难时期的良药。诺大的纽约，我周围却没有可以谈心的朋友，所以每次跟他微信语音都会把我们之间的对话录音，心情低落的时候就听一听。在这期间，我考虑过回国找工作，所以我跟几位国内的朋友聊过回国的前景。朋友们能说什么呢？无非也就是“回国也挺好”罢了。五月份之前，我也在美国投简历，希望一边工作一边再准备申请。投出的简历没有下文，我只好在为我写推荐信的教授的实验室继续做volunteer research assistant来维持OPT学生身份。五月底，过了我28岁生日后，我搬离Manhattan来到Queens，在这里我终于不用再住用屏风围起来的客厅，还有了自己的卫生间和浴室。当你住了三年多客厅后你不知道有自己的房间有多爽（fun fact：在Manhattan住客厅也要大概在$600-$900之间，如果你住的是$1500-$4000的apartment）。</p><p>我们停一下，存个盘。如果换作是你：</p><ul><li>在你读了13封拒信后；</li><li>身边没有父母或知心朋友倾诉；</li><li>找不到工作没有收入；</li><li>不可能知道第二次申请是否能够成功</li></ul><p>你还会再申请第二次吗？我没法替你回答这个问题。你需要全盘考虑自己的人生目标、生活状态、家庭经济收入以及其他因素再决定接下来的第二年要不要再申请一次。但有一点我希望你知道，如果你真的热爱做研究，那就不要害怕再经历一遍申请，不管你选择什么时候再次申请。</p><p>有了第一次失败的经验后，我重新思考了自己的定位：</p><ul><li>我第一次申请的clinical psychology，是心理学博士申请里最难申请的心理学分支（没有之一）。根据APA出版的Graduate Study in Psychology 2016年的申请数据显示，录取率在2%-8%之间。另外从我认识的教授那里得知，有些学校录取后还不保证有funding（你只能自费或自己想办法找funding）。所以，我第二次申请时决定不再申请这个方向了，我没有必要死磕这个方向，因为实在太冒险。</li><li>我意识到，<strong>我无法在不到半年的准备时间里（6月开始到12月1日申请截止日期）实现质的突破</strong>（例如发表一篇高质量的论文）。这种情况下再申请基本是又要浪费一年时间、精力和金钱。</li><li>根据我父母的财力状况和我自己的身心状态，如果我第二次申请再不成，我基本就没有机会再申请第三次了。</li></ul><p>所以为了最大化我的申请成功可能性，我要选择一个相对更容易被录取的专业方向。现在想来，这是我当时做的最恰当的一个关键决定。</p><p>在这里我说一下博士选校的问题，因为我发现有些人博士选校竟然是顺着所谓的专排一口气申请20个学校。博士选校最关键的几点建议：</p><ul><li>请主要按研究兴趣选校。你的研究兴趣应该跟你POI的研究兴趣一致，在一个小领域内。比方说，你想研究抑郁症里的一个topic，POI也研究抑郁症，但POI主要研究的topic不是你想研究的topic，这就没有问题，但如果POI主要是研究精神分裂症的，那你们的研究兴趣就不一致，POI就会觉得你跟他/她不match；实在不确定你们是否match，直接给POI发邮件问！表害羞！</li><li>不要把自己的研究兴趣限制得太小，否则你的选择余地会很小。你可以有一个主要的研究兴趣（如抑郁症），在这个领域里你有比较丰富的研究经验，然后为了扩大选择POI的范围，你可以有一些其他的研究兴趣（比如精神分裂症），这样你就可以选择这个program里其他的POI成为你的potential advisor，具体参考我下面提供的我的PS里如何表述的。扩大自己的研究兴趣是申请成功的一个很重要的一点，因为如果你只选择一个program里的一个POI，如果他录取了其他申请者，你就直接出局了，但如果你选择了两个到三个（不要再多了），那你被录取的可能性就大多了；</li><li>你当然可以参考专排，但如果专排靠前的某个学校里的教授没有你感兴趣的研究领域，你干嘛申请这个学校呢？你没法跟这样的教授套磁，没法把你的PS写得让他们感觉你们之间的fit，所以不要申请这样的学校；</li><li>衡量一个program适不适合你，除了看你的研究兴趣，还要衡量一下自己够不够格申请地到这个学校。硬件方面，你应该至少达到admitted students的GPA、GRE的中位数，软件方面，你需要直接跟current students联系，浏览他们的CV，看他们做了什么，咨询他们如何脱颖而出的。你也可以直接问POI（如果你没考虑好谁是你的POI，那就随便问一个），他们录取学生的标准，他们看重什么特质、经验和技能，然后朝这个方向努力。</li></ul><p>再插一句。Clinical psychology的很多program要求考GRE sub（psych GRE），或者你本科是其他专业的想在读博士转到clinical psychology，这种情况下你也可能被学校要求考sub。在这里我分享一下我当时自己整理出来的考点，可以到这里下载：</p><p><a href="https://drive.google.com/drive/folders/1nyCu4prLMCDU4PdPNfYARLL3rb75MJmx?usp=sharingdrive.google.com" target="_blank" rel="noopener">https://drive.google.com/drive/folders/1nyCu4prLMCDU4PdPNfYARLL3rb75MJmx?usp=sharingdrive.google.com</a></p><p>我用思维导图的方式覆盖了需要你掌握的知识点，你需要配合ETS官方推出的考试大纲、一两本英文心理学入门教材（Psychology and Life之类的）以及一本模拟考题集来准备。我当时还看了这个网站上的视频（不是ETS官方出品，而且是收费的）：</p><p><a href="https://study.com/academy/course/gre-psychology-study-guide-test-prep.htmlstudy.com" target="_blank" rel="noopener">https://study.com/academy/course/gre-psychology-study-guide-test-prep.htmlstudy.com</a></p><p>Sub全是选择题，当时考完感觉非常懵逼，以为要再考一遍了，但最后分数挺好（建议你考到85%甚至90%以上，尤其针对转专业的申请者），究其原因是其中很多题目你不能靠死记硬背来拿分，需要你真正理解这些心理学概念，所以很多题目拿不准。</p><p>这里有一位被哥大clinical psych PhD录取的中国学生在寄托论坛上发的经验帖，大家感受一下从国内被美国常春藤顶尖名校PhD program录取的学生的水平：</p><p>Clinical/Other Psych+Neuroscience PhD - 美国留学 寄托家园留学论坛bbs.gter.net</p><p>如果有其他跟clinical psychology申请相关的问题，也许你可以在下面这个帖子里找到。这个帖子里是一位clinical psych的教授回答申请者的各种问题，很有参考价值：</p><p><a href="https://forums.studentdoctor.net/threads/advice-from-prof-for-applicants-to-phd-programs.489204/forums.studentdoctor.net" target="_blank" rel="noopener">https://forums.studentdoctor.net/threads/advice-from-prof-for-applicants-to-phd-programs.489204/forums.studentdoctor.net</a></p><p>回到正题。第二次申请，我决定申请quantitative psychology和educational measurement and statistics（计量心理学和教育测量和统计）。在看GradCafe上以下几个讨论话题之前我从来没听说过这个分支，但当我读完这些讨论内容后，我脑子里已经做出申请这个方向的决定了（如果你感兴趣，请重点看Spunky的回答，他是UBC的quant psych的博士）：</p><p><a href="https://forum.thegradcafe.com/topic/61618-quantitative-psychology-phd/forum.thegradcafe.com" target="_blank" rel="noopener">https://forum.thegradcafe.com/topic/61618-quantitative-psychology-phd/forum.thegradcafe.com</a></p><p><a href="https://forum.thegradcafe.com/topic/46739-quantitative-psychology-educational-measurement-fall-2014-sop-help/?tab=comments#comment-1058018960forum.thegradcafe.com" target="_blank" rel="noopener">https://forum.thegradcafe.com/topic/46739-quantitative-psychology-educational-measurement-fall-2014-sop-help/?tab=comments#comment-1058018960forum.thegradcafe.com</a></p><p>DIfference in Quantitative Psychology Programsforum.thegradcafe.com</p><p><a href="https://forum.thegradcafe.com/topic/56006-which-field-is-right-for-me/?tab=comments#comment-1058152406forum.thegradcafe.com" target="_blank" rel="noopener">https://forum.thegradcafe.com/topic/56006-which-field-is-right-for-me/?tab=comments#comment-1058152406forum.thegradcafe.com</a></p><p>这里有一个比较完整的学校名单（不过这个单子里落了University of Texas, Austin和我现在所在的学校）：</p><p>Programs in Quantitative Psychology and Quantitative Methodssmep.org</p><p>我做这出这个改变申请方向的决定有以下几个原因：</p><ul><li>申请clinical psychology的难度真的很高。如果你去各个学校的网站上看faculty和current student的介绍，你很少会发现中国人的名字。每个program的申请人数都太多了，我极少见到有低于100个申请者的program：先说几个名校来举例，Graduate Study in Psychology (2016）的数据显示，Berkeley的申请人数达到了397个，仅录取8个（录取率2%）；Yale的申请人数为264，录取5个（录取率2%）；再说几个知名度低的学校：The Catholic University of America的申请人数是222，录了11个（录取率5%）；Georgia State University的申请者达到436人，录了19个（录取率4%）。你可以看出来，竞争激烈程度只能用rediculously competitive来形容。</li><li>我一直很喜欢学心理统计学，同时我这门课的成绩很好，我也非常喜欢用R来做数据分析，这是最根本的原因。我的理念一贯是兴趣第一，如果我对统计不感兴趣，我不会考虑申请这个方向的。我也有认知心理学的研究经验，但我没有申请那个方向，就是因为我对认知心理学不感兴趣；</li><li>所有社会科学分支都需要数据分析，因而计量心理学专业的博士生非常抢手，我可以去很多实验室做助研，跟不同专业的学者合作，可以学到很多不同领域的专业知识；</li><li>我有计量心理学方面的研究经验：我参与过几个meta-analysis project，如果你往下读就会发现，我这方面的经验是我被录取的最关键因素之一；</li><li>知道这个心理学分支的学生很少，所以申请者数量比临床心理学要少的多，竞争压力就小，成功申请的可能性就可能更高一些；</li><li>计量心理学是心理学学科里唯一一个博士毕业数量少于教职需求量的分支，也就是说将来更容易找到教职，如果我打算进入academia的话</li></ul><p>我必须声明的是，我没有任何要劝你们放弃申请临床心理学这个方向的意思。我之所以没有再申请第二次临床心理学的原因已经写在上面了，这是我在当时做出的最符合我实际情况的选择。</p><p>在第二次申请中，我集种精力提高我的软件方面，包括writing sample和statement of purpose，提前告诉我的三位letter writer（推荐人）我要重新申请以及我的新的申请方向是什么并得到了他们的支持。我没有盲目地再去刷GRE成绩。之所以走这条路线，是因为：</p><ul><li>我在GradCafe上阅读了大量经验帖，都是quant psych PhD students分享的很有价值的申请经验—他们强调，申请者应该突出自己对quant方面的热爱和经验，尤其是体现R programming方面的能力；</li><li>APA出版的Graduate Study in Psychology 2016年的申请数据显示，拿到录取的申请人的GRE成绩中位数大概在310-315之间，即便是quant部分只要上160就算是不错的分数了，<strong>并没有国内考试机构鼓吹的要上325分以上，quant要考满分170</strong></li></ul><p>根据我制定的申请路线，在接下来的半年内，我主要做了如下工作：</p><ul><li>为了利用好我做过meta-analysis研究的这一优势，我几乎只申请了以meta-analysis为主要研究兴趣的POI所在的program。这一选校和定位策略后来证明非常有效，在所有我申请的学校里，没有录取我的POI都不是研究meta-analysis的，也就是说录取我的POI都是研究meta-analysis的。所以PhD的申请一定要记住：自己的研究兴趣和POI的研究兴趣要匹配（但不用完全重合，只要在一个研究领域即可）。对于申请PhD，请不要像申硕士那样，顺着排名申前10或前20的。</li><li>花了三个月时间打磨出一篇达到接近发表水平的writing sample—还记得我之前提到过的我参与的meta-analysis项目吗？我在那个项目中自学了如何用R进行单组率的meta-analysis（一种小众的统计方法）。我在这篇writing sample中系统地总结了我自学的内容，写成了一篇60多页的tutorial，既包括理论部分也包括用R编程的部分。头两个多月全都花在coding上了，这个过程颇为艰辛，在coding过程中我遇到过数不清的问题。我在ResearchGate、CrossValidated、Stack Overflow上问问题，或者直接发邮件问一些专家，一个问题一个问题解决的。在我之前还没有哪个学者系统写过这样一篇tutorial，所以这是我的亮点，能把我跟其他申请人区分出来。写成之后，为保证没有任何语法错误，我找了美国的proofreading机构帮我修改润色语法。如果你感兴趣，可以在我的ResearchGate上下载阅读，当前阅读量已经超过2000人，并且得到不少好评：</li></ul><p>(PDF) How to Conduct a Meta-Analysis of Proportions in R: A Comprehensive Tutorial”<a href="https://pic2.zhimg.com/v2-f13a77e5df055b6d49f36a6559fb946d&quot;" target="_blank" rel="noopener">https://pic2.zhimg.com/v2-f13a77e5df055b6d49f36a6559fb946d&quot;</a></p><ul><li>我很清楚我申请学校的POI不可能有耐心和时间读完这篇超长的writing sample。所以我又花了两周录制了一段三十分钟的R programming教学视频并上传到YouTube来演示我的writing sample到底写了些什么。我的视频也得到不少好评（33个赞，其中一个是我自己点的，0差评）。感兴趣的可以在这里观看：</li></ul><p><a href="https://www.youtube.com/watch?v=2wbXTFvaRnM&amp;list=PLmYuhFRfrz_5n_9iZ_kwZcK65IUjDXgqR&amp;index=2&amp;t=1129swww.youtube.com" target="_blank" rel="noopener">https://www.youtube.com/watch?v=2wbXTFvaRnM&amp;list=PLmYuhFRfrz_5n_9iZ_kwZcK65IUjDXgqR&amp;index=2&amp;t=1129swww.youtube.com</a></p><ul><li>到这里还没有结束—我不想发两个链接给POI，所以为了展示自己的特色，我又制作了自己的学术网站，把简历、writing sample以及视频上传到网站上。这样我在发给POI的套磁email里只需要附上网址就可以很优雅地让POI了解我了。我当时是用WordPress做的网站，现在已经失效了，但我又用GitBook做了一个新的，感兴趣的话可以在这里浏览（网站修葺中）。</li></ul><p>以上这三部分绝对是我application package里面最有亮点的部分。接下来我们谈谈如何向POI介绍并展示自己（套磁）。以下是我的套磁信：</p><p>11/7/2017 11:56（我套磁很晚了，但不要紧，只要你真有料，deadline一周前套磁都可以）。补充一点：一般是提交申请前套瓷，这样你可以知道你的POI今年收不收学生，是否鼓励你继续申请。</p><p>Subject: Inquiry from a prospective graduate student for Fall 2018</p><p>Dear Dr. LastName,</p><p>I am interested in doing a PhD in the area of meta-analysis. I would like to inquire whether you are currently accepting graduate students.</p><p>I have research experience in meta-analysis. I hope that you can take a quick look at my paper on how to do a meta-analysis of proportions using R on my personal academic site (website address).</p><p>I also have a few specific questions about your program that I’d like to ask you. I know you are very busy, so I appreciate any time you can give me.</p><p>Regards,</p><p>MyFirstandLastName</p><p>M.A. in Forensic Mental Health Counseling</p><p>John Jay College of Criminal Justice, the City University of New York</p><p>这封套磁信有以下几个特点和功能：</p><ul><li>简洁，切忌长篇大论。教授每天会收到几十封邮件，你要保证他/她能在一分钟内读完你的邮件，记住你并想继续了解你；</li><li>核心功能：我跟你很搭（因为我有你研究领域的经验和你需要的研究技术，通过writing sample或publication来体现）；</li><li>展示自己的强项，impress your POIs so they would want to advocate for you；</li><li>邮件内容包括：</li></ul><ol><li>我想在你（POI）的研究领域读博士；</li><li>请问你今年是否招生（非常重要！）；</li><li>我有相关的研究经验，请见我的CV、论文或网站（最好有自己的网站）；</li><li>我有几个关于这个program的问题想咨询一下你；</li><li>感谢你阅读这封邮件</li></ol><p>之所以要询问POI几个问题，是为了：</p><ul><li>加深POI对你的印象</li><li>展示你真的对该program非常感兴趣</li><li>还有一个很实用的作用，就是POI的回答可以用在你的PS或SOP中</li></ul><p>你可以自行发挥，想出五个问题，但我建议你一定要问这个问题：</p><p>As far as you are concerned, what are the strengths of your program, in terms of faculty, course offerings, job prospects, etc.?</p><p>看到了吧，你可以把POI的回答内容放进你的PS里，向admission committee解释你为什么要申请该项目（因为POI回答中提到的这些优势），这表明你是经过认真调查后才申请该项目的，而不是随意申请的。</p><p>我跟每一个申请学校的POI都发邮件介绍了自己，让他们看到我的网站（我也给每个program里的一到两个老生发了邮件，询问他们program的优缺点，得到很多有用的信息）。我的套磁信的效果让我自己都感到吃惊，举几个例子：</p><p>Response from POI A:</p><p>11/7/2017 12:02</p><p>Yes I am taking students. Frankly your tutorial page is better than my web site… I am tempted to accept you just based on that!</p><p>Actually I can’t do that even if I wanted to but I am happy to talk with you or write back and forth to answer any questions. Also, I am cc’ing one of my new doc students, J.M., who may also share some insights into our program.</p><p>If you want to talk, today I am working at home and am quite open. Or tomorrow morning would also work. Suggest a time and I can see if it works for me, or just reply to my email with your questions.</p><p>Response from POI B:</p><p>11/9/2017</p><p>Thank you for the email and for your interest in our program. Really nice to learn a little bit about your work and interest from your introduction and your website (and good job in maintaining your online presence). Yes, I am currently accepting doctoral students and we definitely need more methodologists working on effect size statistics and meta-analysis. Therefore, I would strongly encourage you to apply for our program. Feel free to ask questions about our program and I look forward to talking to you.</p><p>P.S. Your HTML and YouTube tutorial are really neat. Great job!</p><p>Reponse from POI C:</p><p>11/10/2017</p><p>Thank you for reaching out to me concerning applying for the PhD program in Educational Psychology and Methodology. I am in particular interested in your potential application as your research interest is in line with my research expertise. I have a lot of projects to collaborate on in the area of multilevel meta-analysis of SCEDs (and applying Bayesian estimation). Enclosed you can find my CV.</p><p>Yes, currently we have full assistantships available (for four years) and I encourage you to apply given your research interest.</p><p>I am very much looking forward learning more about you and your background. Of course I am also happy to answer any questions you have concerning the program and such. If you’d like, we can arrange a Skype conversation for the upcoming week. Or you can always send me your questions per email and I will answer them this weekend. Please feel free to send me your application package (if you have that ready) so I can already have a look at that.</p><p>如果你的套磁信能达到这样的效果，那录取就有戏了（我不想说稳了，我比较保守）。</p><p>接下来我讲一下另一个重头戏：personal statement/statement of purpose（以下统称PS）。在这里我推荐各位读两篇同一个人写的PS。作者是Berkeley数学系的博士Piotr Achinger，他第一次申请被拒，第二次申请成功了。</p><ul><li>这是被拒那次写的PS：</li></ul><p><a href="https://drive.google.com/file/d/1f2skv0yq5Xo9RQr29m89dWiUTVycrwgG/view?usp=sharingdrive.google.com" target="_blank" rel="noopener">https://drive.google.com/file/d/1f2skv0yq5Xo9RQr29m89dWiUTVycrwgG/view?usp=sharingdrive.google.com</a></p><ul><li>这是拿到录取的那次写的PS：</li></ul><p><a href="https://drive.google.com/file/d/16QnxOBaZ9jfEKNR4yIF3dfPraXIPr476/view?usp=sharingdrive.google.com" target="_blank" rel="noopener">https://drive.google.com/file/d/16QnxOBaZ9jfEKNR4yIF3dfPraXIPr476/view?usp=sharingdrive.google.com</a></p><p>我来帮你总结一下弱PS和强PS到底有什么差别。</p><p>强PS：</p><ul><li>能让读者知道你做过什么并体会到你的<strong>批判性思维</strong>（PhD的核心能力）；</li><li>是既有广度又有深度的立体结构（体现你的专业性）；</li><li>读起来带劲，能让读者随你一起思考</li></ul><p>弱PS：</p><ul><li>只能让读者知道你做过什么；</li><li>只有广度而缺乏深度的平面结构；</li><li>读起来无聊，因为没人关心陌生人的生平事迹</li></ul><p>那如何体现你的思考能力呢？</p><ul><li>举2-3个科研经历；</li><li>描述你研究的问题，可以是前人没有解决的问题以及/或你发现前人研究中的缺陷：</li></ul><p>- 前人没有解决的问题：你用什么方法解决的这个问题以及解决方法的效果</p><p>- 前人研究中的缺陷：用推理证明为什么是缺陷</p><p>- 提出新问题（optional: 提出可能的大体解决方法）</p><ul><li>不要写成下面这样，这样写仅仅是扩充了你的CV：</li></ul><p>- 我参与了一个什么研究，我负责干什么，我克服了什么困难，我们的成果是什么</p><p>我推荐的写作结构：</p><ul><li>简略：读博原因—因为我对最近进行的一项科研项目很感兴趣，想继续钻研下去（而非童年往事，不要灵光乍现突然顿悟了，也无需参透人生使命是什么）；</li><li>详细：体现思考能力的科研经历并展示你具备这个专业方向所需要的科研技术，例如编程；</li><li>简略：读博期间的研究方向；</li><li>简略：毕业后的目标—academia or industry?</li><li>详细：为什么想申请贵校博士项目（夸赞该项目的强项，你跟POI套磁过程中得到的信息在这里排上用场了）以及想跟哪几位POI读博士（实际上这一段也可以放在PS开头）；</li><li>optional—没有什么意义总结性结尾：我相信自己是跟贵项目匹配的一名出色的申请者</li></ul><p>附上我的一篇PS：</p><p>After conducting a series of meta-analyses <strong>(这是POI的研究兴趣，开头就点出来，吸引POI的眼球)</strong> at the Sex Offender Research Lab led by Dr. E. J. at John Jay College of Criminal Justice where I pursued my master’s degree, I discovered I was passionate about learning to improve existing quantitative approaches and wanted to try to develop new ones to advance the field of psychology. Thus, my first academic objective is to pursue a doctorate in quantitative methods in order to effectively pursue my research interests and attain my professional goals <strong>(因为最近做的一系列研究我对这个研究领域感兴趣啦，我还想继续在这个领域干，所以我要申请读博士)</strong>.</p><p>To better prepare for rigorous doctoral study, I accumulated skills and knowledge that will aid my doctoral studies and research over the course of my involvement in various research projects <strong>(下面我告诉你们我最得意的研究项目，我改进了前人的研究，而且从这个项目里我获取的经验和技能是你们非常value的)</strong>. For example, one of the meta-analytic studies I was involved in was a meta-analysis of proportions, which allowed me to explore my keen interests in meta-analytic methods and gain coding experience. When I first started working on this project, few studies were dedicated to illustrating how a meta-analysis of proportions should be performed in R. To fill this gap, I created comprehensive written and video tutorials showing how to conduct such an analysis in R. My written tutorial consists of a critical review of the meta-analytic approach to synthesizing single-arm studies with proportional outcomes and a step-by-step guide to conducting the analysis in R. The R code can yield exactly the same results as those delivered by Comprehensive Meta-Analysis, software developed by meta-analysis experts. One of the prominent features of the R code is that it incorporated three transformation options for users to choose from to convert proportional data to improve its statistical properties, whereas other statistical software can only perform one.</p><p>While writing the review part of the tutorial, I investigated over 30 published meta-analyses of proportions and compared their results with those calculated by my R code. My research unearthed two problematic practices that plagued these studies <strong>(我开始展示自己很厉害的critical thinking abilities啦，因为我发现了前人研究中的很严重的问题)</strong>. The first issue I identified was related to effect size calculation in the presence of subgroups. Specifically, these studies failed to accurately estimate mean proportions because they did not take study size and within-group estimates of between-study variance (τ^2) into consideration. My R code fixed this problem by enabling users to pool the within-group estimates of τ^2 and apply this common estimate to the included studies. I also highlighted the questionable practice of improperly employing publication bias tests developed for meta-analyses of randomized controlled trials in meta-analyses of singlearm studies. In these faulty cases, test utility was limited due to the violation of underlying assumptions. I believe this practice could result in the failure to identify potential publication bias in observational meta-analyses.</p><p>My experience with this project contributed greatly to my ability to carry out independent research and resulted in a deep interest in meta-analysis and its related research areas. I enjoyed the challenging process of unearthing and evaluating methodological and statistical issues in existing studies <strong>(我对这些研究富有热情，很有动力)</strong>. Specifically, my interest was piqued regarding the impact of violating the normality assumption on meta-analytic results because I found that even after data transformation, very often the distribution of proportional data remained quite skewed. I plan to improve the estimation of effect size for meta-analysis of proportions in such situations by developing a more flexible and accurate alternative to the transformation-based approach based on the betabinomial model in a Bayesian framework <strong>(我未来的研究方向和可能的解决方法是什么)</strong>. Additionally, after reading Meta-analysis: A Structural Equation Modeling Approach by Mike Cheung, I was fascinated by the idea of integrating the two powerful statistical techniques into a unified framework to allow the synthesis of research findings for testing hypothesized models. This is an area that I am looking forward to specializing in <strong>(我的研究兴趣不止一个，所以我对你们program里的好几个POI都感兴趣，保证我有更多的选择余地)</strong>. Furthermore, I also developed an interest in transforming statistical procedures into computer code when I developed the R code for the tutorial. I am eager to learn more sophisticated software that enables latent variable modeling and Monte Carlo simulations, such as Mplus, Python, and SAS.</p><p>My research interests in quantitative methods evolved further during my internship at two criminal courts where I observed mentally ill defendants in forensic evaluations. My clinical observations challenged my deeply ingrained assumptions that mental disorders were single, cohesive disease constructs. Instead, there is considerable heterogeneity in severe mental illnesses, not only in their clinical presentations, but also in etiology, growth trajectory, and laboratory task performance measures. With further study, I was excited to learn that a promising way to account for the high degree of heterogeneity in mental disorders lies in quantitative methods, such as employing latent growth curve modeling and latent growth mixture modeling in the context of SEM. I hope to develop new SEM approaches to longitudinal data analysis and improve existing ones so that researchers can tackle such problems in mental health studies and other realms of psychological research <strong>(同样，这一段也是为了说明我的研究兴趣不局限在一个POI身上，如果这个POI收了另一个申请者，我可以选择其他POI来增加我被录取的机会)</strong>.</p><p>Upon completing the Ph.D. program, I intend to obtain a position in academia to focus on studying and teaching quantitative methods. One of my long-term goals is to lead a team to develop data-analysis software that allow psychologists without a comprehensive statistical background to conduct meta-analyses and all kinds of latent variable modeling. In addition, despite quantitative methods’ undeniable value and promising career opportunities, there is little public information about this area. Therefore, I am also dedicated to promoting its wider use in research and as an area of study among students <strong>(拿到学位后我的未来计划和目标)</strong>.</p><p>The PhD in Measurement and Statistics at Florida State University (FSU) is the ideal program to prepare me with the knowledge and skills needed to accomplish my academic and professional goals. What sets this program apart from many others is the breadth and depth of training that allows students to pursue their research interests and career goals. The program offers a large selection of statistics and methodology courses, which not only cover the standard quantitative methods subjects, but also include many less common ones, such as Meta-analysis, Causal Modeling, and Bayesian Data Analysis, subjects I am passionate about. I was also excited to learn that students in this program can also take classes in the Department of Statisticsto further enhance their understanding of statistical methodologies. In addition, after communicating over email with Dr. B. B. and Dr. Q. Z. and their PhD students C and D<strong>（只有你联系过program里的教授和学生，才能写出其他申请者从program官网得不到的信息，才能把你跟其他申请者区分开，加深committee对你的印象和好感）</strong>, I learned that the mentoring model employed in the program creates an environment where professors not only care about their students, but also encourage early participation in research and collaboration with multiple professors in the program and scholars from across the university. These factors ultimately benefit students in the program by providing the opportunity to gain different perspectives and valuable research experience. I believe I can truly make a difference in the advancement of the quantitative field with the training I will receive at FSU. Furthermore, I am attracted to FSU because it has a large, tight-knit community of graduate students who are able to harmoniously collaborate with one another and the faculty. It appears that everyone gets along and is willing to help each other like a family. I really feel I can thrive in such an environment. Lastly, the program is very successful at finding placements for its students. I believe that when I graduate I can leave with a firm understanding of what it takes to obtain jobs in either academia or industry <strong>(这个夸赞这个program各种强项的段落会让POI感觉我是tailor过我的PS的，而不是写的非常generic，体现出我满满的诚意)</strong>.</p><p>Although my current research experience in quantitative methods is mainly in meta-analysis, I am also eager to tackle problems in other areas of this discipline. I am looking forward to having the opportunity to work with Dr. B, Dr. Z, and Dr. Y. Their expertise in the areas of meta-analysis, SEM, Bayesian statistics, and longitudinal data analysis will be crucial in providing my future research with the appropriate theoretical and practical guidance.</p><p>With my determination and tenacity, along with my academic qualifications, I am confident that l will be able to excel in the doctoral program at FSU. In the future, my international background may also allow me to foster research collaborations between the U.S. and China.</p><p>请注意，没有一个POI会要求你完全按照PS里写的内容让你百分之百地去履行。所以为什么要有PS呢？我认为有以下几点原因：</p><ul><li>像我上面说的，体现你的批判性思维，这是核心能力，可以通过PS看出来；</li><li>你对自己将来的研究和工作生涯是有规划的；</li><li>你不是随便选的学校，你是认真考虑后才申请这个学校的；</li><li>能一定程度上反映你的逻辑和写作能力。</li></ul><p><strong>Bear with me, guys. We’ve still got two important topics to cover: letters of recommendation and how to ace your interview. So, 醒醒！</strong></p><p>关于推荐信，如果你申请心理学的PhD，请务必拿到至少一封美国教授的推荐信（可能申请其他人文社科类的PhD也是如此）。美国的录取委员会不傻，他们知道中国申请者的推荐信是怎么编造出来的，只有美国教授的推荐信才有可信度（除非你的推荐人跟你的POI私交甚好）。想拿到一封强推荐信，说到底是跟你的推荐人培养感情，所以我建议大家：</p><ul><li>尽早参与科研，在不同lab体验科研的过程，这样你才能挑选出你喜欢的科研方向，在这个过程中你同时跟不同教授混熟了；</li><li>上课时积极参与讨论，或者下课后常问教授问题；</li><li>用发邮件、office hour这样的方式多跟教授联络，你们可以讨论问题，不问问题你也可以谈谈最近的学习、科研感悟。不要临近申请的时候才想起教授来，没人喜欢这样的学生。</li><li>如果教授对你有不错的印象的话，你完全可以问教授有没有认识的其他临床心理学的教授在招博士生，这种通过熟人关系推荐来的学生显然比从一个不认识的教授那里推荐来的学生要更可信。但话又说回来，只有你有很强的读博动力、科研经验和水平并且会做人跟教授相处愉快，教授才会真心推荐给自己的同事或朋友。我想强调的是，一定要张口问教授可不可以把自己推荐给其他教授。<strong>如果能拿到一封熟人推荐信，对你非常有利。</strong></li><li>积极争取去美国交流的机会，国内的学生至少去修一个学期的课；</li><li>有条件的学生读美本或美国的master’s</li></ul><p>为什么我非常强调要有在美国读书的经历？这样能证明：</p><ul><li>你有在英语环境中学习并取得高GPA的学术能力；</li><li>如果你能取得高GPA说明你的英语能力得到了实战的证明，而非仅仅是一个苍白的托福分数（托福考得高但写PS满篇语法错误的学生还少吗？）</li></ul><p>In my case，当我的教授Betsy在第二次给我写推荐信后告诉我，她为我写了一封“强得不能再强”的推荐信。之所以我能得到她的支持，是因为我让她看到了我的努力、坚持和科研能力。我从没读过她为我写的推荐信，所以也没法展示在这里。但我可以从她给我回的一封邮件里推断出她是怎样为我写推荐信的（我给她发邮件说我被什么学校录取了并给她寄了一张感谢卡）：</p><p><img src="https://pic3.zhimg.com/50/v2-6e56a0270d79bbb264a591cefb6e2309_hd.jpg" alt="img">![img]</p><p>我的教授能这样称赞我是我荣幸，只有做到这样才能拿到一封名副其实的强推荐信。</p><p>最后我们讲讲PhD interview。</p><p>如果你进入面试环节，那恭喜你！我恭喜你的原因是因为你为自己赢得了展示自己的最佳机会。我想强调的是，面试不是考试，如果你把面试当作回答专业问题的考试，那就大错特错了。</p><p>对于校方，interview有以下几个功能：</p><ul><li>更加深入地了解你；</li><li>让教授和老生感受你是否跟他们的program匹配，观察你是不是真的对他们感兴趣，你有没有red flag</li></ul><p>对于你，interview有以下几个功能：</p><ul><li>直观地感受校园和校外的环境；</li><li>program里的老生是否真的喜欢他们学校；</li><li>更加深入了解这个program的机会：多问教授和学生问题！</li><li>最重要的是，展示你的强项的机会</li></ul><p>下面是我当时准备的几个最关键的面试问题和回答（有些问题真的问到了所以我会用过去式写我当时如何回答的）。这些回答是为了更好地展示我自己而准备的，请不要把它们当作标准答案，因为我再说一遍：面试不是考试：</p><ul><li>Your educational background;</li><li>Your best quality, character, personal trait—this should be something that can facilitate your future research career, such as perseverance;</li><li>Why do you want to do a PhD—I love doing research; I love teaching and tutoring students; being a professor is a rewarding career;</li><li>Why do you want to do a PhD in this area (my area is in quant psych)—I am more interested in methodologies and statistics than substantive topics; this is an area full of opportunities for growth; I love coding; great job prospects;</li><li>What are your research interests—I gave a very general answer (in my area, POIs don’t expect you to have a very clear idea of what topics you want to do in the future. You can just tell them you are interested in such and such areas, such as meta-analysis, SEM, longituidinal data-analysis). A more detailed answer wouldn’t hurt so long as your interests match your POIs’;</li><li>What are your career goals—when it comes to goals, I think you should be specific, like I want to get 5 first author publications by the time I graduate (you can even name a few prestigious journals in which you want to publish); I want to graduate within 5 years; I want to become an assistant professor in 6 years; I want to create very popular statistics packages. I would suggest that you choose some challenging but realistic goals;</li><li>What is your greatest strength and weakness (yes, they asked me what my biggest weakness was)- I said that my greatest strength would be my ability to solve problems in very challenging situations and I gave them an example; as far as my weakness, I said that I don’t have a heavy background in math/stats but I’m planning on taking a few refresher online courses like calculus and that I will take as many stats courses as possible during the progarm (the truth is that most incoming PhD students don’t have a strong background in math/stats in our field, so this is not a fatal weakness and it is something I can improve upon);</li><li>Which research project are you most proud of—they are giving you an opportunity to impress them, so be prepared with a great answer; remember to find out what skills are valued in your program and highlight them in your answer;</li><li>Why do you want to join their program—I said that my research interests match yours very well and I like the courses your program offers.</li></ul><p>If you have these questions prepared, you will talk confidently in the interview.</p><p>面试最后教授们肯定会问你你有没有问题问他们。以下是我准备的一些问题：</p><p>For POIs</p><ul><li>Could you tell me about your current projects? What are you working on? Where do you see your work going in the next few years?</li><li>Could you tell me about your advising style?</li><li>How many students do you advise now?</li><li>How often do you meet with your students?</li><li>What kind of research would I be working on if I were to go to your program?</li><li>Is there a group meeting every week? What do you all talk about?</li><li>What are you looking for in an incoming student? What are your expectations of students as they progress through the program?</li><li>On average, how many publications do students leave this program with?</li><li>What is the academic community like at this university? Is there much collaboration between the faculty members?</li><li>What type of internships do students usually get? Do they have research opportunities during summer or winter?</li><li>How long do most people take to finish the program? What are some common things that prevent people from finishing in this time frame?</li><li>What are your graduated students doing now? What type of placements do students usually get? Did they go into academia, or are they pursuing other careers?</li><li>What kind of financial support/funding opportunities are there for a Ph.D. student? How are students funded? Do you offer tuition waiver and stipend?</li><li>What is the funding package like for admitted students?</li></ul><p>For current students</p><ul><li>What are your favorite and least favorite things about this program?</li><li>What are your favorite and least favorite things about working with your adviser？</li><li>Why did you choose to come here?</li><li>What do you do for fun? How much social time do you spend with other students in the program?</li><li>How supportive is your adviser of you publishing?</li><li>How often do you meet with your adviser? When you want to talk with him/her, how do you go about scheduling a meeting?</li><li>Have you ever had a research idea outside of your adviser’s immediate research agenda? Did you pursue the idea and if so, how?</li><li>Do you need a car to get around?</li><li>What happens in this program to students who make a mistake (give the wrong answer in class, forget to randomize participants in one of their studies, etc.)?</li><li>How common/easy is it for graduate students to collaborate with someone who isn’t their primary adviser?</li><li>What is the biggest challenge you have faced as a graduate student?</li><li>Given my stipend, will I be able to live by myself?</li><li>What do you wish you would have known or understood better before coming here?</li></ul><p>你肯定问不全，所以挑最关键的以及你最感兴趣的问吧。</p><p>几个小tips：</p><ul><li>Keep your answers succinct, easy to understand, and to the point—they don’t expect you to give very detailed and long answers within 30 minutes (that’s the normal length of an interview). A very complicated answer will make interviewers confused because 1) some of them don’t know your background very well; 2) when you are being interviewed, your speech may become unorganized under pressure；</li><li>Show your eagerness to learn and do research! They prefer to hire an indivisual who is driven to do a PhD；</li><li>把你准备要问POI和录取委员会的问题打印出来并让他们看到！这样他们会直观地感受到你是很认真地来面试的，肯定会增加对你的好感！我当时拿出我的问题时，教授们都笑了；</li><li>千万不要尝试去背诵你的回答！可以写逐字稿（script），但不要一字不差地去背诵script，而是要记住要点和那些你临场想不出来的表达。如果你反复修改你的逐字稿，最后定稿以后你基本都会记住你要说什么了，但此时你记住的内容并不是你死记硬背记住的，而是顺着一条逻辑线记下来的。接下来，在琐碎的时间（起床前、睡觉前、坐车时、散步、锻炼、做饭的时候）不停地问自己这些面试问题，按你写好的内容现场组织语言，并反复重复。</li><li>dress code: at least business casual (tie is optional).</li></ul><p>2018年这一次，你能想到的申请的各个方面我都做到了极致。这回我申请了11所学校，拿到了9所录取（拒掉8位POI的感觉其实没有想象中那么简单。我非常感激他们选择我，给我机会去他们学校读博士，所以今年感恩节我给他们每个人发了邮件再次表达感谢）。</p><p>我现在在Texas读博士。</p><p>我觉得我的经历是可以给我孩子讲的一个不错的故事，并且告诉他我总结出的一个大道理：</p><p>At the end of the day, you don’t get what you want, you get what you <strong>deserve</strong>.</p><p><img src="https://pic3.zhimg.com/50/v2-3d4d4a9abd1d47a400422143a47e4663_hd.jpg" alt="img"></p><p>最后我来讲讲我都被哪些学校录取了，以及收offer期间遇到的有（qi）趣（pa）的事情。我选的这些学校一方面有研究meta-analysis的POI，另一方面从整体看faculty研究兴趣比较广，这样即便我将来不想研究meta-analysis了，也可以轻松地换方向（有些program的faculty只有两个教授，我就不会选那样的program）。</p><p>直接给我录取的学校有：</p><p>University of Cincinnati（<a href="https://link.zhihu.com/?target=https%3A//cech.uc.edu/education/programs/educational-studies/phd-in-edst/concentrations/qmrm.html">Quantitative and Mixed Methods Research Methodologies</a>）：这是最早给我录取的学校（1/29）。跟POI用Skype面试得不错，也答应给funding。最有意思的是POI两个月后告诉我他跳槽去USC（University of South California）了，所以今年不能收学生了。当时让我焦虑了几天，害怕其他老师也突然告诉我他/她要跳槽。这位POI就是我现在读博士这个学校毕业的。</p><p>Florida State University（<a href="https://link.zhihu.com/?target=https%3A//education.fsu.edu/degrees-and-programs/measurement-and-statistics">Measurement and Statistics</a>）：1月26号给我的录取。这其实是我最想去的学校，因为POI是meta-analysis这个领域里的“Queen”，最厉害的一位学者之一，也是我现在的导师的导师。当我得知我被录取的时候，我在Houston刚刚结束另一个学校的面试。当时实在是太开心了，所以我去酒店一层一间昏暗但别致的酒吧自己庆祝了一翻：配着一个汉堡，第一次喝了一点威士忌（极少喝酒，平时最多喝一点Mojito），彻底放松了下来。当时感觉一切付出都是值得的。FSU没有面试（估计是因为系里没钱没法给参加面试的申请人报销机票），但POI邀请我去她家住一晚第二天带我去逛逛校园了解一下他们的program。POI跟她老公住在一座森林里的一座别墅里，POI一个学生夜里载着我到的她的住处，如果你们看过《人体蜈蚣》第一部，往森林深处开的感觉就跟电影里那样。家里有两条大狗狗，墙上挂着各种从世界各地带回来的纪念品（她老公是人类学家，以前满世界到处跑）。她老公跟我讲，选PhD的项目要看导师而不是这个学校怎么样，以前他一开始在哥大读博士，后来他从哥大转到City College是因为他认为哥大那个项目里的导师学术上“太保守”，他不喜欢。第二天我逛了一下FSU的校园，认识了一下他们的学生，一切都很满意……直到我们谈钱（划掉），谈funding。我发现这个program里的学生基本要么是贷款读博，要么是家里给钱读博，asisstantship只给10个小时（一般RA和TA都要求每周工作20小时）。我问POI为什么会这样，她跟我讲，系里希望给每个学生都有补助，弊端就是每个人都分的少。POI帮我申请了fellowship，希望我去他们的program，但最终因为funding的问题我没有去。我是在4月16日收到邮件告知我拿到了fellowship，但当时我早就接了现在这个学校的offer了。</p><p>UC Merced（<a href="https://link.zhihu.com/?target=http%3A//psychology.ucmerced.edu/graduate-program/quantitative">Quantitative Psychology</a>）：1月29日给我的录取。UC系统里最新的一个学校，program也比较新，由Dr. William Shadish带领创办的program（可惜他去世了），faculty里有好几个我都很喜欢的，有一个POI是FSU的POI的“academic brother”（FSU的POI用的这个词），另一位POI只有30多岁就成了Associate Professor，是一个rising star。我通过Skype跟POI们interview，还跟他们的学生interview（不是正式的，只是通过学生了解这个program）。面试后一个小时就拿到录取，直接给了我五年的funding。实际上我很喜欢这个项目，除了faculty之外，这个项目另一个大卖点就是campus附近就是Yosemite（优山美地）。最后我没去，实话实说，假设我毕业后回国的话，我还是希望有人听说过我读博士的学校的名字的（我的解释比较婉转，你们懂）。</p><p>Texas A&amp;M University（<a href="https://link.zhihu.com/?target=https%3A//epsy.tamu.edu/academics/research-measurements-and-statistics-doctoral/">Research, Measurement, and Statistics</a>）：2月6日给的我录取。导师是FSU的POI的弟子。导师、funding、校园文化以及各个方面我都满意，最后来这里了。</p><p>University of Iowa（<a href="https://link.zhihu.com/?target=https%3A//education.uiowa.edu/academic-programs/educational-measurement-and-statistics">Educational Measurement and Statistics</a>）：2月7日给的录取。POI也是FSU的POI的弟子。这位POI来自南美某国家，口音非常重，乃至于跟他Skype面试时我只听懂了他问的问题，当他自己介绍自己时，我只能点头微笑随声附和，说一说that’s interesting。最后因为没有funding没去。实际上，我有个阴谋论的观点：因为FSU、TAMU和Iowa三个学校的POI平时联系非常紧密，经常邮件来往，所以他们非常清楚我的情况，最后让我选择TAMU会不会是他们三人商量好的结果呢？这仅仅是阴谋论，我的gut feeling，完全没有根据。</p><p>University of Houston（<a href="https://link.zhihu.com/?target=http%3A//www.uh.edu/education/degree-programs/mqm-ls-phd/">Measurement, Quantitative Methods, and Learning Sciences</a>）：2月9日给的录取，也有funding。没去是因为我更喜欢FSU和TAMU。最有意思的是，我去这个学校面试时，另外来面试的两个女生，一个是我济南老乡（她最后选择了这所学校），我们回国后还在济南见了一面（不要多想，人家都结婚了）；另一个最后选择来TAMU了（人家也结婚了），上学期我们还一起上了一门课；还有一个隐藏的我没见到的电话面试者，他最后也来TAMU了，现在跟我一间办公室。</p><p>SUNY Albany（<a href="https://link.zhihu.com/?target=https%3A//www.albany.edu/graduate/ed-psychology-phd-degree.php">Educational Psychology and Methodology</a>）：2月15日给的我录取（我记得收到录取信时我在去New Port的地铁上）。Skype面试很顺利，很喜欢这个来自荷兰毕业于荷兰最古老大学的POI。Funding也很不错。最后没去一是我更喜欢FSU和TAMU，二是这个program虽然POI是研究meta-analysis的，但整个项目更倾向于Educational Psychology，最后一个原因是我想离开纽约州去其他州生活生活（所以我来到跟New York对角线的Texas）。</p><p>Univerisity of Texas, Austin（<a href="https://link.zhihu.com/?target=https%3A//education.utexas.edu/departments/educational-psychology/graduate-programs/quantitative-methods">Quantitative Methods</a>）：没记住几号给的录取。没去这个学校最主要的原因是这个program要求必须有他们学校的Quant Psych的M.S.（其他学校的也可以），才能申请他们的博士。他们审核我的材料后认为我的心理咨询的硕士不符合他们的要求，最终把我录进他们的M.S.，没有funding，所以没去。</p><p>好吧，我来讲讲最最奇葩的一个学校了：University of Notre Dame<strong>（</strong><a href="https://link.zhihu.com/?target=https%3A//psychology.nd.edu/graduate-programs/areas-of-study/quantitative/">Quantitative Psychology</a><strong>）。</strong></p><p>直接被拒后来又问想不想来我们program啊这种事情是真能发生的。</p><p>我在二月份某天直接从Notre Dame的申请系统里收到他们的拒信，后来四月十几号POI直接发短信询问我还有兴趣来我们这儿吗，当时距离我接TAMU的offer都过去俩月了。虽然我本来也很想去这所学校，但不能反悔已经答应的offer（不然我在学术圈里名声就完蛋了，还记得TAMU的POI跟FSU和Iowa都是什么关系吧？），所以拒掉了。后来我在GradCafe上得知，Notre Dame那一年prematurely拒掉了很多优秀的申请者（圣母大学，waitlist了解一下呀？），我还看到比我还强很多的申请者在GradCafe上面留言说他跟我有同样的遭遇（人家最后去了UCLA，活该啊Notre Dame）。</p><p>最后，两所直接拒我（但最后没有改主意再录我）的学校：</p><p>University of Maryland, College Park（<a href="https://link.zhihu.com/?target=https%3A//education.umd.edu/measurement-statistics-evaluation-program%23faculty">Measurement, Statistics, and Evaluation</a>）</p><p>University of Minnesoda, Twin Cities（<a href="https://link.zhihu.com/?target=http%3A//www.cehd.umn.edu/edpsych/programs/qme/">Quantitative Methods in Education</a>）</p><p><a href="https://www.zhihu.com/question/31712813/answer/176492357" target="_blank" rel="noopener">https://www.zhihu.com/question/31712813/answer/176492357</a></p><h4 id="感悟"><a href="#感悟" class="headerlink" title="感悟"></a>感悟</h4><p>多看看大神们是如何做事的，和作者比起来我的努力程度让自己感到羞愧。作者的座右铭非常应景：you dont get what you want,you get want you derserve，好好努力吧，科研这种事怎么能投机。</p><p>顺着作者的思路，先问自己两个问题：</p><p>老师凭什么推断我可做科研?——–有科研产出:paper或writing sample</p><p>老师凭什么推断我能博士毕业?—–思维能力强,专业基础扎实,心态健康,沟通能力强,语言能力强—-&gt;</p><p>​                            ——–发表优质论文,能证明思维能力和专业基础以及语言能力</p><p>​                             ——-在合适时间点（2020.1-3月）与老师套词，在套词的过程中呈现自己的paper或                              writing sample进展，在getting better的过程中证明自己的心态和沟通技巧</p><p>规律作息,戒咖啡,回避不必要的社交</p><p>在2019年结束前写出像样的writing sample是主要任务，投递论文的技术细节请教小鸿后另开一文。</p><p>​                                    </p><h4 id="新加坡国立调研"><a href="#新加坡国立调研" class="headerlink" title="新加坡国立调研"></a>新加坡国立调研</h4><ul><li>全球排名20</li><li>最低要求<ul><li>研究型    <ul><li>硕士<ul><li>GRE成绩</li><li>通常要求本科相关专业(就只有这个是本科相关专业就好)</li><li>如果有相关工作/科研经历可做加分项</li></ul></li><li>博士<ul><li>相关领域硕士学历</li></ul></li></ul></li><li>授课型<ul><li>硕士<ul><li>本科相关专业</li></ul></li></ul></li></ul></li><li>费用:<ul><li>担任助教的研究型计算机硕士每年20350新元</li><li>不担任助教的研究性计算机硕士每年38500新元</li></ul></li></ul><p>结论:</p><ul><li>硕士没有奖学金/很少</li><li>博士要求必须是本专业的硕士</li><li>硕士对本科专业要求不是特别严</li><li>硕士完了跨校读博基本就得7年</li><li>研究性硕士可以在第一年就申请转博士</li><li>研究性硕士一般要给博士让路,博士没满才招研究性硕士<ul><li>【cs专业就没有研究性硕士这一栏,不存在博士没招满】</li></ul></li><li>新加坡硕士也是要考gre的</li></ul><p>总结:直接读博要求是本专业硕士,cs无硕博连读，pass.</p><p>先把文章投出去与老师套词,然后考语言,这样有希望申请上授课型硕士</p><p>硕士期间花费预估在70w，招行+新加坡本地贷款+助教项目等，问题不大，需注意读完硕还要读博，并不能工作还贷款，所以贷款年限需拉长。</p><p>这条路时间成本巨大，预估得7年。</p><p>科研能建构内心秩序，是一种lifestyle。</p>]]></content>
    
    
    <categories>
      
      <category>5.杂谈</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文写法</tag>
      
      <tag>思考</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>5.杂谈/996 - 超时工作的异化问题</title>
    <link href="undefined2020/02/24/5.%E6%9D%82%E8%B0%88/996%20-%20%E8%B6%85%E6%97%B6%E5%B7%A5%E4%BD%9C%E7%9A%84%E5%BC%82%E5%8C%96%E9%97%AE%E9%A2%98/"/>
    <url>2020/02/24/5.%E6%9D%82%E8%B0%88/996%20-%20%E8%B6%85%E6%97%B6%E5%B7%A5%E4%BD%9C%E7%9A%84%E5%BC%82%E5%8C%96%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h1 id="996-超时工作与异化问题"><a href="#996-超时工作与异化问题" class="headerlink" title="996 - 超时工作与异化问题"></a>996 - 超时工作与异化问题</h1><h5 id="1-感性认识"><a href="#1-感性认识" class="headerlink" title="1.感性认识"></a>1.感性认识</h5><p>我国现阶段各行业超时工作现象十分普遍，有的互联网公司晚上十一二点整个大楼还灯火通明，有的公司虽然晚上按时下班，但回家后仍然要求有工作输出，很多岗位还要求7*24小时on call。</p><p>我前些年所在的公司就有制度要求员工每月加班22天以上才算考核达标，而我经常需要加班27天才能完成工作，每晚九点离开公司后经一小时通勤才能到家，洗洗漱漱刚好十一点上床，十二点入睡，第二天早上6点50分起床后经过一小时通勤抵达公司，在公司门口的快餐店里缩着身子与其他同事挤在餐桌上潦草进食后便赶往工位，开始一天的埋头苦干。算上通勤时间，我的工作强度是7107。高强度劳动和了无希望所积累的疲劳深入骨髓，在此期间我感到生命力与灵魂的飞速流逝。</p><p>深夜的华为大楼</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1305_42_222.png" alt=""></p><p>早上五点半排队等公交上班的年轻人</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1306_00_748.png" alt=""></p><h5 id="2-八小时工作制的来由"><a href="#2-八小时工作制的来由" class="headerlink" title="2.八小时工作制的来由"></a>2.八小时工作制的来由</h5><p>八小时工作制国家法律规定的工作日长度为8小时的工作制度。目前世界各国普遍实行八小时工作制。正常一天工作时间为早上九点至下午五点为8小时。</p><p>理论起源</p><p>八小时工作制最早由社会主义者<a href="https://baike.baidu.com/item/%E7%BD%97%E4%BC%AF%E7%89%B9%C2%B7%E6%AC%A7%E6%96%87" target="_blank" rel="noopener">罗伯特·欧文</a>于1817年8月提出。他还发明了一个口号， “8小时劳动， 8小时休闲， 8个小时休息”1833年，在<a href="https://baike.baidu.com/item/%E6%AC%A7%E6%96%87" target="_blank" rel="noopener">欧文</a>的支持下，具有同情心的工厂主约翰·多赫尔蒂等人发动了一场争取八小时工作制的运动。1866年，第一国际<a href="https://baike.baidu.com/item/%E6%97%A5%E5%86%85%E7%93%A6" target="_blank" rel="noopener">日内瓦</a>代表大会提出了“8小时工作，8小时自己支配，8 小时休息”的口号，要求各国制定法律给予确认。</p><p>历史延革</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1313_15_391.png" alt=""></p><p>19世纪80年代的美国工人运动。当时，美国和欧洲的许多国家，逐步由<a href="https://baike.baidu.com/item/%E8%B5%84%E6%9C%AC%E4%B8%BB%E4%B9%89" target="_blank" rel="noopener">资本主义</a>发展到帝国主义阶段，为了刺激经济的高速发展，榨取更多的剩余价值，以维护这个高速运转的资本主义机器，资本家不断采取增加劳动时间和劳动强度的办法来残酷地剥削工人。</p><p>在美国，工人们每天要劳动14至16个小时，有的甚至长达18个小时，但工资却很低。马萨诸塞州一个鞋厂的监工说：“让一个身强力壮体格健全的18岁小伙子，在这里的任何一架机器旁边工作，我能够使他在22岁时头发变成灰白。”沉重的阶级压迫激起了无产者巨大的愤怒。他们知道，要争取生存的条件，就只有团结起来，通过罢工运动与资本家作斗争。工人们提出的罢工口号，就是要求实行八小时工作制。</p><p>1877年，美国历史上第一次全国罢工开始了。工人阶级走向街头游行示威，向政府提出改善劳动与生活条件，要求缩短工时，实行八小时工作制。罢工不久，队伍日渐扩大，工会会员人数激增，各地工人也纷纷参加罢工运动。</p><p>在工人运动的强大压力下，美国国会被迫制定了八小时工作制的法律。但是，狠毒的资本家根本不予理睬，这项法律只不过是一纸空文，工人们仍然是生活在水深火热之中，倍受资本家的折磨。忍无可忍的工人们决定将这场争取生存权利的斗争，推向一个新的高潮，准备举行更大规模的罢工运动。</p><p>1884年10月，美国和加拿大的八个国际性和全国性工人团体，在<a href="https://baike.baidu.com/item/%E7%BE%8E%E5%9B%BD%E8%8A%9D%E5%8A%A0%E5%93%A5" target="_blank" rel="noopener">美国芝加哥</a>举行一个集会，决定于1886年5月1日举行总罢工，迫使资本家实施八小时工作制。这一天终于来到了。5月1日，美国2万多个企业的35万工人停工上街，举行了声势浩大的示威游行，各种肤色，各个工种的工人一齐进行总罢工。仅<a href="https://baike.baidu.com/item/%E8%8A%9D%E5%8A%A0%E5%93%A5" target="_blank" rel="noopener">芝加哥</a>一个城市，就有4．5万名工人涌上街头。这下，美国的主要工业部门处于瘫痪状态，火车变成了僵蛇，商店更是鸦雀无声，所有的仓库也都关门并贴上封条。</p><p>1889年7月14日，各国马克思主义者召集的社会主义者代表大会在法国巴黎隆重开幕。这次大会上，法国代表拉文提议：把1886年5月1日美国工人争取八小时工作制的斗争日，定为国际无产阶级的共同节日。与会代表一致同意，通过了这项具有历史意义的决议。</p><h5 id="3-996的起因"><a href="#3-996的起因" class="headerlink" title="3.996的起因"></a>3.996的起因</h5><p>行业特征与恶性竞争</p><p>互联网本身时效性极强，如果其他所有公司都能很快推出自己的产品，唯独自己的公司不行，那就会很快被市场淘汰。所以各公司都会延长员工的工作时间，以实现产品产出的最大化。当然，在这种严酷的加班制度下，资方并没有好心地将超时工作的时长以假期或补贴的形式返还给员工，也没有扩招人手来降低人均工作强度。资方常常采取的策略是拼命地使用员工，当员工不再年富力强或疾病缠身时就果断抛弃，反正每年都有大把的应届毕业生兴高采烈地加入被剥削大军。</p><h5 id="4-996本质是剥削"><a href="#4-996本质是剥削" class="headerlink" title="4.996本质是剥削"></a>4.996本质是剥削</h5><p>强迫人超时劳动本质是剥削，剥削的坏处是让一个完整的人变得不完整，这个长期超时工作的人会工具化和异化。</p><p>上班的人在出卖自己的劳动力换得薪水，劳动力是要再生产的。再生产的方式是吃东西，充足的睡眠，维持与家人的健康关系，发展自己的业务爱好，让自己保持身心愉悦，这样才能源源不断产生充沛的劳动力。</p><p>本身人工作8小时后剩下的16个小时是用于恢复劳动力的，但是老板拿掉了剩下恢复生产力的时间，平常8-11，周末只放一天假也用来补平常缺的觉了。长期这样以来人的正常劳动力再生产过程就会被破坏掉，这样人的劳动表现就不会好，跟不上高压的工作节奏，这样以来人干个五年十年身体与灵魂都会垮掉。这个人垮掉之后当然会被炒掉，资方只用再换一个年轻力壮的就OK了。</p><h5 id="5-深刻认识剥削"><a href="#5-深刻认识剥削" class="headerlink" title="5.深刻认识剥削"></a>5.深刻认识剥削</h5><p>作者：李劼</p><p>链接：<a href="https://www.zhihu.com/question/320073567/answer/777226626" target="_blank" rel="noopener">https://www.zhihu.com/question/320073567/answer/777226626</a></p><p>来源：知乎</p><p>如果大家认为“剥削”就是自己挣10块钱，资本家抽走7块，只给留3块。那这个话题就没有讨论的必要了。</p><p>因为这掩盖了剥削的残酷本质。</p><p>资本主义的核心并不在“私有制”（其实在原始社会末期就已经有私有财产了，封建时代也是，但都称不上资本主义）。而在于“资本做主才是第一要义”，简称“资本主义”。</p><p>由此导致的局面就是，资本凌驾于人类之上，人类为资本服务。而并非人类拥有资本，资本听命于人。虽然资本主义环境下有财产私有制度，但这极具迷惑性，容易让人误以为资本家是资本的主人（财产所有人）。但实际上，资本家只是傀儡，人形外壳，传声筒，使魔，皮囊，英灵（随便你们叫什么吧），资本本身才是真正的“主人”和“灵魂”。</p><p>到了这一步，资本即将展现它真正的杀伤力――异化。</p><p>上面那一大坨文字看不懂没关系，下面讲点简单的，举个例子。</p><p>比如你是一个老板，搞到（甭管是捡的研发的还是买的）一把超级牛逼的镐头，能让矿工挖矿的产量提高到原先的10倍。假如你面前有两个人，一个是超级矿工，即便不用这把超级镐头，他也能挖10个单位的矿（用镐头将会是100）。另一个是包身工，他只能挖1个单位的矿（用镐头就会变为10）。</p><p>现在，包身工在你面前跪下来了，他儿子得了白血病，他恳求你把超级镐头分配给他，他不介意被剥削，他愿意挖出来的矿你得7，他得3（换钱给儿子治病），实在是不行，你8他2也行。</p><p>超级矿工的辞职报告也递上来了，他的兴趣是做菜（虽然他的天赋是挖矿），他想改行当厨师。</p><p>你把镐头分配给谁？（无视人情，也无视另一人的天赋）</p><p>别想多了，犹豫必将败北。作为称职的资本家，你给超级矿工跪下了。</p><p>你求他拿上超级镐头去挖矿，你许诺跟他五五开分成，如有必要，你还可以只拿100个单位中的40，他拿60，折成股份也行。你甚至允许他每天只挖半天矿，另外半天可以兼职做厨师。至于那个可怜的包身工，谁叫他不努力呢，是吧。又或者，等超级矿工挣到钱，你的企业也成功上市，你再用财税渠道和福利政策“转移支付”给包身工，那样，你，包身工（和他倒霉儿子），超级矿工，就“共赢”了，是吧。</p><p>怎么样，如此温情脉脉，优胜劣汰，按劳分配，是不是很市场经济，很效率优先，很兼顾公平？是不是跟政治（或政治经济学）教科书里讲的剥削不一样？</p><p>不一样就对了，教科书太仁慈，不敢告诉大家剥削的残酷本质。</p><p>好，再回到前面说的“以资本为主”，“异化”。</p><p>上文就是“以资本为主”，“异化”的例子。</p><p>――“以资本为主”，所以，资本不是分配给最需要它的人，而是分配给最能实现他价值的人。</p><p>――“异化”，所以“劳动”（其实喜欢做菜）被扭曲为“就业”（挖矿最出成果）。</p><p>而就在这“以资本为主”和“异化”的过程中，“剥削”粉墨登场。</p><p>再次强调一遍，剥削并不（或不仅）表现为对成果分配的掠夺（如各种三七开，二八开，“剩余价值”），它真正残忍的面貌在于对时间与自由的剥夺。</p><p>本来，自由与时间对每一个人都是（或应该是）平等的，谁都是一天24小时（就算马云资产是我的一亿倍，他的一天也变不成24亿小时）。</p><p>然而，一旦进入“以资本为主”的“异化”过程，你的自由将不再是你的自由，你的时间将不再是你的时间。</p><p>你不能自由的，随心所欲的去“劳动”，你只能有目标有任务的去“就业”，并为此消耗时间。</p><p>但，第一，时间是不能重叠的（虽然资本允许重复投资）。</p><p>你花时间学习使用超级镐头，就不能同时陪伴白血病儿子或做菜，正所谓“放下砖头没钱养你，抱起砖头没时间陪你”。</p><p>第二，时间是不会倒流的（虽然投资可以撤回）。</p><p>当某一天，你花光了挖矿换来的钱，儿子依然死在了病榻上，又或者经济危机，你就是花时间学了怎么使用超级镐头但依然找不到工作的时候，你为此耗费的时间照样流逝，永不回头。</p><p>这才是剥削的真谛――你必须为了实现资本的价值（而非直接满足自身需要）来消耗时间（然后再诉诸于市场交换），而且，不论你的消耗能否换来自身需要的满足和自身价值的实现，时间都被实打实的消耗了，不可重叠，不可倒流，不可撤回，不可再生，不可增殖。</p><p>现在，我们可以回答问题了。</p><p>为什么资本家明明拥有大量资产，却还要剥削无产阶级？</p><p>因为还需要时间啊！</p><p>资本要实现自身价值（增殖），就必须通过运作（生产，经营，市场交换）的过程才能实现，而维持这一运作过程，需要包括资本家在内的全人类全身心投入，而“私有”却偏偏能保证大家死心塌地去投入。因为“私有”能划分出“所拥有的”和“所需要的”。要满足“所需要的”，就必须提供“所拥有的”来交换。</p><p>作为资本代言人的资本家，他们打开了名为“市场”的炉盖。</p><p>无产阶级，他们往熔炉里投入了名为“时间”（也可写作“花时间做事”）的燃料。</p><p>从此，他们的时间不为自己而生，而为资本支配。</p><p>资本家？就一烧锅炉的。</p><p>这就是为什么马克思主义者的终极追求是“解放全人类”，而非“打倒资本家”。</p><h5 id="6-探究异化"><a href="#6-探究异化" class="headerlink" title="6.探究异化"></a>6.探究异化</h5><p>作者：雾雨之灵</p><p>链接：<a href="https://www.zhihu.com/question/297726264/answer/608965993" target="_blank" rel="noopener">https://www.zhihu.com/question/297726264/answer/608965993</a></p><p>来源：知乎</p><p><img src="!%5B%5D(http://bqlab-pic.test.upcdn.net/pic/20191207_1313_37_944.png)" alt="mark"></p><p>这个问题，马克思早就预言过了，在我们中学的课本也提过了无数次。这就是资本对劳动的异化！同时也是对人的异化！</p><p>劳动，本来是人的本质属性。何为本质属性？就是这个属性不像吃喝拉撒睡，饱暖思淫欲去啪啪啪一样，这个属性别的动物都没有，唯独人有。人是无法不劳动的，让一个人休息一星期、一个月可能会很开心，但让他一直闲着，他就总要找一点事情去实现自己的价值，创造价值就是劳动，价值就是凝结于商品中的人类劳动。</p><p>所以劳动本来应该是快乐的，因为劳动创造价值嘛，劳动是自己本质属性的表达嘛。如果想找这样快乐的劳动者聊聊，就来北京打一辆出租车。北京开出租的本地人好多都不差这几个钱，为啥还要开出租？有一次我出差，出租车司机师傅特别贫，一直和我说“现在好多人都看重钱，我才不在乎，我一分存款都没有，我儿子我也不打算给他一分钱…”我一听就赶紧打住，师傅您先说说自己有几套房吧。师傅说也不多，就两套，不过有一套快拆了，拆了我能再买八套。我边听边带上耳机说，师傅我有些晕车，不说话了啊我眯一会儿……</p><p>同样，马云说他不喜欢钱，我也觉得他是真心的。他犯得着撒谎吗？</p><p>但现在呢？很显然没有年轻人觉得劳动是快乐的，除非你有出租车师傅那样的资本。现在年轻人劳动仅仅是为了活下去，这就是异化。强调一点，异化不是指一个东西失去了他的本义，异化是指，本来是客体的存在却反客为主，进而控制了主体。本来劳动是人实现价值的手段，劳动是客体，现在却成了人不得不劳动，劳动成了主体。本来金钱是为了方便人的生活，现在却是人的生活为了获取金钱。本来是人在玩手机，现在却是手机在玩人…这些才是异化。</p><p>过去的国企工人，虽然钱不多，但是单位有食堂，有房子，国家管了教育、养老和医疗，自己挣多少花多少，可不就“咱们工人有力量”了？现在的年轻人，哪怕在互联网这种高薪行业，工资扣个知道自己一时半会用不上的五险一金，再交一交房租水电，真的也不剩几个子儿了。可不就自嘲是社畜、肥宅？这不是开玩笑，调查一下当年的工人幸福指数绝对爆现在年轻人几条街不止。</p><p>这还没有完，因为马克思说资本是有自己的意志的，它的意志就是实现自己的最大化。所以劳动和价值开始完全脱钩，10年前有个房子，现在抵得上别人一辈子的劳动，另一方面，是分工越来越细，人越来越原子化。我那些进互联网的同学，一个个都是名校研究生毕业，可他们觉得自己的工作随便一个本科生来学段时间就把自己取代了。</p><p>再者，就是加班时间越来越长，八小时工作制和双休日就是个笑话。什么叫“对剩余价值的剥削”？就是劳动者一天劳动所得，仅仅够他恢复劳动力进行第二天的再次劳动。这下好了，广大年轻人累死累活，忙到晚上连性生活都不想有，结果却发现没有多少时间是属于自己的。本来以为自己赚钱就可以想买什么买什么，本来以为工作没有人管，周末时间可以自由支配，但是现实却是下了班累得只想躺在床上，点个外卖刷刷手机。</p><p>另外，资本可以自己进行增殖，即马太效应，强者愈强，弱者愈弱。有了京户，就有了学区房，然后又有了区重点和市重点…一个阶层强者为了巩固和展现自己的优势自然会集中资源再打造一个阶层，于是阶层层层化分。据我了解，去年北京海淀区某区重点高中——按理说也是北京二流学校——的理科状元的分数只能去哈工大。更何况还有多少当年从高考独木桥冲过来的精英，他们的孩子连区重点都没得上？</p><p>这就是所谓的阶级天花板。随着楼上的人越来越多，楼上的人不但拼命踹想要往上爬的人，还要在楼上再起一层高楼，楼下的空间自然越来越小，天花板越来越低。之前当一名工人也挺滋润，现在当个小领导都焦头烂额。之前的孩子放了学扔了书包就可以去玩，现在的孩子放了学就要去补习班。所以现在人们觉得努力没有希望并不是没有道理，而是源自于我们对周围这个世界长时间的认识而得出的结论。</p><p>最后，资本还要做一件事，就是登堂入室，为自己合理化，占领舆论的高地。为这样的不平等冠以各种堂而皇之的理由。宣传各种奋斗实现财务自由、阶级跃升的例子，让年轻人普遍焦虑，觉得自己活成这个样子完全是因为自己不努力；另一方面，资本又是需要市场的，为了扩大市场资本再次鼓吹消费主义，如网红餐厅之类，并且给年轻的脑力劳动者一个华丽丽的名词——白领，中产阶层。让年轻人误以为自己相比父辈们，相比工人农民地位有了本质的提高，而为了配得上这个地位，必须有相应的消费。这就是小资产阶级，这就是小布尔乔亚。</p><p>什么是小资产阶级？明明没有掌握任何生产资料，却把自己当成了资产阶级，明明还是个劳动人民，却总觉得自己比劳动人民要优越。</p><p>最后的最后，马克思的追求是什么？就是人的解放，解放生产力，发展生产力。一个疲于奔命的人是不会有劳动的动力的，你找不到一个奴隶会兴高采烈地为奴隶主干活，就算你举出有的奴隶通过自身的奋斗而当上了宰相，也改变不了这个事实。当然，个人的奋斗还是很重要的，虽然个人的奋斗常常敌不过历史的进程，但是个人的奋斗总会或多或少决定着自己的命运。所以，但行好事，莫问前程!</p><p>It’s not your fault!</p><p>小结：</p><p>资本本身是主，资本家是管家，工人是slave，资本的意志是让自己增殖，增殖需要人类大量投入时间。时间不可再生，我的时间提供给资本增殖，就必定不能提供给自己的天赋，不能提供给心之所向。我应该把时间投入到自己真正感兴趣的地方，而且是straight forward去做。 </p><p>劳动本身该是快乐的，因为创造了价值，也符合人的天性，但是我们的劳动常常被迫超时超量奉献给资本增殖，同时忽略天赋与inner voice，则变得痛苦不堪。上学同理。保持内心的愉悦与平和，如果不能保持，反思问题，付出代价，解决问题。</p><p>不要透支自己的劳动力。如果干的事情符合天赋与内心认知并且干的快乐，工作时间长一点无所谓。如果干的很痛苦，干的过程中需要付出意志力硬抗，则一定严控工作时长，工作之余好好休息放松，必须认清劳动力不是无限供应的资源这一事实。考研废掉/做事无恒心其实就是没有设计好劳动力再造的过程,我不懒而是勤奋地过了头透支了劳动力,以后要要注意劳逸结合。</p><p>警惕消费主义和小资的陷阱，不要买太贵的东西，不要高消费。高消费不能实现阶级跃迁，明明是个在格子间办公鸽子笼睡觉的被剥削的劳动人民，却觉得自己比劳动人民优越，这是精心设计的认知错位陷阱，注意堤防。</p><p>找准研究方向，既要符合自己的天赋，也要考虑历史的进程！</p>]]></content>
    
    
    <categories>
      
      <category>5.杂谈</category>
      
    </categories>
    
    
    <tags>
      
      <tag>思考</tag>
      
      <tag>996</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>5.杂谈/博士这五年-李沐</title>
    <link href="undefined2020/02/24/5.%E6%9D%82%E8%B0%88/%E5%8D%9A%E5%A3%AB%E8%BF%99%E4%BA%94%E5%B9%B4-%E6%9D%8E%E6%B2%90/"/>
    <url>2020/02/24/5.%E6%9D%82%E8%B0%88/%E5%8D%9A%E5%A3%AB%E8%BF%99%E4%BA%94%E5%B9%B4-%E6%9D%8E%E6%B2%90/</url>
    
    <content type="html"><![CDATA[<p>转自知乎-李沐</p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>12年8月提着一个行李箱降落在匹兹堡机场。没找住的地方，也不知道CMU应该怎么去。对未来一片迷茫，但充满乐观。 现在，刚完成了博士期间最后的一场报告，在同样的机场，不过是在等待离开的航班。</p><p>回想过去的五年，是折腾的五年，也是自我感悟和提升的五年。这里我尝试记录这五年主要做过的事情和其中的感想，希望对大家有所启发。</p><h2 id="第0年：3-11-8-12"><a href="#第0年：3-11-8-12" class="headerlink" title="第0年：3/11-8/12"></a>第0年：3/11-8/12</h2><p>我第一次申请美国的博士是在11年，但拿到的offer并没有特别合适的导师，于是就北上投奔文渊去了。 我当时在百度商务搜索部门做广告的点击预估。具体是使用机器学习来预测一个广告是不是会被用户点击。 这时候离“大数据”这个词流行还有两年，但百度那时候的数据即使现在来看仍然是大的。我的任务是如何高效的利用数百台机器快速的在数十T的数据上训练出模型。</p><p>当时产品用的算法基于LBFGS，我于是想是不是可以换个收敛更快的算法。没几天就找到个不错 。但实现上发现了各种问题，包括性能，收敛，和稳定性。而且那时有的就是一个裸的Linux和很老版本的GCC，什么都是需要从头开始写。花了大量时间做系统优化，算法改动，和线上实验，最后一年后在整个广告流量上上了线。</p><p>现在再回顾会觉得整个一年时间都在<strong>打磨</strong>各种细节上，有时候为了5%的性能提升花上上千行代码。这些都导致算法过于复杂，有过度设计之嫌。<strong>但深入各个细节对个人能力提升很大</strong>，而且很多遇到的问题成为了之后研究方向的来源。一些算法上的思考曾写在<a href="https://link.zhihu.com/?target=http%3A//mli.github.io/2013/03/24/the-end-of-feature-engineering-and-linear-model/">这里</a>，当时候深度学习刚刚出来，冥冥中觉得这个应该是大规模机器学习的未来，不过真正开始跟进是好几年以后了。</p><p>11年12月中的时候突然心血来潮随手把材料重新寄了一遍，就选了CMU和MIT，结果意外收到了CMU的offer。有天在百度食堂同凯哥（余凯）和潼哥（张潼）吃饭，我说收了CMU offer，在纠结去不去。他们立马说去跟Alex Smola啊，他要要加入CMU了，我们给你引荐下。</p><p>记得是离开的前一天才开始打包行李，早上去公司开完会，中午离职，跟小伙伴打招呼说出个国，然后就奔机场了。那天北京天气特别好，完全不记得前一天雾霾刚爆了表。</p><h2 id="第一年：9-12-8-13"><a href="#第一年：9-12-8-13" class="headerlink" title="第一年：9/12-8/13"></a>第一年：9/12-8/13</h2><p>第一年的主要事情是熟悉环境和上课。CMU课程比较重，博士需要学8门课，每门课工作量巨大。而且要求做两门课助教，做助教比上课更累。</p><p>这一年上的课中对我最有用的是“高级分布式系统”。之前在上交ACM班的时候已经学过很多质量都还不错课，纯知识性的课程一般对我帮助不大。但这门课主要是读论文，然后大家讨论。不仅仅是关于知识，很多是对设计理念的领悟。大家知道对于系统而言，设计是一门艺术而不是科学，这是设计者审美和哲学理念的体现。同时系统界历史也是由一波又一波的潮流组成，了解历史的发展以及其中不断重复的规律非常有意义。</p><p>那年这门课上课老师是Hui Zhang（神人之一，20多岁就在CMU任教了，学生包括了Ion Stoica，他是Spark作者Matei的导师），他有非常好的大局观，对于“Why”这个问题阐述非常到位。我是通过这门课才对分布式系统有了比较清晰的认识。两年之后我偶然发现我的一篇论文也在这门课的阅读列表里了，算是小成就达成 。</p><p>除了上课，更重要是做研究。我去CMU的时候Alex那时还在Google，而且没经费，所以把我丢给了 Dave Andersen。于是我有了两个导师，一个做机器学习，一个做分布式系统。</p><p>前面半年都是在相互熟悉的过程。我们每周会一起聊一个小时。前半年因为Alex不在，所以我们只能视频。Alex那边信号经常不好，而且他有德国和澳大利亚口音，外加思维跳跃，经常我听不懂他说啥只能卖萌傻笑。还是靠着Dave不断的打字告诉我Alex说了什么才度过了前几次的会。</p><p>两个导师风格迥异。Alex是属于反应特别快，通常你说一点，他已经想好了接下来十点，要跟上他节奏很难。一般抛出问题的时候他就想好了好几个解决方法。这时候要证明自己的想法比他的更好不容易，需要大量的沟通和实验数据支撑。我想我大概是花了两年证明了在某些方向上我的方案一般更好，所以这时候他就不那么hands-on了。</p><p>Dave不会给很多想法，但会帮助把一个东西理解透，然后讲得很清楚。因为我研究方向主要是机器学习上，基本上前两年基本都是我在教Dave什么叫机器学习，而且是尽量不用公式那种教法。</p><p>我的第一个研究工作是关于如果划分数据和计算使得减少机器学习求解中的网络通讯量。Alex体现了他的强项，几分钟就把问题归纳成了一个优化问题，然后我们三各自提出一个解法。我做了做实验发现Dave的算法更好。接下来两个月把算法做了很多优化，然后又做了点理论分析就把论文写了。</p><p>可惜这个想法似乎有点超前，虽然我们一遍又一遍的改进写作，但投了好几个会审稿人就是不理解，或者觉得这个问题不重要。那个时候学术界已经开始吹嘘“大数据”，但我觉得其实大部分人是不懂的，或者他们的“大数据”仍然是几个GB的规模，烤U盘需要十来分钟的那种。</p><p>这是我在CMU的一个工作，我觉得挺有用，但却是唯一没能发表的。</p><p>当时跟我坐同一个办公室的是Richard Peng，他做的是理论研究。我经常跟他讨论问题，然后有了些想法合作了一个工作。大体思想是把图压缩的快速算法做到矩阵的低秩近似上。这个工作写了三十页公式但没有任何实验，我主要当做写代码间隙的悠闲娱乐，不过运气很好的中了FOCS。</p><p>坦白说我不是特别喜欢纯理论这种，例如在bound的证明中很多大量的项直接丢掉了，导致我觉得bound特别的近似。对于做系统的人来说，最后拼的是常数。这个工作中这种大开大合的做法我觉得很不踏实。所以我觉得以后还是应该做更实在点的东西。</p><p>在CMU回到了去百度前的一周七天工作无休的节奏。每周至少80个小时花在学校。如果累了就去健身房，我一般晚上12点去。不仅是我一个人，大家都很努力，例如凌晨的健身房，早3点的办公室，四处都可以见到中国或者印度学生。我那时候的室友田渊栋花在学校的时候比我多很多。</p><p>那一阵子有读了很多关于优化的文章。其中对我启发最大的是Bertsekas写于80年代末的那本关于分布式计算的书。此书可以认为是MIT控制领域黄金一代研究成果总结，换到现在仍然不过时。</p><p>受启发我转去研究异步算法，就是分布式下不保证数据的及时性来提升系统性能。我基于在百度期间做的算法，做了一些改进和理论分析，然后投了NIPS。</p><p>投完NIPS就动身去了Google Research实习。那时候Google Brain成立不久，在“宇宙的答案”42楼，包括Jeff Dean，Geoffrey Hinton，Prabhakar Raghavan好些大牛挤在一起，加起来论文引用率能超80万。</p><p>Alex跟我说，你去读读Jure Leskovec的文章，学学人家怎么讲故事。我在Google也尝试用了些用户GPS数据来对用户行为建模。可是写文章的时候怎么也写不出Jure的那种故事感，发现自己不是那块料。这篇文章因为用了用户数据，恰逢Snowden让大家意识到隐私的重要性，历经艰辛删了一半结果Google才允许发出来。有些累觉不爱。</p><p>不过在Google期间我主要时间花在研究内部代码和文档上。Google的基础架构很好，文档也很健全。虽然没有直接学到了什么，但至少是开了眼界。</p><h2 id="第二年：9-13-8-14"><a href="#第二年：9-13-8-14" class="headerlink" title="第二年：9/13-8/14"></a>第二年：9/13-8/14</h2><p>这学期上了Tuomas Sandholm的机制设计，此乃另一大神，例如最近德州扑克赢了专业选手，之前开公司也卖了上亿。不过这门课我是完完全全没学懂，连承诺的课程大作业都没怎么做出来。之后的两年里我一遇到Tuomas他都会问下有什么进展没。我只能远远看见他就绕开。</p><p>NIPS被拒了，发现审稿人不懂线程和进程的区别，有点沮丧。隔壁实验室一篇想法类似但简单很多的论文倒是中了oral，所以那阵子压力很大。Alex安慰说这种事情常有发生，看淡点，然后举了很多自己的例子。</p><p>之后想了想，一篇好文章自然需要有足够多的“干货”，或者说信息量， 但一篇能被接受的文章需要满足下面这个公式：</p><blockquote><p>文章的信息量 / 文章的易读性 &lt; 审稿人水平 * 审稿人花的时间</p></blockquote><p>对于机器学习会议，因为投稿量大，所以审稿人很多自然平均水平就会下降。而且很多审稿人就花半个小时到一个小时来读文章，所以公式右边数值通常是很小，而且不是我们能控制。</p><p>如果文章的信息量不大，例如是改进前面工作或者一些简单的新想法，那么公式成立的概率很大。而对于信息量大的文章，就需要努力提升易读性，包括清晰的问题设定，足够的上下文解释等等。而前面投的那篇NIPS，以及更早的那个被拒工作，就是因为我们假设了审稿人有足够多的相关专业知识，而我们塞进了太多干货使得大家都读糊涂了。</p><p>即使对于已经发表的文章，上面那个公式同样可以用来衡量一篇论文的引用率。例如经常见到干货很多的文章没有什么人引用，而同时期的某些工作就是考虑了其中简单特殊情况结果被大引特引。</p><p>接下来的半年我主要在做一个通用的分布式机器学习框架，是想以后做实验方便些。名字就叫parameter server，沿用了Alex 10年论文提出的名字。花了很多时间在接口设计上，做了好几个版本实现，也跑了些工业界级别的大规模的实验。</p><p>不过真正花了我大量时间的是在写论文上。目标是把这个工作投到OSDI上，OSDI是系统界两大会之一。我们预计审稿人跟Dave两年前状态差不多，不会有太多机器学习和数学背景，所以需要尽量的少用公式。整整一个月就花在写论文上，14页的<a href="http://www.cs.cmu.edu/~muli/file/parameter_server_osdi14.pdf" target="_blank" rel="noopener">文章</a>满满都是文字和示意图。不过努力没有白费，最终论文被接受了。随后又花了好几周准备大会报告上。相对于平时花一周写论文，两三天准备报告，这次在写作和报告水平上有了很大的提升。没有放进去的公式和定理投了接下来的<a href="<http://www.cs.cmu.edu/~muli/file/parameter_server_nips14.pdf">NIPS</a>，这次运气很好的中了。</p><p>有了文章后稍微心安了点可以更自由的做些事情。</p><p>寒假回了趟国，跑去百度找了凯哥和潼哥。潼哥说他最近有个想法，于是快糙猛的把实验做了然后写了篇论文投了KDD。同时期Alex一个学生也把他一个一直想让我做但我觉得这个小trick不值得我花时间的想法投了KDD，结果中了<a href="http://www.cs.cmu.edu/~muli/file/minibatch_sgd.pdf" target="_blank" rel="noopener">最佳论文</a>作报告那天我在的会场稀稀疏疏几个人，他们隔壁会场人山人海。这个使得好长一段时间我都在琢磨是不是还是要跟着导师走比较好。</p><p>那时凯哥在百度搞少帅计划，觉得蛮合适就加入了。这时凯哥正带着一大帮兄弟轰轰烈烈的搞深度学习，我自然也是跳坑了。试过好几个想法后，我觉得做做分布式的深度学习框架比较对胃口。我挑了CXXNet作为起点，主要是因为跟天奇比较熟。同时也慢慢上手跑一些Alexnet之类的实验。</p><p>我是因为少帅计划才开始开始做深度学习相关项目，凯哥也很支持我做开源开发回馈社会而不是只做公司内部的产品。但在少帅期间并没有做出什么对公司有帮助的事，很是惭愧。</p><h2 id="第三年：9-14-8-15"><a href="#第三年：9-14-8-15" class="headerlink" title="第三年：9/14-8/15"></a>第三年：9/14-8/15</h2><p>回CMU后Alex看见深度学习这么火，说我们也去买点GPU玩玩。但我们比较穷，只能去newegg上掏点便宜货。这个开启了轰轰烈烈的机器折腾之旅。整个一年我觉得我都在买买买装装装上。最终我们可能就花了小几万刀攒出了一个有80块GPU的集群。现在想想时间上花费不值得，而且为了图便宜买了各种型号的硬件导致维护成本高。但当时候乐在其中。具体细节可以看这篇<a href="https://link.zhihu.com/?target=http%3A//mli.github.io/gpu/2016/01/17/build-gpu-clusters/">blog</a></p><p>这一年写了很多parameter server代码，同时花了很时间帮助用户使用这些代码。很难说做得很成功，现在想想有几个原因。写代码时我会优先考虑性能和支持最多的机器学习算法。但正如前面的错误，忽略了代码的易读性，从而导致只有少部分人能理解代码从而做一些开发。例如我尝试让Alex组的学生来使用这些代码，但其中的各种异步和callback让他们觉得很是难懂。其次是没有人能一起审核代码接口，导致这些接口有浓浓的个人味道，很难做到对所有人都简单明了。</p><p>不过幸运的是找到一帮志同道合的小伙伴。最早是我发现天奇在写xgboost的分布式启动脚本，我看了看发现挺好用，就跟他聊了聊。聊下的发现有很多基础部件例如启动脚本，文件读取应该是可以多个项目共同使用，而不是每个项目都造一个轮子。于是跟天奇在Github上创建了一个叫DMLC的组织，用来加强合作和沟通。第一个项目是dmlc-core，放置了启动和数据读取代码。</p><p>DMLC的第二个新项目叫wormhole。想法是提供一系列分布式机器学习算法，他们使用差不多相同的配置参数来统一用户体验。我把parameter server里面的机器学习相关算法移植了过来，天奇移植了xgboost。Parameter server原有的系统代码简化到了ps-lite。</p><p>中途我听百度同学说factorization machine（FM）在广告数据上效果不错，所以在wormhole上实现了下。针对分布式做了一些优化，然后投了WSDM。前后没有花到一个月，但神奇的竟然拿了最佳论文提名。</p><p>在wormhole的开发中发现一个问题，就是各个算法还是挺不一样，他们可以共用一些代码，但又有各自的特点，需要特别的优化来保证性能。这样导致维护有些困难，例如对共用代码的改动导致所有项目都要检查下。总结下来觉得一个项目最好只做一件事情。所以天奇把xgboost代码放回原来项目，我也把FM独立出来一个项目叫difacto。</p><p>通过一系列的项目，我学到的一点是，以目前的水平和人力，做一个通用而且高效的分布式机器学习框架是很难的一件事情。比较可行的是针对一类相似的机器学习算法做针对性的项目。这个项目的接口必须是符合这类算法结构，所以做算法开发的同学也能容易理解，而不是过多暴露底层系统细节。</p><p>真正的让DMLC社区壮大的项目是第三个，叫做MXNet。当时的背景是CXXNet达到了一定的成熟度，但它的灵活性有局限性。用户只能通过一个配置项来定义模型，而不是交互式的编程。另外一个项目是zz和敏捷他们做的Minerva，是一个类似numpy的交互式编程接口，但这个灵活的接口对稳定性和性能优化带来很多挑战。我当时候同时给两个项目做分布式的扩展，所有都有一定的了解。然后一个自然的想法是，把两个项目合并起来取长补短岂不是很好。</p><p>召集了两个项目的开发人员讨论了几次，有了大致的眉目。新项目取名MXNet，可以叫做mixed-net，是前面两个名字（Minerva和CXXNet）的组合。放弃开发了几年的项目不是容易的决定，但幸运的是小伙伴都愿意最求更好，所以 MXNet进展挺顺利。很快就有了可以跑的第一个版本。</p><h2 id="第四年：9-15-8-16"><a href="#第四年：9-15-8-16" class="headerlink" title="第四年：9/15-8/16"></a>第四年：9/15-8/16</h2><p>前半年为difacto和MXNet写了很多代码。其实一开始的时候我觉得difacto更重要些，毕竟它对于线性算法的提升非常显著而且额外的计算开销并不大，这对广告预估之类的应用会有非常大的提升。但有次遇到Andrew Ng，我跟他说我同时在做这两个项目，他立即告诉我我应该全部精力放在MXNet上，这个的未来空间会大很多。我一直很佩服Andrew的眼光，所以听了他的建议。</p><p>11月的时候MXNet就有了很高的完成度。写了个小论文投去了NIPS的workshop也算是歇了口气。但随后就听到了TensorFlow（TF）开源的消息。由 Jeff Dean领导大量全职工程师开发，Google庞大的宣传机器支持，不出意料迅速成为最流行的深度学习平台。TF对我们压力还是蛮大，我们有核心开发者转去用了TF。不过TF的存在让我领悟到一点，与其过分关心和担忧对手，不如把精力集中在把自己的做得更好。</p><p>NIPS的时候MXNet的小伙伴聚了一次，有好几个我其实是第一次见面。随后Nvidia的GTC邀请我们去做报告。在这两次之间大家爆发了一把，做了很多地方的改进。同时用户也在稳步增长。我们一直觉得MXNet是小开发团队所以做新东西快这是一个优势，但随着用户增加，收到抱怨说开发太快导致很多模块兼容性有问题。有段时间也在反思要在新技术开发速度和稳定性之间做一些权衡。</p><p>这时一夜之间大数据不再流行，大家都在谈深度学习了。</p><p>我也花了很多力气在宣传MXNet和争取开发者上。包括微博知乎上吼一吼，四处给报告。在大量的点赞声中有些陶醉，但很多中肯的批评也让我意识到重要的一点，就是应该真诚的分享而不是简单的吹嘘。</p><p>因为大量的媒体介入，整个深度学习有娱乐化的趋势。娱乐化的报道很多都只是一些简单信息，（有偏见）的观点，而没有太多干货。不仅对别人没营养，对自己来说也就是满足虚荣心。与其写这些简单的水文，不如静下心做一些有深度的分享，包括技术细节，设计思路，和其中的体会。</p><p>此类分享一个容易陷入的误区是只关注自己做了什么，结果多么好。这些确实能证明个人能力，对于想重复这个工作的人来说会有很大帮助。但更多的人更关心的是适用范围在哪里，就是什么情况下效果会减弱；为什么结果会那么好；insight是什么。这个需要更多深入的理解和思考，而不是简单的展示结果。</p><p>这个对写论文也是如此。只说自己的结果比基线好多少只能说明这是不错的工作，但结果再好并不能意味这个工作有深度。</p><p>深度学习的火热导致了各种巨资收购初创司不断。Alex也有点按耐不住， 结果是他，Dave，Ash（曾经是YahooCTO）和我合伙弄了一家公司，拿了几十万的天使投资就开工了。Alex写爬虫，Dave写框架，我跑模型，风风火火干了好一阵子。可惜中途Dave跑路去跟Jeff做TF了。后来这个公司卖给了一个小上市公司。再后来我们觉得这个公司不靠谱也就没考虑跟他们干了。</p><p>第一次创业不能说很成功，从中学到几点：一是跟教授开公司一定要注意有太多想法但没死死的掐住一个做，二是找一堆兼职的博士生来干活不是特别靠谱，尤其是产品不明确的时候，三是即使要卖公司也一定要做一个产品出来。我们卖的时候给很多人的感觉是团队人太强但产品太弱，所以他们只想要人而已。四是试图想要通过技术去改变一个非技术公司是很难的事情，尤其是过于新的技术。</p><p>然后我们就奔去折腾下一个公司。Ash早财务自由所以想做一个大的想法，但这时Alex刚在湾区买了个房，有还贷压力，他选择去了Amazon。于是算是胎死腹中。</p><p>随后收到Jeff的邮件说有没有兴趣加入Google，自然这是一个很诱人的机会。同时我觉得小的创业技术性强的公司是不错的选择。但从MXNet的发展上来书，去Amazon是最好选择之一。自己挖的坑，总是要自己填的。所以我以兼职的身份去了Amazon，领着一帮小弟做些MXNet开发和AWS上深度学习的应用。</p><h2 id="第五年：9-16-2-17"><a href="#第五年：9-16-2-17" class="headerlink" title="第五年：9/16-2/17"></a>第五年：9/16-2/17</h2><p>早在15年初Alex就表示我可以毕业了，但作为拖延晚期患者，迟迟没开始准备。这时候感觉不能再拖了，于是窝在湾区写毕业论文。Alex觉得毕业论文应该好好写，但我对把前面都做完的东西再捣鼓写写实在是没兴趣，尤其是加州太阳那么好，大部分时间我都是躺在后院晒太阳。此时B站已经完全被小学生占领，这边买书也不方便，无聊之余刷了很多起点。然后还写了篇<a href="https://zhuanlan.zhihu.com/p/23781756" target="_blank" rel="noopener">炼丹文</a>。</p><p>CMU要求答辩委员会需要有三个CMU老师和一个学校外的。除了两个导师外，我找了Jeff Dean和刚加入CMU的Ruslan Salakhutdinov. 结果Russ随后就加入了Apple，整个委员会的人都在湾区了。Jeff开玩笑说可以来Google答辩。可惜跟CMU争吵了好多次，还是不允许在校外答辩，而且必须要三个人委员会成员在场。这些限制导致答辩一拖再拖，而且临时加了Barnabas Poczos来凑人数。最后是Jeff的助理快刀斩乱麻的协调好了时间把所有东西定好了。没有她估计我还可以拖几个月。</p><p>答辩的时候是一个比较奇异的状态，委员会里有Google, Amazon, Apple的AI负责人，剩下两个和我又分别在这三家公司兼职。这个反应了当下AI领域学术界纷纷跑去工业界的趋势。</p><p>不过答辩这个事情倒是挺简单，跟平常做个报告没什么太多区别。一片祥和，即使Russ问了MXNet和TensorFlow哪家强这个问题也没有打起来。</p><p>答辩后我问委员会说，我在考虑找个学术界的工作，有什么建议没。大家介绍了一大堆经验，不过大家都强调的一个重点是：学术界好忙好忙，而且好穷好穷，工业界的薪水（就差指自己脸了）分分钟秒掉CMU校长。你要好好想。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>答辩前一天的晚上，我想了两个问题，一个是“博士收获最大的是什么”，另一个是“如果可以重来会怎么办”。对于第一个问题，这五年时间自然学到了很多东西，例如系统的学习了分布式系统，紧跟了机器学习这五年的发展，写文章做幻灯片做报告水平有提升，代码能力也加强了些。自信上有所提高，觉得既可以做一流的研究，也可以写跟大团队PK的代码。只要努力，对手没什么可怕的。</p><p>但更重要的是博士的五年的时间可以专注的把一些事情从技术上做到最好，做出新的突破，这个氛围没有其他地方能给予。</p><p>第二个问题的一个选项是当年留在国内会怎么样？ 当年百度的伙伴们多数现在都做得很好，都在引领这一波AI的潮流，甚至有好几个创造了上亿价值的公司。所以从金钱或者影响力角度来看，一直在工业界也不差，说不定现在已经是土豪了。</p><p>不过我觉得还是会选择读博。赚钱以后还有大把时间可以，但是能花几年时间在某个领域从入门到精通甚至到推动这个领域发展的机会就一次。站在这个领域的高点会发现世界虽然很大，但其实其他领域也使用差不多的技术，有着同样的发展规律。博士期间领悟到的学习的方法可以在各个方向上都会大有作为。</p><p>更重要的是理想和情怀。人一生要工作五十年，为什么不花五年来追求下理想和情怀呢？</p><h4 id="我的感悟："><a href="#我的感悟：" class="headerlink" title="我的感悟："></a>我的感悟：</h4><p>–做的东西一定要反复打磨，每个细节都做好了再呈现给这个世界。李沐和小熊都讲深入细节（必须为提升5%性能写千行代码）会明显提升个人能力，细致钻研才能遇到有深度的问题，这些问题可能是以后做项目或者发文章的思路来源。</p><h5 id="“11年12月中的时候突然心血来潮随手把材料重新寄了一遍，就选了CMU和MIT，结果意外收到了CMU的offer。有天在百度食堂同凯哥（余凯）和潼哥（张潼）吃饭，我说收了CMU-offer，在纠结去不去。他们立马说去跟Alex-Smola啊，他要要加入CMU了，我们给你引荐下”"><a href="#“11年12月中的时候突然心血来潮随手把材料重新寄了一遍，就选了CMU和MIT，结果意外收到了CMU的offer。有天在百度食堂同凯哥（余凯）和潼哥（张潼）吃饭，我说收了CMU-offer，在纠结去不去。他们立马说去跟Alex-Smola啊，他要要加入CMU了，我们给你引荐下”" class="headerlink" title="“11年12月中的时候突然心血来潮随手把材料重新寄了一遍，就选了CMU和MIT，结果意外收到了CMU的offer。有天在百度食堂同凯哥（余凯）和潼哥（张潼）吃饭，我说收了CMU offer，在纠结去不去。他们立马说去跟Alex Smola啊，他要要加入CMU了，我们给你引荐下”"></a>“11年12月中的时候突然心血来潮随手把材料重新寄了一遍，就选了CMU和MIT，结果意外收到了CMU的offer。有天在百度食堂同凯哥（余凯）和潼哥（张潼）吃饭，我说收了CMU offer，在纠结去不去。他们立马说去跟Alex Smola啊，他要要加入CMU了，我们给你引荐下”</h5><p>–圈子和良好人际关系的重要性。</p><p>–李一周工作八十个小时，累了就深夜去健身房健身，CMU深夜随处可见中国和印度留学生。自己目前的努力程度还未达到需要拼天赋的时候。</p><h5 id="接下来的半年我主要在做一个通用的分布式机器学习框架，是想以后做实验方便些。名字就叫parameter-server，沿用了Alex-10年论文提出的名字。花了很多时间在接口设计上，做了好几个版本实现，也跑了些工业界级别的大规模的实验。"><a href="#接下来的半年我主要在做一个通用的分布式机器学习框架，是想以后做实验方便些。名字就叫parameter-server，沿用了Alex-10年论文提出的名字。花了很多时间在接口设计上，做了好几个版本实现，也跑了些工业界级别的大规模的实验。" class="headerlink" title="接下来的半年我主要在做一个通用的分布式机器学习框架，是想以后做实验方便些。名字就叫parameter server，沿用了Alex 10年论文提出的名字。花了很多时间在接口设计上，做了好几个版本实现，也跑了些工业界级别的大规模的实验。"></a><strong>接下来的半年我主要在做一个通用的分布式机器学习框架，是想以后做实验方便些。名字就叫parameter server，沿用了Alex 10年论文提出的名字。花了很多时间在接口设计上，做了好几个版本实现，也跑了些工业界级别的大规模的实验。</strong></h5><h5 id="不过真正花了我大量时间的是在写论文上。目标是把这个工作投到OSDI上，OSDI是系统界两大会之一。我们预计审稿人跟Dave两年前状态差不多，不会有太多机器学习和数学背景，所以需要尽量的少用公式。整整一个月就花在写论文上，14页的文章满满都是文字和示意图。不过努力没有白费，最终论文被接受了。随后又花了好几周准备大会报告上。相对于平时花一周写论文，两三天准备报告，这次在写作和报告水平上有了很大的提升。没有放进去的公式和定理投了接下来的NIPS，这次运气很好的中了"><a href="#不过真正花了我大量时间的是在写论文上。目标是把这个工作投到OSDI上，OSDI是系统界两大会之一。我们预计审稿人跟Dave两年前状态差不多，不会有太多机器学习和数学背景，所以需要尽量的少用公式。整整一个月就花在写论文上，14页的文章满满都是文字和示意图。不过努力没有白费，最终论文被接受了。随后又花了好几周准备大会报告上。相对于平时花一周写论文，两三天准备报告，这次在写作和报告水平上有了很大的提升。没有放进去的公式和定理投了接下来的NIPS，这次运气很好的中了" class="headerlink" title="不过真正花了我大量时间的是在写论文上。目标是把这个工作投到OSDI上，OSDI是系统界两大会之一。我们预计审稿人跟Dave两年前状态差不多，不会有太多机器学习和数学背景，所以需要尽量的少用公式。整整一个月就花在写论文上，14页的文章满满都是文字和示意图。不过努力没有白费，最终论文被接受了。随后又花了好几周准备大会报告上。相对于平时花一周写论文，两三天准备报告，这次在写作和报告水平上有了很大的提升。没有放进去的公式和定理投了接下来的NIPS，这次运气很好的中了."></a><strong>不过真正花了我大量时间的是在写论文上。目标是把这个工作投到OSDI上，OSDI是系统界两大会之一。我们预计审稿人跟Dave两年前状态差不多，不会有太多机器学习和数学背景，所以需要尽量的少用公式。整整一个月就花在写论文上，14页的文章满满都是文字和示意图。不过努力没有白费，最终论文被接受了。随后又花了好几周准备大会报告上。相对于平时花一周写论文，两三天准备报告，这次在写作和报告水平上有了很大的提升。没有放进去的公式和定理投了接下来的NIPS，这次运气很好的中了.</strong></h5><p>–要有工程能力，做出框架，设计接口，更新版本，跑了工业界级别的大规模实验。然后再用这个框架的内容去发论文。这样的论文工作扎实，比堆砌算法的好很多。小熊给的nlp论文列表里的文章都是带源代码的，要好好研究一下好的论文是怎么做的。李沐提到的百度速发文章和两篇parameter server的文章都打上了超链接，好好读一读。</p><p>–申请博士是要与全球各位优秀的科研工作者竞争的，当别人有完美的本科GPA和一堆科研经历甚至还有多篇发出去的论文，我投不出去的没有工作量和工程量的文章完全没有竞争力。</p><p>–parameter server项目李沐自己也说做的不够成熟，但是已经够发文章了。后来他与陈天奇成立的DMLC组织又做了很多项目。even大牛也都有一个进步的过程，关键是勇敢去做，以及在做的过程中多总结和思考。</p><p>–李沐、陈天奇的github主页，工作量叹为观止。<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1314_05_679.png" alt=""></p><p>–MXnet遭遇TensorFlow竞争，他们面对竞争的做法是做好自己的工作，而非担忧对手有多强大。</p><p>–当然，时至今日tensorflow面对pytorch的竞争时也显得力不从心了，谁知道呢。</p><h5 id="通过一系列的项目，我学到的一点是，以目前的水平和人力，做一个通用而且高效的分布式机器学习框架是很难的一件事情。比较可行的是针对一类相似的机器学习算法做针对性的项目。这个项目的接口必须是符合这类算法结构，所以做算法开发的同学也能容易理解，而不是过多暴露底层系统细节"><a href="#通过一系列的项目，我学到的一点是，以目前的水平和人力，做一个通用而且高效的分布式机器学习框架是很难的一件事情。比较可行的是针对一类相似的机器学习算法做针对性的项目。这个项目的接口必须是符合这类算法结构，所以做算法开发的同学也能容易理解，而不是过多暴露底层系统细节" class="headerlink" title="通过一系列的项目，我学到的一点是，以目前的水平和人力，做一个通用而且高效的分布式机器学习框架是很难的一件事情。比较可行的是针对一类相似的机器学习算法做针对性的项目。这个项目的接口必须是符合这类算法结构，所以做算法开发的同学也能容易理解，而不是过多暴露底层系统细节"></a><strong>通过一系列的项目，我学到的一点是，以目前的水平和人力，做一个通用而且高效的分布式机器学习框架是很难的一件事情。比较可行的是针对一类相似的机器学习算法做针对性的项目。这个项目的接口必须是符合这类算法结构，所以做算法开发的同学也能容易理解，而不是过多暴露底层系统细节</strong></h5><p>–做项目/发文章要精在某个点上比较靠谱。</p><h5 id="我也花了很多力气在宣传MXNet和争取开发者上。包括微博知乎上吼一吼，四处给报告。在大量的点赞声中有些陶醉，但很多中肯的批评也让我意识到重要的一点，就是应该真诚的分享而不是简单的吹嘘。"><a href="#我也花了很多力气在宣传MXNet和争取开发者上。包括微博知乎上吼一吼，四处给报告。在大量的点赞声中有些陶醉，但很多中肯的批评也让我意识到重要的一点，就是应该真诚的分享而不是简单的吹嘘。" class="headerlink" title="我也花了很多力气在宣传MXNet和争取开发者上。包括微博知乎上吼一吼，四处给报告。在大量的点赞声中有些陶醉，但很多中肯的批评也让我意识到重要的一点，就是应该真诚的分享而不是简单的吹嘘。"></a><strong>我也花了很多力气在宣传MXNet和争取开发者上。包括微博知乎上吼一吼，四处给报告。在大量的点赞声中有些陶醉，但很多中肯的批评也让我意识到重要的一点，就是应该真诚的分享而不是简单的吹嘘。</strong></h5><h5 id="赚钱以后还有大把时间可以，但是能花几年时间在某个领域从入门到精通甚至到推动这个领域发展的机会就一次。"><a href="#赚钱以后还有大把时间可以，但是能花几年时间在某个领域从入门到精通甚至到推动这个领域发展的机会就一次。" class="headerlink" title="赚钱以后还有大把时间可以，但是能花几年时间在某个领域从入门到精通甚至到推动这个领域发展的机会就一次。"></a><strong>赚钱以后还有大把时间可以，但是能花几年时间在某个领域从入门到精通甚至到推动这个领域发展的机会就一次。</strong></h5><p>–想方设法读博，在选择方向时注意跟紧时代的发展。</p>]]></content>
    
    
    <categories>
      
      <category>5.杂谈</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文写法</tag>
      
      <tag>思考</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_搜索_A[星号]寻路算法</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E6%90%9C%E7%B4%A2_A%5B%E6%98%9F%E5%8F%B7%5D%E5%AF%BB%E8%B7%AF%E7%AE%97%E6%B3%95/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E6%90%9C%E7%B4%A2_A%5B%E6%98%9F%E5%8F%B7%5D%E5%AF%BB%E8%B7%AF%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="A-寻路算法"><a href="#A-寻路算法" class="headerlink" title="A*寻路算法"></a>A*寻路算法</h1><p>算法要解决的问题：</p><p>在有障碍的格子地图上已知终点和起点，横走代价小于斜走，求由始到终代价最小的路径。</p><p>F = G + H </p><p>总代价  = 到起点的格子数 + 到终点的曼哈顿距离</p><p>算法描述：</p><ol><li><p>初始节点即为当前节点，放入open列表</p></li><li><p>初始节点从open列表移除，放入close列表</p></li></ol><ol start="3"><li><p>当前格子周围八个格子用如下三选一方式处理:</p><ol><li>若在close列表中，忽略</li><li>若不在open/close列表中，加入open列表，并将当前格子设为它的父节点</li><li>若在open列表中，计算这个格子经过当前格子的F值，如果F值更小，更新之，并将当前格子设为它的新父节点。</li></ol></li><li><p>把当前格子从open列表拿入close列表；</p><p>设置刚才加入open列表中F值最小的格子为当前格子。</p></li></ol><ol start="5"><li>重复3.4.两步，知道找到终点，随后按照父节点顺序回溯，找到最优路径</li></ol><p>不需要更新F值的图示</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1226_56_264.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1227_17_162.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1227_37_630.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1227_53_344.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1228_18_128.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1228_32_410.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1228_44_755.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1228_58_530.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1229_12_312.png" alt=""></p><p>需要更新F值的图示</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1229_30_596.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1229_40_541.png" alt=""></p><p>小结：</p><p>A*寻路算法需要对每个格子维持三个状态值：总代价，到起点的距离，到终点的距离。下一步如何选择由总代价值指导。通常比DFS和BFS更高效。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>A*寻路算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_搜索_回溯算法</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E6%90%9C%E7%B4%A2_%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E6%90%9C%E7%B4%A2_%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="8-查找"><a href="#8-查找" class="headerlink" title="8.查找"></a>8.查找</h1><h4 id="8-1回溯"><a href="#8-1回溯" class="headerlink" title="8.1回溯"></a>8.1回溯</h4><p>回溯算法也叫试探法，是一种系统地解决搜索问题的解的方法</p><p>用回溯算法解决问题的一般步骤：</p><p>1.针对问题，定义解空间，使得能用回溯法方便地搜索整个解空间</p><p>2.确定易于搜索的解空间结构，使得能用回溯法方便地搜索整个空间结构</p><p>3.以深度优先的方式搜索解空间，并且在搜索过程中使用剪枝函数避免无效搜索</p><p>回溯法在解空间树里对结果进行深度优先遍历，如果发现本节点不属于解的范围，则退出，剪枝，然后递归地对子节点搜索</p><p>经典问题是八皇后问题，见代码</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>查找</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_网络流，最短路径，字典树</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E7%BD%91%E7%BB%9C%E6%B5%81%EF%BC%8C%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%EF%BC%8C%E5%AD%97%E5%85%B8%E6%A0%91/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E7%BD%91%E7%BB%9C%E6%B5%81%EF%BC%8C%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%EF%BC%8C%E5%AD%97%E5%85%B8%E6%A0%91/</url>
    
    <content type="html"><![CDATA[<h1 id="7-最短路径-迪杰斯特拉算法"><a href="#7-最短路径-迪杰斯特拉算法" class="headerlink" title="7.最短路径:迪杰斯特拉算法"></a>7.最短路径:迪杰斯特拉算法</h1><h3 id="7-1释义"><a href="#7-1释义" class="headerlink" title="7.1释义"></a>7.1释义</h3><p>迪杰斯特拉算法就是求一个顶点到其他所有顶点的最短路径</p><p>需要维持两个数据集,一个最开始只有顶点,另一个最开始有除了顶点之外的所有点</p><p>然后每次都把距离顶点最近的点放入前一个数据集,然后遍历一次顶点到和前一个数据集接触的点的最短路径,有更近的就更新一次</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1238_23_066.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1238_45_004.png" alt=""></p><h1 id="8-网络流建模"><a href="#8-网络流建模" class="headerlink" title="8.网络流建模"></a>8.网络流建模</h1><h3 id="8-1几个概念"><a href="#8-1几个概念" class="headerlink" title="8.1几个概念"></a>8.1几个概念</h3><ul><li><p>原点:起点</p></li><li><p>汇点:目标点</p></li><li><p>流:从原点到汇点的一条路径</p></li><li><p>流量:通过一条边的水的体积</p></li><li><p>容量:每条管道允许通过的最大流量</p></li><li><p>()实际流量:取决于流上最小的容量,最小流量是流的短板</p></li><li><p>最小割:如下图,切断哪些管道后,源头的水就不能流向汇点了?</p><ul><li><p>最小割=最大流</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1239_14_419.png" alt=""></p></li></ul></li></ul><h1 id="9-哈希表"><a href="#9-哈希表" class="headerlink" title="9.哈希表"></a>9.哈希表</h1><h4 id="9-1什么是哈希表"><a href="#9-1什么是哈希表" class="headerlink" title="9.1什么是哈希表"></a>9.1什么是哈希表</h4><p>举例说明</p><p>去商场停车,有三种策略</p><ul><li>随机停,然后找车的时候从头到尾顺序搜索,时间复杂度是O(N)</li><li>所有车辆按照牌照顺序停,然后每次二分查找找车,时间复杂度是log(n)</li><li>每个车都有一个与牌照对应的停车位,专车专位,时间复杂度是O(1)<ul><li>这种策略时间复杂度确实低,但是空间开销过大,每个车牌照都需要分配一个存储空间</li></ul></li></ul><p>所以在时间复杂度和空间复杂度之间做个取舍，降低专车专位的空间开销，同时尽量保留时间开销的优势</p><p>哈希表：比如车牌照是０　－－　９８５７３６２８之间</p><p>取末尾四位作为下标依据，塞入散列表</p><ul><li>碰撞问题<ul><li>如果两个车的尾号都是3628怎么办?<ul><li>开链表法:如果两个车都是3628,就在原来的3628后边跟一个链表,把后一个3628接在后边</li><li>再散列法:如果3628碰撞,第二个3628就放在3629好了</li></ul></li></ul></li><li>哈希表的评估标准:<ul><li>优秀的哈希表一定是通过巧妙的散列函数,尽量避免碰撞问题的</li></ul></li></ul><h1 id="10-1后缀树"><a href="#10-1后缀树" class="headerlink" title="10.1后缀树"></a>10.1后缀树</h1><p>字典树和压缩字典树    </p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1239_59_443.png" alt=""></p><ul><li>把不会引起歧义的节点串压缩为一个节点</li></ul><p>后缀树就是把一颗以所有的后缀为关键字建立的压缩字典树<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1240_13_018.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>字典树</tag>
      
      <tag>最小生成树</tag>
      
      <tag>Kruskal</tag>
      
      <tag>Prim</tag>
      
      <tag>迪杰斯特拉算法</tag>
      
      <tag>哈希表</tag>
      
      <tag>后缀树</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_搜索_贪心算法与最小生成树</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E6%90%9C%E7%B4%A2_%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E4%B8%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E6%90%9C%E7%B4%A2_%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E4%B8%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91/</url>
    
    <content type="html"><![CDATA[<h1 id="贪心算法"><a href="#贪心算法" class="headerlink" title="贪心算法"></a>贪心算法</h1><p>贪心算法是指，在对问题求解时，总是做出当前来看最好的选择，比较短视，不从总体上考虑最优解，每一步都在向局部最优解迈进（类似于梯度下降算法）</p><p>贪心算法并不能总是得到最优解，关键是贪心策略的选择，选择贪心策略必须具备无后效性，即某个状态以前的过程只与当前状态有关，不会影响到以后的状态。</p><h5 id="引入找零问题"><a href="#引入找零问题" class="headerlink" title="引入找零问题"></a>引入找零问题</h5><p>尽可能用少的硬币和纸币加出一个指定金额总数。思路是从尽可能大的面额的货币开始处理，附上代码</p><pre><code class="python">denom = [10000, 5000, 2000, 1000, 500, 200, 100, 50, 25, 10, 5, 1]owed = 9876payed = []for d in denom:    while owed &gt;= d:        owed -= d        payed.append(d)print(sum(payed))print(payed)</code></pre><p>输出如下结果</p><pre><code class="python">9876[5000, 2000, 2000, 500, 200, 100, 50, 25, 1]</code></pre><h5 id="引入背包问题"><a href="#引入背包问题" class="headerlink" title="引入背包问题"></a>引入背包问题</h5><p>背包问题可视为纸币找零问题的泛化版，背包问题是组合优化的NP完全问题，描述是给定一组物品，每种物品都有自己的重量和价格，在限定的总重量内，我们如何选择才能使得物品的总价值最高。</p><ul><li><p>分数背包问题</p><p>比如去野餐，往背包里边放什么？可以放金沙，威士忌，和水。</p><p>我们先放价值最高的金沙，然后放价值稍次的威士忌，最后放水。背包问题的核心是找到权重比，然后按照权重比排序，从高到低顺序一个个装包就好了。</p></li></ul><h5 id="引入哈夫曼算法"><a href="#引入哈夫曼算法" class="headerlink" title="引入哈夫曼算法"></a>引入哈夫曼算法</h5><pre><code class="python"># 哈弗曼算法from heapq import heapify, heappush, heappopfrom itertools import countdef huffman(seq, frq):    num = count()    trees = list(zip(frenquence, num, seq))    heapify(trees)    # 数组按照频率组建成小顶堆，每次pop都pop出值最小的    while len(trees) &gt; 1:        fa, _, a = heappop(trees)        fb, _, b = heappop(trees)        n = next(num)        heappush(trees, (fa+fb, n, [a, b]))    # 将刚才弹出的两个频率相加，然后合并回堆的右下角，堆自然会按照小顶堆的构建规则做调整。    # 频率值在小顶堆的秩序里做操作，跟他一起绑定的哈夫曼编码值也在两两合并中通过多层数组构建为哈夫曼树    return trees[0][-1]seq = &quot;abcdefghi&quot;frq = [4, 5, 6, 9, 11, 12, 15, 16, 20]print(huffman(seq, frenquence))</code></pre><p>哈夫曼算法在构建哈夫曼树的过程中就运用了贪心算法，每次都先解决看起来最好的选择（不管全局规划，先将最小的两个频率值相加合并）。</p><h5 id="引入Kruskal算法和Prim算法"><a href="#引入Kruskal算法和Prim算法" class="headerlink" title="引入Kruskal算法和Prim算法"></a>引入Kruskal算法和Prim算法</h5><p>最小生成树定义</p><ul><li><p>定义</p><ul><li>一个有向带权图,需要删除一些边,使这个图变成权值最小的树</li></ul></li><li><p>实际应用</p><ul><li>几个城市之间如何修高速公路/通信网络实现几个城市互联最有效率?(点组网)</li><li>皇帝如何保持通知体系的运转,又能清理大臣之间跨部门的互相勾结(原图删除边成为最小生成树)</li></ul></li><li><p>构建最小生成树的两种算法</p><ul><li><p>Kruskal算法        —        常用于稀疏图(先取出边再判断节点,稠密的话很慢)</p><ul><li><p>贪心算法,将图的每个边按照权重排序,每次从边集中取出权重最小且两个顶点都不在同一个集合的边加入生成树中,反复执行,直到所有节点都链接成功</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1230_10_308.png" alt=""></p></li><li><p>上图节点的颜色代表他们是否属于同一个集合,寻找到本步最小边后需判断两点是否在同一集合?</p><ul><li>具体判断步骤为:并查集<ul><li>对所有节点遍历建立并查集,按照边的权重建立最小堆</li><li>取出最小堆堆顶数据,并判断两端节点是否在同一集合</li><li>如果不在,将两个节点添加到同一集合,接着将边加入生成边,如果在,则不进行操作.</li><li>重复上述步骤,直到所有的边都检查完</li></ul></li></ul></li></ul></li></ul></li></ul><ul><li><p>Prim算法   —   常用于稠密图(只是对节点遍历)</p><ul><li><p>Kruskral算法主要对边操作</p></li><li><p>Prim算法主要对节点操作</p><ul><li><p>Prim算法没有判断两个点是否在同一集合的步骤,所以不使用并查集</p></li><li><p>具体步骤如下</p><ul><li><p>建立基本容器</p><ul><li>建立已完事儿集合存放已经完事儿节点</li><li>建立未完事儿集合存放没有被处理节点</li><li>建立完事儿节点相连的边的最小堆</li></ul></li><li><p>遍历所有节点,若没有被访问,则添加进节点set,然后将其相连的边加入最小堆</p></li><li><p>从堆中取最小的边,然后判断to节点是否被访问过,若没有,将此边加入生成树,并标记此节点被访问过</p></li><li><p>然后将to节点所连的边加入最小堆中,不然这个网络就不会扩展了</p></li><li><p>循环,知道所有节点遍历完</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1230_39_087.png" alt=""></p></li></ul></li></ul></li></ul></li></ul><p>小结：</p><p>贪心算法是在一步步构建出属于自己的解决方案时，慢慢逼近最优解，在到达终点时，就变成最优解了。</p><p>贪心算法的优缺点：</p><p>优点：简单，高效，省去了为了找最优解可能需要穷举操作，通常作为其它算法的辅助算法来使用</p><p>缺点：不从总体上考虑其它可能情况，每次选取局部最优解，不再进行回溯处理，所以很少情况下得到最优解。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>Kruskal</tag>
      
      <tag>Prim</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_排序_7.2-归并排序</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E6%8E%92%E5%BA%8F_7.2-%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E6%8E%92%E5%BA%8F_7.2-%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/</url>
    
    <content type="html"><![CDATA[<h1 id="7-2归并排序"><a href="#7-2归并排序" class="headerlink" title="7.2归并排序"></a>7.2归并排序</h1><p>归并排序思路：</p><p>1.将序列中待排序数字分为若干组，每个数字分为一组</p><p>2.将若干组两两合并，保证合并后的组是有序的</p><p>3.重复第二步，知道所有组都被处理完成</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1224_39_967.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1225_05_846.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>排序</tag>
      
      <tag>归并排序</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_排序_7.3-计数排序</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E6%8E%92%E5%BA%8F_7.3-%E8%AE%A1%E6%95%B0%E6%8E%92%E5%BA%8F/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E6%8E%92%E5%BA%8F_7.3-%E8%AE%A1%E6%95%B0%E6%8E%92%E5%BA%8F/</url>
    
    <content type="html"><![CDATA[<h1 id="7-3计数排序"><a href="#7-3计数排序" class="headerlink" title="7.3计数排序"></a>7.3计数排序</h1><p>适用范围：</p><p>数据量很大但是取值范围很小</p><p>适用案例：</p><p>对某企业三万员工的年龄排序，快速取得高考成绩等</p><p>桶排序的思想：</p><p>以年龄排序为例子：</p><p>假设年龄以0-60为区间</p><p>创建一个新的数组，长度为61，遍历待排序数组，将出现的数字频率记录在以本数字为下标的计数数组中，</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1225_35_726.png" alt=""></p><p>然后按照计数数组记录的频率值输出数字就OK了</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>排序</tag>
      
      <tag>计数排序</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_7.字典树Trie</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_7.%E5%AD%97%E5%85%B8%E6%A0%91Trie/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_7.%E5%AD%97%E5%85%B8%E6%A0%91Trie/</url>
    
    <content type="html"><![CDATA[<h1 id="7-字典树"><a href="#7-字典树" class="headerlink" title="7.字典树"></a>7.字典树</h1><p>在实际的搜索引擎中，当我们在数据库中搜索一个关键字的时候，如何快速准确的进行定位是一个关键的问题，在面临大规模数据的时候，使用暴力的手段往往会造成检索和查找性能的低下，因此我们需要更加高效的数据结构。</p><p>这时候我们引入一种新的数据结构：Trie树（字典树）。</p><p>又称单词查找树，<a href="https://baike.baidu.com/item/Trie%E6%A0%91" target="_blank" rel="noopener">Trie树</a>，是一种<a href="https://baike.baidu.com/item/%E6%A0%91%E5%BD%A2%E7%BB%93%E6%9E%84/9663807" target="_blank" rel="noopener">树形结构</a>，是一种哈希树的变种。典型应用是用于统计，排序和保存大量的<a href="https://baike.baidu.com/item/%E5%AD%97%E7%AC%A6" target="_blank" rel="noopener">字符</a>串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。它的优点是：利用字符串的公共前缀来减少查询时间，最大限度地减少无谓的字符串比较，查询效率比哈希树高。</p><p>字典树每一个节点代表一个字符,有相同前缀的树就有相同的根节点,每个节点结束的时候用一个特殊的标记来表示(-1)</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1239_46_215.png" alt=""></p><p>从图中可以看出：</p><p>1.每一个节点代表一个字符</p><p>2.有相同前缀的单词在树中就有公共的前缀节点，由于一共有26个小写英文字母（在这篇文章中，我们主要讨论小写的英文字母查询），因此每个节点最多有26个子节点。</p><p>3.整棵树的根节点是空的（这里我们设置根节点为root=0），这便于查找和插入，可以通过根节点快速的进入树结构，稍后就会明白。</p><p>4.每个节点结束的时候用一个特殊的标记来表示，这里我们用-1来表示结束，从根节点到-1所经过的所有的节点对应一个英文单词。</p><pre><code class="java">packagecom.suning.search.test.tree.trie;public class Trie{    private int SIZE=26;    private TrieNode root;//字典树的根    Trie() //初始化字典树    {        root=new TrieNode();    }    private class TrieNode //字典树节点    {        private int num;//有多少单词通过这个节点,即由根至该节点组成的字符串模式出现的次数        private TrieNode[]  son;//所有的儿子节点        private boolean isEnd;//是不是最后一个节点        private char val;//节点的值p       private boolean haveSon;        TrieNode()        {            num=1;            son=new TrieNode[SIZE];            isEnd=false;h           haveSons=false;        }    }//建立字典树    public void insert(String str) //在字典树中插入一个单词    {        if(str==null||str.length()==0)        {            return;        }        TrieNode node=root;        char[]letters=str.toCharArray();        for(int i=0,len=str.length(); i&lt;len; i++)        {            int pos=letters[i]-&#39;a&#39;;            if(node.son[pos]==null)            {                node.haveSon = true;                node.son[pos]=newTrieNode();                node.son[pos].val=letters[i];            }            else            {                node.son[pos].num++;            }            node=node.son[pos];        }        node.isEnd=true;    }//计算单词前缀的数量    public int countPrefix(Stringprefix)    {        if(prefix==null||prefix.length()==0)        {            return-1;        }        TrieNode node=root;        char[]letters=prefix.toCharArray();        for(inti=0,len=prefix.length(); i&lt;len; i++)        {            int pos=letters[i]-&#39;a&#39;;            if(node.son[pos]==null)            {                return 0;            }            else            {                node=node.son[pos];            }        }        return node.num;    }//打印指定前缀的单词    public String hasPrefix(String prefix)    {        if (prefix == null || prefix.length() == 0)        {            return null;        }        TrieNode node = root;        char[] letters = prefix.toCharArray();        for (int i = 0, len = prefix.length(); i &lt; len; i++)        {            int pos = letters[i] - &#39;a&#39;;            if (node.son[pos] == null)            {                return null;            }            else            {                node = node.son[pos];            }        }        preTraverse(node, prefix);        return null;    }// 遍历经过此节点的单词.    public void preTraverse(TrieNode node, String prefix)    {        if (node.haveSon)        {for (TrieNode child : node.son)            {                if (child!=null)                {                    preTraverse(child, prefix+child.val);                }            }            return;        }        System.out.println(prefix);    }//在字典树中查找一个完全匹配的单词.    public boolean has(Stringstr)    {        if(str==null||str.length()==0)        {            return false;        }        TrieNode node=root;        char[]letters=str.toCharArray();        for(inti=0,len=str.length(); i&lt;len; i++)        {            intpos=letters[i]-&#39;a&#39;;            if(node.son[pos]!=null)            {                node=node.son[pos];            }            else            {                return false;            }        }        return node.isEnd;    }//前序遍历字典树.    public void preTraverse(TrieNodenode)    {        if(node!=null)        {            System.out.print(node.val+&quot;-&quot;);for(TrieNodechild:node.son)            {                preTraverse(child);            }        }    }    public TrieNode getRoot()    {        return this.root;    }    public static void main(String[]args)    {        Trietree=newTrie();        String[]strs= {&quot;banana&quot;,&quot;band&quot;,&quot;bee&quot;,&quot;absolute&quot;,&quot;acm&quot;,};        String[]prefix= {&quot;ba&quot;,&quot;b&quot;,&quot;band&quot;,&quot;abc&quot;,};for(Stringstr:strs)        {            tree.insert(str);        }        System.out.println(tree.has(&quot;abc&quot;));        tree.preTraverse(tree.getRoot());        System.out.println();//tree.printAllWords();for(Stringpre:prefix)        {            int num=tree.countPrefix(pre);            System.out.println(pre+&quot;&quot;+num);        }    }}</code></pre>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>字典树</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_排序_7.1-快排</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E6%8E%92%E5%BA%8F_7.1-%E5%BF%AB%E6%8E%92/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E6%8E%92%E5%BA%8F_7.1-%E5%BF%AB%E6%8E%92/</url>
    
    <content type="html"><![CDATA[<h1 id="7-排序"><a href="#7-排序" class="headerlink" title="7.排序"></a>7.排序</h1><h3 id="7-1快速排序"><a href="#7-1快速排序" class="headerlink" title="7.1快速排序"></a>7.1快速排序</h3><p>快排是对冒泡排序的改进.</p><p>基本思想:</p><p>通过一趟排序将数据分割成独立的两部分,其中一部分的数据要小于另一部分</p><p>然后用同样方法递归地对这两部分数据做快速排序,直到整个数据都变成有序序列.</p><p>如下图所示</p><h4 id="7-1-左指针放最左-右指针放最右-Basic指针随机放"><a href="#7-1-左指针放最左-右指针放最右-Basic指针随机放" class="headerlink" title="7.1.左指针放最左,右指针放最右,Basic指针随机放"></a>7.1.左指针放最左,右指针放最右,Basic指针随机放</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1222_45_969.png" alt=""></p><h4 id="7-2-左指针向右移动-遇见比basic大的停下来"><a href="#7-2-左指针向右移动-遇见比basic大的停下来" class="headerlink" title="7.2.左指针向右移动,遇见比basic大的停下来"></a>7.2.左指针向右移动,遇见比basic大的停下来</h4><h4 id="右指针向左移动-遇见比basic小的停下来"><a href="#右指针向左移动-遇见比basic小的停下来" class="headerlink" title="右指针向左移动,遇见比basic小的停下来"></a>右指针向左移动,遇见比basic小的停下来</h4><h4 id="左右指针指的对象交换位置"><a href="#左右指针指的对象交换位置" class="headerlink" title="左右指针指的对象交换位置"></a>左右指针指的对象交换位置</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1222_54_965.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1223_04_091.png" alt=""></p><h4 id="7-3-交换位置后重复刚才步骤-直到左右指针相碰"><a href="#7-3-交换位置后重复刚才步骤-直到左右指针相碰" class="headerlink" title="7.3.交换位置后重复刚才步骤,直到左右指针相碰"></a>7.3.交换位置后重复刚才步骤,直到左右指针相碰</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1223_12_618.png" alt=""></p><h4 id="7-4-相碰后交换本对象和basic所指对象的位置"><a href="#7-4-相碰后交换本对象和basic所指对象的位置" class="headerlink" title="7.4.相碰后交换本对象和basic所指对象的位置"></a>7.4.相碰后交换本对象和basic所指对象的位置</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1223_20_577.png" alt=""></p><h4 id="7-5-于是将数据一分为二-可递归地操作剩下数据"><a href="#7-5-于是将数据一分为二-可递归地操作剩下数据" class="headerlink" title="7.5.于是将数据一分为二,可递归地操作剩下数据"></a>7.5.于是将数据一分为二,可递归地操作剩下数据</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1223_43_080.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>排序</tag>
      
      <tag>快速排序</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_5.堆</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_5.%E5%A0%86/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_5.%E5%A0%86/</url>
    
    <content type="html"><![CDATA[<h1 id="5-堆"><a href="#5-堆" class="headerlink" title="5.堆"></a>5.堆</h1><ul><li><p>堆中存储的值是偏序</p></li><li><p>堆属于完全二叉树</p></li><li><p>大顶堆：父节点的值小于或等于子节点的值</p></li><li><p>小顶堆：父节点的值大于或等于子节点的值</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1236_05_814.png" alt=""></p></li></ul><h4 id="1-堆的存储"><a href="#1-堆的存储" class="headerlink" title="1.堆的存储"></a>1.堆的存储</h4><p>一般都用数组来表示堆，i节点的父节点下标就是（i - 1）/ 2,它的左右节点的下标分别是2 * i + 1和2 * i + 2。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1236_15_981.png" alt=""></p><h4 id="2-堆的操作：insert"><a href="#2-堆的操作：insert" class="headerlink" title="2.堆的操作：insert"></a>2.堆的操作：insert</h4><p>插入元素时，新元素被加到heap的末尾，然后更新树以恢复堆的次序。从新加元素到根节点之间必为一个有序数列，现在的任务是将这个新数据插入到原来的有序数列中，使数列仍保持有序。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1236_56_211.png" alt=""></p><h4 id="3-堆的操作：删除"><a href="#3-堆的操作：删除" class="headerlink" title="3.堆的操作：删除"></a>3.堆的操作：删除</h4><p>以大顶堆为例，删除元素时，每次都删除堆的第0个元素，为了便于重建堆，实际的操作是将堆的最后一个数据的值赋给根节点，然后从根节点开始依次从上往下调整。调整时先在左右儿子中找到最小的，若父节点比小子节点还还小就不交换，完成。如果不是就和大子节点交换位置，一直到交换不了为止。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1237_11_033.png" alt=""></p><h4 id="4-堆的操作：堆化数组"><a href="#4-堆的操作：堆化数组" class="headerlink" title="4.堆的操作：堆化数组"></a>4.堆的操作：堆化数组</h4><p>堆化数组就是把数组从头到尾过一遍，把不满足大/小顶堆要求的数据依次交换。</p><p>需注意叶子节点一律不用处理，应该从非叶子节点的右下角开始处理，处理时将本节点与下方节点比较，不符合要求的直接交换。本次交换后下沉的节点要再次与下方节点比较并交换，但本次交换后上浮的节点不需要与上方节点比较。（确定比较的方向是向下）</p><p>如下图例子，从2号节点开始依次处理到0号节点，2号1号节点均与下方节点交换，下沉节点无比较，上浮节点不比较。0号节点发生交换，上浮节点不比较，下沉节点要与下方节点比较并交换。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1237_40_622.png" alt=""></p><h4 id="小结："><a href="#小结：" class="headerlink" title="小结："></a>小结：</h4><p>堆是完全二叉树，存储在数组中，以大顶堆为例，数据不是完全有序，但是每一条从叶子到根节点的数列都是严格降序的。</p><p>pop操作就是把最大值（root）删除，也就是把之前的root换成右下角最小的元素，然后依次从上往下调整。push操作就是把新加入值放在右下角，然后依次进行交换操作。<br>注意pop和push操作都仅限于在某个叶子节点到root节点这一条通路上做交换排序，其他通路不受影响。</p><p>数组初始化为堆的时候注意交换操作从非叶子结点的右下角开始。每个节点都与下方节点做比较并交换，交换完毕后两个被交换节点仍可能与分别的上下节点存在不符合大顶堆条件的情况，这时进一步的交换和比较仅限于下方节点与更下方节点，上方节点先不管，因为自然会轮到他。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>堆</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_3.树</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_3.%E6%A0%91/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_3.%E6%A0%91/</url>
    
    <content type="html"><![CDATA[<h1 id="树"><a href="#树" class="headerlink" title="树"></a>树</h1><ul><li>树的特征<ul><li>树是非线性结构</li><li>树是由一个集合以及在集合上定义的一种关系构成的<ul><li>集合的元素称树的节点</li><li>定义的关系成为父子关系</li><li>父子关系在树的节点之间建立了一个层次关系</li></ul></li></ul></li></ul><ul><li><p>二叉树的定义</p><ul><li>递归定义:二叉树或者是一个空树,或者是由一个根节点和两颗互不相交分别为根的左子树和右子树钩构成的</li></ul></li><li><p>遍历</p><ul><li>先序</li><li>中序</li><li>后序</li></ul></li><li><ul><li><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1232_34_492.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1232_46_883.png" alt=""></p></li></ul></li></ul><ul><li><p>哈弗曼树</p><ul><li><p>哈弗曼编码</p><ul><li><p>在一个字符集的编码过程中,为了避免出现前缀歧义的情况,通常把待编码的项都安排在树的叶子节点上,因为没有任何一个叶子节点与其他叶子节点共享到根节点的路径,遂可保证无歧义性</p></li><li><p>把待编码的项目根据使用频率可赋值一个”权”,权大的使用频率高,放在更靠近根节点的位置,这样它的编码就较短,可拉低整体编码的长度.</p></li><li><p>哈弗曼树的构建过程如下图所示,代码在study项目中</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1233_11_078.png" alt=""></p></li></ul></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>树</tag>
      
      <tag>二叉树</tag>
      
      <tag>哈夫曼树</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_4.图</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_4.%E5%9B%BE/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_4.%E5%9B%BE/</url>
    
    <content type="html"><![CDATA[<h1 id="4-图"><a href="#4-图" class="headerlink" title="4.图"></a>4.图</h1><p>图是一种网状数据结构,由顶点和边构成,Graph = (V,E)</p><h3 id="4-1子图"><a href="#4-1子图" class="headerlink" title="4.1子图"></a>4.1子图</h3><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1233_28_077.png" alt=""></p><h3 id="4-2强连通分量"><a href="#4-2强连通分量" class="headerlink" title="4.2强连通分量:"></a>4.2强连通分量:</h3><ul><li><p>构造镜像图G’</p><ul><li>对G 和 G’中s的可达分量求并集既可得强连通分量</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1233_51_550.png" alt=""></p></li></ul><h3 id="4-3图的存储方式"><a href="#4-3图的存储方式" class="headerlink" title="4.3图的存储方式"></a>4.3图的存储方式</h3><p>首先,从图的逻辑结构定义来看,无法将图的顶点排列成唯一的线性序列,在图中任意顶点都可以是图的第一个顶点.</p><p>对任意顶点来说他的邻接点也不存在顺序关系</p><p>所以顶点在图中的位置就是指该顶点在图中[人为]确定的序列的位置</p><p>由于图的结构比较复杂,任意两个顶点之间都可能存在联系,所以无法将数据元素存储区的位置来表示元素之间的关系,即图没有顺序映像的存储结构,但可借助数组来表示数据元素之间的关系</p><h3 id="4-3-1邻接矩阵"><a href="#4-3-1邻接矩阵" class="headerlink" title="4.3.1邻接矩阵"></a>4.3.1邻接矩阵</h3><p>图的邻接矩阵表示法就是用数组来存放图的结构,也成为[数组表示法],采用两个数组来表示图</p><ul><li><p>存储所有顶点信息的一维数组</p></li><li><p>存储图中顶点之间关系的二维数组,这个关联数组也叫邻接矩阵</p><p>邻接矩阵的示意图(∞代表没有相连)</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1234_24_383.png" alt=""></p></li></ul><h3 id="4-3-2邻接表"><a href="#4-3-2邻接表" class="headerlink" title="4.3.2邻接表"></a>4.3.2邻接表</h3><p>临接矩阵的问题:空间使用效率低,因为大量的单元所对应的边可能没有在图中出现</p><p>按照改进空间使用效率的思路想,可以将静态数组的存储结构改成链式存储结构<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1235_28_643.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>图</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_0.常见算法总览</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_0.%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95%E6%80%BB%E8%A7%88/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_0.%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95%E6%80%BB%E8%A7%88/</url>
    
    <content type="html"><![CDATA[<p><strong>十大排序算法</strong></p><ul><li>简单排序：插入排序、选择排序、冒泡排序（必学）</li><li>分治排序：快速排序、归并排序（必学，快速排序还要关注中轴的选取方式）</li><li>分配排序：桶排序、基数排序</li><li>树状排序：堆排序（必学）</li><li>其他：计数排序（必学）、希尔排序</li></ul><p>对于十大算法的学习，假如你不大懂的话，那么我还是挺推荐你去看书的，因为看了书，你可能不仅仅知道这个算法怎么写，还能知道他是怎么来的。推荐书籍是《算法第四版》，这本书讲的很详细，而且配了很多图演示，还是挺好懂的。</p><p>推荐文章：</p><p><a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/IAZnN00i65Ad3BicZy5kzQ">必学十大经典排序算法，看这篇就够了(附完整代码/动图/优质文章)(修订版)</a></p><p><strong>2、图论算法</strong></p><ul><li>图的表示：邻接矩阵和邻接表</li><li>遍历算法：深度搜索和广度搜索(必学)</li><li>最短路径算法：Floyd，Dijkstra（必学）</li><li>最小生成树算法：Prim，Kruskal（必学）</li><li>实际常用算法：关键路径、拓扑排序（原理与应用）</li><li>二分图匹配：配对、匈牙利算法（原理与应用）</li><li>拓展：中心性算法、社区发现算法（原理与应用）</li></ul><p>图还是比较难的，不过我觉得图涉及到的挺多算法都是挺实用的，例如最短路径的计算等，图相关的，我这里还是建议看书的，可以看《算法第四版》。</p><p><a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/4JEHZWanGtsQHYrZ0MDq7Q">漫画：什么是 “图”？（修订版）</a></p><p><a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/WA5hQXkcACIarcdVnRnuiw">漫画：深度优先遍历 和 广度优先遍历</a></p><p><a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/gjjrsj95X4w7QdWBlAKnaA">漫画：图的 “最短路径” 问题</a></p><p><a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/ALQntqQJkdWf4RbPaGOOhg">漫画：Dijkstra 算法的优化</a></p><p><a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/qnPSzv_xWSZN0VpdUgwvMg">漫画：图的 “多源” 最短路径</a></p><p><strong>3、搜索与回溯算法</strong></p><ul><li>贪心算法（必学）</li><li>启发式搜索算法：A*寻路算法（了解）</li><li>地图着色算法、N 皇后问题、最优加工顺序</li><li>旅行商问题</li></ul><p>这方便的只是都是一些算法相关的，我觉得如果可以，都学一下。像贪心算法的思想，就必须学的了。建议通过刷题来学习，leetcode 直接专题刷。</p><p><strong>4、动态规划</strong></p><ul><li>树形DP：01背包问题</li><li>线性DP：最长公共子序列、最长公共子串</li><li>区间DP：矩阵最大值（和以及积）</li><li>数位DP：数字游戏</li><li>状态压缩DP：旅行商</li></ul><p>我觉得动态规划是最难的一个算法思想了，记得当初第一次接触动态规划的时候，是看01背包问题的，看了好久都不大懂，懵懵懂懂，后面懂了基本思想，可是做题下不了手，但是看的懂答案。一气之下，再leetcdoe专题连续刷了几十道，才掌握了动态规划的<strong>套路</strong>，也有了自己的一套模板。不过说实话，动态规划，是考的真他妈多，学习算法、刷题，一定要掌握。这里建议先了解动态规划是什么，之后 leetcode 专题刷，反正就一般上面这几种题型。后面有时间，我也写一下我学到的<strong>套路</strong>，有点类似于我之前写的递归那样，算是一种经验。也就是我做题时的模板，不过感觉得写七八个小时，，，，，有时间就写。之前写的递归文章：<a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/mJ_jZZoak7uhItNgnfmZvQ">为什么你学不会递归？告别递归，谈谈我的一些经验</a></p><p><strong>5、字符匹配算法</strong></p><ul><li>正则表达式</li><li>模式匹配：KMP、Boyer-Moore</li></ul><p>我写过两篇字符串匹配的文章，感觉还不错，看了这两篇文章，我觉得你就差不多懂 kmp 和 Boyer-Moore 了。</p><p><a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/7IZTuLrPSuxvFRqsv5PiXQ">字符串匹配Boyer-Moore算法：文本编辑器中的查找功能是如何实现的？</a></p><p><strong>6、流相关算法</strong></p><ul><li>最大流：最短增广路、Dinic 算法</li><li>最大流最小割：最大收益问题、方格取数问题</li><li>最小费用最大流：最小费用路、消遣</li></ul><p>这方面的一些算法，我也只了解过一些，感兴趣的可以学习下。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_1.数组,链表</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_1.%E6%95%B0%E7%BB%84,%E9%93%BE%E8%A1%A8/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_1.%E6%95%B0%E7%BB%84,%E9%93%BE%E8%A1%A8/</url>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h1 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h1><ul><li>数组是用来存放一组有相同数据类型的数据结构,通过整形下标来访问数组中的值</li><li>如果越过数据的下标访问数据,会返回ArrayIndexOutOfBoundException</li><li>Java中数组是一个类:所以两个数组变量可以指向同一个数组</li></ul><h1 id="线性表"><a href="#线性表" class="headerlink" title="线性表"></a>线性表</h1><ul><li>顺序存储结构<ul><li>用数组实现</li><li>插入元素的平均时间复杂度是O(n)</li><li>查找的平均时间复杂度是O(1)</li><li>适合多查找的场景</li></ul></li><li>链式存储结构<ul><li>典型的node节点由[data]和[next]两个域组成</li><li>插入元素的平均时间复杂度是O(1)</li><li>查找的平均时间复杂度是O(n)</li><li>适合多增删的场景</li></ul></li></ul><h1 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h1><p>迭代器是程序设计模式中的行为模式,功能是提供一种方法顺序访问一个聚集对象中各个元素,又不暴露该对象的内部表示.</p><p>简单来说迭代器就是对遍历操作的抽象.</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>数组</tag>
      
      <tag>链表</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_2.栈与队列-</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_2.%E6%A0%88%E4%B8%8E%E9%98%9F%E5%88%97-/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_2.%E6%A0%88%E4%B8%8E%E9%98%9F%E5%88%97-/</url>
    
    <content type="html"><![CDATA[<h1 id="4-1栈"><a href="#4-1栈" class="headerlink" title="4.1栈"></a>4.1栈</h1><ul><li><p>栈的定义</p><ul><li><p>栈又称堆栈,是运算首先的线性表,仅允许从表的一端进行插入和删除.</p></li><li><p>插入,删除的一端称为栈顶(top),另外一端称为栈底(bottom)</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1231_10_277.png" alt=""></p></li><li><p>栈的顺序存储和链式存储和线性表一模一样,只是在栈顶加上了增删元素的限制.</p></li><li><p>有后进先出的特征</p></li></ul></li></ul><h1 id="4-2队列"><a href="#4-2队列" class="headerlink" title="4.2队列"></a>4.2队列</h1><ul><li>队列的定义<ul><li>队列和栈一样,是运算受限制的线性表,仅允许从一头插入元素(rear),从另一头读取(head)</li><li>有先进先出的特征</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>栈</tag>
      
      <tag>队列</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据库_概论</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E5%BA%93_%E6%A6%82%E8%AE%BA/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E5%BA%93_%E6%A6%82%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="1-数据库的四个基本概念"><a href="#1-数据库的四个基本概念" class="headerlink" title="1.数据库的四个基本概念"></a>1.数据库的四个基本概念</h2><ul><li><p>数据语义</p><ul><li>“93”的语义<ul><li>可以是年龄</li><li>也可以是班级人数</li></ul></li></ul></li></ul><ul><li><p>数据和自然语言</p><ul><li><p>自然语言可以通过上下文完整描述事物</p></li><li><p>数据是计算机的记录:</p><ul><li><p>(李明,男,199005,江苏南京,计算机,自行车协会)</p><p>记录不能完整表达事物内容,因缺少语义(列信息)</p></li></ul></li></ul></li></ul><ul><li>什么是数据库<ul><li>长期储存在计算机内,有组织的,可共享的大量数据集合</li></ul></li><li>为什么要建立数据库<ul><li>可收集所需的大量数据,然后进一步加工处理,转换为有用的知识</li></ul></li><li>数据库的基本特征<ul><li>数据按照一定的数据模型(结构)存储,组织,描述</li><li>冗余度小,可扩展</li><li>数据独立性高</li></ul></li><li>什么是数据库管理系统DBMS<ul><li>位于用户和操作系统之间的一层数据管理软件</li><li>基础软件,也是一层服务</li></ul></li><li>DBMS的用途<ul><li>科学的组织和存储数据,高效的获取和维护数据 </li></ul></li><li>DBMS的主要功能<ul><li>数据定义功能<ul><li>提供数据定义语言DDL</li><li>定于数据库中的数据对象</li></ul></li><li>数据组织/存储/管理<ul><li>管理数据</li><li>确定数据在存储级别上的结构和存取方式</li><li>实现数据之间的联系</li><li>提供多种存储方法提高存储效率</li></ul></li><li>数据操纵功能<ul><li>增删改查</li><li>提供数据操纵语言(DML)<ul><li>关系代数</li><li>关系演算</li><li>SQL</li></ul></li></ul></li><li>数据库的事务管理和运行管理<ul><li>数据的安全,高并发管理</li><li>故障恢复</li></ul></li></ul></li></ul><h2 id="2-数据模型"><a href="#2-数据模型" class="headerlink" title="2.数据模型"></a>2.数据模型</h2><ul><li>数据模型应满足三个特点<ul><li>能真实模拟现实世界</li><li>容易理解</li><li>易在计算机实现</li></ul></li><li>数据模型是数据库系统的核心和基础</li></ul><p>两类数据模型</p><ul><li>概念模型<ul><li>用户视角,用于数据库设计</li></ul></li><li>逻辑模型和物理模型<ul><li>逻辑模型<ul><li>计算机视角,用于DBMS的实现</li></ul></li><li>物理模型<ul><li>描述数据在系统内的表示方法和存取方法,是对数据底层的抽象</li></ul></li></ul></li></ul><p>三层建模</p><ul><li>现实世界–&gt;概念模型<ul><li>人类的认识抽象,概念模型只是形成概念,计算机不懂</li><li>由数据库设计人员完成</li></ul></li><li>概念模型–&gt;逻辑模型<ul><li>把概念转换为计算机可读的逻辑模型</li><li>由数据库设计人员完成</li></ul></li><li>逻辑模型–&gt;物理模型<ul><li>把逻辑转换为物理执行方案</li><li>由DBMS完成</li></ul></li></ul><h2 id="3-常用的数据模型"><a href="#3-常用的数据模型" class="headerlink" title="3.常用的数据模型"></a>3.常用的数据模型</h2><ul><li>层次模型</li><li>网状模型</li><li>关系模型(现在最常用的模型)</li><li>面向对象数据模型</li><li>对象关系数据模型</li><li>半结构化数据模型</li><li>非结构化数据模型,图模型</li></ul><p>关系模型</p><ul><li>在用户观点下,关系模型中数据的逻辑结构是一张二维表</li><li>概念<ul><li>关系(Relation)   —-一个关系对应一张表</li><li>元组(Tuple)        —-表中的一行就是一个元组</li><li>属性(Attribute)  —-表中的一列为一个属性</li><li>主键(Key])           —-表中的属性组,可唯一确定一个元组</li><li>域(Domain)        —-一组具有相同数据结构类型的值的集合,属性的取值范围来自某个域<ul><li>​    域的举例:<ul><li>一个属性的全集,例如学校的系名的域就是全部系</li></ul></li></ul></li><li>关系模型—对关系的描述<ul><li>学生(学号,姓名,年龄,性别,系名,年级等)</li></ul></li></ul></li></ul><h2 id="4-数据库系统的结构"><a href="#4-数据库系统的结构" class="headerlink" title="4.数据库系统的结构"></a>4.数据库系统的结构</h2><ul><li><p>数据库开发人员角度:</p><ul><li><p>模式Schema的概念</p><ul><li><p>是对数据库逻辑结构和特征的描述</p></li><li><p>是类的描述,不涉及具体值</p><ul><li>模式是稳定的</li></ul></li><li><p>模式举例:学生选课数据库</p></li><li><p>模式类比：数据库是仓库，模式是一个个房间，表格是房间里的床。</p><p>用户的权限的对房间分配，不对床分配</p><ul><li>学生,课程,学生选的课三个关系模式(三张表)<ul><li>学生表Student(Sno,Sname,Ssex,Sage,Sdept)</li><li>课程表Coursera(Cno,Cname,Cpno,Ccredit)</li><li>学生选课表SC(SNo,Cno,Grade)</li></ul></li></ul></li></ul></li><li><p>实例(instance)</p><ul><li>数据库某时刻的状态,模式的一个具体值</li><li>同一个模式有很多实例</li><li>实例举例:学生选课数据库实例<ul><li>2014年的学生数据库实例<ul><li>2014年学校所有学生的记录</li></ul></li><li>2013年的学生数据库实例<ul><li>2013年学校所有开设的所有课程的记录</li></ul></li></ul></li></ul></li></ul></li><li><p>从数据库用户角度看:</p><ul><li>单用户结构</li><li>分布式结构</li><li>客户-服务器结构</li><li>浏览器-数据服务器结构</li></ul></li></ul><ul><li>数据库三级模式(schema)结构,是数据库系统内部系统结构<ul><li>(中)模式<ul><li>数据库中全体数据的逻辑结构和特征的描述</li><li>所有用户的公共数据视图</li><li>定义模式<ul><li>DDL定义数据的逻辑结构,以某种数据模型为基础</li><li>定义数据之间的联系</li><li>定义数据的完整性/安全性</li></ul></li></ul></li><li>外模式<ul><li>又称子模式/用户模式<ul><li>外模式是模式的子集,通常是与某一应用有关的数据的逻辑表示</li><li>通常可以有多个外模式</li></ul></li></ul></li><li>内模式<ul><li>是数据物理结构和存储方式的描述</li><li>是数据在数据库内部的表示方式<ul><li>记录的存储方式    <ul><li>例如顺序存储,堆存储,hash存储等</li></ul></li><li>索引的组织方式<ul><li>B+树,Bitmap,Hash等</li></ul></li><li>数据是否被压缩</li><li>数据是否被加密</li><li>数据存储记录的规定–如定长还是变长</li></ul></li><li>一个数据库只有一个内模式</li></ul></li></ul></li></ul><h2 id="5-关系代数"><a href="#5-关系代数" class="headerlink" title="5.关系代数"></a>5.关系代数</h2><p>1.选择select—&gt;取出满足条件的信息</p><p>2.投影—-&gt;列变换</p><p>3.链接—&gt;  R(A@B)S  链接运算从R和S的广义笛卡尔积R*S中选取R关系在A属性组上的值与S关系在B属性组上的值满足比较关系@的元组</p><p>​                我的理解:R和S拼成一行,选取满足条件A@B的行</p><p>​                当@等于=时,即为等值链接,这种链接最常用</p><p>​                R(A=B)S :从关系R和S的广义笛卡尔积中选取A/B属性值相等的那些元组</p><p>​    悬浮元组:R和S元组在链接时有些公共属性上值不等的元组,导致这些元组在连接时被舍弃了,这些舍弃的元祖被成         为悬浮元组</p><p>外链接:如果把悬浮元组也保留在结果关系中,而在其他属性上填空值,就叫做外连接</p><p>左外连接:只保留左边关系R中的悬浮元祖</p><p>右外连接:只保留右边关系S中的悬浮元祖</p><p>​    </p><p>4.除运算</p><p>​    </p><h2 id="6-关系演算"><a href="#6-关系演算" class="headerlink" title="6.关系演算"></a>6.关系演算</h2><ul><li>关系演算以数理逻辑中的谓词演算为基础<ul><li>按谓词变元不同可以分为<ul><li>元祖关系演算<ul><li>以元祖变量作为谓词变量的 基本对象</li><li>元祖关系演算语言ALPHA</li></ul></li><li>域关系演算<ul><li>以与变量作为谓词变量的基本对象</li><li>域关系演算语言GBE</li></ul></li></ul></li></ul></li></ul><h2 id="7-sql语言概述"><a href="#7-sql语言概述" class="headerlink" title="7.sql语言概述"></a>7.sql语言概述</h2><p>关系型数据库标准语言SQL</p><ul><li><p>关系代数太数学了,性能差接受度差,于是IBM开展了system R的研制工作</p><ul><li>system R 以关系模型为基础,但是摒弃了数学语言,以自然语言为方向,诞生了结构化英语查询语言<ul><li>Structed English Query  Language–&gt;SEQUEL</li><li>后来改名为SQL,发音不变,为seeQu</li></ul></li><li>目前为止,sql应用最为广泛,但是没有一个数据库能支持sql所有概念和特性</li></ul></li><li><p>Sql例子</p><ul><li><p>SELECT Sno,Grade</p><p>FROM SC</p><p>WHERE Cno = ‘3’</p><p>Order by Grade</p></li><li><p>在SC表中选取Cno=3的行,抽取除Sno和Grade成表,按照Grade排序</p></li></ul></li><li><p>SQL特点</p><ul><li>综合统一,集成了数据定义语言/数据操纵语言/数据控制语言为一体</li><li>可以独立完成数据库生命周期中的全部活动<ul><li>增删改查</li><li>维护</li><li>安全性完整性控制</li><li>嵌入式sql和动态sql定义</li></ul></li><li>高度非过程化:<ul><li>这对于面向过程的语言来说,不必指定路径,不必指定细节</li><li>它只要提出做什么,无需提出做的详细过程</li><li>例如做两个表的链接,不必说哪个表做外循环,哪个表做内循环,只用join就可以了</li></ul></li><li>面向集合的操作方式<ul><li>操作的对象,返回的对象都是元组/元组的集合</li><li>同一种语法结构提供多种使用方式<ul><li>独立性,可以独立地用于联机交互</li><li>嵌入性,又能够嵌入到高级语言C,C++,JAVA等中设计使用</li></ul></li></ul></li><li>功能实现只用了9个动词<ul><li>数据查询:select</li><li>数据定义:create,drop,alter</li><li>数据操纵:insert,update,delete</li><li>数据控制:grant,revoke</li></ul></li><li>sql支持数据库的三级模式结构<ul><li>视图                                外模式<ul><li>从一个或者几个基本表导出的表</li><li>数据库中只存放视图的定义不存放视图对应的数据</li><li>视图是一个虚表</li><li>用户可以在视图在再定义视图</li></ul></li><li>基本表                           模式<ul><li>本身独立存在的表</li><li>sql一个关系对应一个基本表</li><li>一个基本表对应一个存储文件</li><li>一个表可以带若干索引</li></ul></li><li>存储文件                       内模式<ul><li>物理结构对用户是隐藏的</li></ul></li></ul></li></ul></li></ul><h2 id="8-sql数据定义"><a href="#8-sql数据定义" class="headerlink" title="8.sql数据定义"></a>8.sql数据定义</h2><ul><li><p>数据定义是对数据库中的各种对象进行创建的过程</p></li><li><p>数据库中有各种对象:</p><ul><li><p>模式</p></li><li><p>表</p></li><li><p>视图</p></li><li><p>索引等</p></li><li><p>sql的数据定义语句</p><table><thead><tr><th></th><th>创建</th><th>删除</th><th>修改</th></tr></thead><tbody><tr><td>模式(schema)</td><td>create schema</td><td>drop schema</td><td></td></tr><tr><td>表(table)</td><td>create table</td><td>drop table</td><td>alter table</td></tr><tr><td>视图(view)</td><td>create view</td><td>drop view</td><td></td></tr><tr><td>索引(index)</td><td>create index</td><td>drop index</td><td>alter index</td></tr></tbody></table></li></ul></li></ul><ul><li><p>数据定义详解</p><ul><li><p>模式定义</p><ul><li>模式就是一个目录(命名空间),在此模式中包含了数据库对象,例如基本表,视图,索引等<ul><li>创建:create schema “name”</li><li>删除:drpo schema <ul><li>drop schema “name”&lt;cascade&gt;级联删除<ul><li>删除schema时同时删除其中所有数据库对象</li></ul></li><li>drop schema “name”&lt;restrict&gt;限制删除<ul><li>如果该schema中有定义的对象(表,视图,索引等),则拒绝该语句的删除</li><li>仅当该模式中没有任何下属对象的时候才能执行</li></ul></li></ul></li></ul></li></ul></li><li><p>表定义</p><ul><li>表是关系数据库的核心<ul><li>create table&lt;表名&gt;</li></ul></li></ul></li><li><p>索引定义</p><ul><li><p>索引的目的:加快查询速度</p></li><li><p>关系数据库管理系统常见索引:</p><ul><li>顺序文件上的索引</li><li>B+树索引<ul><li>特点时可以动态平衡</li></ul></li><li>散列hash索引<ul><li>特点是速度快</li></ul></li><li>位图索引</li></ul></li><li><p>创建索引</p><ul><li>create &lt;unique&gt;&lt;cluster&gt;index&lt;索引名&gt;<ul><li>unique索引是否唯一</li><li>cluster是否是聚簇索引</li></ul></li><li>举例<ul><li>create unique index Stusno On Student(sno)</li></ul></li></ul></li></ul></li></ul></li></ul><pre><code>  ## 9.数据查询  * SELECT             指定要显示的属性列  * from                   制定查询对象  * where                制定查询条件  * group by           对查询结果按指定列的值分组  * order by            对查询结果按指定列值的升序或者降序排序  * 别名    * 使用列的别名改变查询结果中的列标题    * SELECT Sname NAME,Year of birth BIRTH,    ​                NAME,BIRTH就是别名  ​                              * DISTINCT关键词取消重复的行    * SELECT DISTINCT sno From SC      * 执行结果        * sno        * 201215706        * 201215707      * 没有DISTINCT关键词的执行结果        * sno        * 201215706        * 201215706        * 201215706        * 201215707        * 201215707  * 聚集函数    * 统计个数count(*)    * 计算一列值的总和sun    * 计算一列值的平均值avg    * 求一列值中的最大值和最小值      * max      * min  ## 10.链接查询  * 等值与非等值链接查询    * 链接条件运算符是=    * 例子      * SELECT Student.\*,SC.\*        From student,sc        where student.sno=sc.sno      * 链接操作的执行过程        - 嵌套循环法          - for表一{for{表二}}        - 排序合并法sort-merge          - 常用于等值链接          - 按链接属性对表一表二进行排序          - 设置指针,指向表一和表二的第一个元祖,如果这两个元祖满足链接条件就拼接,然后            i++,j++            否则            i++,j不动或者            j++,i不动  * 自身链接    * 一个表与自己链接    * 由于所有属性都是同名属性,必须使用别名前缀      * 例如        * SELECT FIRST.Cname,SECOND.Cname          FROM Coursera FIRST,Courera SECOND           取别名          WHERE FIRST.Cpno = SECOND.Cno  * 外链接    * 普通链接只输出满足链接条件的元组    * 外链接以指定表为链接主体,将主表中不满足链接条件的元组一起输出    * 左链接      * 列出左边关系中的所有元组    * 右链接      * 列出右边关系中的所有元组    * 外链接举例      * SELECT student.sno,Sname,Ssex,Sage,Sdept,Cno,Grade        From Student LEFT OUT JOIN SC ON        (Student.sno = SC.Sno)      * 左链接以左表为主,左边满足匹配条件的就链接了,不满足匹配条件的也把左边的信息列出来,右边本该链接的地方空着      * 右链接与左链接完全对称  * 两个以上的表的链接    * 举例      * SELECT student.sno,Sname,Cname,Grade        From Student,SC,Coursera     //有多个from就是链接        WHERE Student.sno =SC.Sno        And SC.Cno = Coursera.Cno;  ## 10.嵌套查询  嵌套查询概览  * 一个 SELECT-FROM-WHERE语句称谓一个查询模块  * 将一个查询模块嵌套在另一个查询模块where或者having中的查询称谓嵌套查询  * 举例    * SELECT Sname       from Student      where sno in      ​                    (select sno       ​                     from sc      ​                     where Cno = &#39;2&#39; )  * 上层的查询称为外查询,下层的查询称谓内查询  * sql允许多层嵌套  * 子查询不能使用order by  * 不相关子查询    * 子查询的查询条件不依赖于父查询    * 由里向外逐层查询  * 相关子查询    * 相关子查询的查询条件依赖于父查询      * 首先取外层查询中表的第一个元祖,根据它与内层查询相关的属性值处理内层查询      * 若where子句返回值为真,则此元组放入结果表      * 然后再取外层表的下一个元组  * 带有in谓词的子查询    * 举例    * SELECT sno,sname,sdept      from student       where sdept IN      ​                            (select sdept      ​                            from student       ​                             where sname = &#39;刘成&#39;)      此例为不相关子查询  * 带有比较运算的子查询  * 带有any(some)或者all的子查寻    * 例子:查询非计算机系中比计算机系任意一个学生年龄小的学生姓名和年龄      SELECT Sname,Sage      From Student      where Sage &lt; ANY(SELECT Sage      ​                                        FROM Student       ​                                        where Sdept  = &#39;CS&#39;)      处理过程:      首先计算子查询,得到一个结果集合(19,20)      然后求出Sage小于任何(19,20)也就是19的所有集合  * 带有exist的子查询    * 是个存在量词    * 带有exist的查询不返回任何数据只返回true或者false</code></pre><h2 id="11-集合查询"><a href="#11-集合查询" class="headerlink" title="11.集合查询"></a>11.集合查询</h2><ul><li>集合操作的种类<ul><li>并操作union</li><li>交操作intersect</li><li>差操作except</li></ul></li><li>参加集合操作的各查询结果的列数必须相同,对应的数据类型也必须相同</li></ul><ul><li><p>union举例</p><ul><li><p>查询计算机科学系的学生,以及年龄不大于19岁的学生</p></li><li><p>SELECT * </p><p>FROM Student</p><p>WHERE Sdept  = ‘CS’</p><p>UNION </p><p>Select *</p><p>FROM     Student </p><p>WHERE Sage &lt;= 19;</p><p>两个查询用union合并</p><ul><li>union可以自动合并重复的元素</li><li>union all会保留重复的元素</li></ul></li></ul></li><li><p>intersect交集举例</p><ul><li><p>查询计算机的学生和年龄不大于19岁的学生的交集</p></li><li><p>SELECT *</p><p>from Student</p><p>where Sdept  = ‘CS’</p><p>INTERSECT </p><p>SELECT *</p><p>FROM Student </p><p>WHERE Sage &lt;= 19</p></li></ul></li><li><p>except差集举例</p><ul><li><p>查询计算机的学生和年龄不大于19岁的学生的差集</p><p>SELECT * </p><p>From Student </p><p>where Sdept = ‘CS’</p><p>EXCEPT </p><p>SELECT *</p><p>FROM Student </p><p>WHERE Sage &lt;= 19;</p></li></ul></li></ul><p>聚集函数的一般模式</p><ul><li>Count</li><li>sum</li><li>avg</li><li>max</li><li>min</li></ul><h2 id="12-数据更新"><a href="#12-数据更新" class="headerlink" title="12.数据更新"></a>12.数据更新</h2><ul><li>增</li><li>删</li><li>改</li></ul><ul><li><p>增</p></li><li><p>两种插入数据方式</p><ul><li><p>插入元组 </p><ul><li><p>插入数据举例</p><ul><li><p>将一个新生元组学号:201215128,姓名:陈冬,性别:男,所在系:is,年龄:18—-&gt;插入到Student表中</p></li><li><p>insert</p><p>INTO student (Sno,Sname,Ssex,Sdept,Sage)</p><p>VALUES(‘201215128,’陈冬’,’男’,18</p></li><li><p>关键字into就是插入的元素种类</p><p>关键字VALUES就是插入的具体内容</p></li></ul></li></ul><ul><li><p>插入子查询结果</p><ul><li><p>可以一次插入多个元组</p></li><li><p>插入子查询结果举例</p><ul><li><p>建表</p><ul><li><p>CREATE TABLE Dept_age</p><p>(Sdept    CHAR(15)</p><p>Avg_age SMALLINT);</p></li></ul></li><li><p>插入数据</p><ul><li><p>INSERT INTO Dept_age(Sdept,Avg_age)     //插入以下查询结果</p><p>select Sdept,AVG(Sage)</p><p>FROM Student </p><p>Group by Sdept</p></li></ul></li></ul></li></ul></li></ul></li></ul></li><li><p>改</p></li><li><p>update关键字</p></li><li><p>举例</p><ul><li><p>将学生201215121的年龄改成22岁 </p></li><li><p>update Student</p><p>set Sage= ‘22’</p><p>where SNO = ‘201215121’</p></li></ul></li></ul><ul><li><p>删除</p></li><li><p>关键字delete</p><ul><li><p>delete from</p><p>where &lt;条件&gt;</p></li></ul></li><li><p>举例</p><ul><li><p>删除学号为201215121的学生记录</p><p>delete FROM stduent</p><p>where Sno = ‘201215121’</p></li></ul></li></ul><h2 id="13-视图"><a href="#13-视图" class="headerlink" title="13.视图"></a>13.视图</h2><ul><li><p>视图的特点</p><ul><li><p>视图是一种虚拟的表</p></li><li><p>形式上和表一致</p></li><li><p>不是用create table来一个字段一个字段定义的</p></li><li><p>使用sql的SELECT 来定义的</p></li><li><p>所以视图是一种虚表,普通表是实表</p></li><li><p>视图由一个或者几个普通表构建的</p><ul><li>视图中只存放视图的定义,不存放视图的数据</li></ul></li><li><p>视图举例</p><ul><li><p>建立信息系的学生的视图</p><ul><li><p>Creat VIEW IS_STUDENT</p><p>AS </p><p>SELECT Sno,Sname,Sage</p><p>FROM STUDENT</p><p>WHERE Sdept = ‘IS’</p><p>WITH CHECK OPTION;</p></li></ul></li><li><p>建立信息系选修了1号课程的学生的视图(包括学号,姓名,成绩)</p></li><li><p>CREATE VIEW IS_S1(Sno,Sname,Grade)</p><p>AS</p><p>SELECT  Student.sno,Sname,Grade</p><p>FROM  Student,SC</p><p>WHERE Sdept = ‘IS’  And </p><p>​                Student.Sno=SC.Sno AND</p><p>​                SC.Cno = ‘1’</p></li></ul></li></ul></li><li><p>视图消解法</p><ul><li><p>进行有效性检查</p></li><li><p>转换成等价的对基本表的查询</p><ul><li><p>转换举例</p><ul><li><p>SELECT Sno,Sage</p><p>From IS_Student</p><p>WHERE SAGE  &lt; 20;</p></li><li><p>转换为</p><p>SELECT Sno,Sage</p><p>FROM STUDENT </p><p>WHERE Sdept = ‘IS’  AND Sage &lt;  20;</p></li></ul></li></ul></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据库</tag>
      
      <tag>sql</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/网站部署记录</title>
    <link href="undefined2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E7%BD%91%E7%AB%99%E9%83%A8%E7%BD%B2%E8%AE%B0%E5%BD%95/"/>
    <url>2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E7%BD%91%E7%AB%99%E9%83%A8%E7%BD%B2%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<h4 id="1-获取root权限"><a href="#1-获取root权限" class="headerlink" title="1.获取root权限"></a>1.获取root权限</h4><p>-sudo passwd</p><p>-su root</p><h4 id="2-云主机安装git"><a href="#2-云主机安装git" class="headerlink" title="2.云主机安装git"></a>2.云主机安装git</h4><p>-yum update</p><p>-yum install git</p><p>-mkdir App</p><p>-cd App</p><h4 id="3-云主机与github建立sshkey连接"><a href="#3-云主机与github建立sshkey连接" class="headerlink" title="3.云主机与github建立sshkey连接"></a>3.云主机与github建立sshkey连接</h4><p>返回：Permissiondenied (publickey)，fatal:Could not read from remote repository.</p><p>cd ~/.ssh ls  //查看是否有ssh key</p><p>ssh-keygen -t rsa -C “<a href="mailto:youremail@example.com">youremail@example.com</a>“    //一路回车</p><p>ssh -v <a href="mailto:git@github.com">git@github.com</a> </p><p>ssh-agent -s</p><p>ssh-add ~/.ssh/ github new SSH KEY</p><p>ssh -T <a href="mailto:git@github.com">git@github.com</a></p><p>若出错：“Could not open a connection to your authentication agent”</p><p>eval `ssh-agent -s``</p><p>ssh-add ~/.ssh/id_rsa，</p><p>github new SSH KEY</p><h3 id="4-克隆项目，安装maven"><a href="#4-克隆项目，安装maven" class="headerlink" title="4.克隆项目，安装maven"></a>4.克隆项目，安装maven</h3><pre><code>git clone https://git@github.com:BqLion/SpringBoot_HelloWorld.gitls //显示名称cd SpringBoot_HelloWorldyum install mavenmvn -vmvn complie package</code></pre><h4 id="5-编译，运行"><a href="#5-编译，运行" class="headerlink" title="5.编译，运行"></a>5.编译，运行</h4><p>more src/main/resources/application.properties</p><p>cp src/main/resources/application.properties src/main/resources/application-production.properties</p><p>vim src/main/resources/application.properties</p><p>vim src/main/resources/application-production.properties</p><p>【修改配置文件为服务端配置】</p><p>【编译】mvn package</p><p>报错后改用：mvn clean compile package</p><p>报错后改用：mvn compile package</p><p>查看端口号netstat -a -n</p><p>【运行】</p><p>本次启动：</p><p>java -jar -Dspring.profiles.active=production target/springboothellloworld-0.0.1-SNAPSHOT.jar</p><p>直接启动：</p><p>java -jar springboothellloworld-0.0.1-SNAPSHOT.jar</p><hr><h4 id="6-服务器连接mysql数据库"><a href="#6-服务器连接mysql数据库" class="headerlink" title="6.服务器连接mysql数据库"></a>6.服务器连接mysql数据库</h4><p>1.谷歌云sql，用Cloud Shell远端操作本机</p><p>2.mysql -u root -p </p><p>​    报错：Access denied</p><p>3.尝试mysql -h 127.0.0.1 -P 3306 -u root -p</p><p>​    报错：RROR 2002 (HY000): Can’t connect to local MySQL server through socket ‘/var/run/mysqld/mysqld.sock’</p><p>4.apt-get update</p><p>5.sudo apt-get install mysql-server</p><p>​    如果不能sudo：  sudo passwd root重置密码</p><p>6.RROR 2002没有解决</p><pre><code class="sql">sudo /etc/init.d/mysqld sto</code></pre><pre><code class="sql">sudo mysqld_safe --skip-grant-tables &amp;</code></pre><pre><code class="sql">mysql -uroot</code></pre><pre><code class="sql">use mysql;</code></pre><pre><code class="sql">update user set password=PASSWORD(&quot;math1122&quot;) where User=&#39;root&#39;;</code></pre><p>flush privileges;</p><pre><code class="sql">quit</code></pre><pre><code class="sql">sudo /etc/init.d/mysql stop...sudo /etc/init.d/mysql start</code></pre><p>7.解决</p><h4 id="7-检查端口号是否打开："><a href="#7-检查端口号是否打开：" class="headerlink" title="7.检查端口号是否打开："></a>7.检查端口号是否打开：</h4><p>1.控制面板-程序-打开或关闭windows功能 -勾选telent</p><p>2.cmd - telent 192.168.1.1 3306</p><h4 id="8-远程连接sql："><a href="#8-远程连接sql：" class="headerlink" title="8.远程连接sql："></a>8.远程连接sql：</h4><p>首先要在谷歌控制台SQL-用户-创建用户账号</p><p>创建一个不限制ip的账号和密码，之后才能连接</p><h4 id="9-项目在服务端持久运行"><a href="#9-项目在服务端持久运行" class="headerlink" title="9.项目在服务端持久运行"></a>9.项目在服务端持久运行</h4><p>nohup java -jar springboothellloworld-0.0.1-SNAPSHOT.jar&amp; &gt;/dev/null 2&gt;&amp;1 &amp;</p><p>exit</p><p>PID:2385</p><p><code>ps</code> <code>-p 2385 -o etime</code>查看运行时间</p><p>​    </p><p>chmod +x springboothellloworld-0.0.1-SNAPSHOT.jar</p><p>nohup java -jar -Dspring.profiles.active=production springboothellloworld-0.0.1-SNAPSHOT.jar &gt; /dev/null 2&gt; /dev/null &amp;</p><h3 id="10-数据库连接不上："><a href="#10-数据库连接不上：" class="headerlink" title="10.数据库连接不上："></a>10.数据库连接不上：</h3><ol><li>进入数据库的shell<ol><li>grant all privileges on <em>.</em> to ‘root’@’%’ identified by ‘password’ with grant option;</li></ol></li><li>flush privileges;</li><li>关了VPN!!!!!</li></ol>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>博客</tag>
      
      <tag>java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/重装Git和博客</title>
    <link href="undefined2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E9%87%8D%E8%A3%85Git%E5%92%8C%E5%8D%9A%E5%AE%A2/"/>
    <url>2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E9%87%8D%E8%A3%85Git%E5%92%8C%E5%8D%9A%E5%AE%A2/</url>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h1 id="重装Git和博客："><a href="#重装Git和博客：" class="headerlink" title="重装Git和博客："></a>重装Git和博客：</h1><h4 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h4><p>下载安装git.exe，官网速度慢就从腾讯下载</p><p>git bash</p><ol><li><p>git config –global user.name “yubin（自定义用户名）”</p></li><li><p>git config –global user.email “<a href="mailto:488459564@qq.com">488459564@qq.com</a>（自定义邮箱）”</p></li><li><p>ssh-keygen -t rsa -C “你的邮箱”　</p><p>三个回车</p></li></ol><p>在C:\users\bqlion\.ssh找到公钥，复制，粘贴到github里的new sshkey中</p><p>git clone <a href="mailto:git@github.com">git@github.com</a>::BqLion/_post.git</p><p>推送时若报转义问题：warning: LF will be replaced by CRLF in 字典树.md.<br>The file will have its original line endings in your working directory</p><p>输入git config –global core.autocrlf false解决</p><h4 id="hexo"><a href="#hexo" class="headerlink" title="hexo"></a>hexo</h4><p>下载安装node：<a href="https://nodejs.org/en/download/" target="_blank" rel="noopener">https://nodejs.org/en/download/</a></p><p>检查是否安好：node -v，npm -v</p><p>在Project目录：</p><p>git bash here</p><p>npm install -g hexo-cli</p><p>hexo init bqlion.github.io</p><p>cd bqlion.github.io</p><p>hexo new test</p><p>hexo g</p><p>hexo s</p><p>localhost:4000查看是否拉起</p><p>替换根目录下的_config.yml文件</p><p>将fliud压缩包解压到theme文件夹中，重命名为fliud</p><p>将老文件夹中 - 根目录 - source的CNAME复制到对应位置</p><p>将老文件夹中 - 根目录 - source的_data文件夹复制到对应位置</p><p>npm install hexo-deployer-git –save</p><p>hexo clean</p><p>hexo g</p><p>hexo d</p><p>在弹出的session中输入github账号密码</p><p>成功重新拉起网站</p>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/博客建站记录</title>
    <link href="undefined2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E5%8D%9A%E5%AE%A2%E5%BB%BA%E7%AB%99%E8%AE%B0%E5%BD%95/"/>
    <url>2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E5%8D%9A%E5%AE%A2%E5%BB%BA%E7%AB%99%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<h1 id="windows环境下使用hexo搭建博客记录"><a href="#windows环境下使用hexo搭建博客记录" class="headerlink" title="windows环境下使用hexo搭建博客记录"></a>windows环境下使用hexo搭建博客记录</h1><h3 id="搭建步骤"><a href="#搭建步骤" class="headerlink" title="搭建步骤"></a>搭建步骤</h3><ul><li><p>购买域名</p></li><li><p>github创建个人仓库</p></li><li><p>安装node.js</p></li><li><p>安装hexo</p></li><li><p>推送网站</p></li><li><p>域名解析</p></li><li><p>更换主题</p></li><li><p>更改配置</p></li><li><p>发布文章</p></li><li><p>使用图床</p></li></ul><h4 id="1-域名购买"><a href="#1-域名购买" class="headerlink" title="1.域名购买"></a>1.域名购买</h4><p>godaddy.com: 域名很全但价格较高，首年年费均价￥60左右,往后每年续费￥120左右。</p><p>阿里云：价格适中但个性化后缀较少,cc后缀域名无法备案</p><p>namesilo.com: 域名较全,价格最低,bqlab.cc十年￥280,最终在此购买。 </p><h4 id="2-github创建仓库"><a href="#2-github创建仓库" class="headerlink" title="2.github创建仓库"></a>2.github创建仓库</h4><p>仓库名命名为bqlion.github.io，亲测改成其他域名网站不能正常访问，</p><h4 id="3-安装node-js"><a href="#3-安装node-js" class="headerlink" title="3.安装node.js"></a>3.安装node.js</h4><p>下载地址：<a href="https://nodejs.org/en/download/" target="_blank" rel="noopener">https://nodejs.org/en/download/</a></p><p>安装完成后检测是否成功：node -v</p><p>检测npm是否一起安装成功: npm - v</p><h4 id="4-安装hexo"><a href="#4-安装hexo" class="headerlink" title="4.安装hexo"></a>4.安装hexo</h4><p>创建名为bqlion.github.io的文件夹，在这里打开cmd命令行</p><p>输入</p><pre><code>npm install -g hexo-cli</code></pre><p>初始化博客</p><pre><code>hexo bqlion.github.io</code></pre><p>检查网站雏形</p><pre><code>cd bqlion.github.iohexo new test_my_sitehexo ghexo s</code></pre><p>打开localhost:4000 检查网站是否已经拉起</p><p>常用hexo命令</p><pre><code>npm install hexo -g #安装Hexonpm update hexo -g #升级 hexo init #初始化博客命令简写hexo n  &quot;我的博客&quot; == hexo new  &quot;我的博客&quot;   #新建文章hexo g == hexo generate   #生成hexo s == hexo server   #启动服务预览hexo d == hexo deploy   #部署hexo server     #Hexo会监视文件变动并自动更新，无须重启服务器hexo server -s    #静态模式hexo server -p 5000     #更改端口hexo server -i 192.168.1.1     #自定义 IPhexo clean     #清除缓存，若是网页正常情况下可以忽略这条命令</code></pre><h4 id="5-推送网站"><a href="#5-推送网站" class="headerlink" title="5.推送网站"></a>5.推送网站</h4><p>根目录下的_config.yml是站点配置文件，themes文件夹里的_config.yml文件是主题配置文件</p><p>将hexo与github关联起来：</p><p>打开_config.yml，修改最后几行为：</p><pre><code>deploy: type: gitrepo: https://github.com/BqLion/bqlion.github.io.gitbranch: master</code></pre><p>其实就是给 hexo d这个命令做配置，让hexo知道要把blog部署在哪个位置</p><p>安装git部署插件</p><pre><code>npm install hexo-deployer-git --save</code></pre><p>分别输入三条命令（清理，生成，部署）</p><pre><code>hexo clean hexo g hexo d</code></pre><p>到这里博客就上线了</p><h4 id="6-域名解析"><a href="#6-域名解析" class="headerlink" title="6.域名解析"></a>6.域名解析</h4><p>阿里云在添加两条bqlab.cc中添加两条记录</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1217_10_817.png" alt=""></p><p>第一条记录值是bqlion.github.io，第二条记录值是bqlion.github.io的ip</p><p>ip地址获取方法是打开cmd</p><pre><code>ping bqlion.github.io</code></pre><p>点开github之前创建的仓库，点settings，设置custom domain,输入bqlab.cc</p><p>在blog根目录/source文件夹里创建记事本，内容就是自己的网址</p><pre><code>bqlab.cc</code></pre><p>退出保存为CNAME,注意保存为所有文件而不是txt文件</p><p>然后输入</p><pre><code class="bash">hexo cleanhexo ghexo d</code></pre><p>即可看到博客</p><h4 id="7-更换主题"><a href="#7-更换主题" class="headerlink" title="7.更换主题"></a>7.更换主题</h4><p>我使用的是fliud主题，参照<a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="noopener">https://github.com/fluid-dev/hexo-theme-fluid</a>设置</p><p>_config.yml修改红框主题为fliud</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1217_43_784.png" alt=""></p><p>下载相应版本<a href="https://github.com/fluid-dev/hexo-theme-fluid/releases" target="_blank" rel="noopener">https://github.com/fluid-dev/hexo-theme-fluid/releases</a></p><p>解压到themes目录下，重命名为fluid</p><ul><li>配置</li></ul><p>按如下代码块修改hexo根目录的_config.yml</p><pre><code># 设置语言，需要对应下面目录内的文件名，可以自定义文件内容# https://github.com/fluid-dev/hexo-theme-fluid/tree/master/languageslanguage: zh-CN# 关闭默认的代码高亮highlight:  enable: false</code></pre><ul><li>启用主题</li></ul><p>修改根目录的_config.yml</p><pre><code># Extensionstheme: fluid</code></pre><ul><li></li></ul><h4 id="8-配置主题（需特别注意yml的缩进问题）"><a href="#8-配置主题（需特别注意yml的缩进问题）" class="headerlink" title="8.配置主题（需特别注意yml的缩进问题）"></a>8.配置主题（需特别注意yml的缩进问题）</h4><p>参考文件<a href="https://fluid-dev.github.io/hexo-fluid-docs/guide" target="_blank" rel="noopener">https://fluid-dev.github.io/hexo-fluid-docs/guide</a></p><p>覆盖配置</p><ol><li><p>根目录的source目录，创建_data文件夹和_post文件夹，</p></li><li><p>在 <code>_data</code> 目录下创建 <code>fluid_config.yml</code> 文件，将 <code>/theme/fluid/_config.yml</code> 中被修改的配置或者全部配置，复制到 <code>fluid_config.yml</code> 中；</p></li><li><p>以后配置都在 <code>fluid_config.yml</code> 中修改，配置会在 <code>hexo g</code> 时自动覆盖。</p></li><li><p>所有资源静态文件的 Url 可以通过 <code>_static_prefix</code> 自定义配置，同样也支持覆盖配置，写入 <code>_data/fluid_static_prefix.yml</code> 即可。</p></li><li><p>其他情况，建议更新前备份 <code>/theme/fluid/_config.yml</code>，以免覆盖自定义的配置项。</p></li></ol><p>config.yml</p><ul><li>site</li></ul><pre><code>title: 刘秉乾的技术博客 :)subtitle: &quot;Life Oriented Programming...&quot;description: &#39;desc&#39;keywords:author: LiuBingqianlanguage: zh-CNtimezone: Asia/Shanghai</code></pre><p>subtitle:首页打印信息</p><p>language 和 timezone为必填项</p><ul><li>url</li></ul><pre><code>url: bqlab.ccroot: /permalink: :year/:month/:day/:title/permalink_defaults:pretty_urls:  trailing_index: true # Set to false to remove trailing index.html from permalinks</code></pre><p>url 必填</p><ul><li>extension/deployment</li></ul><pre><code># Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: fluid# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy:  type: git  repo: https://github.com/BqLion/bqlion.github.io.git  branck: master</code></pre><p>之前的步骤中已经修改</p><p>fliud_config.yml</p><p>修改头图</p><pre><code>favicon: /img/myicon.png # 网站标签页的 iconapple_touch_icon: /img/myicon.png # 用于苹果设备的 icon</code></pre><p>修改导航栏</p><pre><code>navbar:  blog_title: LiuBingqian`s Blog # 导航栏左侧的标题，为空则按 hexo config.title 显示  menu:  # 可自行增减，若想减去某个入口，可以将值留空    &quot;主页&quot;: /    &quot;目录&quot;: /categories/    &quot;归档&quot;: /archives/    &quot;标签&quot;: /tags/    &quot;关于&quot;: /about/</code></pre><p>修改各页图片和高度</p><pre><code>banner_img: https://static.zkqiang.cn/images/20190728023253.jpg-slim # 首页 Banner 头图，以下相同banner_img_height: 100  # 头图高度，屏幕百分比，available: 0 - 100</code></pre><p>修改about页信息</p><pre><code>#---------------------------# 关于页# About Page#---------------------------about:  # 以下仅为页面顶部的基本信息，更多内容请在 ./pages/about.md 中编辑，支持 markdown 和 HTML  md_path: ./pages/about.md  # 关于页文档的相对路径，可以按相对文档设置主题之外的路径，从而避免更新冲突  banner_img: https://static.zkqiang.cn/images/20190728021541.jpg-slim  banner_img_height: 90  # available: 0 - 100  avatar: /img/slava.png # 头像  name: 刘秉乾  introduce: 程序猿 / 健身爱好者 / 搬砖狂魔 # 支持 HTML  icons: # 更多图标可从 https://fontawesome.com/icons 查找，并以 &quot;图标名: url&quot; 的格式添加在下方    &quot;fab fa-github&quot;: https://github.com/BqLion    #&quot;fab fa-twitter&quot;:    #&quot;fab fa-weixin&quot;: # 微信图标，这个是特殊图标，点击不会跳转而是悬浮二维码，所以链接需要对应二维码图片地址</code></pre><p>参考博客：</p><p><a href="https://zkqiang.cn/" target="_blank" rel="noopener">https://zkqiang.cn/</a></p><p><a href="https://0x2e.github.io/" target="_blank" rel="noopener">https://0x2e.github.io/</a></p><h4 id="9-发布文章"><a href="#9-发布文章" class="headerlink" title="9.发布文章"></a>9.发布文章</h4><p>可直接把.md文件拖进_post目录，注意在每个.md文件头部插入yml块，输入如下内容配置好标签和分类</p><pre><code>categories:- 算法- CS229tags:- 机器学习- 梯度下降</code></pre><h4 id="10-使用图床"><a href="#10-使用图床" class="headerlink" title="10.使用图床"></a>10.使用图床</h4><ol><li>注册七牛云，实名认证，新建存储空间</li><li>使用MPic图床神器，配置AK SK后即可使用，直接把图片拖拽上去即可在剪切板生成md插入图片链接</li><li>使用snipaste截屏工具，比windows自带工具好用</li></ol><h4 id="11-常用命令"><a href="#11-常用命令" class="headerlink" title="11.常用命令"></a>11.常用命令</h4><p>hexo clean</p><p>hexo g</p><p>hexo d</p><p>hexo new &lt;title&gt;</p>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>博客</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/更新图床+截屏工作流</title>
    <link href="undefined2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E6%9B%B4%E6%96%B0%E5%9B%BE%E5%BA%8A+%E6%88%AA%E5%B1%8F%E5%B7%A5%E4%BD%9C%E6%B5%81/"/>
    <url>2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E6%9B%B4%E6%96%B0%E5%9B%BE%E5%BA%8A+%E6%88%AA%E5%B1%8F%E5%B7%A5%E4%BD%9C%E6%B5%81/</url>
    
    <content type="html"><![CDATA[<p>1.七牛云的试用域名只能用一个月，过期后要用自己已备案的域名。不管是否有备案域名所有已上传图片都要改链接，坑。</p><p>2.改用sm.ms图床，username:bqlion,email:<a href="mailto:398703917@qq.com">398703917@qq.com</a>,password:8</p><p>3.sm.ms限流，改用又拍云。</p><p>4.又拍云的shareX配置信息如下，密码用又拍云操作员控制台生成。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1220_19_603.png" alt=""></p><p>4.shareX简直神器。可以取代Snipaste+mpic。</p><p>5.ShareX更改快捷键：</p><ul><li>捕捉矩形区域为F1</li><li>设置“上传至”为FTP，</li><li>动作设置-高级设置 - after upload - chipboardContentFormat  - 修改为![]($result)</li></ul>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图床</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/win安装linux子系统</title>
    <link href="undefined2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/win%E5%AE%89%E8%A3%85linux%E5%AD%90%E7%B3%BB%E7%BB%9F/"/>
    <url>2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/win%E5%AE%89%E8%A3%85linux%E5%AD%90%E7%B3%BB%E7%BB%9F/</url>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h3 id="1-安装"><a href="#1-安装" class="headerlink" title="1.安装"></a>1.安装</h3><p>​    win10设置-针对开发者人员-勾选开发人员模式</p><p>​    控制面板-程序-程序和功能-启动或关闭windows功能-勾选适用于linux的windows子系统</p><p>​    windows appstore下载ubuntu</p><h3 id="2-优化"><a href="#2-优化" class="headerlink" title="2.优化"></a>2.优化</h3><p>​    备份源路径文件sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak</p><p>​    编辑源路径文件sudo vim  /etc/apt/sources.list</p><p>​    切换镜像源    vim删除当前文件所有内容：ggdG</p><pre><code class="cpp">deb http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiverse</code></pre><pre><code>deb https://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse</code></pre><p>​    sudo apt-get update</p><h3 id="3-图形界面"><a href="#3-图形界面" class="headerlink" title="3.图形界面"></a>3.图形界面</h3><p>​    windows10下载xming</p><pre><code>sudo apt-get install x11-apps </code></pre><p>​    运行程序的图形界面版：加DISPLAY=:0前缀，例如</p><p>​    DISPLAY=:0 geany</p><h2 id="4-cmder"><a href="#4-cmder" class="headerlink" title="4.cmder"></a>4.cmder</h2><p>设置可随时bash：settings-startup-command line -输入bash -cur_console:p</p><p>配置环境变量：控制面板-系统和安全-系统-高级系统设置-高级-环境变量-Path编辑</p><p>-新建D:\programs\cmder</p><p>cmd设置：管理员权限打开cmd-Cmder.exe /REGISTER ALL</p><p>5.安装yum后卡死或者报错</p><p>error: db5 error(-30973) from db_create: BDB0087 DB_RUNRECOVERY: Fatal error, run database recovery<br>error: rpmdb: BDB0060 PANIC: fatal region error detected; run recovery</p><p>原因：rpm数据库被损坏了</p><p>解决：<em>rpm –rebuilddb</em> </p>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/ShadowSocks在AWS搭梯子</title>
    <link href="undefined2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/ShadowSocks%E5%9C%A8AWS%E6%90%AD%E6%A2%AF%E5%AD%90/"/>
    <url>2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/ShadowSocks%E5%9C%A8AWS%E6%90%AD%E6%A2%AF%E5%AD%90/</url>
    
    <content type="html"><![CDATA[<h1 id="ShadowSocks在AWS搭梯子"><a href="#ShadowSocks在AWS搭梯子" class="headerlink" title="ShadowSocks在AWS搭梯子"></a>ShadowSocks在AWS搭梯子</h1><p>注册AWS账号，启动新的实例。</p><p>配置安全组，给实例增加端口，点击添加规则（类型：自定义TCP规则，协议：默认TCP，端口：8989，来源：任何位置）</p><p>下载密钥对</p><p>下载putty，把密钥放在putty根目录里</p><p>打开puttygen，选择Type of key to generate为RSA</p><p>点击puttygen里边的load，文件类型改成所有文件，打开putty根目录里的密钥</p><p>点save private key,弹出是否不要密码地保存私钥点”是”</p><p>打开putty，Connection页面，seconds between keepalives填写10。</p><p>打开putty，第一页HostName填写user_name<code>@</code>public_dns_name，也就是</p><pre><code>ec2-user@ec2-18-191-169-15.us-east-2.compute.amazonaws.com</code></pre><p>putty - connection - ssh - auth - browser,选中刚才生成的pkk文件，点击open，即可连上实例。</p><p>sudo yum install python-pip</p><p>sudo pip install shadowsocks</p><p>sudo ssserver -p 8989 -k password -m aes-256-cfb -d start</p><p>至此服务启动（关闭服务:输入ps - A查看进程号3960，输入sudo kill -s 9 3960关闭）</p><p>netstat -tln检验8989端口是否启动</p><p>下载shadowsocks客户端，填写ip，密码和端口号，加密方式和上文相同，aes-256-cfb 。最后右键app图标，选择启动系统代理。</p>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SSR搭梯子</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/Docker在AWS搭梯子</title>
    <link href="undefined2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/Docker%E5%9C%A8AWS%E6%90%AD%E6%A2%AF%E5%AD%90/"/>
    <url>2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/Docker%E5%9C%A8AWS%E6%90%AD%E6%A2%AF%E5%AD%90/</url>
    
    <content type="html"><![CDATA[<h1 id="使用Docker在AWS上搭梯子"><a href="#使用Docker在AWS上搭梯子" class="headerlink" title="使用Docker在AWS上搭梯子"></a>使用Docker在AWS上搭梯子</h1><h4 id="1-在aws上安装docker"><a href="#1-在aws上安装docker" class="headerlink" title="1.在aws上安装docker"></a>1.在aws上安装docker</h4><p>sudo apt-get update</p>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>test</title>
    <link href="undefined2020/02/24/test/"/>
    <url>2020/02/24/test/</url>
    
    <content type="html"><![CDATA[<p><img src="https://pic1.zhimg.com/v2-5ad8ddc664115133c73a40cb0c9662b0_r.jpg" alt="preview"></p>]]></content>
    
    
    <categories>
      
      <category>test</category>
      
    </categories>
    
    
    <tags>
      
      <tag>test</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/SpringBoot_Debug记录</title>
    <link href="undefined2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/SpringBoot_Debug%E8%AE%B0%E5%BD%95/"/>
    <url>2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/SpringBoot_Debug%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<h2 id="0-问题解决记录"><a href="#0-问题解决记录" class="headerlink" title="0.问题解决记录"></a>0.问题解决记录</h2><h4 id="0-1链接不上下文远程数据库"><a href="#0-1链接不上下文远程数据库" class="headerlink" title="0.1链接不上下文远程数据库"></a>0.1链接不上下文远程数据库</h4><p>–建立本地数据库，修改了application-dev.yml,</p><pre><code class="java">  datasource:#    url: jdbc:mysql://127.0.0.1:3306/libingteam?useUnicode=true&amp;characterEncoding=utf8&amp;serverTimezone=UTC##    url: jdbc:log4jdbc:mysql://120.78.150.152:3306/checky?useUnicode=true&amp;characterEncoding=utf8&amp;serverTimezone=UTC#    username: root#    password: 123456</code></pre><h4 id="0-2链接本地数据库之后时区报错"><a href="#0-2链接本地数据库之后时区报错" class="headerlink" title="0.2链接本地数据库之后时区报错"></a>0.2链接本地数据库之后时区报错</h4><p>报错信息如下</p><pre><code class="java">The server time zone value &#39;�й���׼ʱ��&#39; is unrecognized or represents more than one time zone</code></pre><p>–参考之前数据库配置，在配置语句最后加上?useUnicode=true&amp;characterEncoding=utf8&amp;serverTimezone=UTC</p><p>拉完代码：</p><p>application.yml</p><p>改成 active：dev</p><pre><code>#开发环境配置spring:  profiles: dev  datasource:    ##############################这两条线之间的内容修改为本机数据库地址######################    url: jdbc:mysql://localhost:3306/libingteam?useUnicode=true&amp;characterEncoding=utf8&amp;serverTimezone=UTC    username: root    password: math1122    ##############################这两条线之间的内容修改为本机数据库地址######################    driver-class-name: com.mysql.cj.jdbc.Driver    #连接池配置    hikari:      max-lifetime: 60000      maximum-pool-size: 20      idle-timeout: 60000      connection-timeout: 60000      validation-timeout: 3000      LoginTimeout: 60000  redis:    host: 127.0.0.1    port: 6379    timeout: 5000    database: 0    password:mybatis:  typeAliasesPackage: com.whu.libingteam.*.entity  mapperLocations: classpath*:mapper/**/*.xmllogging:  level:    com.whu.libingteam.system.dao : debug    com.whu.libingteam.user.dao : debug</code></pre>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Springboot</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_9 - Automatic Speech Recognition</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_9%20-%20Automatic%20Speech%20Recognition/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_9%20-%20Automatic%20Speech%20Recognition/</url>
    
    <content type="html"><![CDATA[<h1 id="9-Auto-Speech-Recognition-语音转文字"><a href="#9-Auto-Speech-Recognition-语音转文字" class="headerlink" title="9. Auto Speech Recognition - 语音转文字"></a>9. Auto Speech Recognition - 语音转文字</h1><p>Spoken language understanding is a difficult task.The goal of <strong>Automatic speech recognition（ASR）</strong>is to address this this task computationally by building system  that <strong>map from an acoustic signal to a string of word</strong>.And <strong>Automatic speech understanding(ASU)</strong> extends this goal to understand more than just words,also sentences.</p><p>影响语音转文字系统性能表现的几个参数：</p><ul><li>需要转换的词汇量：如果只是判断对错/数字，容易。判断词汇库是整体英语，难。</li><li>speech的流畅度：如果是切分好的单个单词，容易。流畅长篇幅对话，难。</li><li>声音质量：apparently，噪音少识别起来容易。</li><li>讲话者的类型：apparently，native speaker识别起来容易，日本口音难</li></ul><p>本章节阐述：</p><p>主要聚焦于大词汇量连续篇幅的语音识别问题（LVCSR : Large vocabulary continuous speech recognition）。</p><p>LVCSR主要使用的范式是HMM模型。本章涉及的核心知识在之前章节均有提及：第七章的音素，第五六章的贝叶斯法则，HMM模型，维特比算法，BW前向算法，第四章的N-gram模型和困惑度。</p><p>本章将介绍HMM-based的speech-recognizer、信号处理技术如何分离MFCC特征、高斯音学模型、维特比算法训练ASR模型的步骤（embedded training）。第十章会接着介绍ASR的其他细节。</p><p><strong>9.1)语音识别系统架构</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/ApplicationFrameHost_tFCT614otB.png" alt=""></p><p><strong>9.3</strong>）<strong>从waveform中提取MFCC feature</strong></p><p>1.把音波变成观察矩阵</p><p>首先把音波分帧，每帧25ms，两帧之间重叠10ms。分帧后音波变成了一小段一小段的，因为波形在时域上几乎没有描述能力，因此必须对波形做MFCC提取。根据人耳朵的生理特征，把每一个波形变换成一个多维向量，可以简单理解为这个向量提取了这帧语音的内容信息，这个过程就是声学特征提取。</p><p>因此，声音就成了一个十二行,N列的一个矩阵，称之为观察矩阵，这里N是总帧数。如下图，每一个帧都用12维的向量表示，色块颜色的深浅表示向量值的大小。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_W2LnNViUEa.png" alt=""></p><p>如何把这个矩阵变成文本？首先介绍两个概念：</p><ul><li>音素：单词的发音由音素构成（常见的英语用音素是CMU的39音素表构成，汉语由声母+韵母构成）</li><li>状态：这里可理解为比音素更细致的语音单位。通常把音素划分成3个状态。</li></ul><p>语音识别总体是如何工作的？</p><ul><li>把帧识别成状态（难点）</li><li>状态组合成音素</li><li>音素组合成单词</li></ul><p>如下图所示，竖条代表一个帧，若干个帧组成一个状态，三个状态组合成一个音素。若干个音素组合成一个单词。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_rMS54NmxEe.png" alt=""></p><p>那么每个帧对应哪个状态呢？可以看某个帧对应哪个状态的概率最大，这个帧就属于哪个状态。如下所示，这个帧在S3上的条件概率最大，所以就判断这个帧属于S3状态。这个条件概率在“高斯声学模型”中查阅的。这个模型里边训练了一大堆参数，通过这些参数，就可以知道帧对应状态的概率。训练参数的过程（<strong>embedding training</strong>）需要用到巨大的语音数据，具体过程这里省略。</p><p>这样做有一个问题，每帧都会对应一个状态号，最后整个语音都会得到一堆乱七八糟的状态号。假设语音有1000帧，就会有1000个状态号和333个音素，111个单词，但是其实1000帧就是25秒，30个单词左右，说明这里每个帧一个状态号的分配方式有问题，前后交叠有大量重复。其实相邻帧应该有相同的状态号才合理，因为每帧很短。</p><p>解决这个合理给帧分配状态号的问题，可使用HMM模型，一共两步;</p><ul><li>构建一个状态网络</li><li>从状态网络中寻找与声音最匹配的路径</li></ul><p>这样就把输出的结果限定在了刚才搭建的状态网络中，只要网络搭建的足够大，就可以包含任意文本路径，但是这个网络搭建的越大，就会越稀释，想达到比较好的准确率就很难。所以要根据任务特点，合理选择网络大小。</p><p>搭建状态网络，就是从单词级网络展开成音素网络，再展开成状态网络，语音识别其实就是在状态网络中搜索一条最佳路径，语音对应的这条路径是概率最大的路径。这其实也就是得知<strong>输出结果</strong>“语音，求解他的<strong>隐藏状态</strong>“状态”，是HMM的decoding问题，使用维特比算法（路径搜索的维特比算法是，概率一种动态规划的剪枝算法）可解。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_TjYucmLpiT.png" alt=""></p><p>这里有三个累积概率，分别是：</p><ul><li>观察概率：每帧和每个状态转移的概率（高斯声学模型得知）</li><li>转移概率：每个状态转移到自身和下个状态的概率（高斯声学模型得知）</li><li>语言概率：根据语言统计规律得到的规律（语言模型N-gram etc得知）</li></ul>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_总结</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_%E6%80%BB%E7%BB%93/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<p>自然语言是表达者对某状态(state)[比如<em>“饥饿”</em>]的<strong>编码(encode)</strong>[比如<em>“我饿了,I`m hungry,Я голоден”</em>],接收者会对这段音波/文字进行解码(decode)来揣测表达者的状态(state)。</p><p>自然语言处理是对state转换、code、encode/decode过程的一系列数学建模，通常基于规则+机器学习的模型效果不错。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_Dic</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_Dic/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_Dic/</url>
    
    <content type="html"><![CDATA[<table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Phonetics</td><td>语音学</td></tr><tr><td>Phonology</td><td>音韵学</td></tr><tr><td>Morphology</td><td>形态学</td></tr><tr><td>orthographic</td><td>正字法</td></tr><tr><td>Syntax</td><td>语法</td></tr><tr><td>Pragmatics</td><td>语用学</td></tr><tr><td>Semantics</td><td>语义学</td></tr><tr><td>Ambiguity</td><td>一词多义，模棱两可</td></tr><tr><td>Dative pronoun</td><td>与格代词</td></tr><tr><td>Possessive pronoun</td><td>所有格代词</td></tr><tr><td>part-of-speech</td><td>词性</td></tr><tr><td>lexical disambiguation</td><td>词义消歧</td></tr><tr><td>speech act interpretation</td><td>言语行为解释</td></tr><tr><td>regular grammars</td><td>正则文法</td></tr><tr><td>critical role</td><td>关键角色</td></tr><tr><td>crucial</td><td>重要的</td></tr><tr><td>symbolic</td><td>象征的</td></tr><tr><td>stochastic</td><td>随机的</td></tr><tr><td>retrieval</td><td>检索</td></tr><tr><td>corpora</td><td>语料库</td></tr><tr><td>finite-state automaton</td><td>有限状态自动机</td></tr><tr><td>punctuation</td><td>标点符号</td></tr><tr><td>appendix</td><td>附录</td></tr><tr><td>Disjunction</td><td>析取</td></tr><tr><td>Precedence</td><td>优先级</td></tr><tr><td>Symmetrically</td><td>对称地</td></tr><tr><td>sketched out</td><td>勾勒出来</td></tr><tr><td>concatenation</td><td>串联</td></tr><tr><td>union</td><td>并联</td></tr><tr><td>anchor</td><td>锚</td></tr><tr><td>intersection</td><td>交叉</td></tr><tr><td>complementation</td><td>互补</td></tr><tr><td>closure</td><td>关闭</td></tr><tr><td>singular</td><td>单数的</td></tr><tr><td>plural</td><td>复数的</td></tr><tr><td>stemming</td><td>堵塞</td></tr><tr><td>nominal inflection</td><td>名词的曲折变化</td></tr><tr><td>colon</td><td>冒号</td></tr><tr><td>intuitively</td><td>直觉</td></tr><tr><td>interpretation</td><td>解释</td></tr><tr><td>notion</td><td>概念</td></tr><tr><td>estimators</td><td>估计器</td></tr><tr><td>assign</td><td>分配</td></tr><tr><td>essential</td><td>本质</td></tr><tr><td>augmentative</td><td>增强的</td></tr><tr><td>extraction</td><td>提取</td></tr><tr><td>utterance</td><td>语言</td></tr><tr><td>strip out</td><td>剥离</td></tr><tr><td>lumped</td><td>集中</td></tr><tr><td>lemma</td><td>词根</td></tr><tr><td>picnic</td><td>野餐</td></tr><tr><td>metric</td><td>米制的，标准的</td></tr><tr><td>perplexity</td><td>困惑</td></tr><tr><td>extrinsic evaluation</td><td>外在评价</td></tr><tr><td>sparse data</td><td>稀疏数据</td></tr><tr><td>recapitulates</td><td>概述</td></tr><tr><td>ration</td><td>定额</td></tr><tr><td>implicit</td><td>隐性的</td></tr><tr><td>decomposing</td><td>分解</td></tr><tr><td>transcribing</td><td>抄写</td></tr><tr><td>acoustic</td><td>声学的</td></tr><tr><td>perspective</td><td>观点</td></tr><tr><td>phones</td><td>音素</td></tr><tr><td>hoax</td><td>骗局</td></tr><tr><td>Gaussian density function,pdfs</td><td>高斯概率密度函数</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_7 - Phonetics</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_7%20-%20Phonetics/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_7%20-%20Phonetics/</url>
    
    <content type="html"><![CDATA[<h1 id="7-Phonetics-语音学-–-本章对研究帮助不大，跳过"><a href="#7-Phonetics-语音学-–-本章对研究帮助不大，跳过" class="headerlink" title="7.Phonetics(语音学) – 本章对研究帮助不大，跳过"></a>7.Phonetics(语音学) – 本章对研究帮助不大，跳过</h1><p>A speech recognition system needs to have a pronunciation for every word it can recognize,and a text-to-speech system needs to have a pronunciation for every word it can say.</p><ol><li><p>In this chapter,we introduces <strong>phonetic alphabets</strong>音标字母 for describing these pronunciations.</p></li><li><p>Then we introduces two main areas of phonetics, <strong>articulatory phonetics</strong>发音语音学,and <strong>acoustic phonetics</strong>声学语音学。</p></li><li><p>Also briefly touch on <strong>phonology</strong>音韵学</p></li></ol><p><strong>7.1）</strong>Speech sounds and phonetic transciption</p><p>音标表格如下：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191229_1536_52_924.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Phonetics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_8 - Speech Synthesis</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_8%20-%20Speech%20Synthesis/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_8%20-%20Speech%20Synthesis/</url>
    
    <content type="html"><![CDATA[<h1 id="8-Speech-Synthesis-语音合成（文字转语音）"><a href="#8-Speech-Synthesis-语音合成（文字转语音）" class="headerlink" title="8. Speech Synthesis - 语音合成（文字转语音）"></a>8. Speech Synthesis - 语音合成（文字转语音）</h1><p>Speech Synthesis detail is as following:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191229_1557_04_707.png" alt=""></p><p>本单元讲解如何将文本转换成语音，主要分成如下四个任务：</p><ul><li>text normalization  文本标准化</li><li>phonetic analysis 语音分析（把token化的词汇转换成音标）</li><li>prosodic analysis 韵律分析( 把音标组成的集合拼凑地和谐一点)</li></ul><ul><li>waveform synthesis 波形合成（让拼凑好的音标们-IR转wave-发音）</li></ul><h3 id="8-1-Text-Normalization"><a href="#8-1-Text-Normalization" class="headerlink" title="8.1) Text Normalization"></a><strong>8.1</strong>) Text Normalization</h3><p>Text normalization was combined as follwoing:</p><ul><li>sentence tokenization</li><li>non-standard words</li><li>homograph disambiguation</li></ul><h4 id="8-1-1-Sentence-tokenization"><a href="#8-1-1-Sentence-tokenization" class="headerlink" title="8.1.1)Sentence tokenization"></a><strong>8.1.1)</strong>Sentence tokenization</h4><p>tokenization理解为把句子拆分成小块（token），token之间可以是被空格键分隔，也可以是被句号，逗号，或者单纯是被语义分隔，被token化的单词已经解决了大小写，缩写，等等问题。</p><p>做tokenzation的时候一般采用机器学习的方法，在训练集上人们用手工的方法标注各个token，用&lt;EOS&gt;符号间隔开各个token。</p><p>机器学习方法可能学习到的特征有：缩写，前缀，后缀，前边后边的单词之间的相互关系等等。</p><h4 id="8-1-2）non-standard-words"><a href="#8-1-2）non-standard-words" class="headerlink" title="8.1.2）non-standard words"></a><strong>8.1.2）</strong>non-standard words</h4><p>dealing with non-standard words need three steps:</p><ul><li>tokenization - separete and identify non-standard words.</li><li>classification - classification to label them with a type in following pictures.</li><li>expansion - convert each type into a string of standard words.</li></ul><p>the kind of non-standard words:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191229_1620_31_807.png" alt=""></p><p><strong>tokenization:</strong></p><p>大体上，我们通过空格把各个toke分隔开，然后逐一与字典对比，任何不在字典里的词语我们都理解为一个non-standard words</p><p><strong>classification：</strong></p><p>对这些non-standard words，我们需要对他们分类，分类的结果集就是上图出现的各个项。</p><p>分类的具体做法可以是desision list算法，这个算法会在第20章讲解。</p><p>我们现在在构建分类器的时候可以先使用手工标注的训练集，使用监督机器学习的方法去训练分类器。SVM,逻辑回归等等。</p><p><strong>expansion:</strong></p><p>expansion can deal with abbr problem,all the abbr can be expanded to <strong>ordinary words</strong>,by using <strong>abbr dictionary</strong>,.In which ambiguities problem can be solved by <strong>homonym disambiguation</strong></p><p>​    homonym disambiguation:</p><p>​        Most disambiguation problem can be sovled by different part-of-speech role,most disambiguation         abbr/word also different pronunciation.</p><p>​        The rest of disambiguation problem which have same POS and same pronunciation,can be solved         by <strong>word sense disambiguation algorithms</strong>,like <strong>decision list algorithm</strong>.which will be introduced         in chapter 20.</p><h3 id="8-3-Phonetic-analysis-语音分析"><a href="#8-3-Phonetic-analysis-语音分析" class="headerlink" title="8.3 Phonetic analysis - 语音分析"></a>8.3 Phonetic analysis - 语音分析</h3><p>上一小段介绍的是如何把文本内容拆成token，这一小段介绍的是如何把token转化成发音。</p><p>这一小段比较重要的一个组件是<strong>音素字典</strong>。单独使用字典效率是不高的，因为总有一些token你在字典里边查不到（比如稀奇古怪的人名等，书中举例子说在两个不同的词典里运用华尔街日报的语料库，分别有5%左右的token在字典里查不到）。</p><p>本小段分为三个小节，分别介绍：<strong>音素字典</strong>、<strong>名字</strong>、<strong>字符-音素转换规则</strong>。</p><p><strong>8.3.1</strong>）音素字典</p><p>最常用的音素字典是CMU Pronounceing Dictionary.包含了120000多个单词的读音。但是他的设计目的为为了用于语音识别，而不是为了把文字合成语音。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191230_1418_05_932.png" alt=""></p><p>另一个设计目的就是为了科研的合成语音的字典是UNISYN，他是免费的。如下是使用示例。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191230_1419_52_341.png" alt=""></p><p>​    </p><p><strong>8.3.2</strong>)名字</p><p>如本小结开头所说，有很多名字在词典里边是查不到的，因为人名/公司名等等实在太多了，仅仅在美国预估都有两百万人名，为了解决有些人名在词典里查不到，通常的做法是使用 字符-音素转换规则（下小结介绍，通常构建两套预测系统，一个是名字专用，另一个是用于其他）</p><p><strong>8.3.3</strong>）字符-音素装换规则</p><p>grapheme-to-phoneme,or g2p.核心做法就是将cake转换成[K EY K].</p><p>早期做法是letter-to-sound，or LTS rule。英语发音中比较难掌握的是重读的界定，之前都是采用复杂的rule-based的规则，现在多采用机器学习的方法对重读标注。</p><h3 id="8-4-Prosodic-Prominence-韵律"><a href="#8-4-Prosodic-Prominence-韵律" class="headerlink" title="8.4 Prosodic Prominence - 韵律"></a>8.4 Prosodic Prominence - 韵律</h3><p>在语音中，通常会有某些词比较<strong>突出</strong>，突出的方法可能是重读也可能是减慢发音速度，这样的突出的词汇会更感性，对听者有更深的印象。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191230_1443_07_979.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191230_1443_19_973.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191230_1443_40_551.png" alt=""></p><p><strong>8.4.1</strong>）文本转音波系统的输出：Internal Representation</p><p>最终，经过TOBi或者其他系统，成功将文字转换成internal representation,然后这个IR会输入下一节的双音波形合成系统，被转换为读音输出。</p><p>IR如下所示</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191230_1452_07_167.png" alt=""></p><h3 id="8-5-将上文输出的Internal-representation转换为发音（WaveForm）"><a href="#8-5-将上文输出的Internal-representation转换为发音（WaveForm）" class="headerlink" title="*8.5 *将上文输出的Internal representation转换为发音（WaveForm）"></a>*<em>8.5 *</em>将上文输出的Internal representation转换为发音（WaveForm）</h3><p>介绍两种将internal representatiton转换为音波的方法：一是双音波形合成，二是单元综合合成(unit selection synthesis)</p><p><strong>8.5.1</strong>）Diphone音波合成</p><p>粗略地讲，Diphone是一个音素的中间，到另一个音素的中间，的东西。</p><p><strong>DiPhone拼接合成模型</strong>通过查询diphone数据库，对一系列音素生成波形。</p><p>此模型可以被如下步骤表针:</p><ol><li>训练<ol><li>挨个记录每一个话筒对单个diphone的发音</li><li>把每一个diphone从一篇speech中切分出来，存到数据库中</li></ol></li><li>合成<ol><li>按需从di-phone数据库中取出一系列需要的diphone</li><li>在各个di-phone的边界进行简单的信号处理之后，串联起来</li><li>使用信号处理技术，调整基频和持续时间等。</li></ol></li></ol><p><strong>8.5.2</strong>）Unit selection(Waveform)Synthesis</p><p>刚才介绍的diphone waveform synthesis有两个毛病：</p><p>一是依赖于diphone数据库，数据库中的内容都是从speech中切分出来的，没有质量稳定性。任何时候取出来的数据都有可能听起来不自然。</p><p>二是只能使用相邻的音素，但实际应用中很多时候需要使用并不相邻的音素。</p><p>于是就有了Unit selectioin synthesis系统，可以视作上一个 DiPhone系统的新一代产品。他做了如下两个改进：</p><p>1.之前的系统中，每个音素就是单独的音素，在新系统中，每一个音素是几个小时时长的存储单元，包含了同一音素在各个场景下的细微不同的发音。</p><p>2.在之前的系统中，音素串联采用例例如PSOLA算法等，新的系统中，采用了信号处理系统处理串联。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191230_1635_42_296.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191230_1636_30_296.png" alt=""></p><p>Making choice by cepstral distance algorithm.</p><p>代价函数和分项（Target cost、Join cost）如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191230_1640_56_763.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191230_1641_13_154.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191230_1641_25_194.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_6 - HMM &amp; HEMM</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_6%20-%20HMM%20&amp;%20HEMM/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_6%20-%20HMM%20&amp;%20HEMM/</url>
    
    <content type="html"><![CDATA[<h1 id="6-Hidden-Markov-and-Maximum-Entropy-Models（HMM-MEMM）"><a href="#6-Hidden-Markov-and-Maximum-Entropy-Models（HMM-MEMM）" class="headerlink" title="6.Hidden Markov and Maximum Entropy Models（HMM,MEMM）"></a>6.Hidden Markov and Maximum Entropy Models（HMM,MEMM）</h1><p>HMM and HEMM are both sequence classifiers.</p><p>Sequence classifier or sequence labeler is a model whose job is to assign some label or class to each unit in a sequence.The FST we studied in Chapter 3 is a kind of non-probabilisitic sequence classifier.</p><p>We have seen on important sequence classification task:POS tagging.</p><p>This chapter is roughtly divided into 2 section:HMM , MEMM.</p><p><strong>6.1</strong>) Markov Chains - 马尔科夫链 - 详见概念6 - 马尔科夫链</p><p><strong>6.2</strong>)The Hidden Markov Model - 隐性马尔科夫模型 - 详见概念6-马尔科夫模型</p><p>Both were defined in appendix.</p><p><strong>6.3</strong>)HMM的三个问题 - 详见概念6-三个问题</p><ul><li>likelihood</li><li>learning</li><li>decoding</li></ul><p><strong>6.6)</strong></p><p>HEMM和CRF和HMM</p><p>【详见概念_6_HMM,HEMM,CRF的比较】</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_5 - Part-of-Speech Tagging</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_5%20-%20Part-of-Speech%20Tagging/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_5%20-%20Part-of-Speech%20Tagging/</url>
    
    <content type="html"><![CDATA[<h1 id="5-Part-of-Speech-Tagging"><a href="#5-Part-of-Speech-Tagging" class="headerlink" title="5.Part-of-Speech Tagging"></a>5.Part-of-Speech Tagging</h1><p>Introduce three algorithms:</p><ul><li>rule-based tagging</li><li>HMM tagging</li><li>transformation-based tagging</li></ul><p>词性标注是一项消歧任务，很多情况下词具有多于一个的意思，我们的工作是为这种情况找到正确的标签。</p><h2 id="5-2）Tagset-for-English"><a href="#5-2）Tagset-for-English" class="headerlink" title="5.2）Tagset for English"></a><strong>5.2</strong>）Tagset for English</h2><p>There are 3 different tagset.</p><ul><li>45-tag Penn Treebank tagset</li><li>61-tag C5 tagset</li><li>87-tag tagset </li></ul><ul><li>Small Tagset:</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191225_1326_22_350.png" alt=""></p><ul><li>Middle Tagset</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191225_1328_08_570.png" alt=""></p><ul><li>Large Tagset</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/n7NLrfur0L.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191225_1330_02_268.png" alt=""></p><h2 id="5-3-Part-of-Speech-Tagging"><a href="#5-3-Part-of-Speech-Tagging" class="headerlink" title="5.3) Part-of-Speech Tagging"></a><strong>5.3</strong>) Part-of-Speech Tagging</h2><p>Sometimes,tagging can be difficult（unambiguous）,For example ,book is ambiguous.Book can be a noun as a read book,or be a verb, as booking a hotel.Below picture shows the number of word type with different level of part-of-speech ambiguity.</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191225_1344_07_777.png" alt=""></p><p>There are three kinds of tagging:</p><ul><li>rule-based tagging</li><li>HMM tagging</li><li>transformation-based tagging</li></ul><h3 id="5-3-1-rule-based-tagging"><a href="#5-3-1-rule-based-tagging" class="headerlink" title="5.3.1 rule-based tagging"></a><strong>5.3.1 rule-based tagging</strong></h3><p>like the code:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191225_1604_00_167.png" alt=""></p><h3 id="5-3-2）HMM-tagging"><a href="#5-3-2）HMM-tagging" class="headerlink" title="5.3.2）HMM tagging:"></a><strong>5.3.2）HMM tagging</strong>:</h3><h4 id="HMM-Tagging的组件"><a href="#HMM-Tagging的组件" class="headerlink" title="HMM Tagging的组件:"></a><strong>HMM Tagging的组件:</strong></h4><p>HMM Tagging有两个组件，即A和B概率矩阵。</p><p>矩阵A包<strong>含标签转移概率</strong> <img src="https://www.zhihu.com/equation?tex=P%28t_i%7Ct_%7Bi-1%7D%29" alt="[公式]"> ，<img src="https://www.zhihu.com/equation?tex=P%28t_i%7Ct_%7Bi-1%7D%29" alt="[公式]"> <strong>表示给定前一个标签(隐藏状态)的当前标签发生的概率</strong>。例如，像will这样的情态动词很可能后面跟着一个基本形式的动词，比如race，所以我们认为这种可能性很高。<strong>我们通过计数来计算这个转移概率的最大似然估计</strong>，<strong>在我们看到标记语料库中的第一个标记的次数中，第一个标记后面紧跟着第二个标记的频率(计算第一个标记后第二个标记出现的次数再除第一个标记总的出现次数)：</strong></p><p><img src="https://pic4.zhimg.com/v2-821f5c0d8508923acb74ced907900d53_b.png" alt="img"></p><p>通俗写法：P（race|will） = (will,race)次数   / （will）次数</p><p>例如，在华尔街日报语料库中，MD(情态动词)发生13124次，紧随其后的是VB(动词原形)出现10471, MLE估计为：</p><p><img src="https://pic4.zhimg.com/v2-28d6d7976b3bf9d6526b95923f9e3653_b.png" alt="img"></p><p>在返回解码算法之前，让我们看一个示例。</p><p><img src="https://pic3.zhimg.com/v2-39d989ee5a949ae21123c1f704229416_b.png" alt="img"></p><p>分子是数据中标记为 <img src="https://www.zhihu.com/equation?tex=t_i" alt="[公式]"> 时，单词也是 <img src="https://www.zhihu.com/equation?tex=w_i" alt="[公式]"> 的出现次数，分母是 <img src="https://www.zhihu.com/equation?tex=t_i" alt="[公式]"> 出现的次数，在WSJ语料库中出现的13124个MD中，与will一起出现4046次：即，P（will是情态动词 |will ） = C(will,will情态动词)/C(所有情态动词).</p><p><img src="https://pic4.zhimg.com/v2-6f53212d4bcbe071656d95948eeba477_b.png" alt="img"></p><p>我们先前看到过这种贝叶斯模型; 回想一下，这个可能性术语并不是在问“词will最可能的标签是什么？”那将是后验 <img src="https://www.zhihu.com/equation?tex=P%28MD%7Cwill%29" alt="[公式]"> 。相反，<img src="https://www.zhihu.com/equation?tex=P%28will+%7C+MD%29" alt="[公式]"> 回答了一个有点违反直觉的问题“如果我们要生成一个MD，这个情态动词(MD)有多大可能是will？”</p><p>图8.4所示为HMM词性标记器中三种状态的A转移概率矩阵和B观察概率矩阵;完整的标记器将为每个标记提供一个状态。</p><p><img src="https://pic2.zhimg.com/v2-b0c92f0f469c96823534cc857f0c0a0d_b.jpg" alt="img"></p><p>Figure 8.4。HMM表示的两部分的一个示例:用于计算先验概率的A转换概率，以及与每个状态相关的B观察概率，每个可能的观察词的一种可能性</p><p>*<em>HMM Tagging工作原理 *</em></p><p>对于任何包含隐变量的模型，例如HMM，确定与观察序列对应的隐变量序列的任务称为解码.</p><p><strong>Tagging</strong> 给定一个HMM模型 <img src="https://www.zhihu.com/equation?tex=%5Clambda+%3D+%28A%2C+B%29" alt="[公式]"> 和一个观察序列 <img src="https://www.zhihu.com/equation?tex=O+%3D%5C%7Bo_1%2Co_2%2C...o_T%5C%7D" alt="[公式]"> ，找出最可能的状态序列 <img src="https://www.zhihu.com/equation?tex=Q+%3Dq_1q_2...q_T" alt="[公式]"> </p><p>对于词性标注，<strong>隐马尔可夫解码的目标是在给定n个单词 <img src="https://www.zhihu.com/equation?tex=w_%7B1%7D%5E%7Bn%7D" alt="[公式]"> 的观察序列的情况下，选择最可能的标记序列 <img src="https://www.zhihu.com/equation?tex=t_%7B1%7D%5E%7Bn%7D" alt="[公式]"> 作为标记序列</strong>：</p><p><img src="https://pic2.zhimg.com/v2-ed2abb5d28fcaf4cc0f3c178bf7eeb29_b.png" alt="img"></p><p>我们在HMM中做这件事的方法是用<strong>贝叶斯规则来代替计算:</strong></p><p><img src="https://pic3.zhimg.com/v2-716e8251d6c81015640c7dcedfdb46ea_b.png" alt="img"></p><p>此外，我们通过去掉分母 <img src="https://www.zhihu.com/equation?tex=P%28w_%7B1%7D%5E%7Bn%7D%29" alt="[公式]"> （分母可去对结果无影响）来简化式8.14:</p><p><img src="https://pic4.zhimg.com/v2-d300cff6cc2fc091149d5b6702054c53_b.png" alt="img"></p><p>标记器进一步简化了两个假设。首先，<strong>一个单词出现的概率只取决于它自己的标签，与相邻的单词和标签无关:</strong></p><p><img src="https://pic3.zhimg.com/v2-717897d2ccbbdef5cf21a77c841d2f1a_b.png" alt="img"></p><p><strong>第二个假设，bigram假设，一个标签的概率只依赖于前一个标签，而不是整个标签序列;</strong></p><p><img src="https://pic4.zhimg.com/v2-64b9f026b07a0b5153ec3746b1fe0c0f_b.png" alt="img"></p><p>将式8.16和式8.17的简化假设代入式8.15，得到bigram tagger中最可能的标签序列如下式：</p><p><img src="https://pic4.zhimg.com/v2-850351b4b6b59972589d018319d55153_b.png" alt="img"></p><p>式8.18的两部分与我们刚刚定义的B状态概率和转移概率完全对应。</p><p>Viterbi algo on POS Tagging<img src="http://bqlab-pic.test.upcdn.net/pic/20191225_1951_16_048.png" alt=""></p><h3 id="5-3-3-Transformation-based-Tagging"><a href="#5-3-3-Transformation-based-Tagging" class="headerlink" title="5.3.3) Transformation-based Tagging"></a>5.3.3) Transformation-based Tagging</h3><p>This kind of tagging draws inspiration from both rule-based and HMM tagging:it based on the rules and also have machine learning technique.</p><p>How TBT applied?</p><p>First,every words was initilized by most likely label.</p><p>Secondly,It exams every possible transformation and select the one that result in most improved tagging</p><p>Finally,it then re-tags the data according to this rule.</p><p>TBT will repeat last two stage until it reaches some stopping criterion.</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_4 - N-Grams</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_4%20-%20N-Grams/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_4%20-%20N-Grams/</url>
    
    <content type="html"><![CDATA[<h1 id="4-N-Grams"><a href="#4-N-Grams" class="headerlink" title="4 - N-Grams"></a>4 - N-Grams</h1><p>N-gram is a language model,is a N-token sequence of words.</p><h3 id="1-the-way-to-calculate-conditional-probability"><a href="#1-the-way-to-calculate-conditional-probability" class="headerlink" title="1.the way to calculate conditional probability."></a>1.the way to calculate conditional probability.</h3><p>For example,how can we calculate <strong>P(the|its water is so transparent that)</strong> ?</p><p><strong>Method1):</strong> counting 2 sentences</p><p>Counting the times of “its water is so transparent that the” and “its water is so transparent that” in the whole corpus.It works sometime,but language is creative,many sentences is not exist.</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191222_1112_48_111.png" alt=""> </p><p><strong>Method2):</strong> chain rule of probability,then use method 1.</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191222_1149_27_804.png" alt=" m"></p><p>notice that:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191222_1331_13_781.png" alt=""></p><p>means  P(w1w2w3w4……wn)</p><p>But <strong>P(its|water is) * P(water|is) * P(is) *<em>still don`t work fine.The reason is same to *</em>Method1</strong>,some sentence not exist.</p><p><strong>Method3):</strong> transform the question to <strong>P(the | that)</strong></p><p>Markov assumption: We can predict the probability of future words without looking too far away.So here is the <strong>bigram</strong> model : <strong>P(the|Walden Pond`s water is so transparent that)</strong> = <strong>P(the|that)</strong>.</p><p>How can we caclute <strong>P(the | that)</strong>?</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191222_1342_51_243.png" alt=""></p><p>As above,we count the number of <strong>（”that the“ / “that”)</strong>.</p><p>Example for method3；</p><p>mini-corpus:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191222_1355_44_172.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191222_1356_06_426.png" alt=""></p><p>例如上图第一个概率等式：分母是&lt;s&gt;出现的总次数，一共是3。分子是&lt;s&gt;后面跟着I 的次数，一共是2。</p><h3 id="2-randomly-genarate-sentences-by-n-gram"><a href="#2-randomly-genarate-sentences-by-n-gram" class="headerlink" title="2.randomly genarate sentences by n-gram"></a>2.randomly genarate sentences by n-gram</h3><p>Sentences randomly generated by using corpora of <strong>Shakespeare`s works</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191223_1438_50_162.png" alt=""></p><p>Sentences randomly generated by using corpora of <strong>Wall Street Journal</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191223_1441_18_717.png" alt=""></p><h3 id="3-How-to-evaluating-N-gram-Perplexity"><a href="#3-How-to-evaluating-N-gram-Perplexity" class="headerlink" title="3.How to evaluating N-gram : Perplexity"></a>3.How to evaluating N-gram : Perplexity</h3><p>How to evaluating a language model from good to bad?</p><ul><li><p>extrinsic evalation ：</p><p>Extrinsic evalation is applying language model to different specific question(machine translator,speech recognition,spelling corrector etc),and compare two LM the cost of time.Is`s intution,but usually cost too much time.</p></li><li><p>intrinsic evalation :</p><p>Just calculate perplexity : 当语言模型训练完后，测试集中的句子都是正常的句子，模型在测试集上验证概率越高，说明句子概率越大，困惑度越小，模型越好。</p></li></ul><p>The <strong>Perplexity</strong> was defined as follow:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191223_1504_16_492.png" alt=""></p><p>a <strong>example</strong> of calculate <strong>perplexity</strong> in a mini corpora:</p><p>​    mini corpora is just 10 digits:(0,1,2,3,4,5,6,7,8,9)</p><p>​    <img src="http://bqlab-pic.test.upcdn.net/pic/20191223_1512_43_979.png" alt=""></p><p>​    <img src="http://bqlab-pic.test.upcdn.net/pic/20191223_1513_07_960.png" alt=""></p><p>如下，另一个在同一个语料库上生成的un / bi / tri - gram模型的困 惑值有显著不同。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191223_1529_59_667.png" alt=""></p><h3 id="4-Smoothing"><a href="#4-Smoothing" class="headerlink" title="4. Smoothing"></a>4. Smoothing</h3><p>为什么需要平滑（Smoothing）？</p><p>（平滑通过）</p><p>Smoothing.Smooth provides a better way of estimating the probability of N-grams than MLE.Commonly used smoothing algorithms for N-grams rely on lower-order N-gram counts through backoff or interpolation</p><p>3.Both backoff and interpolation require discounting such as Kneser-Ney,WittenBell,or Good-Turing discounting.</p><p>假设语料库由如下三个句子构成：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1317_13_202.png" alt=""></p><p>按照最大似然估计的方法来计算p(BROWN READ A BOOK)的概率如下：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1317_57_370.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1318_14_432.png" alt=""></p><p>结果是0.06。这时候求p（David read a book）的概率是多少呢？因为David从来没有在语料库中出现过。所以如下所示，求出来的概率为0.但这与事实不符。因为语言是creative的，但语料库是有限的，一定会有一些语料没有在语料库中出现，所以需要对这部分没有出现在语料库中的creative的语料做平滑处理（也就是劫富济贫，把出现多的语料劫一部分出来分给他们）。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1321_41_820.png" alt=""></p><p><strong>Laplace Smoothing</strong></p><p>Laplace smoothing just add 1 to any tokens.It`s not a good enough smoothing method,but it`s a good start-up baseline.</p><p>Before Laplace smoothing:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1117_36_583.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1120_07_549.png" alt=""></p><p>After Laplace smoothing</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1117_06_667.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1121_21_187.png" alt=""></p><p><strong>Kneser-Ney smoothing</strong></p><p>Equation:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1340_17_404.png" alt=""></p><p>c是词语出现的次数。N(c+1)是出现次数为（c+1）次的词语的总数，如下是两个语料库的示例：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1342_40_890.png" alt=""></p><p><strong>插值与回退（Interlation and backoff）</strong></p><p>插值：interlation</p><p>在一些情况下，无3-gram example，只得用2-gram，1-gram代替之</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1513_27_145.png" alt=""></p><p>λ的其他构成</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1513_40_098.png" alt=""></p><p>回退：backoff</p><p>回退法恐怕是最好理解的一种平滑方法了，它的思路就是：如果一个n-gram的条件概率为0，则用(n-1)-gram的条件概率取代，如果(n-1)-gram的条件概率依然为0，继续回退，直到1-gram概率，如果1-gram依然为0，就直接忽略掉该词。用式子表示如下：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1525_53_830.png" alt=""></p><p><strong>交叉熵cross-entropy</strong></p><p>cross entropy of m on p is defined by</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1539_52_534.png" alt=""></p><pre><code>Summary:1.N-gram probability is conditioinal probability of a word given the previous N-1 words.N-gram probabilities can be computed by simply counting in a corpus and normalizing(the maximum likelihood estimate),or they can be computed by more sophisticated algorithms.2.Smoothing.Smooth provides a better way of estimating the probability of N-grams than MLE.Commonly used smoothing algorithms for N-grams rely on lower-order N-gram counts through backoff or interpolation3.Both backoff and interpolation require discounting such as Kneser-Ney,WittenBell,or Good-Turing discounting.4.N-gram LM are evaluated by separating the corpus int a training set and a test set.training the model on the training set,and evaluating on the test set.The perplexity of LM on a test set is used to compare LM performance.</code></pre>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_3 - Word &amp; Transducers</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_3%20-%20Word%20&amp;%20Transducers/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_3%20-%20Word%20&amp;%20Transducers/</url>
    
    <content type="html"><![CDATA[<h1 id="3-Word-amp-Transducers"><a href="#3-Word-amp-Transducers" class="headerlink" title="3.Word &amp; Transducers"></a>3.Word &amp; Transducers</h1><p><strong>3.1</strong>)Word</p><p>Before processing, words in speech should be Stemming、lemmatization and tokenization.</p><p>Tokenization means put”New York”in one word,separate “I`m” into “I” and “am”。</p><p>Using FSA to build Stemming net of words</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191220_1600_40_308.png" alt=""></p><p><strong>3.2</strong>)FST : Finite state transducers.</p><p>I define it my way:</p><p>If you input “aa”,”b”will be output,and the state in still “q0”,if you input “b”,”a or b”will out put,state will be “q1”.For example,if you input “aa aa b a”，output is “b b a ba”.</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191220_1655_35_867.png" alt=""></p><p>Defination with details is in another md file.</p><p>FSA is isomorphic to <strong>regular languages</strong>, FSTs are isomorphic to <strong>regular relations</strong></p><p><strong>3.9</strong>)Tokenization</p><p>“Space空格键” is not a sufficient tool to separate words from sentences,and many languages dont have spaces(Chinese,Japanes,Thai).A example:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191220_1704_30_824.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191220_1704_51_270.png" alt=""></p><p>Tokenizer can be build with simple regular expression,which can be easily implemented in FSTs.See the script as follow:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191220_1708_03_391.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_23 - QA</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_23%20-%20QA/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_23%20-%20QA/</url>
    
    <content type="html"><![CDATA[<h1 id="23-Question-Answering-and-Summarization"><a href="#23-Question-Answering-and-Summarization" class="headerlink" title="23 - Question Answering and Summarization"></a>23 - Question Answering and Summarization</h1><p>本章介绍基于事实的回答问题系统+文章的总结系统。如果我们查找的是结构化的数据，可用上一章介绍的信息提取算法来词查找。如果查找的是非结构化的信息，则需要用本章介绍的“回答问题系统”处理，非结构化信息的查找就是一种“使用非正式的单词或句子”来表达查找需求的场景，这种场景下客户通常期望能返回一些回答or一些文本，或something in between。</p><p>第一个session介绍向量空间模型，第二个session介绍基于事实的一问一答系统。第六个session介绍基于句子的总结系统/基于注意力机制的总结系统。</p><h3 id="23-1-Information-Retrieval-信息检索"><a href="#23-1-Information-Retrieval-信息检索" class="headerlink" title="23.1 Information Retrieval 信息检索"></a>23.1 Information Retrieval 信息检索</h3><p><strong>术语定义</strong></p><p>文档document指的是一个有索引的，可被检索系统直接定位的最小单元，对应到web就是一个网页</p><p>集合collection指的是一系列用来满足用户需求的文档documents</p><p>term指的是一个词汇元素。</p><p>query指的是一系列terms</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_3w6aY5INY5.png" alt=""></p><p><strong>词向量空间</strong></p><p>词义相近的在向量空间中距也近，概念在CS224N中学过了。空间和点积的直观图如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_WZMYCVDLrg.png" alt=""></p><p>点积：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_dnt6s0Vt8D.png" alt=""></p><p><strong>Term weighting</strong>词汇权重</p><p>Tf-idf表示，概念在CS224N中学过。就是词频 - 逆文档频率系数。词频就是这个词在本文中出现的频率，频率越高越重要；逆文档系数是语料库总文数/包含该词的文树  再取对数，如果词语在各个文章中都有，例如“的”，它在本文中重要性为零，如果词语只在一篇文章中出现，也就是本文，那重要性很高，例如“对<strong>阿尔法脱氧核糖核酸逆转录蛋白</strong>的研究”中的这个BioNER。</p><p>词频数学定义</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_1dDqjUrMmt.png" alt=""></p><p>逆文档频率数学定义</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_5KlTJHKTsq.png" alt=""></p><p>总定义</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_TpTuxSwx34.png" alt=""></p><ul><li><p>词干化和构建stop list也对performance有帮助。</p></li><li><p>评价IR系统performance的benchmark无外乎是召回率和准确率，在前文有提及。</p></li></ul><h3 id="23-2）Factoid-Question-Answering"><a href="#23-2）Factoid-Question-Answering" class="headerlink" title="23.2）Factoid Question Answering"></a><strong>23.2</strong>）Factoid Question Answering</h3><p>基于事实的问答系统示例如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_8h2ysn9tiw.png" alt=""></p><p>The fundamental problem in factiod question answering is : <strong>Gap between the way that questions are posed and the way that answers are expressed in a text.</strong></p><p>问答系统架构思路是，先用开销较小的IR技术从document库中抽取一小部分相关文件，在这一步中，不使用开销大的NLP技术比如parsing或者role labeling处理海量ducuments，把这一小部分文件用IR技术抽取出来后，再用NLP技术进行处理。</p><p>下图演示了Arch，主要是三块,在接下来的session中逐一介绍</p><ul><li>Question Processing</li><li>Passage Retrieval</li><li>Answer Processing</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_plJqei3v64.png" alt=""></p><h4 id="23-2-1）Question-Processing-问题语句预处理"><a href="#23-2-1）Question-Processing-问题语句预处理" class="headerlink" title="23.2.1）Question Processing 问题语句预处理"></a><strong>23.2.1</strong>）Question Processing 问题语句预处理</h4><p>问题处理这一步分两个大目标，在架构图中可见。分别是对<strong>查询语句方程式化</strong>和对<strong>查询分类</strong>。</p><p><strong>查询语句方程式化 Query Formulation</strong></p><p>目标是从问题中提取出一系列单词which form an IR query,具体要提取成什么样的form视application而定。通常我们会移除stopword和疑问词（who,when,where,etc..）。然后对query使用基于叙词表的算法来扩充它（expansion），然后我们就能得到一个更大的keyword sets。很多系统都使用WordNet作为叙词表。</p><p>还有一种查询语句方程式化的方法是reformulation。也就是把疑问句变形为陈述句，如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_tFty9IZgjU.png" alt=""></p><p>问题分类<strong>Question Classification</strong></p><p>问题分类的依据是<strong>答案的种类</strong>。回答是“A person”和“A City”肯定不是一类。显而易见的思路是使用命名实体识别来做详细分类，但其实这种分类方式不够科学，性能表现更好的分类方式如下图，是人工设计的。而新的问题如何能按照如下的类别分好类？当然是采用反复提到的基于统计学的机器学习方法。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_stMfDtVi3V.png" alt=""></p><h4 id="23-2-2-Passage-Retrieval-段落检索"><a href="#23-2-2-Passage-Retrieval-段落检索" class="headerlink" title="23.2.2 Passage Retrieval 段落检索"></a><strong>23.2.2</strong> Passage Retrieval 段落检索</h4><p>本session介绍QA系统架构的第二大部分 段落检索</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_aAyDZJkhRN.png" alt=""></p><p>接收到上一session中的问题处理模块输出的query后，把这个query放进本模块的IR system中。也就是上图左一中，这个IR system可以是典型的IR引擎也可以是一个Web搜索引擎。总之他会输出出a set of docs.也就是开篇提到的使用IR方法而非NLP方法对文件库中的文件做一个开销较小的筛选，接下来对筛选出来的候选文件上开销大的工具精细处理。</p><p>虽然 a set of doc输出出来经过了相关度排序，但是排在首位的doc不一定就是需要的doc。所以接下来的任务是对选出来的set of doc做extract信息提取的工作。可以采用21章提到的 paragraph segmentation algorithm of type.</p><p>接下来要做的是段落检索，这一步中，我们已经把不相关的段落从doc中过滤掉了，然后把剩下的相关的段落按照相关性排序。然后根据如下的features，使用机器学习方法对剩下的段落重排序，然后挑选之。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_cBEmClxtbL.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_nMimu5gN6E.png" alt=""></p><h4 id="23-2-3-Answer-Processing-回答处理"><a href="#23-2-3-Answer-Processing-回答处理" class="headerlink" title="23.2.3 Answer Processing 回答处理"></a><strong>23.2.3</strong> Answer Processing 回答处理</h4><p>本session介绍QA系统架构的第三大部分回答处理。</p><p>![1578475614060](C:\Users\Bq Lion\AppData\Roaming\Typora\typora-user-images\1578475614060.png)</p><p>上个session已经把有用的段落排好序输进本模块了，本模块作用是提取出有用信息，组织成答案返回给用户。</p><p>两个算法可用于此模块：分别是<strong>答案类型模式提取算法answer-type pattern extraction</strong>和<strong>N-gram平铺算法</strong>。</p><p><strong>答案类型模式提取算法：</strong></p><p>利用了可能的回答类型信息 + 正则表达式。比如预计答案类型是“HUMAN”，那就用正则表达式在候选段落中搜索任何和HUMAN有关的句子并返回，如下例子中带下划线的词汇（NER和长度信息）就是期待的答案类型，被Answer Processing在candidate段落中检索到并返回。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_HaKyl4DMDB.png" alt=""></p><p>当然有些问题的期待回答并不是上述例子中的NER或者长度信息那么简单明了，而是一个较为复杂的句子，面对这种情况只能采用人工标注的正则表达式展开对candidate的搜索。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_661wCiHOAy.png" alt=""></p><p>模式提取算法和20章22章中介绍的关系抽取算法相似，模式抽取算法的目的是在学习<strong>一个特定的回答类型</strong>比如YEAR-OF-BIRTH与一个特定的问题的aspect之间的关系。可以把这个问题抽象为如下表述：</p><p>Relation between (PERSON-NAME/YEAR-OF-BIRTH)和（TERM-TO-BE-DEFINED/DEFINITION）。</p><p>这个表述和20章介绍的学习 Wordnet 句法集中的（下位词/下位词）之间的关系和</p><p>22章中介绍的学习词汇与词汇之间的ACE relations相似。</p><p>算法伪代码描述如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_hINbmRHkOK.png" alt=""></p><p>对于回答类型是BIRTH-OF-DATE的模式识别，学习到了如下两种模式</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_wVvo2x94bY.png" alt=""></p><p>其实仅借助命名实体探测和回答模式分离这两个技术是不够的，通常我们还使用采用了如下特征的分类器对candidate排序。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_kqaxyLw9Hp.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_fMOcu4fulA.png" alt=""></p><p>N-gram tiling算法通常仅用于web搜索。</p><p><strong>23.2.4</strong>对Factoid Answer系统的评价标准</p><p>采用平均导数排序，or MRR，也就是存在一个训练集里边包含了正确答案，将QA系统的回答与正确答案交叉验证。系统给的回答通常有五个，如果第一个就对了得分是1/1，第二个才对得分是1/2，第五个才对得分1/5，没有对的得分0，然后总体套上其他数学形式sum后求均值，表达式如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_DbnWMLsu2Q.png" alt=""></p><h3 id="23-3-Summarization文本总结"><a href="#23-3-Summarization文本总结" class="headerlink" title="23.3 Summarization文本总结"></a>23.3 Summarization文本总结</h3><p>目前为止，我们学习的算法都是对用户呈现整个文档（IR）或者是一个短的基于事实的回答（factoid answer phrase）。有些时候我们需要的是一个篇幅夹在整个文档/一句话 中间的文档，例如对一篇文档的总结，或者对几篇文档的总结（summarization）。</p><p>文本总结（text summarization）的正式定义：是对一个特定任务和用户，输出对一段删节版的text蒸馏出来的信息。如下几种summarization是研究热点。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_CQr6UaQhiI.png" alt=""></p><p>其实之前的IR和QA中的核心任务也包含在了文本总结里边，本质都是对信息的查找和遴选，只不过规模不同叫法不同而已。文本总结的任务通常分为三步：</p><ol><li>内容选择(content selection)：决定要从candidate文档中选择那些句子/语料做为原材料</li><li>Information ordering：如何组织排序刚才选择的原材料s</li><li>Sentence Realization：如何呈现出流利的candidate。</li></ol><h4 id="23-4）单个文件总结"><a href="#23-4）单个文件总结" class="headerlink" title="23.4）单个文件总结"></a><strong>23.4</strong>）单个文件总结</h4><p>继承紧接上文的三步骤，内容选择，信息ordering和句子实现。</p><p>总arch如下图</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_2y4pmgvOly.png" alt=""></p><p><strong>23.4.1</strong>无监督学习的内容选择任务</p><p>内容选择任务可以归类为 classification的机器学习任务。分类器classifier的任务是把candidate文档中每一句话都按照binary标签分类：important VS unimportant。</p><p>最早期的基于直觉设计的用于summary任务中选择句子的分类器就是筛选句子中是否含有大量<strong>关键词</strong>。如何界定关键词多少？建议使用TF-IDF或者对数似然ratio界定。</p><p>具体给句子打分到底有多关键的centrality(x)可以用数学表达式描述如下：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_e8jvcK6FD0.png" alt=""></p><p><strong>23.4.2</strong>基于修辞分析的无监督内容选择任务</p><p>importance veries from diff structures</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_R9MeB15UCu.png" alt=""></p><p>23.4.3监督机器学习的内容选择任务</p><p>也就是存在人手工标注的训练集，Ziff-Davis corpus for example。</p><p>​    <img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_FvEvwciSOi.png" alt=""></p><p><strong>23.4.4</strong>) 句子简化</p><p>句子简化也多是使用监督学习算法，对句子运行parser删除特定的语法单元。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_7ViipL2riN.png" alt=""></p><h3 id="23-5-多文件的总结Muti-DOC-Summarization"><a href="#23-5-多文件的总结Muti-DOC-Summarization" class="headerlink" title="23.5 多文件的总结Muti - DOC Summarization"></a>23.5 多文件的总结Muti - DOC Summarization</h3><p>想真正解决多文件总结任务距离还远得很，一个多文件总结应用场景是在对同一件新闻的各家媒体稿做总结输出。多文件的总结问题的处理架构和之前的单文件总一致，也是<strong>内容选择，信息排序，句子生成</strong>三大步骤。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_ughTbZwJTW.png" alt=""></p><p><strong>23.5.1）</strong>多文件总结中的内容选择模块</p><p>单文件总结中的内容选择，无监督学习和有监督学习都可以用。多文件的内容选择则不行，只能用无监督学习。</p><p>单文件总结与多文件总结的最大区别是<strong>冗余</strong>。所以多文件总结算法的一个重要任务就是避免冗余信息，输出一句新话时，要检查不要与其他已输出的话重复过多。</p><p>数学上的设计是generate一个冗余factor，给重复过多的句子增加罚项。一个可用的factor是MMR系统，也就是<strong>最大边际相关scoring系统。</strong>Sim（s,si）表示s句子和已经分类出来的si句子组之间有多大的相似性，λ是手动调节的重要性参数。</p><p><strong>23.5.2</strong>） 多文件总结中的信息ordering</p><p>把去除了冗余信息的句子们提取出来后，接下来要做的就是对这些信息做ordering，组织他们重新有序。放入具体的实例中就是，对同一个新闻事件的不同媒体新闻稿做总结。一个思路是使用时间序列排序（chronological ordering）。使用时间序列排序的缺点是生成的句子缺乏语序的<strong>一致性</strong>，这个不难理解，不同语篇按照时间顺序硬拼凑在一起必然缺乏一致性。为了解决硬拼缺乏一致性的问题，需要使用coreference-based algorithm.(基于共指算法)。共指算法使用了centering<strong>定心</strong>的概念。</p><p>centering<strong>定心</strong> 的概念是每个语段都有一个突出的实体，也就是<strong>focus焦点</strong>。定心的精髓是聚焦于<strong>焦点</strong>和<strong>焦点之前转换</strong>的<strong>句法实现</strong>。通常两个命名实体和实体间的转换被preferred。这样可以在ordering的时候输出更有一致性的语料。</p><p>一个常用的方法是，对每个句子分配一致性得分，一个between两句子的转换得分，通常包含了词汇的连贯性和基于实体的连贯性。有了这样的评分系统后，选择最优化所有local句子pair的distance的choosing order操作，是时分苦难的，可以转化为<strong>循环排序</strong>和<strong>旅行商问题</strong>，也就是NP完全问题。</p><p>如上文所讲，其实information ordering 和上个session的内容选择是完全分离的两个任务，有一种协同处理他们的模型是HMM模型，文件的隐状态是，内容的topics和sentences observations，for example,对于加州地震新闻报道文稿中，隐状态就是<strong>地震震级，地点，营救，伤亡</strong>。</p><p><strong>23.5.3</strong>）Sentence Realization</p><p>上个模块输出的句子，仍然存在缺乏一致性和语义不同的问题，需要做最后的调整，调整的手段是，使用一些rule，比如下文两个rule。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_LLVpr9e1yK.png" alt=""></p><p>如下一个最终调整实例</p><p>![1578563125318](C:\Users\Bq Lion\AppData\Roaming\Typora\typora-user-images\1578563125318.png)</p><h3 id="23-6-QA系统设计思路"><a href="#23-6-QA系统设计思路" class="headerlink" title="23.6 QA系统设计思路"></a>23.6 QA系统设计思路</h3><p>如何设计QA系统？首先，QA回答的内容必定是从文件库中搜索+整理出来的。按照Q搜索的工作接下来会详细介绍，是一种基于Query-focused 的Summarization；整理的工作就是上个session介绍的多文件整理的工作。</p><p>如何对着问题Q有目的性的搜索？我们可以在上个session中介绍的多文件总结系统上做一些调整，让这个系统在内容选择这个模块上，ranking各个内容的时候，偏重于让含有Q的关键词的句子，ranking的高一些。</p><p>另一个提高performance的思路是，对不同的提问分类，比如分成<strong>查询人物档案，查询药物，查询某物定义等</strong>，然后对这些不同的分类的提问，构建定制化的<strong>内容选择</strong>算法。</p><p>如下是几个常见的提问的definition</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_K4mMtos4p0.png" alt=""></p><p>Sum up，如下是基于<strong>问题导向搜索的，多文件整理的，QA系统架构</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_TDA0lvTTyZ.png" alt=""></p><p>左一模块，文件检索，L是问题长度，N是待查文件数量。返回1127个句子。</p><p>左二模块，谓词识别，这个是根据Q的导向来筛选1127个句子，输出9个根据词问题定制化的句子，和383个非定制化句子。</p><p>左三模块，数据分析，输入上模块383句子个非定制化句子，输出ordering后的句子。</p><p>左四模块，Answer生成。输入上模块的ordering之后的句子+第二个模块输出的9个定制化的句子，然后整理，输出一篇合格的answer。</p><p>具体实例如下：</p><p><strong>输入</strong>：What is a Water Spinach?</p><p><strong>输出</strong>:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_vsWyk0nwtw.png" alt=""></p><h3 id="23-7-文件总结的评估方法"><a href="#23-7-文件总结的评估方法" class="headerlink" title="23.7 文件总结的评估方法"></a>23.7 文件总结的评估方法</h3><p>收到BLEU启发，文件总结系统的评价方式多用ROUGE-2评估（Recall - Oriented Understudy For Gisting Evaluation）。数学定义如下；</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_obUcS6BCt1.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_24 - 对话agent</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_24%20-%20%E5%AF%B9%E8%AF%9Dagent/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_24%20-%20%E5%AF%B9%E8%AF%9Dagent/</url>
    
    <content type="html"><![CDATA[<h1 id="24-Dialogue-and-Conversational-Agents"><a href="#24-Dialogue-and-Conversational-Agents" class="headerlink" title="24 - Dialogue and Conversational Agents"></a>24 - Dialogue and Conversational Agents</h1><p>这一章介绍问答助手的基本结构+算法。Session24.1介绍人类对话的基本概念，如对话的交替，表达技巧，grounding，对话结构等。Session24.2介绍口语系统的组件和评价标准。Session24.5和Session24.6介绍信息状态架构和马尔科夫对话代理模型，以及高阶话题如BDI范式（belief - desire -intention）信念 - 渴望 - 意图范式。</p><h3 id="24-1-人类对话的Properties"><a href="#24-1-人类对话的Properties" class="headerlink" title="24.1 人类对话的Properties"></a>24.1 人类对话的Properties</h3><h4 id="24-1-1-Turns-and-turn-talking"><a href="#24-1-1-Turns-and-turn-talking" class="headerlink" title="24.1.1 Turns and turn-talking"></a>24.1.1 Turns and turn-talking</h4><p>人类对话的模式是一个人说完了之后另一人说，交替进行，通常情况下，两人对话的重叠部分不超过5%。两个人交替间的停顿时间在100ms左右。为了实现这种模式，人类对话通常有如下三个规律来规范交替的进行，非常显而易见。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_GQuE6iM6Bj.png" alt=""></p><p>通常一个比正常情况更长的听读怒会有额外的表达效果，如下所示一个长停顿代表了不想积极回应问题。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_RxTCJd6Uy7.png" alt=""></p><h4 id="24-1-2-Language-as-Action-Speech-Acts-语言是会产生后果的行动"><a href="#24-1-2-Language-as-Action-Speech-Acts-语言是会产生后果的行动" class="headerlink" title="24.1.2 Language as Action:Speech Acts 语言是会产生后果的行动"></a>24.1.2 Language as Action:Speech Acts 语言是会产生后果的行动</h4><p>语言能对现实世界产生具体影响。根据语言产生不同的影响，把语言分为如下三类：</p><ul><li>说话行为Locutionary act: 单纯说话，表达某个东西，传递某种意思。</li><li>施事行为Illocutionary act:提问，回答，许诺，通过话语影响听众，改变其观点和行动。</li><li>取效行为Perlocutionary act：故意改变听众的观点和行为，听众会采取具体的举措。</li></ul><p><strong>“外边真冷”</strong>这句话</p><ul><li><p>仅仅说出来听众不在意是说话行为</p></li><li><p>说出来了听众也觉得确实很冷想把窗户关上是施事行为</p></li><li><p>听众停了之后起身把窗户关上了是取效行为。</p></li></ul><p>speech act主要用来描述<strong>施事行为</strong>。细分为如下五类：</p><ul><li>论证:承诺某事就是这样的。（示意，论证，发誓，吹嘘，下结论等）</li><li>指令:让听众做某事。(要求，命令，请求，邀请，建议，恳求)</li><li>承诺:向听众承诺未来某事会发生（承诺，计划，发誓，打赌，不同意）</li><li>表达:向听众表达心理状态（感谢，抱歉，欢迎，谴责）</li><li>声明:为世界带来新状态（辞职，宣战，辞退，分手）</li></ul><h4 id="24-1-3-Language-as-Joint-Action-Grounding"><a href="#24-1-3-Language-as-Joint-Action-Grounding" class="headerlink" title="24.1.3 Language as Joint Action: Grounding"></a>24.1.3 Language as Joint Action: Grounding</h4><p>和独白不同，对话需要演讲者与听众保持持续的 common ground，听众需要不停给研究者反馈，代表自己听懂/没听懂。正如电梯按钮按完了会亮一样，信息发出者<strong>需要</strong>即时反馈。</p><p>以下五种反馈按照积极程度递增的顺序排序</p><ul><li>持续注意：听众显示他一直保持对演讲者的注意力</li><li>开启类似话题：听众开启类似话题</li><li>认可：听众点头，说对或者太好了之类肯定短语</li><li>示范：听众通过调整语序/套用研究者的发言，来示范给演讲者看</li><li>演示：听众逐字逐句演示给演讲者</li></ul><h4 id="24-1-4-Conversational-Struture"><a href="#24-1-4-Conversational-Struture" class="headerlink" title="24.1.4 Conversational Struture"></a>24.1.4 Conversational Struture</h4><p>了解了对话的一般结构才能设计对话系统。如下。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_lypkQUT0m9.png" alt=""></p><h4 id="24-1-5-Conversational-Implicature-会话含义"><a href="#24-1-5-Conversational-Implicature-会话含义" class="headerlink" title="24.1.5 Conversational Implicature 会话含义"></a>24.1.5 Conversational Implicature 会话含义</h4><p>目前为止我们只学习了对话系统的“基础设施”，还没有学习信息究竟是如何从研究者传递到听众的。17章有演示如何计算一句话的meaning。在对话中，通常meaning要比word所表达的本身含义再多出来（extend）一点点。如下一个例子，回答者并未直接回答问题的，但是传达出了enough info.</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_EdfV3Innth.png" alt=""></p><p>以下4个“最大”是指导会话含义理解</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_iEJLDOMIvj.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_tQ2PwVZI99.png" alt=""></p><h3 id="24-2-基础对话系统"><a href="#24-2-基础对话系统" class="headerlink" title="24.2 基础对话系统"></a>24.2 基础对话系统</h3><p>上文对人类对话有所介绍，下文将对对话系统的六个模块做逐一介绍。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_YPJpxMWo9o.png" alt=""></p><p>左一到左四好理解。右一用于控制对话domain，例如航班预定，生活信息查询等。右二是总控。</p><h4 id="24-2-1-ASR模块-语音识别模块"><a href="#24-2-1-ASR模块-语音识别模块" class="headerlink" title="24.2.1 ASR模块 -  语音识别模块"></a>24.2.1 ASR模块 -  语音识别模块</h4><p>在第九章中介绍过，语音识别模块输入的是音素，输出的是字符串。对于domain-based的对话系统，语音识别模块只需要辨识身后的自然语言理解模块能理解的特定领域语料即可。为此，商业对话系统基本都用<strong>基于有限状态语法规则的非概率语言模型</strong>。这些语法规则由人为规定，对所有系统可能understand的语言做出特定response。在Session24.3中我们会介绍适用于VoiceXML系统的人为构建的语法规则。</p><p>通常对话系统中的语言模型是要依赖<strong>对话state</strong>的。如果一个对话系统进行到提出问题“Which city are you departing from?”的时候，这个系统的语言识别模块只能识别后边这一句话的括号内内容“I (leave|depart)from [CITYNAME]”。</p><p>在对话系统期待用户说话的场景下，系统通常运用restrictive grammar，来限制对话的状态数量。</p><h4 id="24-2-2-NLU-模块-自然语言理解模块"><a href="#24-2-2-NLU-模块-自然语言理解模块" class="headerlink" title="24.2.2 NLU 模块 - 自然语言理解模块"></a>24.2.2 NLU 模块 - 自然语言理解模块</h4><p>如何实现理解自然语言？要借鉴22章提到的信息提取技术。也就是frame-and-slot技术。例如一个“帮助用户寻找合适航班”的系统，它的frame-with-slot就是如下这样的：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_YtyIywxGbT.png" alt=""></p><p>如何把句子拆解成frame需要的格式从而填补slot？使用一些手工制作的semantic grammars来parse句子。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_J9qCVHmSEk.png" alt=""></p><p>另一种办法是使用HMM的反编码技术，把句子当成发射值，待填补的slot当成隐状态。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_wmjBFvCtqm.png" alt=""></p><p>数学过程老生常谈，贝叶斯概率对换+分母可忽略+单独N概率用语言模型N-gram计算。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_IAtyar6XGU.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_O2o3TGwHWE.png" alt=""></p><h4 id="24-2-3-Generation-and-TTS-Components"><a href="#24-2-3-Generation-and-TTS-Components" class="headerlink" title="24.2.3 Generation and TTS Components"></a>24.2.3 Generation and TTS Components</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/Typora_HxU6DdyGmC.png" alt=""></p><p>输出模块，核心任务有两个，分别是<strong>what to say</strong>和<strong>how to say it</strong>。</p><p><strong>what to say</strong> 的任务用content planner实现，它决定了要给用户表达什么，是否要问问题，如何回答等等，这个<strong>content planner</strong>组件通常和<strong>Dialogue Manager</strong> merge在一起,we come back later on this.</p><p><strong>how to say it</strong>的任务用language generation模块生成。一种实现是用template - based generation,它有很多模块可被填充，如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_mJ9hbrSOcs.png" alt=""></p><p>还有一种实现how to say it的方法是使用自然语言生成。它由如下三个模块组成。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_o0136Nyaer.png" alt=""></p><h4 id="24-2-4-Dialogue-Manager-核心：对话管理模块"><a href="#24-2-4-Dialogue-Manager-核心：对话管理模块" class="headerlink" title="24.2.4 Dialogue Manager 核心：对话管理模块"></a>24.2.4 Dialogue Manager 核心：对话管理模块</h4><p>本模块接受识别模块输入的信息，把输出传递给上个session将的Generation模块。对话管理模块最常见的架构是有限状态-框架基础(finite-state,frame-based)的架构，将在本session介绍。下个session将讨论更强大的information-state dialogue manager（基于马尔科夫决策过程的概率版本的information-state manager）。最后介绍plan-based architecutes.</p><p>最简单的对话管理模块架构就是finite-state manager。如下图所示，对话由彼此相连的状态（system）完全控制，这个系统的工作模式就是连续问用户好几个问题，忽略掉用户说的其他任何内容，这种系统也叫<strong>system-initiative</strong> system。这种系统要求用户exactly回答它提出的任何问题，这会让对话非常生硬sometimes annoying。在设计这种finite-state系统的架构时，当然也可以在每个state上都链接一些subset of state，但是这样会使state数量爆炸，bad arch。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_ICuRoueyFG.png" alt=""></p><p>于是有了<strong>mixed initative</strong>，这种设计就不是完全系统主导了，而是用户和系统一起主导对话。这种系统的常见架构是frame-based,system问一些问题，然后提取用户的回答来填满slot。这种slot-用户answer的设计如下所示</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_AZioJu8Ck1.png" alt=""></p><p>这种系统会一直攒slot，直到攒够slot足够在database中做查询和返回后，返回给用户回答。如果用户同时回答两三个问题，那系统就是并行打开两三个slot来攒用户的信息。</p><h4 id="24-4-Dialogue-System-Design-and-Evaluation-对话系统设计思路"><a href="#24-4-Dialogue-System-Design-and-Evaluation-对话系统设计思路" class="headerlink" title="24.4 Dialogue System Design and Evaluation 对话系统设计思路"></a>24.4 Dialogue System Design and Evaluation 对话系统设计思路</h4><ol><li><p>研究用户和任务类型。</p><p>了解潜在用户画像，评估任务类型。常见的研究方法包括对类似系统调研，对潜在用户发调查问卷，研究类似的人类之间的对话等。</p></li><li><p>构建模拟和原型（simulationas and prototypes）</p></li><li><p>迭代地测试设计思路</p></li></ol><h3 id="24-5-Information-State-and-Dialogue-Acts"><a href="#24-5-Information-State-and-Dialogue-Acts" class="headerlink" title="24.5 Information-State and Dialogue Acts"></a>24.5 Information-State and Dialogue Acts</h3><p>目前我们介绍的frame-based 的对话系统都只能在限定的domain运行。因为语义解释和基于框架的语义生成过程都要根据slot-filling的需求开展。但是更实用的对话系统应该不仅仅是用来满足内部的插槽而已，而是能决定何时应该打断用户的话，在不清楚的地方能想用户提出更明确的问题，如何给出几个靠谱的suggestion等。所以这个session我们讲解一个更先进的架构，<strong>information - state arch</strong>下个session我们还会讲更深入的马尔科夫决策过程。本session讲解的information-state arch包括如下五个组件；</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_KJsYGsm55W.png" alt=""></p><p>第一个组件，信息状态，是一个高度抽象的概念，比上一session中介绍的有限状态机中的状态更复杂。它包含了语义上下文信息，两个对话者的共识，当下使用的模型，两个对话者的注意力等。</p><p>第二三组件一起讲，是一个对话act组件。他是speech act组件的一个扩展，以情感分析和dialogue act作为输入，以text/speech为输出。</p><p>最后，update rule组件用于更新information state。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_5ksCF31ab0.png" alt=""></p><h4 id="24-5-1-Using-Dialogue-Acts-对话行为"><a href="#24-5-1-Using-Dialogue-Acts-对话行为" class="headerlink" title="24.5.1 Using Dialogue Acts 对话行为"></a>24.5.1 Using Dialogue Acts 对话行为</h4><p>Dialogue Act是Speech Act的延伸，如下是scheduling domain（主要用于在某领域预定一个meeting）的18个Dialogue Act。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_i89KX4vp2C.png" alt=""></p><p>每个Act有如下四种大类</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_9xs5XUQMpq.png" alt=""></p><h4 id="24-5-2-解释对话行为"><a href="#24-5-2-解释对话行为" class="headerlink" title="24.5.2 解释对话行为"></a>24.5.2 解释对话行为</h4><p>我们如何解释一个对话行为？如何判断输入字段是一个Suggestion/Question/Statement ？如下是三种问题的示例。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_GfIZXDkpXs.png" alt=""></p><p>有些间接的speech act，比如一个看上去是Statement的句子实际是一个question等等。为了判断出一个句子到底是哪种act，需要引入supervised classification task，这个分类任务使用的feature是act的micro grammar,grammar 由如下三个组件构成。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_UMoQ4SAUM5.png" alt=""></p><ol><li>从单词上分析，一个“please”或者’’would you”通常一个Request的象征。“Are you”是一个YES-NO        questiond的象征，这种单词分析可以在Dialogue-Specific N-gram grammars中进行。</li><li>Prosody音韵分析:升降调/气息强弱通常可判断肯定或否定.</li><li>对话结构:yeah后边跟一个建议通常就是agreement等。</li></ol><p>总体来说，给定一个观测句子o，确定这个句子的对话act是多少的 概率由如下数学表达式定义，还是熟悉的贝叶斯对换位置公式。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_vtEvtiY9Eh.png" alt=""></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_22 - Information Extraction</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_22%20-%20Information%20Extraction/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_22%20-%20Information%20Extraction/</url>
    
    <content type="html"><![CDATA[<h1 id="22-Information-Extraction"><a href="#22-Information-Extraction" class="headerlink" title="22 - Information Extraction"></a>22 - Information Extraction</h1><p>本章主要概念：</p><ul><li>NER：命名实体识别</li><li>relation detection and classification：关系检测与分类</li><li>event detection and classification：事件检测与分类</li><li>temporal expression recognition：事件表达式识别</li><li>template filling：模板填充</li></ul><h3 id="22-1-NER-命名实体识别"><a href="#22-1-NER-命名实体识别" class="headerlink" title="22.1)NER : 命名实体识别"></a><strong>22.1</strong>)NER : 命名实体识别</h3><p>命名实体识别分类的举例</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_gAytSQ6xFK.png" alt=""></p><p><strong>22.1.2</strong>）NER as Sequence Labeling</p><p>标准的命名实体识别的步骤是使用word-by-word sequence labeling任务。其实进行NER的方法与第五章的POS tagging和十三章的syntactic chunking方法相同。</p><p>PS：提一下第五章的POS tagging：使用的还是HMM base 的 维特比算法（decoding）。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/Typora_2EY2kXShLb.png" alt=""></p><p>问题的本质如下所示，观察到句子/词语，猜测对应的词性/NER类型。也就是观察到结果，猜测其隐状态。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/Typora_mhyTE7MGA1.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/Typora_VaaEPeouLb.png" alt=""></p><p>具体的 word-by-word IOB-style tagging技术细节会在后边的session详细介绍。</p><p><strong>22.1.3</strong>） Evaluation of Named Entity Recognition</p><p>标准的<strong>训练NER system</strong>的步骤如下：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_CkvnPGXNZF.png" alt=""></p><p>首先是人工标注，构建test set，然后构建training set，然后训练之（MEMM,CRF,SVM,HMM等）。</p><p>NER系统的评价方法是F1 measure，和Chunking-System Evaluation中的F1 measure一致。F1 measure就是一个对召回率和准确率之间的取舍，控制取舍的参数是β，β&gt;1偏好召回率，β&lt;1偏好准确率，如下：</p><p>![1578297475664](C:\Users\Bq Lion\AppData\Roaming\Typora\typora-user-images\1578297475664.png)</p><p>其中P是precision，准确率，R是召回率，recall，具体定义如下：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_9o9lFPvgDg.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_SCyRkoJMHq.png" alt=""></p><p><strong>22.1.4</strong>）NER架构</p><ul><li>首先，使用高准确率的rule-based 来tag不模糊的实体。</li><li>搜索substring，which matches之前探测到的命名实体识别。这里使用的方法是19章中提到的基于概率的字符串匹配方法。</li><li>查阅特定domain的 NER list</li><li>使用基于概率的sequence labeling techs</li></ul><h3 id="22-2-关系探测和分类"><a href="#22-2-关系探测和分类" class="headerlink" title="22.2 关系探测和分类"></a>22.2 关系探测和分类</h3><p>命名实体之间必定存在关系，厘清NER之间的关系就能基本上完成语义理解。NER之间的关系如下所示，一部分，并列，树级关系等等。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_0x9NRGiPHR.png" alt=""></p><p><strong>22.2.1</strong>）关系探测中的监督学习方法</p><p>监督学习方法肯定已经不陌生，无外乎就是给定一个人工标注的test集，然后对剩下的没有标注的训练集进行监督学习（无外乎是一种统计学上的模式识别并模仿之）。</p><p>最straghtforward的关系探测的方法是：</p><p>1.列出本句子中任何两对关系 </p><p>2.遍历所有的关系，逐一作出判断</p><p>在第二步中，遍历所有关系，作出判断肯定是使用监督学习的机器学习模型。想要让模型工作的好，特征工程非常重要。构建优秀的特征分为如下三个方面：</p><ul><li>feature of the named entities<ul><li>两个命名实体的种类</li><li>两个种类的级联</li><li>参数的中心词</li><li>参数的的词袋模型</li></ul></li><li>the words in the text<ul><li>词袋和bi-gram词袋 between 两个命名实体</li><li>两个命名实体的词干</li><li>实体上下文的 词干和word</li><li>参数的词距</li><li>参数的实体数量</li></ul></li><li>the syntactic structure<ul><li>在组成结构中是否存在特定的结构</li><li>块 基本路径</li><li>块头部的 Bags</li><li>树的依赖路径</li><li>成分树路径</li><li>参数见的树距离</li></ul></li></ul><p>三个参数的实例如下所示</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_DgRYDGFRft.png" alt=""></p><h3 id="22-3-Temporal-and-Event-Processing-时间和事件处理"><a href="#22-3-Temporal-and-Event-Processing-时间和事件处理" class="headerlink" title="22.3 Temporal and Event Processing(时间和事件处理)"></a>22.3 Temporal and Event Processing(时间和事件处理)</h3><p>至此，我们了解了NER提取和分析他们之间的关系。还有一个非常重要的概念将在本session中介绍：“时间实体”。在一些NLP应用中，对时间这个概念的处理至关重要（其实想要厘清语义，尤其是有一定篇幅的语言的语义，把时间线弄清楚必不可少）。</p><p><strong>22.3.1</strong>）时间表达式的识别</p><p>关于<strong>绝对时间</strong>，<strong>相对时间</strong>，<strong>时间间隔</strong>。是表达时间的三个重要概念。也就是坐标-坐标的加减-基于坐标的距离。如下图片。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_bXYfeu8bND.png" alt=""></p><p>时间实体识别的具体方法有如下三个：</p><ul><li>rule-based system</li><li>基于统计的机器学习（sequence classifiers based on token by tokne IOB enconding）</li><li>作为语义角色标记的组成分类</li></ul><p>对于上文提到的第二个，IOB encoding，训练IOB style 的时间表达式的taggers的典型特征如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_X0B2ceJvbr.png" alt=""></p><p>对时间实体的正则化</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_2fh07ii32E.png" alt=""></p><p><strong>22.3.3</strong>）事件实体的探测和分析</p><p>在英语中，一般时间都和动词相关，几乎都是动词后边跟着介绍事件（event）。rule-based和基于统计的机器学习方法都被应用到了事件实体的探测和分析上。这两种方法都借鉴了句子中的POS tagging，和动词的分类等等。</p><p>当事件（event）和时间因素（temporal expression）都被探测到后，接下来的逻辑任务就是把这些时间信息何来地放到timebank中排序。</p><p><strong>22.3.4</strong>）TimeBank</p><p>TimeBank of Allen`s(1984) relations, as follows:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_c0fOlkkKNn.png" alt=""></p><p>一个抽取事件+时间的句例：</p><p>“Delta Ari Lines <strong>soared[e1]</strong> 33% to a record[t1] in the fiscal first quarter,<strong>bucking[e2]</strong> the industry trend toward <strong>declining[e3]</strong> profits”</p><p>事件有三个，通常用动词标注，上文中黑体标注。</p><p>时间有两个，分别是record和fiscal first quarter。</p><p>事件之间的关系：</p><ul><li>soar[e1] is <strong>before</strong> record[t1]</li><li>soar[e1] is <strong>simultaneous</strong> with the bucking[e3]</li><li>declining[e3] includes soaring[e1]</li></ul><h3 id="22-4-Template-Filling模板填充"><a href="#22-4-Template-Filling模板填充" class="headerlink" title="22.4 Template Filling模板填充"></a>22.4 Template Filling模板填充</h3><p>模板由各个slot插槽组成，插槽中填充的是命名实体。如下是一个例子</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_IiChHL8sBn.png" alt=""></p><p><strong>22.4.1</strong>）使用统计方法的模板填充</p><p>统计方法中的序列标签问题，是一个解决模板填充的有效方法。序列标签标注模型会被训练如何正确地把候选项分配给slot。（多分类问题HMM等，24章有更深入的讨论）。</p><p><strong>22.4.2</strong>）使用Finite-state有限状态机的模板填充</p><p>基于有限状态机的模板填充的典型步骤：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_6LsKRrb3em.png" alt=""></p><p>实例</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_n0n7Ao8jNm.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_ZNfJVh1wlX.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_bW2awuxgOK.png" alt=""></p><h3 id="22-5-生物医学命名实体识别"><a href="#22-5-生物医学命名实体识别" class="headerlink" title="22.5 生物医学命名实体识别"></a>22.5 生物医学命名实体识别</h3><p>BioNER的需求很大，如下所示，新词逐年稳步增长</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_xOjwRd2yNs.png" alt=""></p><p>生物医学的命名实体识别要难于一般的命名实体识别，一般的命名实体识别仅仅需要识别<strong>人名</strong>，<strong>组织名</strong>和<strong>地名</strong>，而BioNER在识别项上分类更多，如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_CWQaKmMnPk.png" alt=""></p><p>基本上所有NER领域内的武器都被应用到了BioNER上。</p><p>找到并正则化所有的生物医学命名实体，只是厘清各实体在文本中扮演着怎样角色的初级步骤。</p><p>其他的厘清实体在文本中扮演怎样角色的方法，目前热门的技术还有两个，分别是<strong>遍历各实体对关系</strong>和<strong>搞清各实体与中心事件的关系</strong>。</p><p><strong>遍历各个实体对的关系</strong>举例</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_KMvWk9IVa6.png" alt=""></p><p>如上提到的命名实体之间的关系就是<strong>疾病</strong>被<strong>药物治疗</strong>，寻找这种关系的方法上文已经提到，可以使基于规则的有限状态机，也可以是基于统计学的HMM机器学习方法。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_2 - Regular expressions and automata</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_2%20-%20Regular%20expressions%20and%20automata/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_2%20-%20Regular%20expressions%20and%20automata/</url>
    
    <content type="html"><![CDATA[<h1 id="2-Regular-expressions-and-automata"><a href="#2-Regular-expressions-and-automata" class="headerlink" title="2 - Regular expressions and automata"></a>2 - Regular expressions and automata</h1><p><strong>2.2</strong>) Relationship between FSA and RE</p><p>Any regular expression(RE) can be implemented as a finite state automata(FSA),symmetrically,any finite-state automata can be described with a regular expression.</p><p>Both RE and FSA can be used to describe regular languages:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191220_1321_39_996.png" alt=""></p><p><strong>Using FSA to understand sheep talk</strong></p><p>sheep language can be defined as any string from the following set:</p><p>baa!</p><p>baaaa!</p><p>baaaaa!</p><p>baaaaaa!</p><p>baaaaaaa!</p><p>baaaaaaaa!</p><p>baaaaaaaaa!</p><p>…</p><p>Sheep language can be described as following finite state automata(FSA):</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191220_1335_21_266.png" alt=""></p><p>Same FSA adding with fail state:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191220_1359_38_175.png" alt=""></p><p>Another example FSA of dollar changing；</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191220_1421_18_177.png" alt=""></p><h4 id="Idea-flash"><a href="#Idea-flash" class="headerlink" title="Idea flash:"></a>Idea flash:</h4><h4 id="The-RNN-model-in-CS224N-is-a-implement-of-FSA-Especially-the-circle-on-node-Somehow-I-feel-if-diving-deep-there-is-not-as-much-difficult-things-as-I-used-to-image-The-guys-who-inviting-models-and-theroy-are-also-HUMAN"><a href="#The-RNN-model-in-CS224N-is-a-implement-of-FSA-Especially-the-circle-on-node-Somehow-I-feel-if-diving-deep-there-is-not-as-much-difficult-things-as-I-used-to-image-The-guys-who-inviting-models-and-theroy-are-also-HUMAN" class="headerlink" title="The RNN model in CS224N is a implement of FSA???????????Especially the circle on node.Somehow I feel if diving deep,there is not as much difficult things as I used to image.The guys who inviting models and theroy are also HUMAN."></a>The RNN model in CS224N is a implement of FSA???????????Especially the circle on node.Somehow I feel if diving deep,there is not as much difficult things as I used to image.The guys who inviting models and theroy are also HUMAN.</h4><p><strong>Summary:</strong></p><ul><li>most important fundamental concept in NLP:  <strong>finite automaton</strong></li><li>the practical tool based on automation: <strong>regular expression</strong></li><li>Basic operations in regular expressions include concatenation of symbols, disjunction<br>of symbols ([], |, and .), counters (*, +, and {n,m}), anchors (ˆ,$) and precedence operators ((,)).</li><li>Any regular expression can be realized as a finite state automaton (FSA).</li></ul>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_10 - Automatic Speech RecongnitionAdvanced Topics</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_10%20-%20Automatic%20Speech%20RecongnitionAdvanced%20Topics/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_10%20-%20Automatic%20Speech%20RecongnitionAdvanced%20Topics/</url>
    
    <content type="html"><![CDATA[<h1 id="10-Auto-Speech-Recognition-Advanced-Topic-：语音转文字进阶话题"><a href="#10-Auto-Speech-Recognition-Advanced-Topic-：语音转文字进阶话题" class="headerlink" title="10. Auto Speech Recognition Advanced Topic ：语音转文字进阶话题"></a>10. Auto Speech Recognition Advanced Topic ：语音转文字进阶话题</h1><p>之前企图对输入的语音转换成音素的处理办法是：构建一个由全体语言组成的HMM状态网络，然后在网络中采用维特比算法进行全局搜索。这种算法太expensive了。</p><p>改进思路是采用多路编码的decoding技术，使用新的<strong>上下文相关声学模型(triphone)</strong>。本章还会介绍<strong>判别训练(discriminative training)</strong>和模型的一些变体；</p><p><strong>10.1</strong>）多路编码decoding : N-Best List and Lattices</p><p>首先，维特比算法在进行对语音输入的decoding的时候，有如下两个问题：</p><ul><li>在应对一词多音/一音多词的语言时，维特比算法表现很差</li><li>维特比算法很难take advantage of 复杂的语言模型：2-gram还行，3-gram就不行了。因为3-gram violates the <strong>dynamic programming invariant</strong></li></ul><p>改进如上两个问题的思路有：</p><ul><li>改进维特比算法，将原本只返回单一值，变成返回多值。以改进一词多音的问题。</li><li>使用其他的的decoding算法，比如<strong>stack decoder</strong>,或者<strong>A* decoder</strong>。</li></ul><p>Multiple-decoding：</p><p>N-best list：先使用一个开销小的、简单的语言模型处理语音输入，然后输出一个N-best句子，然后将这个N-best句子输入到一个开销大的、复杂的模型中去。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_ZYnDyJFty2.png" alt=""></p><p>N-best句子如下图所示，直接处理这个N-best句子也较为费劲，可以把N-best句子转换成Lattice格式。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_3eoTFxVLf8.png" alt=""></p><p>Lattice</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_CVXNZkamQs.png" alt=""></p><p>And，Lattice可以转换成有限状态机</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_4jl5MpmHUs.png" alt=""></p><p><strong>10.2</strong>) A* （Stack） Decoding</p><p><strong>10.3</strong>) 上下文声学模型：Triphones HMM</p><p>A tripphones HMM model represent a phone in a particulaar left and right context.For example,   triphone[y-eh+l] means [eh] in the middle of [y] and [l].</p><p>带了上下文的声学（音素）模型比单独的音素模型要更精细一些。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_b97JYOwns2.png" alt=""></p><p>在上图决策树中可以看到，同一个“<strong>ih</strong>”可以有不同的triphone。</p><p>下图是triphone的训练过程，（1）（2）步是将iy音素复制进triphone，（3）是连接（4）是将其扩张至GMM高斯模型。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_J9c2W8kdKi.png" alt=""></p><p><strong>10.4</strong>）判别训练（discriminative training）</p><p>以下用<img src="https://www.zhihu.com/equation?tex=X" alt="[公式]">)代表训练数据中的语音信号，<img src="https://www.zhihu.com/equation?tex=W" alt="[公式]">)代表训练数据中的文本，<img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]">代表语音模型（acoustic model）的参数。语言模型（language model）是固定的。</p><p>最大似然训练法（maximum likelihood, ML）的目标函数是这样的：<br><img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D_%7B%5Ctext%7BML%7D%7D+%3D+%5Carg%5Cmax_%5Ctheta+P_%5Ctheta%28X%7CW%29" alt="[公式]"></p><p>而区分性训练（discriminative training, DT）的目标函数是这样的：<br><img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D_%7B%5Ctext%7BDT%7D%7D+%3D+%5Carg+%5Cmax_%5Ctheta+P_%5Ctheta%28W%7CX%29" alt="[公式]"></p><p>区别在于条件概率不同。ML中，只要训练文本产生训练语音的概率大就行了；而DT要求的是训练语音对应训练文本的概率大，换句话说，就是要训练文本产生训练语音的概率，与其它文本产生训练语音的概率之差大。对DT的目标函数用一次贝叶斯公式就能看出这一点：<br><img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D_%5Ctext%7BDT%7D+%3D+%5Carg+%5Cmax_%5Ctheta+%5Cfrac%7BP_%5Ctheta%28X%7CW%29+P%28W%29%7D%7BP_%5Ctheta%28X%29%7D+%3D+%5Carg+%5Cmax_%5Ctheta+%5Cfrac%7BP_%5Ctheta%28X%7CW%29+P%28W%29%7D%7B%5Csum_w+P_%5Ctheta%28X%7Cw%29+P%28w%29%7D" alt="[公式]"><br>分子上的<img src="https://www.zhihu.com/equation?tex=P_%5Ctheta%28X%7CW%29" alt="[公式]">，正是ML的目标函数；而分母则是所有文本（包括训练文本和它的所有竞争者）产生训练语音的概率的（按语言模型加权的）和。</p><p>由于分母上要枚举所有可能的文本并不现实，所以实际中，一般是用一个已有的ML训练的语音系别系统对训练语音做一次解码，得到n-best list或lattice，用这里面的文本来近似分母上的求和。n-best list或lattice中包含了训练文本的足够接近的竞争者。</p><p>把DT的目标函数取个对数，可以得到：</p><p>右边第一项</p><p>是常数，可忽略；第二项是ML的目标函数的对数；第三项的形式与第二项有相似之处。</p><p>ML训练问题一般是用EM算法来解决的。DT训练多了第三项，同样有Generalized EM算法来求解。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_11 - Computational Phonology</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_11%20-%20Computational%20Phonology/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_11%20-%20Computational%20Phonology/</url>
    
    <content type="html"><![CDATA[<h1 id="11-Computational-Phonology"><a href="#11-Computational-Phonology" class="headerlink" title="11. Computational Phonology"></a>11. Computational Phonology</h1><p>本章较艰涩，对NLP发文帮助不大，Skip，以后需用再补。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_12 - 16,17 - 21语法学，语义语用学</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_12%20-%2016,17%20-%2021%E8%AF%AD%E6%B3%95%E5%AD%A6%EF%BC%8C%E8%AF%AD%E4%B9%89%E8%AF%AD%E7%94%A8%E5%AD%A6/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_12%20-%2016,17%20-%2021%E8%AF%AD%E6%B3%95%E5%AD%A6%EF%BC%8C%E8%AF%AD%E4%B9%89%E8%AF%AD%E7%94%A8%E5%AD%A6/</url>
    
    <content type="html"><![CDATA[<h1 id="12-16语法学，17-21语义语用学"><a href="#12-16语法学，17-21语义语用学" class="headerlink" title="12 - 16语法学，17 - 21语义语用学"></a>12 - 16语法学，17 - 21语义语用学</h1><p>以上几章主要侧重于对英语这门语言本身的讲解，对发表论文帮助不大。先跳过，回头需用再补。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_1 - Introdution</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_1%20-%20Introdution/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_1%20-%20Introdution/</url>
    
    <content type="html"><![CDATA[<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h1><p><strong>1.1</strong>) Required knowledge for NLP:</p><ul><li>Phonetics and Phonology : knowledge about linguistic sounds</li><li>Morphology : knowledge of meaningful components of words</li><li>Syntax :knowledge of the structural relationships between words</li><li>Pragmatics :knowledge of the relationship of meaning to the goals and intentions of speaker(what is )</li><li>Semantics :knowledge of meaning(what is said)</li><li>Discourse : knowledge about linguistic units larger than a single utterance</li></ul><p><strong>1.2</strong>) Key task : Disambiguation at variety level </p><p>Because of ambiguous,A sentence “<strong>I make her duck</strong>“can have so much meanings as behind:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191219_1519_21_672.png" alt=""></p><p><strong>Models</strong> and <strong>Algorithms</strong> are introduced to do disambiguating.</p><table><thead><tr><th>Problem solved</th><th>Method</th></tr></thead><tbody><tr><td>duck is a verb or noun?</td><td>part-of-speech tagging</td></tr><tr><td>make means “create” or “cook”?</td><td>word sense disambiguation</td></tr><tr><td>“her” and “duck” same entity?</td><td>probabilistic parsing</td></tr><tr><td>sentence is a statement or quesition?</td><td>speech act interpretation</td></tr></tbody></table><p><strong>1.3</strong>）Toolkits</p><p>This behind are all useful models</p><ul><li>State machine[状态机的概念见单独文件]<ul><li>finite-state automata</li><li>finite-state transducer</li></ul></li><li>formal rule system<ul><li>regular grammars</li><li>regluar relations</li><li>context-free grammars</li><li>feature-augmented grammars</li></ul></li><li>first order logic <ul><li>lambda-calculus</li><li>features-structures</li><li>semantic-primitives</li></ul></li><li>probabilisitc model – augment_all_above<ul><li>Hidden Markov Models</li></ul></li><li>vector-space model</li><li>machine learning model<ul><li>classifiers<ul><li>decision trees</li><li>support vector machines</li><li>Gaussian Mixture Models</li><li>Logistic regression</li></ul></li><li>sequence models<ul><li>HMM</li><li>Maximum Entropy Markov Model</li><li>Conditioanl Random Fields</li></ul></li></ul></li></ul><p><strong>1.7</strong>)Summary</p><p><strong>Conference:</strong></p><ul><li>NLP:<ul><li>ACL,EACL,NAACL</li><li>SIGs</li><li>COLING</li><li>CoNLL</li><li>EMNLP</li></ul></li><li>AI work for NLP <ul><li>AAAI</li><li>IJCAI</li></ul></li><li>Speech recogniton<ul><li>ICSLP</li><li>EUROSPEECH</li><li>IEEE ICASSP</li><li>SIGDial</li></ul></li></ul><p><strong>Book</strong>:</p><ul><li><p>Manning and Sch¨utze (1999) (Foundations of Statistical Language Processing)  :</p><p> focuses on statistical models of tagging, parsing, disambiguation,collocations, and other areas.</p></li><li><p>Charniak (1993) (Statistical Language Learning) :</p><p>an accessible, though older and less-extensive, introduction to similar material.</p></li><li><p>Manning et al. (2008) focuses on information retrieval, text classification, and clustering. </p></li><li><p>NLTK,the Natural Language Toolkit (Bird and Loper, 2004):</p><p>a suite of Python modules and data for natural language processing, together with a Natural Language Processing book based on the NLTK suite</p></li><li><p>Pereira and Shieber (1987) gives a Prolog-based introduction to parsing and interpretation. </p></li><li><p>Russell and Norvig (2002) is an introduction to artificial intelligence that includes chapters on<br>natural language processing. </p></li><li><p>Partee et al. (1990) has a very broad coverage of mathematical linguistics. </p></li><li><p>Grosz et al. (1986) (Readings in Natural Language Processing)</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1.科研/课题组信息搜集</title>
    <link href="undefined2020/02/24/1.%E7%A7%91%E7%A0%94/%E8%AF%BE%E9%A2%98%E7%BB%84%E4%BF%A1%E6%81%AF%E6%90%9C%E9%9B%86/"/>
    <url>2020/02/24/1.%E7%A7%91%E7%A0%94/%E8%AF%BE%E9%A2%98%E7%BB%84%E4%BF%A1%E6%81%AF%E6%90%9C%E9%9B%86/</url>
    
    <content type="html"><![CDATA[<h1 id="NLP课题组信息搜集20191218"><a href="#NLP课题组信息搜集20191218" class="headerlink" title="NLP课题组信息搜集20191218"></a>NLP课题组信息搜集20191218</h1><p>作者：cstghitpku</p><p>链接：<a href="https://zhuanlan.zhihu.com/p/48529628" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/48529628</a></p><p>来源：知乎</p><p>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p><p>根据这几年的积累，整理了一份国内外学术界和工业界的牛人和大牛团队，供大家申请硕士、博士、博士后和找工作参考。</p><p>学校（排名不分先后）：</p><p><a href="https://link.zhihu.com/?target=http%3A//ir.hit.edu.cn/">哈工大社会计算与信息检索实验室</a>：刘挺老师坐镇，教师包括：秦兵、张宇、车万翔、赵妍妍、刘铭、张伟男、丁效等老师，实验室共7个组，另外王海峰老师也是实验室兼职博导。</p><p><a href="https://link.zhihu.com/?target=http%3A//insun.hit.edu.cn/">哈工大智能技术与自然语言处理实验室</a>：王晓龙老师坐镇，教师包括刘秉权、刘远超、孙承杰等老师</p><p><a href="https://link.zhihu.com/?target=http%3A//mitlab.hit.edu.cn/">哈工大机器智能与翻译研究室</a>：赵铁军老师坐镇，教师包括杨沐昀、郑德权、徐冰老师等，另外周明老师是实验室兼职博导。</p><p><a href="https://link.zhihu.com/?target=http%3A//icrc.hitsz.edu.cn/index.htm">哈工大深圳智能计算研究中心</a>：王晓龙老师坐镇，包括陈清才、汤步洲、徐睿峰、刘滨等老师，实力很强。</p><p><a href="https://link.zhihu.com/?target=http%3A//www.hitsz-hlt.com/">哈工大深圳人类语言技术组</a>：徐睿峰老师坐镇，情感原因发现做的比较好。</p><p>哈工大另外做NLP的老师包括：关毅、王轩等。</p><p><a href="https://link.zhihu.com/?target=http%3A//nlp.csai.tsinghua.edu.cn/site2/index.php/zh">清华大学自然语言处理与社会人文计算实验室</a>：孙茂松老师坐镇，包括刘洋、刘知远等老师。论文发的非常多。</p><p><a href="https://link.zhihu.com/?target=http%3A//coai.cs.tsinghua.edu.cn/">清华大学交互式人工智能（CoAI）课题组</a>：朱小燕老师坐镇，包括黄民烈等老师。Dialogue System做的非常好，论文非常多。</p><p><a href="https://link.zhihu.com/?target=http%3A//www.thuir.cn/">清华大学智能技术与系统国家重点实验室信息检索课题组</a>：马少平老师坐镇，包括张敏、刘奕群等老师。信息检索做的非常好，论文非常多，前段时间刚拿了CIKM唯一的最佳论文（因为一作是学生，跟最佳学生论文合二为一了）。</p><p>清华大学另外做NLP的老师还有李涓子、唐杰、朱军等老师，李老师知识图谱做得好，唐老师数据挖掘（尤其是学者画像）做得好，朱老师偏向机器学习和贝叶斯等做的很好。</p><p><a href="https://link.zhihu.com/?target=http%3A//klcl.pku.edu.cn/">北京大学计算语言学教育部重点实验室</a>：教师包括：王厚峰、万小军、常宝宝、李素建、孙栩、严睿、穗志方、吴云芳等（包含其他实验室的老师）。万老师、李老师、常老师等发论文很多。</p><p><a href="https://link.zhihu.com/?target=http%3A//www.icst.pku.edu.cn/lcwm/index.php%3Ftitle%3D%25E9%25A6%2596%25E9%25A1%25B5">北京大学语言计算与互联网挖掘研究组</a>：<a href="https://link.zhihu.com/?target=http%3A//www.icst.pku.edu.cn/lcwm/wanxj/">万小军</a>老师、孙薇薇老师。万老师主要做自动摘要、文本生成、情感分析与计算等，论文非常多。</p><p>中科院NLP组主要集中在自动化所模式识别国家重点实验室下属的<a href="https://link.zhihu.com/?target=http%3A//nlpr-web.ia.ac.cn/cip/introduction.htm">中文信息处理研究组</a>，另外计算所有刘群老师组和软件所也有孙乐老师做。具体老师包括刘群、宗成庆、赵军、孙乐、王斌、徐君、张家俊、刘康、韩先培、何世柱等老师。论文非常多。</p><p><a href="https://link.zhihu.com/?target=http%3A//nlp.fudan.edu.cn/">复旦大学自然语言处理组</a>：黄萱菁、邱锡鹏等老师，发论文很多。</p><p><a href="https://link.zhihu.com/?target=http%3A//kw.fudan.edu.cn/">复旦大学知识工场</a>：肖仰华老师知识图谱做的非常好，论文发的很多。</p><p><a href="https://link.zhihu.com/?target=http%3A//nlp.suda.edu.cn/index.html">苏州大学自然语言处理组</a>：做机器翻译、情感分析、信息抽取等，论文发的很多。教师包括张民、周国栋、姚建民、李正华、熊得意、李军辉、洪宇、陈文亮等老师。其中张老师、姚老师、李老师都是哈工大毕业的，张老师也是哈工大的兼职博导，论文很多。</p><p><a href="https://link.zhihu.com/?target=http%3A//www.nlplab.com/">东北大学自然语言处理实验室</a>：机器翻译做的非常好，还成立了自己的公司，对外合作很多。姚天顺老师是创始人，朱靖波老师坐镇，教师包括肖桐、任飞亮、张春良、王会珍等老师。</p><p>另外东北大学的王大玲老师、冯时老师情感分析做的不错。</p><p>浙江大学：陈华钧、赵洲等老师，陈老师知识图谱做的很厉害。</p><p>中国人民大学：文继荣、赵鑫、徐君、窦志成等老师。文老师现在是院长，之前在MSRA，信息检索非常厉害。</p><p>上海交大：<a href="https://link.zhihu.com/?target=http%3A//bcmi.sjtu.edu.cn/~zhaohai/">赵海</a>老师，主要做机器翻译、句法分析等。</p><p>东南大学：漆桂林老师，知识图谱做的很厉害。</p><p><a href="https://link.zhihu.com/?target=http%3A//ir.dlut.edu.cn/">大连理工信息检索实验室：</a>林鸿飞老师坐镇，包括杨志豪、王健、张绍武、孙媛媛、张冬瑜、杨亮等老师。主要做信息检索，隐喻、幽默等语料库做的非常好。</p><p>大连理工大学：<a href="https://link.zhihu.com/?target=http%3A//faculty.dlut.edu.cn/dlut_nlp/zh_CN/index.htm">黄德根</a>老师团队，擅长词法分析、命名实体自动识别、短语自动识别、中日机器翻译、社交媒体文本处理等NLP研究。近年来围绕词向量、NLP深度学习、神经网络机器翻译等开展研究。</p><p>西湖大学：张岳老师，之前在新加坡，论文发的非常非常非常多，剑桥2016年统计的全世界发论文的数量好像排第二。</p><p><a href="https://link.zhihu.com/?target=http%3A//nlp.nju.edu.cn/">南京大学自然语言处理研究组</a>：包括陈家俊、<a href="https://link.zhihu.com/?target=http%3A//cs.nju.edu.cn/daixinyu/">戴新宇</a>、黄书剑等老师。</p><p>天津大学：<a href="https://link.zhihu.com/?target=http%3A//cs.tju.edu.cn/faculty/zhangpeng/">张鹏</a>老老师，信息检索做的很好。</p><p>北京理工大学：<a href="https://link.zhihu.com/?target=http%3A//cs.bit.edu.cn/szdw/jsml/js/hhy/index.htm">黄河燕</a>老师、张华平老师。黄老师是北京理工大学计算机学院院长，主要研究机器翻译，担任好几个副理事长，享受国务院特殊津贴。<a href="https://link.zhihu.com/?target=http%3A//www.nlpir.org/">中科院的自然语言处理工具包</a>就是张老师做的，另外跟刘群老师合作发了不少论文，我之前工作时实习生开发的NER就是借鉴的他的层叠马尔可夫模型而二次开发和优化的。</p><p>武汉大学语言与信息研究中心：姬东鸿、李晨亮等老师。</p><p><a href="https://link.zhihu.com/?target=http%3A//nlp.xmu.edu.cn/">厦门大学智能科学与技术系自然语言处理实验室</a>：包括史晓东等老师，主要做机器翻译、知识图谱、信息抽取等。</p><p>昆明理工大学：<a href="https://link.zhihu.com/?target=http%3A//222.197.200.10/MHWZ/MHWQTGL/jslist.do%3Fzgh%3Dzdh001%26lmlxdm%3D02%26lmlbdm%3D0204">余正涛</a>老师团队，有100多人，主要做信息检索、机器翻译和智能系统与决策分析。</p><p>山东大学：<a href="https://link.zhihu.com/?target=http%3A//ir.sdu.edu.cn/~liqiangnie/index.html">聂礼强</a>老师，信息检索做的非常好，论文很多。之前在新加坡，新加坡发SIGIR太多了。。。</p><p>南开大学：<a href="https://link.zhihu.com/?target=http%3A//cc.nankai.edu.cn/teachers/introduce/yangzl">杨征路</a>老师，主要做信息检索。</p><p>北京邮电大学：王小捷老师。</p><p>北京语言大学：<a href="https://link.zhihu.com/?target=http%3A//xxkx.blcu.edu.cn/art/2014/10/28/art_3082_1090594.html">于东</a>老师，主要做机器翻译、人机对话等。</p><p>华东师范大学：<a href="https://link.zhihu.com/?target=https%3A//faculty.ecnu.edu.cn/s/2627/main.jspy">吴苑</a>斌老师，记得应该是复旦大学黄萱菁老师的学生。</p><p>山西大学：李茹老师，山西大学计算机学院副院长。</p><p><a href="https://link.zhihu.com/?target=http%3A//www5.zzu.edu.cn/nlp">郑州大学自然语言处理实验室</a>：教师包括昝红英、<a href="https://link.zhihu.com/?target=http%3A//www5.zzu.edu.cn/nlp/info/1004/1166.htm">柴玉梅</a>、<a href="https://link.zhihu.com/?target=http%3A//www5.zzu.edu.cn/nlp/info/1004/1163.htm">穆玲玲</a>、<a href="https://link.zhihu.com/?target=http%3A//www5.zzu.edu.cn/nlp/info/1004/1151.htm">赵丹</a>、<a href="https://link.zhihu.com/?target=http%3A//www5.zzu.edu.cn/nlp/info/1004/1626.htm">钱晓捷</a>、牛桂玲、马玉汴等。</p><p><a href="https://link.zhihu.com/?target=https%3A//nlp.heida.me/">黑龙江大学</a>：付国宏、张梅山等老师。</p><p>以下是港澳台地区的高校（排名不分先后）：</p><p><a href="https://link.zhihu.com/?target=http%3A//nlg.csie.ntu.edu.tw/">台湾大学自然语言处理实验室</a>：主要研究方向包括知识图谱、机器翻译、问答、自动摘要、信息检索等，论文非常多。</p><p><a href="https://link.zhihu.com/?target=http%3A//www.cse.ust.hk/~hltc/">香港科技大学人类语言技术中心</a>：论文非常多，牛人也不少。</p><p><a href="https://link.zhihu.com/?target=http%3A//www1.se.cuhk.edu.hk/~textmine/">香港中文大学文本挖掘组</a>：主要研究方向包括文本挖掘和信息检索。</p><p><a href="https://link.zhihu.com/?target=http%3A//nlp2ct.cis.umac.mo/">澳门大学自然语言处理与中葡翻译实验室</a>：主要做机器翻译，做的非常好，论文也非常多。</p><p><a href="https://link.zhihu.com/?target=http%3A//www4.comp.polyu.edu.hk/~cswjli/Group.html">香港理工大学社会媒体挖掘组</a>：主要研究方向包括社会影响力建模、社会媒体分析、观点摘要、观点追踪、跨语言情感分析等，这个实验室对外合作很多，比如北大李素建老师、MSRA的韦福如老师等。</p><p>国内工业界（排名不分先后）：</p><p>百度王海峰老师以及带领的自然语言处理部+百度研究院做NLP的一些组，内部NLPC平台集成了几十个NLP算子，一些算子每天调用量都能上亿次（不要问我为啥知道这么详细，因为15-16年我参与开发过2个算子，被加到开发者组了，经常有群邮件告知各算子的调用情况），几乎涵盖所有的NLP任务，部分技术在<a href="https://link.zhihu.com/?target=http%3A//ai.baidu.com/">百度AI开放平台-全球领先的人工智能服务平台-百度AI开放平台</a>开放API，少量技术在<a href="https://link.zhihu.com/?target=https%3A//github.com/baidu">github</a>开源代码；</p><p>MSRA周明老师带领的NLC组，组内论文发的非常多，尤其是几个高级研究员、主管研究员；</p><p>哈工大和科大讯飞联合实验室：实验室主任是刘挺老师，阅读理解做的非常好；</p><p><a href="https://link.zhihu.com/?target=http%3A//www.noahlab.com.hk/">华为诺亚方舟</a>的刘群老师以及带领的团队；</p><p>小米：王斌老师坐镇，王老师翻译的书相信大部分人都看过吧。</p><p><a href="https://link.zhihu.com/?target=https%3A//ailab.bytedance.com/">今日头条</a>的李航老师；</p><p>阿里巴巴达摩院语言技术实验室，在全球6个地点（杭州，北京，西雅图，硅谷，纽约，新加坡）组建了100多人的国际化团队。包括司罗，黄非，骆卫华，陈博兴，刘晓钟等，内部搭建了平台，也做了一些技术评测取得不俗成绩。</p><p><a href="https://link.zhihu.com/?target=https%3A//ai.tencent.com/ailab/nlp/">腾讯NLP</a>做的也不少，只是个人感觉都是很多团队在做，比较分散，没有集中到一起。如果有人总结比较好的话，欢迎告知。</p><p>其他很多创业公司也有大牛坐镇，只是太多、太分散了，不再赘述，感兴趣的可以私聊。</p><p>国外学术界：</p><p><a href="https://link.zhihu.com/?target=http%3A//nlp.csail.mit.edu/">麻省理工学院</a>：<a href="https://link.zhihu.com/?target=http%3A//people.csail.mit.edu/regina/"> Regina Barzilay</a> ,<a href="https://link.zhihu.com/?target=https%3A//people.csail.mit.edu/tommi/"> Tommi S. Jaakkola</a>。</p><p><a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~nasmith/nlp-cl.html">卡内基梅隆大学</a>：<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~jgc"> Jaime Carbonell</a> ,<a href="https://link.zhihu.com/?target=http%3A//www.justinecassell.com/"> Justine Cassell</a> ,<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~wcohen"> William Cohen</a>（主要做信息抽取）,<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~cdyer"> Chris Dyer</a>（主要做机器翻译）,<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~sef"> Scott Fahlman</a> ,（主要做只是表示和知识推理），<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~ref"> Robert Frederking</a> ,<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~hovy"> Eduard Hovy</a> ,<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~alavie"> Alon Lavie</a> ,<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~lsl"> Lori Levin</a> ,<a href="https://link.zhihu.com/?target=http%3A//psyling.psy.cmu.edu/brian"> Brian MacWhinney</a> ,（做的比较杂），<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~teruko"> Teruko Mitamura</a> ,（主要做QA），<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~tom"> Tom Mitchell</a> ,<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~ehn"> Eric Nyberg</a>，<a href="https://link.zhihu.com/?target=http%3A//www.andrew.cmu.edu/user/ko"> Kemal Oflazer</a> ,<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~cprose"> Carolyn Penstein Rosé</a> ,（主要做聊天），<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~roni"> Roni Rosenfeld</a> ,<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~nasmith"> Noah Smith</a> ,<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~epxing"> Eric Xing</a>。</p><p><a href="https://link.zhihu.com/?target=https%3A//www.clsp.jhu.edu/">约翰·霍普金斯大学</a>：Andreas Andreou，Raman Arora，Jason Eisner, Sanjeev Khudanpur, David Yarowsky, Hynek Hermansky，Mark Dredze, Tom Lippincott，Philipp Koehn，Najim Dehak，Ben van Durme。绝对的NLP领域顶级牛校，研究几乎涵盖所有NLP任务，而且做的都非常好，如果非要说主要研究内容的话：句法分析、机器翻译。</p><p><a href="https://link.zhihu.com/?target=https%3A//www.cs.princeton.edu/research/areas/nlp">普林斯顿大学</a>：<a href="https://link.zhihu.com/?target=https%3A//www.cs.princeton.edu/people/profile/arora"> Sanjeev Arora</a>，<a href="https://link.zhihu.com/?target=https%3A//www.cs.princeton.edu/people/profile/karthikn"> Karthik Narasimhan</a>。</p><p><a href="https://link.zhihu.com/?target=https%3A//nlp.stanford.edu/">斯坦福大学</a>： Christopher Manning, Daniel Jurafsky, Percy Liang，这几个人不用赘述了吧，实验室做的很广泛，句法分析和词性标注的工具很有名。</p><p><a href="https://link.zhihu.com/?target=http%3A//nlp.seas.harvard.edu/">哈弗大学</a>：Stuart Shieber，Alexander Rush，主要做MT、自动摘要和文本生成。</p><p><a href="https://link.zhihu.com/?target=https%3A//www.cl.cam.ac.uk/research/nl/">剑桥大学</a>：Edward J. Briscoe，Ann Copestake，Simone Teufel，Paula Buttery，Andreas Vlachos，摘要、文本生成、NLU、句法分析、IR做的都不错。</p><p><a href="https://link.zhihu.com/?target=http%3A//www.clg.ox.ac.uk/">牛津大学</a>：Stephen Pulman，Phil Blunsom（MT非常非常厉害）。</p><p><a href="https://link.zhihu.com/?target=http%3A//nlp.cs.berkeley.edu/">加州大学伯克利分校</a>：<a href="https://link.zhihu.com/?target=http%3A//www.eecs.berkeley.edu/~klein"> Dan Klein</a>（主要做IE和MT）。</p><p><a href="https://link.zhihu.com/?target=http%3A//nlp.cs.illinois.edu/">伊利诺伊大学香槟分校</a>：<a href="https://link.zhihu.com/?target=http%3A//www.cs.uiuc.edu/homes/mfleck/"> Margaret M. Fleck</a> (<a href="https://link.zhihu.com/?target=http%3A//www.cs.uiuc.edu/"> CS</a> )，<a href="https://link.zhihu.com/?target=https%3A//netfiles.uiuc.edu/girju/index.html"> Roxana Girju</a> (<a href="https://link.zhihu.com/?target=http%3A//www.linguistics.uiuc.edu/"> Linguistics</a> )，<a href="https://link.zhihu.com/?target=http%3A//www.ifp.uiuc.edu/~hasegawa/"> Mark Hasegawa-Johnson</a> (<a href="https://link.zhihu.com/?target=http%3A//www.ece.uiuc.edu/"> ECE</a> )，<a href="https://link.zhihu.com/?target=http%3A//www.cs.uiuc.edu/homes/juliahmr/"> Julia Hockenmaier</a> (<a href="https://link.zhihu.com/?target=http%3A//www.cs.uiuc.edu/"> CS</a> )，<a href="https://link.zhihu.com/?target=http%3A//l2r.cs.uiuc.edu/~danr"> Dan Roth</a> (<a href="https://link.zhihu.com/?target=http%3A//www.cs.uiuc.edu/"> CS</a> )，<a href="https://link.zhihu.com/?target=http%3A//www-faculty.cs.uiuc.edu/~czhai"> ChengXiang Zhai</a> (<a href="https://link.zhihu.com/?target=http%3A//www.cs.uiuc.edu/"> CS</a> )。</p><p><a href="https://link.zhihu.com/?target=http%3A//nlp.cis.upenn.edu/">宾夕法尼亚大学</a>：<a href="https://link.zhihu.com/?target=http%3A//www.cis.upenn.edu/~mitch/"> Mitch Marcus</a>，<a href="https://link.zhihu.com/?target=http%3A//www.cis.upenn.edu/~danroth/"> Dan Roth</a>，<a href="https://link.zhihu.com/?target=http%3A//www.seas.upenn.edu/~ungar/"> Lyle Ungar</a>，<a href="https://link.zhihu.com/?target=http%3A//www.cis.upenn.edu/~nenkova/"> Ani Nenkova</a>，<a href="https://link.zhihu.com/?target=http%3A//www.cis.upenn.edu/~ccb/"> Chris Callison-Burch</a>，句法分析做的非常屌，LTAG、Penn Treebank不用过多解释了吧。</p><p>芝加哥大学：John Lafferty（CRF发明人，机器翻译做的也不错）, John Goldsmith</p><p>哥伦比亚大学：Kathy McKeown, Julia Hirschberg，Owen Rambow。</p><p>康奈尔大学：Lillian Lee（主要做SA（情感分析））, Thorsten Joachims（深入学习SVM的话应该知道他）, Claire Cardie, Yoav Artzi。</p><p><a href="https://link.zhihu.com/?target=https%3A//u.osu.edu/cllt/">俄亥俄州立大学（OSU）</a>：Eric Fosler-Lussier(我是因为做对话知道的他), Michael White（主要做NLG）, William Schuler（主要做句法分析和MT）, Micha Elsner, Alan Ritter, Wei Xu（社交媒体）。</p><p><a href="https://link.zhihu.com/?target=http%3A//www.isp.pitt.edu/about">匹兹堡大学</a>：Ashley Kevin， Brusilovsky Peter, Lewis Michael。</p><p><a href="https://link.zhihu.com/?target=https%3A//www.cs.toronto.edu/compling/">多伦多大学</a>：Graeme Hirst, Gerald Penn，Frank Rudzic，Suzanne Stevenson，主要做句法分析、语义分析。</p><p><a href="https://link.zhihu.com/?target=http%3A//rl.cs.mcgill.ca/people.html">麦吉尔大学</a>：<a href="https://link.zhihu.com/?target=http%3A//www.cs.mcgill.ca/~dprecup"> Doina Precup</a>，<a href="https://link.zhihu.com/?target=http%3A//www.cs.mcgill.ca/~jcheung/"> Jackie Chi Kit Cheung</a>，<a href="https://link.zhihu.com/?target=http%3A//www.cs.mcgill.ca/~jpineau"> Joelle Pineau</a>，<a href="https://link.zhihu.com/?target=http%3A//www.cs.mcgill.ca/~prakash"> Prakash Panangaden</a></p><p>蒙特利尔大学：Yoshua Bengio，不过多解释。</p><p>佐治亚理工：Eric Gilbert（社会计算领域很有名）。</p><p><a href="https://link.zhihu.com/?target=https%3A//cl.usc.edu/">南加州大学</a>：Jerry Hobbs，Ron Artstein，David DeVault，Kallirroi Georgila，Panayiotis (Panos) Georgiou， Andrew Gordon，Jerry Hobbs，Khalil Iskarous，Kevin Knight，Sungbok Lee， Anton Leuski，Jonathan May，Prem Natarajan，MT、IE、关系挖掘、对话做的都不错。</p><p><a href="https://link.zhihu.com/?target=https%3A//www.cs.washington.edu/research/nlp">华盛顿大学</a>：<a href="https://link.zhihu.com/?target=http%3A//www.timalthoff.com/"> Tim Althoff</a>，<a href="https://link.zhihu.com/?target=http%3A//melodi.ee.washington.edu/people/bilmes/pgs/index.html"> Jeffrey Bilmes</a>，Yejin Choi，Pedro Domingos，Oren Etzioni，<a href="https://link.zhihu.com/?target=https%3A//homes.cs.washington.edu/~hannaneh/"> Hannaneh Hajishirzi</a>，<a href="https://link.zhihu.com/?target=https%3A//homes.cs.washington.edu/~nasmith"> Noah Smith</a>，Daniel S. Weld，Luke Zettlemoyer，主要做句法分析、MT、对话、IR等。</p><p><a href="https://link.zhihu.com/?target=http%3A//edinburghnlp.inf.ed.ac.uk/">爱丁堡大学</a>：Shay Cohen（句法分析），Sharon Goldwater，Kenneth Heafield（MT），Frank Keller（句法分析），Mirella Lapata（NLU、NLG），Adam Lopez，Walid Magdy（IR、DM、社会计算），Rico Sennrich （句法分析、MT），Mark Steedman（对话），Ivan Titov（句法分析、NLU），Bonnie Webber（QA）。</p><p><a href="https://link.zhihu.com/?target=https%3A//www.comp.nus.edu.sg/~nlp/">新加坡国立大学</a>：NG Hwee Tou（主要做MT和句法纠错）。</p><p>马里兰大学：Philip Resnik, Naomi Feldman，Marine Carpuat,Hal Daumé, 主要做MT和IR。</p><p><a href="https://link.zhihu.com/?target=https%3A//nlp.ccis.northeastern.edu/">东北大学</a>：David A. Smith, Byron Wallace, Lu Wang。</p><p>加州大学伯克利分校：Dan Klein，主要做NLP和ML交叉研究。</p><p>加州大学圣巴巴拉分校：William Wang, 主要做IE（信息抽取）和ML。</p><p>加州大学圣克鲁兹分校：Marilyn Walker，主要做dialogue。</p><p><a href="https://link.zhihu.com/?target=http%3A//nlpatcuny.cs.qc.cuny.edu/">纽约市立学院（CUNY）</a>：Martin Chodorow，Liang huang，Andrew Rosenberg，William Sakas，Virginia Teller。</p><p>University of Massachusetts Amherst：<a href="https://link.zhihu.com/?target=https%3A//people.cs.umass.edu/~mccallum/">Andrew McCallum</a>（CRF、主题模型）、Bruce Croft、James Allan（IR做的非常屌）。</p><p><a href="https://link.zhihu.com/?target=https%3A//wp.nyu.edu/ml2/">纽约大学</a>：Sam Bowman, Kyunghyun Cho，NLU做的非常好。</p><p><a href="https://link.zhihu.com/?target=https%3A//nlp.cs.unc.edu/">北卡罗来纳大学教堂山分校</a>（UNC）：Mohit Bansal, Tamara Berg,主要做句法分析、多模态对话。</p><p>罗切斯特大学：Len Schubert, James Allen（篇章分析、对话做的很好），Dan Gildea（句法分析、MT）。</p><p><a href="https://link.zhihu.com/?target=https%3A//www.sheffield.ac.uk/dcs/research/groups/nlp">谢菲尔德大学</a>：<a href="https://link.zhihu.com/?target=http%3A//www.dcs.shef.ac.uk/~robertg"> Rob Gaizauskas</a> (Head of Group），<a href="https://link.zhihu.com/?target=http%3A//www.dcs.shef.ac.uk/~hepple"> Mark Hepple</a>，<a href="https://link.zhihu.com/?target=http%3A//staffwww.dcs.shef.ac.uk/people/L.Specia/"> Lucia Specia</a>（MT很厉害），<a href="https://link.zhihu.com/?target=http%3A//www.dcs.shef.ac.uk/~marks"> Mark Stevenson</a>（主要做IR和IE），<a href="https://link.zhihu.com/?target=http%3A//www.dcs.shef.ac.uk/~yorick"> Yorick Wilks</a>（ACL终身成就奖）。</p><p>还有很多学校、很多学术界大佬没整理了，后续再补充吧。另外美国工业界的NLP大牛也很多，比如google、facebook、microsoft、amazon、IBM等公司。</p>]]></content>
    
    
    <categories>
      
      <category>1.科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>课题组信息</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/修改conda源_下载问题</title>
    <link href="undefined2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E4%BF%AE%E6%94%B9conda%E6%BA%90_%E4%B8%8B%E8%BD%BD%E9%97%AE%E9%A2%98/"/>
    <url>2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E4%BF%AE%E6%94%B9conda%E6%BA%90_%E4%B8%8B%E8%BD%BD%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h4 id="首次改为国内源："><a href="#首次改为国内源：" class="headerlink" title="首次改为国内源："></a>首次改为国内源：</h4><p>打开cmd</p><p>conda config –add channels <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</a><br>conda config –add channels <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge</a><br>conda config –add channels <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/</a><br>conda config –add channels <a href="https://mirrors.ustc.edu.cn/anaconda/pkgs/main/" target="_blank" rel="noopener">https://mirrors.ustc.edu.cn/anaconda/pkgs/main/</a><br>conda config –add channels <a href="https://mirrors.ustc.edu.cn/anaconda/pkgs/free/" target="_blank" rel="noopener">https://mirrors.ustc.edu.cn/anaconda/pkgs/free/</a><br>conda config –add channels <a href="https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/" target="_blank" rel="noopener">https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/</a><br>conda config –add channels <a href="https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/" target="_blank" rel="noopener">https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/</a><br>conda config –add channels <a href="https://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/" target="_blank" rel="noopener">https://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/</a><br>conda config –add channels <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</a><br>conda config –add channels <a href="https://mirrors.ustc.edu.cn/anaconda/cloud/menpo/" target="_blank" rel="noopener">https://mirrors.ustc.edu.cn/anaconda/cloud/menpo/</a><br>conda config –set show_channel_urls yes</p><p>C:\Users&lt;你的用户名&gt; 下就会生成配置文件.condarc</p><p>删除第三行-default保存</p><p>conda info检查修改是否生效</p><h4 id="改回默认源："><a href="#改回默认源：" class="headerlink" title="改回默认源："></a>改回默认源：</h4><p>打开C:\Users&lt;bqlion&gt;.condarc文件，改成:</p><pre><code>channels:  - defaultsshow_channel_urls: true</code></pre><p>保存</p><h4 id="再改回国内源"><a href="#再改回国内源" class="headerlink" title="再改回国内源;"></a>再改回国内源;</h4><p>打开C:\Users&lt;bqlion&gt;.condarc文件，改成:</p><pre><code>channels:  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/  - https://mirrors.ustc.edu.cn/anaconda/cloud/menpo/  - https://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/  - https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/  - https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/  - https://mirrors.ustc.edu.cn/anaconda/pkgs/free/  - https://mirrors.ustc.edu.cn/anaconda/pkgs/main/  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/show_channel_urls: truessl_verify: false</code></pre><h4 id="CondaHTTPError-HTTP-000-CONNECTION-FAILED的问题："><a href="#CondaHTTPError-HTTP-000-CONNECTION-FAILED的问题：" class="headerlink" title="CondaHTTPError: HTTP 000 CONNECTION FAILED的问题："></a>CondaHTTPError: HTTP 000 CONNECTION FAILED的问题：</h4><p>conda config –set ssl_verify false</p><p>若还不行就输入：</p><p>conda config –set ssl_verify no</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>1.科研/WebScience讲座笔记</title>
    <link href="undefined2020/02/24/1.%E7%A7%91%E7%A0%94/WebScience%E8%AE%B2%E5%BA%A7%E7%AC%94%E8%AE%B0/"/>
    <url>2020/02/24/1.%E7%A7%91%E7%A0%94/WebScience%E8%AE%B2%E5%BA%A7%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="Web-Science讲座笔记"><a href="#Web-Science讲座笔记" class="headerlink" title="Web Science讲座笔记"></a>Web Science讲座笔记</h1><ul><li><p>写论文步骤</p><p>在感兴趣的方向提出问题和假设</p><p>验证前人是否已经做过？</p><p>设计实验</p><p>做实验,收集数据</p><p>写论文</p></li></ul><h3 id="开题案例"><a href="#开题案例" class="headerlink" title="开题案例"></a>开题案例</h3><hr><p>案例:人工智能</p><p>阅读中科院，工程院前沿报告,阅读各种综述，阅读下文专项报告</p><p>两个月更新一次:ESI Reaserach front专项报告</p><p>在web产品里找到,网页右边可以关键词聚类</p><p>AI子课题:自然语言处理,主页可显示高被引用论文(数据结构网络的核心)</p><p>能顺便找到本学科的raising star</p><ul><li><p>在web science里写检索式,如何寻找文章引用网络</p><p><a href="https://www.clarivate.com.cn/e-clarivate/wos_video_wos_research.htm" target="_blank" rel="noopener">https://www.clarivate.com.cn/e-clarivate/wos_video_wos_research.htm</a></p></li></ul><ul><li><p>如何在上行的检索之后筛选自己需要的信息?(上个结果返回30w+文章)</p><ul><li><p>检索结果:</p><p>这个功能能找到哪些基金在资助哪些项目</p><p>哪些导师在做,找导师/合伙人利器</p><ul><li><p>分析机构:</p><p>能找到哪些机构近几年发表了哪些文章</p><ul><li>分析机构中的作者</li></ul></li></ul></li></ul></li></ul><ul><li><p>几个产品按钮</p><p>在网页上方有个文章被引用次数的功能,发布在哪个期刊</p></li></ul><ul><li><p>左侧功能栏:</p><p>review是学科综述</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>1.科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Web_of_Science</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1.科研/深度学习论文集[Done]</title>
    <link href="undefined2020/02/24/1.%E7%A7%91%E7%A0%94/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%9B%86%5BDone%5D/"/>
    <url>2020/02/24/1.%E7%A7%91%E7%A0%94/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%9B%86%5BDone%5D/</url>
    
    <content type="html"><![CDATA[<h2 id="1-深度学习历史和基础"><a href="#1-深度学习历史和基础" class="headerlink" title="1 深度学习历史和基础"></a>1 深度学习历史和基础</h2><h3 id="1-0-书籍"><a href="#1-0-书籍" class="headerlink" title="1.0 书籍"></a>1.0 书籍</h3><p>█[0] Bengio, Yoshua, Ian J. Goodfellow, and Aaron Courville. <strong>“Deep learning</strong>.” An MIT Press book. (2015). [pdf] (Ian Goodfellow 等大牛所著的教科书，乃深度学习圣经。你可以同时研习这本书以及以下论文) ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//github.com/HFTrader/DeepLearningBook/raw/master/DeepLearningBook.pdf">https://github.com/HFTrader/DeepLearningBook/raw/master/DeepLearningBook.pdf</a></p><h3 id="1-1-调查"><a href="#1-1-调查" class="headerlink" title="1.1 调查"></a>1.1 调查</h3><p>█[1] LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. “<strong>Deep learning</strong>.” Nature 521.7553 (2015): 436-444. [pdf] (三巨头做的调查)  ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%7Ehinton/absps/NatureDeepReview.pdf">http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf</a></p><h3 id="1-2-深度置信网络-DBN，深度学习前夜的里程碑"><a href="#1-2-深度置信网络-DBN，深度学习前夜的里程碑" class="headerlink" title="1.2 深度置信网络 (DBN，深度学习前夜的里程碑)"></a>1.2 深度置信网络 (DBN，深度学习前夜的里程碑)</h3><p>█[2] Hinton, Geoffrey E., Simon Osindero, and Yee-Whye Teh. “<strong>A fast learning algorithm for deep belief nets</strong>.” Neural computation 18.7 (2006): 1527-1554. [pdf] (深度学习前夜) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%7Ehinton/absps/ncfast.pdf">http://www.cs.toronto.edu/~hinton/absps/ncfast.pdf</a></p><p>█[3] Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. “<strong>Reducing the dimensionality of data with neural networks.</strong>“ Science 313.5786 (2006): 504-507. [pdf] (里程碑，展示了深度学习的前景) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%7Ehinton/science.pdf">http://www.cs.toronto.edu/~hinton/science.pdf</a></p><h3 id="1-3-ImageNet-的进化（深度学习从此萌发）"><a href="#1-3-ImageNet-的进化（深度学习从此萌发）" class="headerlink" title="1.3 ImageNet 的进化（深度学习从此萌发）"></a>1.3 ImageNet 的进化（深度学习从此萌发）</h3><p>█[4] Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. “<strong>Imagenet classification with deep convolutional neural networks.</strong>“ Advances in neural information processing systems. 2012. [pdf] (AlexNet, 深度学习突破)  ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-    networks.pdf</a></p><p>█[5] Simonyan, Karen, and Andrew Zisserman. “<strong>Very deep convolutional networks for large-scale image recognition.</strong>“ arXiv preprint arXiv:1409.1556 (2014). [pdf] (VGGNet，神经网络变得很深层) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1409.1556.pdf">https://arxiv.org/pdf/1409.1556.pdf</a></p><p>█[6] Szegedy, Christian, et al. “<strong>Going deeper with convolutions.</strong>“ Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015. [pdf] (GoogLeNet) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf">http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf</a></p><p>█[7] He, Kaiming, et al. “<strong>Deep residual learning for image recognition.</strong>“ arXiv preprint arXiv:1512.03385 (2015). [pdf](ResNet，特别深的神经网络, CVPR 最佳论文)  ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1512.03385.pdf">https://arxiv.org/pdf/1512.03385.pdf</a></p><h3 id="1-4-语音识别的进化"><a href="#1-4-语音识别的进化" class="headerlink" title="1.4 语音识别的进化"></a>1.4 语音识别的进化</h3><p>█[8] Hinton, Geoffrey, et al. “<strong>Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups.</strong>“ IEEE Signal Processing Magazine 29.6 (2012): 82-97. [pdf] (语音识别的突破) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//cs224d.stanford.edu/papers/maas_paper.pdf">http://cs224d.stanford.edu/papers/maas_paper.pdf</a></p><p>█[9] Graves, Alex, Abdel-rahman Mohamed, and Geoffrey Hinton. “<strong>Speech recognition with deep recurrent neural networks.</strong>“ 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013. [pdf] (RNN) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1303.5778.pdf">http://arxiv.org/pdf/1303.5778.pdf</a></p><p>█[10] Graves, Alex, and Navdeep Jaitly. “<strong>Towards End-To-End Speech Recognition with Recurrent Neural Networks.</strong>“ ICML. Vol. 14. 2014. [pdf] ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.jmlr.org/proceedings/papers/v32/graves14.pdf">http://www.jmlr.org/proceedings/papers/v32/graves14.pdf</a></p><p>█[11] Sak, Haşim, et al. “<strong>Fast and accurate recurrent neural network acoustic models for speech recognition.</strong>“ arXiv preprint arXiv:1507.06947 (2015). [pdf] (谷歌语音识别系统) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1507.06947">http://arxiv.org/pdf/1507.06947</a></p><p>█[12] Amodei, Dario, et al. “<strong>Deep speech 2: End-to-end speech recognition in english and mandarin</strong>.” arXiv preprint arXiv:1512.02595 (2015). [pdf] (百度语音识别系统) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1512.02595.pdf">https://arxiv.org/pdf/1512.02595.pdf</a></p><p>█[13] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, G. Zweig “<strong>Achieving Human Parity in Conversational Speech Recognition.</strong>“ arXiv preprint arXiv:1610.05256 (2016). [pdf] (最前沿的语音识别, 微软) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1610.05256v1">https://arxiv.org/pdf/1610.05256v1</a></p><p>研读以上论文之后，你将对深度学习历史、模型的基本架构（包括 CNN, RNN, LSTM）有一个基础的了解，并理解深度学习如何应用于图像和语音识别问题。接下来的论文，将带你深入探索深度学习方法、在不同领域的应用和前沿尖端技术。我建议，你可以根据兴趣和工作/研究方向进行选择性的阅读。</p><h2 id="2-深度学习方法"><a href="#2-深度学习方法" class="headerlink" title="2 深度学习方法"></a>2 深度学习方法</h2><h3 id="2-1-模型"><a href="#2-1-模型" class="headerlink" title="2.1 模型"></a>2.1 模型</h3><p>█[14] Hinton, Geoffrey E., et al. “<strong>Improving neural networks by preventing co-adaptation of feature detectors.</strong>“ arXiv preprint arXiv:1207.0580 (2012). [pdf] (Dropout) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1207.0580.pdf">https://arxiv.org/pdf/1207.0580.pdf</a></p><p>█[15] Srivastava, Nitish, et al. “<strong>Dropout: a simple way to prevent neural networks from overfitting.</strong>“ Journal of Machine Learning Research 15.1 (2014): 1929-1958. [pdf] ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf">http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf</a></p><p>█[16] Ioffe, Sergey, and Christian Szegedy. “<strong>Batch normalization: Accelerating deep network training by reducing internal covariate shift.</strong>“ arXiv preprint arXiv:1502.03167 (2015). [pdf] (2015 年的杰出研究) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1502.03167">http://arxiv.org/pdf/1502.03167</a></p><p>█[17] Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. “<strong>Layer normalization.</strong>“ arXiv preprint arXiv:1607.06450 (2016). [pdf] (Batch Normalization 的更新) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1607.06450.pdf%3Futm_source%3Dsciontist.com%26utm_medium%3Drefer%26utm_campaign%3Dpromote">https://arxiv.org/pdf/1607.06450.pdf?utm_source=sciontist.com&amp;utm_medium=refer&amp;utm_campaign=promote</a></p><p>█[18] Courbariaux, Matthieu, et al. “<strong>Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to+ 1 or−1.</strong>“ [pdf] (新模型，快) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//pdfs.semanticscholar.org/f832/b16cb367802609d91d400085eb87d630212a.pdf">https://pdfs.semanticscholar.org/f832/b16cb367802609d91d400085eb87d630212a.pdf</a></p><p>█[19] Jaderberg, Max, et al. “<strong>Decoupled neural interfaces using synthetic gradients.</strong>“ arXiv preprint arXiv:1608.05343 (2016). [pdf] (训练方法的创新，研究相当不错) ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1608.05343">https://arxiv.org/pdf/1608.05343</a></p><p>█[20] Chen, Tianqi, Ian Goodfellow, and Jonathon Shlens. “<strong>Net2net: Accelerating learning via knowledge transfer.</strong>“ arXiv preprint arXiv:1511.05641 (2015). [pdf] (改进此前的训练网络，来缩短训练周期) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1511.05641">Accelerating Learning via Knowledge Transfer</a></p><p>█[21] Wei, Tao, et al. “<strong>Network Morphism.</strong>“ arXiv preprint arXiv:1603.01670 (2016). [pdf] (改进此前的训练网络，来缩短训练周期) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1603.01670">[1603.01670] Network Morphism</a></p><h3 id="2-2-优化-Optimization"><a href="#2-2-优化-Optimization" class="headerlink" title="2.2 优化 Optimization"></a>2.2 优化 Optimization</h3><p>█[22] Sutskever, Ilya, et al. “<strong>On the importance of initialization and momentum in deep learning.</strong>“ ICML (3) 28 (2013): 1139-1147. [pdf] (Momentum optimizer) ★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.jmlr.org/proceedings/papers/v28/sutskever13.pdf">http://www.jmlr.org/proceedings/papers/v28/sutskever13.pdf</a></p><p>█[23] Kingma, Diederik, and Jimmy Ba. “<strong>Adam: A method for stochastic optimization.</strong>“ arXiv preprint arXiv:1412.6980 (2014). [pdf] (Maybe used most often currently) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1412.6980">http://arxiv.org/pdf/1412.6980</a></p><p>█[24] Andrychowicz, Marcin, et al. “<strong>Learning to learn by gradient descent by gradient descent.”</strong> arXiv preprint arXiv:1606.04474 (2016). [pdf] (Neural Optimizer,Amazing Work) ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1606.04474">https://arxiv.org/pdf/1606.04474</a></p><p>█[25] Han, Song, Huizi Mao, and William J. Dally. <strong>“Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding.</strong>“ CoRR, abs/1510.00149 2 (2015). [pdf] (ICLR best paper, new direction to make NN running fast,DeePhi Tech Startup) ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//pdfs.semanticscholar.org/5b6c/9dda1d88095fa4aac1507348e498a1f2e863.pdf">https://pdfs.semanticscholar.org/5b6c/9dda1d88095fa4aac1507348e498a1f2e863.pdf</a></p><p>█[26] Iandola, Forrest N., et al. “<strong>SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&lt; 1MB model size.</strong>“ arXiv preprint arXiv:1602.07360 (2016). [pdf] (Also a new direction to optimize NN,DeePhi Tech Startup) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1602.07360">http://arxiv.org/pdf/1602.07360</a></p><h3 id="2-3-无监督学习-深度生成模型"><a href="#2-3-无监督学习-深度生成模型" class="headerlink" title="2.3 无监督学习/深度生成模型"></a>2.3 无监督学习/深度生成模型</h3><p>█[27] Le, Quoc V. “<strong>Building high-level features using large scale unsupervised learning.</strong>“ 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013. [pdf] (里程碑, 吴恩达, 谷歌大脑, Cat) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1112.6209.pdf%26embed">http://arxiv.org/pdf/1112.6209.pdf&amp;embed</a></p><p>█[28] Kingma, Diederik P., and Max Welling. “<strong>Auto-encoding variational bayes.</strong>“ arXiv preprint arXiv:1312.6114 (2013). <a href="VAE">pdf</a> ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1312.6114">http://arxiv.org/pdf/1312.6114</a></p><p>█[29] Goodfellow, Ian, et al. “<strong>Generative adversarial nets.</strong>“ Advances in Neural Information Processing Systems. 2014. <a href="GAN，很酷的想法">pdf</a> ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5423-generative-adversarial-nets.pdf">http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf</a></p><p>█[30] Radford, Alec, Luke Metz, and Soumith Chintala. “<strong>Unsupervised representation learning with deep convolutional generative adversarial networks.</strong>“ arXiv preprint arXiv:1511.06434 (2015). [pdf] (DCGAN) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1511.06434">http://arxiv.org/pdf/1511.06434</a></p><p>█[31] Gregor, Karol, et al. “<strong>DRAW: A recurrent neural network for image generation.</strong>“ arXiv preprint arXiv:1502.04623 (2015). [pdf] (VAE with attention, 很出色的研究) ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v37/gregor15.pdf">http://jmlr.org/proceedings/papers/v37/gregor15.pdf</a></p><p>█[32] Oord, Aaron van den, Nal Kalchbrenner, and Koray Kavukcuoglu. “<strong>Pixel recurrent neural networks.</strong>“ arXiv preprint arXiv:1601.06759 (2016). [pdf] (PixelRNN) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1601.06759">http://arxiv.org/pdf/1601.06759</a></p><p>█[33] Oord, Aaron van den, et al. <strong>“Conditional image generation with PixelCNN decoders.</strong>“ arXiv preprint arXiv:1606.05328 (2016). [pdf] (PixelCNN) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1606.05328">https://arxiv.org/pdf/1606.05328</a></p><h3 id="2-4-递归神经网络（RNN）-Sequence-to-Sequence-Model"><a href="#2-4-递归神经网络（RNN）-Sequence-to-Sequence-Model" class="headerlink" title="2.4 递归神经网络（RNN） / Sequence-to-Sequence Model"></a>2.4 递归神经网络（RNN） / Sequence-to-Sequence Model</h3><p>█[34] Graves, Alex. “<strong>Generating sequences with recurrent neural networks.</strong>“ arXiv preprint arXiv:1308.0850 (2013). [pdf] (LSTM, 效果很好，展示了 RNN 的性能) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1308.0850">http://arxiv.org/pdf/1308.0850</a></p><p>█[35] Cho, Kyunghyun, et al. “<strong>Learning phrase representations using RNN encoder-decoder for statistical machine translation.</strong>“ arXiv preprint arXiv:1406.1078 (2014). [pdf] (第一篇 Sequence-to-Sequence 的论文) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1406.1078">http://arxiv.org/pdf/1406.1078</a></p><p>█[36] Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. “<strong>Sequence to sequence learning with neural networks.</strong>“ Advances in neural information processing systems. 2014. [pdf] (杰出研究) ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5346-information-based-learning-by-agents-in-unbounded-state-spaces.pdf">http://papers.nips.cc/paper/5346-information-based-learning-by-agents-in-unbounded-state-spaces.pdf</a></p><p>█[37] Bahdanau, Dzmitry, KyungHyun Cho, and Yoshua Bengio. “<strong>Neural Machine Translation by Jointly Learning to Align and Translate.</strong>“ arXiv preprint arXiv:1409.0473 (2014). [pdf] ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1409.0473v7.pdf">https://arxiv.org/pdf/1409.0473v7.pdf</a></p><p>█[38] Vinyals, Oriol, and Quoc Le. “<strong>A neural conversational model.</strong>“ arXiv preprint arXiv:1506.05869 (2015). [pdf] (Seq-to-Seq 聊天机器人) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1506.05869.pdf%2520%28http%3A//arxiv.org/pdf/1506.05869.pdf%29">http://arxiv.org/pdf/1506.05869.pdf%20(http://arxiv.org/pdf/1506.05869.pdf)</a></p><h3 id="2-5-神经网络图灵机"><a href="#2-5-神经网络图灵机" class="headerlink" title="2.5 神经网络图灵机"></a>2.5 神经网络图灵机</h3><p>█[39] Graves, Alex, Greg Wayne, and Ivo Danihelka. “<strong>Neural turing machines.</strong>“ arXiv preprint arXiv:1410.5401 (2014). [pdf] (未来计算机的基础原型机) ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1410.5401.pdf">http://arxiv.org/pdf/1410.5401.pdf</a></p><p>█[40] Zaremba, Wojciech, and Ilya Sutskever. “<strong>Reinforcement learning neural Turing machines.</strong>“ arXiv preprint arXiv:1505.00521 362 (2015). [pdf] ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//pdfs.semanticscholar.org/f10e/071292d593fef939e6ef4a59baf0bb3a6c2b.pdf">https://pdfs.semanticscholar.org/f10e/071292d593fef939e6ef4a59baf0bb3a6c2b.pdf</a></p><p>█[41] Weston, Jason, Sumit Chopra, and Antoine Bordes. “<strong>Memory networks.</strong>“ arXiv preprint arXiv:1410.3916 (2014). [pdf] ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1410.3916">http://arxiv.org/pdf/1410.3916</a></p><p>█[42] Sukhbaatar, Sainbayar, Jason Weston, and Rob Fergus. “<strong>End-to-end memory networks.</strong>“ Advances in neural information processing systems. 2015. [pdf] ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf">http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf</a></p><p>█[43] Vinyals, Oriol, Meire Fortunato, and Navdeep Jaitly. “<strong>Pointer networks.</strong>“ Advances in Neural Information Processing Systems. 2015. [pdf] ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5866-pointer-networks.pdf">http://papers.nips.cc/paper/5866-pointer-networks.pdf</a></p><p>█[44] Graves, Alex, et al. “<strong>Hybrid computing using a neural network with dynamic external memory.</strong>“ Nature (2016). [pdf] (里程碑，把以上论文的想法整合了起来) ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//www.dropbox.com/s/0a40xi702grx3dq/2016-graves.pdf">https://www.dropbox.com/s/0a40xi702grx3dq/2016-graves.pdf</a></p><h3 id="2-6-深度强化学习"><a href="#2-6-深度强化学习" class="headerlink" title="2.6 深度强化学习"></a>2.6 深度强化学习</h3><p>█[45] Mnih, Volodymyr, et al. “<strong>Playing atari with deep reinforcement learning</strong>.” arXiv preprint arXiv:1312.5602 (2013). [pdf]) (第一个以深度强化学习为题的论文)  ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1312.5602.pdf">http://arxiv.org/pdf/1312.5602.pdf</a></p><p>█[46] Mnih, Volodymyr, et al. “<strong>Human-level control through deep reinforcement learning</strong>.” Nature 518.7540 (2015): 529-533. [pdf] (里程碑) ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf">https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf</a></p><p>█[47] Wang, Ziyu, Nando de Freitas, and Marc Lanctot. “<strong>Dueling network architectures for deep reinforcement learning.</strong>“ arXiv preprint arXiv:1511.06581 (2015). [pdf] (ICLR 最佳论文，很棒的想法)  ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1511.06581">http://arxiv.org/pdf/1511.06581</a></p><p>█[48] Mnih, Volodymyr, et al. “<strong>Asynchronous methods for deep reinforcement learning.</strong>“ arXiv preprint arXiv:1602.01783 (2016). [pdf] (前沿方法) ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1602.01783">http://arxiv.org/pdf/1602.01783</a></p><p>█[49] Lillicrap, Timothy P., et al. “<strong>Continuous control with deep reinforcement learning.</strong>“ arXiv preprint arXiv:1509.02971 (2015). [pdf] (DDPG)  ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1509.02971">http://arxiv.org/pdf/1509.02971</a></p><p>█[50] Gu, Shixiang, et al. “<strong>Continuous Deep Q-Learning with Model-based Acceleration.</strong>“ arXiv preprint arXiv:1603.00748 (2016). [pdf] (NAF)  ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1603.00748">http://arxiv.org/pdf/1603.00748</a></p><p>█[51] Schulman, John, et al. “<strong>Trust region policy optimization.</strong>“ CoRR, abs/1502.05477 (2015). [pdf] (TRPO)  ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.jmlr.org/proceedings/papers/v37/schulman15.pdf">http://www.jmlr.org/proceedings/papers/v37/schulman15.pdf</a></p><p>█[52] Silver, David, et al. “<strong>Mastering the game of Go with deep neural networks and tree search.</strong>“ Nature 529.7587 (2016): 484-489. [pdf] (AlphaGo) ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//willamette.edu/%7Elevenick/cs448/goNature.pdf">http://willamette.edu/~levenick/cs448/goNature.pdf</a></p><h3 id="2-7-深度迁移学习-终生学习-强化学习"><a href="#2-7-深度迁移学习-终生学习-强化学习" class="headerlink" title="2.7 深度迁移学习 /终生学习 / 强化学习"></a>2.7 深度迁移学习 /终生学习 / 强化学习</h3><p>█[53] Bengio, Yoshua. “<strong>Deep Learning of Representations for Unsupervised and Transfer Learning</strong>.” ICML Unsupervised and Transfer Learning 27 (2012): 17-36. [pdf] (这是一个教程) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.jmlr.org/proceedings/papers/v27/bengio12a/bengio12a.pdf">http://www.jmlr.org/proceedings/papers/v27/bengio12a/bengio12a.pdf</a></p><p>█[54] Silver, Daniel L., Qiang Yang, and Lianghao Li. “<strong>Lifelong Machine Learning Systems: Beyond Learning Algorithms.</strong>“ AAAI Spring Symposium: Lifelong Machine Learning. 2013. [pdf] (对终生学习的简单讨论) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//citeseerx.ist.psu.edu/viewdoc/download%3Fdoi%3D10.1.1.696.7800%26rep%3Drep1%26type%3Dpdf">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.696.7800&amp;rep=rep1&amp;type=pdf</a></p><p>█[55] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. “<strong>Distilling the knowledge in a neural network.</strong>“ arXiv preprint arXiv:1503.02531 (2015). [pdf] (大神们的研究)  ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1503.02531">http://arxiv.org/pdf/1503.02531</a></p><p>█[56] Rusu, Andrei A., et al. “<strong>Policy distillation.</strong>“ arXiv preprint arXiv:1511.06295 (2015). [pdf] (RL 领域) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1511.06295">http://arxiv.org/pdf/1511.06295</a></p><p>█[57] Parisotto, Emilio, Jimmy Lei Ba, and Ruslan Salakhu★★★tdinov. “<strong>Actor-mimic: Deep multitask and transfer reinforcement learning.</strong>“ arXiv preprint arXiv:1511.06342 (2015). [pdf] (RL 领域) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1511.06342">http://arxiv.org/pdf/1511.06342</a></p><p>█[58] Rusu, Andrei A., et al. “<strong>Progressive neural networks.</strong>“ arXiv preprint arXiv:1606.04671 (2016). [pdf] (杰出研究, 很新奇的想法) ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1606.04671">https://arxiv.org/pdf/1606.04671</a></p><h3 id="2-8-One-Shot-深度学习"><a href="#2-8-One-Shot-深度学习" class="headerlink" title="2.8 One Shot 深度学习"></a>2.8 One Shot 深度学习</h3><p>█[59] Lake, Brenden M., Ruslan Salakhutdinov, and Joshua B. Tenenbaum. “<strong>Human-level concept learning through probabilistic program induction.</strong>“ Science 350.6266 (2015): 1332-1338. [pdf] (不含深度学习但值得一读) ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//clm.utexas.edu/compjclub/wp-content/uploads/2016/02/lake2015.pdf">http://clm.utexas.edu/compjclub/wp-content/uploads/2016/02/lake2015.pdf</a></p><p>█[60] Koch, Gregory, Richard Zemel, and Ruslan Salakhutdinov. “<strong>Siamese Neural Networks for One-shot Image Recognition.</strong>“(2015) [pdf] ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.cs.utoronto.ca/%7Egkoch/files/msc-thesis.pdf">http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf</a></p><p>█[61] Santoro, Adam, et al. “<strong>One-shot Learning with Memory-Augmented Neural Networks.</strong>“ arXiv preprint arXiv:1605.06065 (2016). [pdf] (one shot 学习的基础一步) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1605.06065">http://arxiv.org/pdf/1605.06065</a></p><p>█[62] Vinyals, Oriol, et al. “<strong>Matching Networks for One Shot Learning.</strong>“ arXiv preprint arXiv:1606.04080 (2016). [pdf] ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1606.04080">https://arxiv.org/pdf/1606.04080</a></p><p>█[63] Hariharan, Bharath, and Ross Girshick. “<strong>Low-shot visual object recognition.</strong>“ arXiv preprint arXiv:1606.02819 (2016). [pdf] (通向更大规模数据的一步) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1606.02819">http://arxiv.org/pdf/1606.02819</a></p><h2 id="3-应用"><a href="#3-应用" class="headerlink" title="3 应用"></a>3 应用</h2><h3 id="3-1-自然语言处理-NLP"><a href="#3-1-自然语言处理-NLP" class="headerlink" title="3.1 自然语言处理 (NLP)"></a>3.1 自然语言处理 (NLP)</h3><p>█[1] Antoine Bordes, et al. “<strong>Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing.</strong>“ AISTATS(2012) [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//www.hds.utc.fr/~bordesan/dokuwiki/lib/exe/fetch.php%3Fid%3Den%253Apubli%26cache%3Dcache%26media%3Den%3Abordes12aistats.pdf">https://www.hds.utc.fr/~bordesan/dokuwiki/lib/exe/fetch.php?id=en%3Apubli&amp;cache=cache&amp;media=en:bordes12aistats.pdf</a></p><p>█[2] Mikolov, et al. “<strong>Distributed representations of words and phrases and their compositionality.</strong>“ ANIPS(2013): 3111-3119 [pdf] (word2vec) ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf</a></p><p>█[3] Sutskever, et al. ““<strong>Sequence to sequence learning with neural networks.</strong>“ ANIPS(2014) [pdf] ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf</a></p><p>█[4] Ankit Kumar, et al. ““<strong>Ask Me Anything: Dynamic Memory Networks for Natural Language Processing.</strong>“ arXiv preprint arXiv:1506.07285(2015) [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1506.07285">https://arxiv.org/abs/1506.07285</a></p><p>█[5] Yoon Kim, et al. “<strong>Character-Aware Neural Language Models.</strong>“ NIPS(2015) arXiv preprint arXiv:1508.06615(2015) [pdf] ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1508.06615">[1508.06615] Character-Aware Neural Language Models</a></p><p>█[6] Jason Weston, et al. “<strong>Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks.</strong>“ arXiv preprint arXiv:1502.05698(2015) [pdf] (bAbI tasks) ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1502.05698">A Set of Prerequisite Toy Tasks</a></p><p>█[7] Karl Moritz Hermann, et al. “<strong>Teaching Machines to Read and Comprehend.</strong>“ arXiv preprint arXiv:1506.03340(2015) <a href="CNN/每日邮报完形填空风格的问题">pdf</a> ★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1506.03340">[1506.03340] Teaching Machines to Read and Comprehend</a></p><p>█[8] Alexis Conneau, et al. “<strong>Very Deep Convolutional Networks for Natural Language Processing.</strong>“ arXiv preprint arXiv:1606.01781(2016) [pdf] (文本分类的前沿技术) ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1606.01781">[1606.01781] Very Deep Convolutional Networks for Text Classification</a></p><p>█[9] Armand Joulin, et al. “<strong>Bag of Tricks for Efficient Text Classification.</strong>“ arXiv preprint arXiv:1607.01759(2016) [pdf] (比前沿技术稍落后, 但快很多) ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1607.01759">[1607.01759] Bag of Tricks for Efficient Text Classification</a></p><h3 id="3-2-物体检测"><a href="#3-2-物体检测" class="headerlink" title="3.2 物体检测"></a>3.2 物体检测</h3><p>█[1] Szegedy, Christian, Alexander Toshev, and Dumitru Erhan. “<strong>Deep neural networks for object detection</strong>.” Advances in Neural Information Processing Systems. 2013. [pdf] ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf">http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf</a></p><p>█[2] Girshick, Ross, et al. “<strong>Rich feature hierarchies for accurate object detection and semantic segmentation.</strong>“ Proceedings of the IEEE conference on computer vision and pattern recognition. 2014. [pdf] (RCNN) ★★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf">http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf</a></p><p>█[3] He, Kaiming, et al. “<strong>Spatial pyramid pooling in deep convolutional networks for visual recognition.</strong>“ European Conference on Computer Vision. Springer International Publishing, 2014. [pdf] (SPPNet) ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1406.4729">http://arxiv.org/pdf/1406.4729</a></p><p>█[4] Girshick, Ross. “<strong>Fast r-cnn.</strong>“ Proceedings of the IEEE International Conference on Computer Vision. 2015. [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//pdfs.semanticscholar.org/8f67/64a59f0d17081f2a2a9d06f4ed1cdea1a0ad.pdf">https://pdfs.semanticscholar.org/8f67/64a59f0d17081f2a2a9d06f4ed1cdea1a0ad.pdf</a></p><p>█[5] Ren, Shaoqing, et al. “<strong>Faster R-CNN: Towards real-time object detection with region proposal networks.</strong>“ Advances in neural information processing systems. 2015. [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5638-analysis-of-variational-bayesian-latent-dirichlet-allocation-weaker-sparsity-than-map.pdf">http://papers.nips.cc/paper/5638-analysis-of-variational-bayesian-latent-dirichlet-allocation-weaker-sparsity-than-map.pdf</a></p><p>█[6] Redmon, Joseph, et al. “<strong>You only look once: Unified, real-time object detection.</strong>“ arXiv preprint arXiv:1506.02640 (2015). [pdf] (YOLO，杰出研究，非常具有使用价值） ★★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//homes.cs.washington.edu/~ali/papers/YOLO.pdf">http://homes.cs.washington.edu/~ali/papers/YOLO.pdf</a></p><p>█[7] Liu, Wei, et al. “<strong>SSD: Single Shot MultiBox Detector.</strong>“ arXiv preprint arXiv:1512.02325 (2015). [pdf] ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1512.02325">http://arxiv.org/pdf/1512.02325</a></p><p>█[8] Dai, Jifeng, et al. “<strong>R-FCN: Object Detection via Region-based Fully Convolutional Networks.</strong>“ arXiv preprint arXiv:1605.06409 (2016). [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1605.06409">Object Detection via Region-based Fully Convolutional Networks</a></p><h3 id="3-3-视觉追踪"><a href="#3-3-视觉追踪" class="headerlink" title="3.3 视觉追踪"></a>3.3 视觉追踪</h3><p>█[1] Wang, Naiyan, and Dit-Yan Yeung. “<strong>Learning a deep compact image representation for visual tracking.</strong>“ Advances in neural information processing systems. 2013. [pdf] (第一篇使用深度学习做视觉追踪的论文，DLT Tracker) ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5192-learning-a-deep-compact-image-representation-for-visual-tracking.pdf">http://papers.nips.cc/paper/5192-learning-a-deep-compact-image-representation-for-visual-tracking.pdf</a></p><p>█[2] Wang, Naiyan, et al. “<strong>Transferring rich feature hierarchies for robust visual tracking.</strong>“ arXiv preprint arXiv:1501.04587 (2015). [pdf] (SO-DLT) ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1501.04587">http://arxiv.org/pdf/1501.04587</a></p><p>█[3] Wang, Lijun, et al. “<strong>Visual tracking with fully convolutional networks.</strong>“ Proceedings of the IEEE International Conference on Computer Vision. 2015. [pdf] (FCNT) ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//www.cv-foundation.org/openaccess/content_iccv_2015/papers/Wang_Visual_Tracking_With_ICCV_2015_paper.pdf">http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Wang_Visual_Tracking_With_ICCV_2015_paper.pdf</a></p><p>█[4] Held, David, Sebastian Thrun, and Silvio Savarese. “<strong>Learning to Track at 100 FPS with Deep Regression Networks.</strong>“ arXiv preprint arXiv:1604.01802 (2016). [pdf] (GOTURN，在深度学习方法里算是非常快的，但仍比非深度学习方法慢很多) ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1604.01802">http://arxiv.org/pdf/1604.01802</a></p><p>█[5] Bertinetto, Luca, et al. “<strong>Fully-Convolutional Siamese Networks for Object Tracking.</strong>“ arXiv preprint arXiv:1606.09549 (2016). [pdf] (SiameseFC，实时物体追踪领域的最新前沿技术) ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1606.09549">https://arxiv.org/pdf/1606.09549</a></p><p>█[6] Martin Danelljan, Andreas Robinson, Fahad Khan, Michael Felsberg. “<strong>Beyond Correlation Filters: Learning Continuous Convolution Operators for Visual Tracking.</strong>“ ECCV (2016) [pdf] (C-COT) ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/C-COT_ECCV16.pdf">http://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/C-COT_ECCV16.pdf</a></p><p>█[7] Nam, Hyeonseob, Mooyeol Baek, and Bohyung Han. “<strong>Modeling and Propagating CNNs in a Tree Structure for Visual Tracking.</strong>“ arXiv preprint arXiv:1608.07242 (2016). [pdf] (VOT2016 获奖论文,TCNN) ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1608.07242">https://arxiv.org/pdf/1608.07242</a></p><h3 id="3-4-图像标注"><a href="#3-4-图像标注" class="headerlink" title="3.4 图像标注"></a>3.4 图像标注</h3><p>█[1] Farhadi,Ali,etal. “<strong>Every picture tells a story: Generating sentences from images</strong>“. In Computer VisionECCV 2010. Springer Berlin Heidelberg:15-29, 2010. [pdf] ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//www.cs.cmu.edu/~afarhadi/papers/sentence.pdf">https://www.cs.cmu.edu/~afarhadi/papers/sentence.pdf</a></p><p>█[2] Kulkarni, Girish, et al. “<strong>Baby talk: Understanding and generating image descriptions</strong>“. In Proceedings of the 24th CVPR, 2011. [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//tamaraberg.com/papers/generation_cvpr11.pdf">http://tamaraberg.com/papers/generation_cvpr11.pdf</a></p><p>█[3] Vinyals, Oriol, et al. “<strong>Show and tell: A neural image caption generator</strong>“. In arXiv preprint arXiv:1411.4555, 2014. [pdf] ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1411.4555.pdf">https://arxiv.org/pdf/1411.4555.pdf</a></p><p>█[4] Donahue, Jeff, et al. “<strong>Long-term recurrent convolutional networks for visual recognition and description</strong>“. In arXiv preprint arXiv:1411.4389 ,2014. [pdf]</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1411.4389.pdf">https://arxiv.org/pdf/1411.4389.pdf</a></p><p>█[5] Karpathy, Andrej, and Li Fei-Fei. “<strong>Deep visual-semantic alignments for generating image descriptions</strong>“. In arXiv preprint arXiv:1412.2306, 2014. [pdf] ★★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//cs.stanford.edu/people/karpathy/cvpr2015.pdf">https://cs.stanford.edu/people/karpathy/cvpr2015.pdf</a></p><p>█[6] Karpathy, Andrej, Armand Joulin, and Fei Fei F. Li. “<strong>D**</strong>eep fragment embeddings for bidirectional image sentence mapping**”. In Advances in neural information processing systems, 2014. [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1406.5679v1.pdf">https://arxiv.org/pdf/1406.5679v1.pdf</a></p><p>█[7] Fang, Hao, et al. “<strong>From captions to visual concepts and back</strong>“. In arXiv preprint arXiv:1411.4952, 2014. [pdf] ★★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1411.4952v3.pdf">https://arxiv.org/pdf/1411.4952v3.pdf</a></p><p>█[8] Chen, Xinlei, and C. Lawrence Zitnick. “<strong>Learning a recurrent visual representation for image caption generation</strong>“. In arXiv preprint arXiv:1411.5654, 2014. [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1411.5654v1.pdf">https://arxiv.org/pdf/1411.5654v1.pdf</a></p><p>█[9] Mao, Junhua, et al. “<strong>Deep captioning with multimodal recurrent neural networks (m-rnn)</strong>“. In arXiv preprint arXiv:1412.6632, 2014. [pdf] ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1412.6632v5.pdf">https://arxiv.org/pdf/1412.6632v5.pdf</a></p><p>█[10] Xu, Kelvin, et al. “<strong>Show, attend and tell: Neural image caption generation with visual attention</strong>“. In arXiv preprint arXiv:1502.03044, 2015. [pdf] ★★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1502.03044v3.pdf">https://arxiv.org/pdf/1502.03044v3.pdf</a></p><h3 id="3-5-机器翻译"><a href="#3-5-机器翻译" class="headerlink" title="3.5 机器翻译"></a>3.5 机器翻译</h3><p>部分里程碑研究被列入 RNN / Seq-to-Seq 版块。</p><p>█[1] Luong, Minh-Thang, et al. “<strong>Addressing the rare word problem in neural machine translation.</strong>“ arXiv preprint arXiv:1410.8206 (2014). [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1410.8206">http://arxiv.org/pdf/1410.8206</a></p><p>█[2] Sennrich, et al. <strong>“Neural Machine Translation of Rare Words with Subword Units</strong>“. In arXiv preprint arXiv:1508.07909, 2015. [pdf] ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1508.07909.pdf">https://arxiv.org/pdf/1508.07909.pdf</a></p><p>█[3] Luong, Minh-Thang, Hieu Pham, and Christopher D. Manning. “<strong>Effective approaches to attention-based neural machine translation</strong>.” arXiv preprint arXiv:1508.04025 (2015). [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1508.04025">http://arxiv.org/pdf/1508.04025</a></p><p><strong>█</strong>[4] Chung, et al. “<strong>A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation</strong>“. In arXiv preprint arXiv:1603.06147, 2016. [pdf] ★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1603.06147.pdf">https://arxiv.org/pdf/1603.06147.pdf</a></p><p>█[5] Lee, et al. “<strong>Fully Character-Level Neural Machine Translation without Explicit Segmentation</strong>“. In arXiv preprint arXiv:1610.03017, 2016. [pdf] ★★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1610.03017.pdf">https://arxiv.org/pdf/1610.03017.pdf</a></p><p>█[6] Wu, Schuster, Chen, Le, et al. “<strong>Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</strong>“. In arXiv preprint arXiv:1609.08144v2, 2016. [pdf] (Milestone) ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1609.08144v2.pdf">https://arxiv.org/pdf/1609.08144v2.pdf</a></p><h3 id="3-6-机器人"><a href="#3-6-机器人" class="headerlink" title="3.6 机器人"></a>3.6 机器人</h3><p>█[1] Koutník, Jan, et al. “<strong>Evolving large-scale neural networks for vision-based reinforcement learning.</strong>“ Proceedings of the 15th annual conference on Genetic and evolutionary computation. ACM, 2013. [pdf] ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//repository.supsi.ch/4550/1/koutnik2013gecco.pdf">http://repository.supsi.ch/4550/1/koutnik2013gecco.pdf</a></p><p>█[2] Levine, Sergey, et al. “<strong>End-to-end training of deep visuomotor policies.</strong>“ Journal of Machine Learning Research 17.39 (2016): 1-40. [pdf] ★★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//www.jmlr.org/papers/volume17/15-522/15-522.pdf">http://www.jmlr.org/papers/volume17/15-522/15-522.pdf</a></p><p>█[3] Pinto, Lerrel, and Abhinav Gupta. “<strong>Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours.</strong>“ arXiv preprint arXiv:1509.06825 (2015). [pdf] ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1509.06825">http://arxiv.org/pdf/1509.06825</a></p><p>█[4] Levine, Sergey, et al. “<strong>Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection</strong>.” arXiv preprint arXiv:1603.02199 (2016). [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1603.02199">http://arxiv.org/pdf/1603.02199</a></p><p>█[5] Zhu, Yuke, et al. “<strong>Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning.</strong>“ arXiv preprint arXiv:1609.05143 (2016). [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1609.05143">https://arxiv.org/pdf/1609.05143</a></p><p>█[6] Yahya, Ali, et al. “<strong>Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search.</strong>“ arXiv preprint arXiv:1610.00673 (2016). [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1610.00673">https://arxiv.org/pdf/1610.00673</a></p><p>█[7] Gu, Shixiang, et al. “<strong>Deep Reinforcement Learning for Robotic Manipulation.</strong>“ arXiv preprint arXiv:1610.00633 (2016). [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1610.00633">https://arxiv.org/pdf/1610.00633</a></p><p>█[8] A Rusu, M Vecerik, Thomas Rothörl, N Heess, R Pascanu, R Hadsell.”<strong>Sim-to-Real Robot Learning from Pixels with Progressive Nets.</strong>“ arXiv preprint arXiv:1610.04286 (2016). [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1610.04286.pdf">https://arxiv.org/pdf/1610.04286.pdf</a></p><p>█[9] Mirowski, Piotr, et al. “<strong>Learning to navigate in complex environments.</strong>“ arXiv preprint arXiv:1611.03673 (2016). [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1611.03673">https://arxiv.org/pdf/1611.03673</a></p><h3 id="3-7-艺术"><a href="#3-7-艺术" class="headerlink" title="3.7 艺术"></a>3.7 艺术</h3><p>█[1] Mordvintsev, Alexander; Olah, Christopher; Tyka, Mike (2015). “<strong>Inceptionism: Going Deeper into Neural Networks</strong>“. Google Research. [html] (Deep Dream) ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html">https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html</a></p><p>█[2] Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. “<strong>A neural algorithm of artistic style.</strong>“ arXiv preprint arXiv:1508.06576 (2015). [pdf] (杰出研究，迄今最成功的方法) ★★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1508.06576">http://arxiv.org/pdf/1508.06576</a></p><p>█[3] Zhu, Jun-Yan, et al. “<strong>Generative Visual Manipulation on the Natural Image Manifold.</strong>“ European Conference on Computer Vision. Springer International Publishing, 2016. [pdf] (iGAN) ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1609.03552">https://arxiv.org/pdf/1609.03552</a></p><p>█[4] Champandard, Alex J. “<strong>Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks.</strong>“ arXiv preprint arXiv:1603.01768 (2016). [pdf] (Neural Doodle) ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1603.01768">http://arxiv.org/pdf/1603.01768</a></p><p>█[5] Zhang, Richard, Phillip Isola, and Alexei A. Efros. “<strong>Colorful Image Colorization</strong>.” arXiv preprint arXiv:1603.08511 (2016). [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1603.08511">http://arxiv.org/pdf/1603.08511</a></p><p>█[6] Johnson, Justin, Alexandre Alahi, and Li Fei-Fei. “<strong>Perceptual losses for real-time style transfer and super-resolution</strong>.” arXiv preprint arXiv:1603.08155 (2016). [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1603.08155.pdf">https://arxiv.org/pdf/1603.08155.pdf</a></p><p>█[7] Vincent Dumoulin, Jonathon Shlens and Manjunath Kudlur. “<strong>A learned representation for artistic style.</strong>“ arXiv preprint arXiv:1610.07629 (2016). [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1610.00633">https://arxiv.org/pdf/1610.00633</a></p><p>█[8] Gatys, Leon and Ecker, et al.”<strong>Controlling Perceptual Factors in Neural Style Transfer.</strong>“ arXiv preprint arXiv:1611.07865 (2016). [pdf] (control style transfer over spatial location,colour information and across spatial scale) ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1610.04286.pdf">https://arxiv.org/pdf/1610.04286.pdf</a></p><p>█[9] Ulyanov, Dmitry and Lebedev, Vadim, et al. “<strong>Texture Networks: Feed-forward Synthesis of Textures and Stylized Images.</strong>“ arXiv preprint arXiv:1603.03417(2016). [pdf] (纹理生成和风格变化) ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1611.03673">https://arxiv.org/pdf/1611.03673</a></p><h3 id="3-8-目标分割-Object-Segmentation"><a href="#3-8-目标分割-Object-Segmentation" class="headerlink" title="3.8 目标分割 Object Segmentation"></a>3.8 目标分割 Object Segmentation</h3><p>█[1] J. Long, E. Shelhamer, and T. Darrell, “<strong>Fully convolutional networks for semantic segmentation.</strong>” in CVPR, 2015. [pdf] ★★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1411.4038v2.pdf">https://arxiv.org/pdf/1411.4038v2.pdf</a></p><p>█[2] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. “<strong>Semantic image segmentation with deep convolutional nets and fully connected crfs.</strong>“ In ICLR, 2015. [pdf] ★★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1606.00915v1.pdf">https://arxiv.org/pdf/1606.00915v1.pdf</a></p><p>█[3] Pinheiro, P.O., Collobert, R., Dollar, P. “<strong>Learning to segment object candidates.</strong>“ In: NIPS. 2015. [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1506.06204v2.pdf">https://arxiv.org/pdf/1506.06204v2.pdf</a></p><p>█[4] Dai, J., He, K., Sun, J. <strong>“Instance-aware semantic segmentation via multi-task network cascades.</strong>“ in CVPR. 2016 [pdf] ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1512.04412v1.pdf">https://arxiv.org/pdf/1512.04412v1.pdf</a></p><p>█[5] Dai, J., He, K., Sun, J. “<strong>Instance-sensitive Fully Convolutional Networks.</strong>“ arXiv preprint arXiv:1603.08678 (2016). [pdf] ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1603.08678v1.pdf">https://arxiv.org/pdf/1603.08678v1.pdf</a></p>]]></content>
    
    
    <categories>
      
      <category>1.科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文集</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1.科研/语言学荐书</title>
    <link href="undefined2020/02/24/1.%E7%A7%91%E7%A0%94/%E8%AF%AD%E8%A8%80%E5%AD%A6%E8%8D%90%E4%B9%A6/"/>
    <url>2020/02/24/1.%E7%A7%91%E7%A0%94/%E8%AF%AD%E8%A8%80%E5%AD%A6%E8%8D%90%E4%B9%A6/</url>
    
    <content type="html"><![CDATA[<h1 id="语言学方向书籍推荐–Serena-Gao"><a href="#语言学方向书籍推荐–Serena-Gao" class="headerlink" title="语言学方向书籍推荐–Serena Gao"></a>语言学方向书籍推荐–Serena Gao</h1><p>首先，做nlp不一定要很懂语言学，也不一定要跟语言学扯上关系。nlp可以仅是data mining，features engineering, 也的确有很多work目前在用文本或者对话做为数据集，然后用统计学方法实现目的，比如deep learning 。在某些任务上统计学模型功不可没，比如machine translation, speech recognition, question answering, etc. </p><p>如果题主只是对nlp的应用感兴趣，想泛泛了解一下目前进展的话，以上几个回答已经非常详细了，我接下来的回答可以不用看。许多主流大公司目前的力度都在deep learning, 学好nlp基本知识，做工程就够了(当然你还需要cs的background)， 语言学的东西不用太深入研究。</p><p>————————————-3.17 update——————————-</p><p>看了一下其他答案，大家的讨论和见解都很有趣，上来更新一点。</p><p>大多数人对nlp和语言学联系的了解，在于认为rule-based的nlp就是基于语言学。的确rule-based是语言学里广泛使用的，尤其是语法(syntax, syntactic structure)。现在machine learning的发展已经可以将rules转换为hidden states,人不用去操心提出大量rules来做exhaustive search。 </p><p><strong>但computational linguistics所包含的，远远大于rules。人类语言是漫长历史进化的高级产物，远不是成千上万个rules能描述清楚的。能被nlp利用的语言学，除了枚举rules外还有很多很多。</strong></p><p>比如定义。个人认为，在研究任何问题前，都必须要想清楚你的问题是什么，怎么定义。许许多多nlp research都是基于语言学上的定义，像我下文会提到的semantics, grammar。可是如果没有从沿用语言学的定义到nlp，这个0到1的过程，最早做researchers的人该如何想明白他们的research question？</p><p>做对话系统的同学应该很熟悉dialogue acts. 现在的对话系统发展迅猛，很多新应用都基于reinforcement learning, 并且取得显著成就。尤其是某些task-oriented dialogue generator, 早就不是十多年前的rule-based system了。但任何一个系统在设计之初都要采用dialouge acts定义（当然还有其他定义），来明确该系统的目的。不然该系统如何区分wh-question, yes-no question, greetings, 还有其他？（如果觉得见到“wh-”开头，问号结尾，就是一个wh-question rule, 那我不知道该说什么好了）</p><p><strong>以上讨论不在于反驳其他答主</strong>。很开心看到这么多人对nlp有热情并且愿意分享。<strong>只是做为一个看过很多nlp research，并且投身于natural language understanding(还稍微有点爱较真)的科研工作者，想要澄清人们对nlp和computational linguistics的一些误解做的小贡献。</strong> </p><p>明确自己的research task并且贯彻到底是好事，如果要做language modeling，基于machine learning/deep learning, 那真的不用费时间在语言学上。但觉得语言学是rule based已经过时了被淘汰了，这个锅语言学真的背的有点冤呀。</p><p>———————-(我真的不能再答题了该赶due了….)—————————</p><p>接下来的回答是，给真正对computational linguistics和nlp本身感兴趣的，对某些语言现象感兴趣，并打算在这条路上开始钻研的同学的一些建议。（想忽略细节的同学请直接拉到答案最后找reference）</p><p>=========================枯燥理论高能预警========================</p><p>人大脑工作不是靠probablistic language modeling，咱们谁的脑袋里都不会听到一个词然后跑一遍hidden markov，毕竟也进化了这么多年了不是。</p><p>与nlp相关，跟概率论并进的，除了传统的语言学，还有logic呢，Lofti Zadeh老爷爷研究了一辈子的fuzzy logic，也是在探究semantics&amp;world knowledge (再次感谢老爷爷的贡献，r.i.p)。</p><p>我也并不是在强调概率模型不重要，概率模型和现在很火的deep learning architecture像是基本功一样，而且是很好用的工具，其他答主已经强调很多，我就不再重复了。 除了这些，还有很多知识可以深入了解。</p><p>另外，语言学自身是个很大又很宽泛，又互相交叉的学科。有很多研究是跟literatures and arts有关，有的是跟cognitive science有关，还有neuroscience, mathematics, education, psychology, etc。我涉猎有限，在此只能回答跟computational linguistics有关(“to the best of my knowledge”)。</p><p>回归正题。语言学方面的书籍有很多，我接下来谈一下应该如何选和如何看。以下讨论只限英文，中文的工作我不了解，希望其他答主可以帮忙。</p><p><strong>Grammar</strong>是我会首先推荐的方向。Grammar分为morphology&amp;syntax. 在这里我主要指syntax.细节可以看Chomsky, Michael Colins, Jason Eisner等人的工作。现在大家用的最多的应该是stanford的syntactic parsing吧。这方面的工作已经很成熟，要处理语言基本是拿来就能用了。 但是语法树到底是什么，怎么构建，syntatic parsing优势，如何处理ambiguity,  想要做computational linguistics的话，这些很有必要知道。最基本的例子是，当用parser来处理你的句子，你起码要能看懂这个parser output是否make sense. </p><p>接着我会推荐<strong>Semantics</strong>. 这个部分是我做最多的，感觉也是被误解最多的。尤其推荐 “Meaning in language: An introduction to Semantics and Pragmatics.” 我并没有看完。Semantics是个很复杂的研究，可以涉及到语法，句法，world knowledge, 但最终还是回归semantics自身。目前nlp里很火的有distributional semantic representation (word embedding, phrase embedding, sentence embedding, etc), semantic parsing (logical form, etc), 等等等等。同一句话可以表达的意思太多了，同一个意思带来的表达形式也太多了。一个简单句子里包含的意思会涉及到当下对话双方的情景，以前或者以后会发生的事，等等。举个个人很喜欢的例子：</p><ul><li>2016年美国大选first presidential debate, Clinton vs Trump, 当trump被问到 “does the public’s right to know outweigh your personal .. (taxes)”, Trump: “… I will release my tax returns – against my lawyer’s wishes – when she releases her 33000 emails that have been deleted. <strong>As soon as she releases them, I will release</strong>. “. 最后一句话（粗体）包含的语意有：</li></ul><ol><li>等Hilary公开邮件记录之后，我就公开我的税务信息(动作和时间点)；</li><li>Hilary没公开，我也没公开(当下既定事实)；</li><li>Hilary不愿公开，我也不愿公开(sentiment)。</li><li>She – Clinton, I, my, – Trump, them – 33000 emails (co-reference).</li></ol><p>第一层意思是直观semantics, 能够被目前的semantic representation捕捉到。第二层是presupposition, 代表着在说话当下当事人双方默认已经发生的事情，是semantics研究中的难点；第三层包含了sentiment, 做情感分析的同学应该很了解，能否被目前的classifier捕捉到我不清楚。第四层是现在也很火的coreference resolution, 虽然原文里没有明确指代每个人称代词，但听众和当事人很直接能把每个人物代入，甚至包括Trump省略的”I will release (<strong>my taxes</strong>)”. 目前的co-reference resolution,e.g. stanford corenlp,  可以解决前三个代词，但省略的部分似乎还做不到。</p><p>（我还有很多例子，有空再上来更新）</p><p>对Semantic要求最高也是最难的，在nlp中应该是在natural language understanding相关应用了。Semantics里包含了太多太多的现象，如果能稍微研究并且model其中一小部分，对downstream application来说都会是一个很大的boost。前段时间有个shared task，叫 “hedge detection”,目的是找出文本信息中的hedges and cues。大部分人会关注这个shared  task下哪个模型做的最好，个人认为难点是在定义。有“but”,”however”出现语意就一定转折了么？如果被转折，是所在句子，还是段落还是一个小phrase呢？有dependency存在么？ 另一个相似shared task是negation detection. 想要理解这些问题本身和其难点所在，computational linguistics的前期知识储备是并不可少的。</p><p>以上两个方面应该可以展现一个big picture：前者代表语言结构是如何构建的，后者代表meaning是如何被赋予到某种结构里面的。</p><p>————小分割线————-</p><p>除了大框架外，小的方向取决于你的兴趣和目标所在。对话？文本？natural language understanding or natural language generation? </p><p>另外提两个我觉得必看的，很重要的理论，是computational pragmatics范畴里的：<strong>Grice’s maxims,</strong> 和<strong>Rational Speech Act(RSA)</strong>. 这两个理论其实紧密相关。 前者理论关于谈话双方为了有效沟通会有意识的遵守的一些principle, (同时可见“cooperative principle”), 后者关于为了达到这种有效沟通，对话当中存在的一种recursive process, 并且是bayesian inference. 如果你的工作跟 inference, reasoning相关，请一定要阅读。做对话系统的应该已经很熟悉了。</p><p>最后一个比较偏门的方向是我前面提到的fuzzy logic。目前还是有researcher继承Zadeh老爷爷的衣钵，并且用fuzzy logic做出了很多natural language generation, information extraction方面的成就。个人经验而言，我博士第一年(2014)一直在关注deep learning/machine learning方面，当时觉得它们是万能的。直到第二年夏天在忙一个project, 阅读了Zadeh老爷爷的大量工作，才感觉自己一直在以很片面的眼光看research。当时真的做了满满一本笔记。</p><p><img src="https://pic2.zhimg.com/v2-2d38d8c27831d6ceec69edf6535b8e19_b.jpg" alt="img">)<img src="https://pic2.zhimg.com/80/v2-2d38d8c27831d6ceec69edf6535b8e19_hd.jpg" alt="img"></p><p>===========================好累先写到这=========================</p><p>最后，如果兴趣在建modeling，deep learning architecture, 语言学方面的part-of-speech也好，parsing也好，都只是你的工具； </p><p>同样，如果兴趣在computational linguistics,语言现象，deep learning/machine learning都是你的工具。</p><p>取决与你的任务是什么，取决于你有没有完全dedicated的信心。毕竟巴菲特和Geff Hinton是少数，大多数人都无法预测20年后火的适合什么。</p><p>感谢阅读。希望能给在犹豫是否开始computational linguistics和nlp研究同学们一些帮助。</p><p>(任何不准确的地方还请大家指正)</p><p>=============================================================</p><p><strong>Reference:</strong></p><p>（大方向书籍，我要是能全部买下来就好了…并没有全部看完，有的只是看过某一章节。Grammar和syntax知乎里面有很多问答跟这方面有关，在此不重复了。）</p><p>Cruse, Alan. “Meaning in language: An introduction to semantics and pragmatics.” (2011).</p><p>Karttunen, Lauri (1974) <a href="https://link.zhihu.com/?target=http%3A//www.stanford.edu/~laurik/publications/archive/presupplingcontext.pdf">[1]</a>. <em>Theoretical Linguistics</em> 1 181-94. Also in Pragmatics: A Reader, Steven Davis (ed.), pages 406-415, Oxford University Press, 1991.</p><p>Kadmon, Nirit. “Formal pragmatics semantics, pragmatics, presupposition, and focus.” (2001).</p><p>Levinson, Stephen C. <em>Pragmatics.</em>Cambridge: Cambridge University Press, 1983, pp. 181-184.</p><p>Wardhaugh, Ronald. <em>An introduction to sociolinguistics</em>. John Wiley &amp; Sons, 2010. (这本书的影响力很大，有很多跟social science的讨论)</p><p>(具体其他上面提到的，每一篇我都仔细读过的)</p><p><a href="https://link.zhihu.com/?target=https%3A//www.sas.upenn.edu/~haroldfs/dravling/grice.html">Grice’s Maxims</a></p><p>Monroe, Will, and Christopher Potts. “Learning in the rational speech acts model.” <em>arXiv preprint arXiv:1510.06807</em> (2015).(这篇是关于rsa如何被用于具体task上的)</p><p>Farkas, Richárd, et al. “The CoNLL-2010 shared task: learning to detect hedges and their scope in natural language text.” <em>Proceedings of the Fourteenth Conference on Computational Natural Language Learning—Shared Task</em>. Association for Computational Linguistics, 2010. (上文提到的hedge and cues shared task,关于linguistics里的现象是如何被formulate成nlp问题的)</p><p>Morante, Roser, and Eduardo Blanco. “* SEM 2012 shared task: Resolving the scope and focus of negation.” <em>Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation</em>. Association for Computational Linguistics, 2012. (negation 的shared task)</p><p>最后附上两篇老爷爷对我影响最大的：</p><p>Zadeh, Lotfi Asker. “The concept of a linguistic variable and its application to approximate reasoning—I.” <em>Information sciences</em> 8.3 (1975): 199-249.</p><p>Zadeh, Lotfi A. “The concept of a linguistic variable and its application to approximate reasoning—II.” <em>Information sciences</em> 8.4 (1975): 301-357.（ 这系列work分两部。）</p><p>Zadeh, Lotfi A. “Toward a theory of fuzzy information granulation and its centrality in human reasoning and fuzzy logic.” <em>Fuzzy sets and systems</em> 90.2 (1997): 111-127.</p><p><img src="https://pic2.zhimg.com/v2-2b2d9b7cd97948eb565daa7adececf15_b.jpg" alt="img">)<img src="https://pic2.zhimg.com/80/v2-2b2d9b7cd97948eb565daa7adececf15_hd.jpg" alt="img"></p><p><a href="">编辑于 2018-03-18</a></p>]]></content>
    
    
    <categories>
      
      <category>1.科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>语言学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1.科研/3 - 选题</title>
    <link href="undefined2020/02/24/1.%E7%A7%91%E7%A0%94/3%20-%20%E9%80%89%E9%A2%98/"/>
    <url>2020/02/24/1.%E7%A7%91%E7%A0%94/3%20-%20%E9%80%89%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h1 id="论文选题"><a href="#论文选题" class="headerlink" title="论文选题"></a>论文选题</h1><ol><li>经过调研，已对本领域有基本认识，具备了得到idea的条件。</li><li>idea要新颖，要能推动科学的发展，同时要有可复现性和可实现性。</li><li>把握好与现存结果之间的delta</li><li>要因时而动，像语音识别和人脸识别这种已经落地的项目，可能已经没有什么突破的空间了，现在在业界拼的是数据和算力。而常识，知识推理，复杂语境，跨模态理解，可解释智能。这些点目测不能通过数据驱动的方式解决。更高层次的联想、创造、顿悟等认知能力，更是连边还没碰到。这些问题是有远见的研究者应该关注的方向。</li></ol><p>补全了相关知识，阅读了大量的文献，走访了各位前辈，观察了各圈风向标，调整首文合理预期后，</p><p><strong>我的idea是：</strong>_<strong><strong><strong><strong><strong><strong><strong>____</strong></strong></strong></strong></strong></strong></strong>.</p><p><strong>建议2：如何选择第一个好题目？</strong></p><h3 id="什么算是好的idea"><a href="#什么算是好的idea" class="headerlink" title="什么算是好的idea"></a>什么算是好的idea</h3><p>作者：刘知远 </p><p>2015年，我在微博上写过一个调侃的小段子：</p><blockquote><p>ML派坐落美利坚合众山中，百年来武学奇才辈出，隐然成江湖第一大名门正派，门内有三套入门武功，曰：图模型加圈，神经网加层，优化目标加正则。有童谣为证：熟练ML入门功，不会作文也会诌。</p></blockquote><p>到了2018年，我又续了一小段：</p><blockquote><p>不期数年，北方DL神教异军突起，内修表示学习，外练神经网络，心法众多，曰门，曰注意，曰记忆，曰对抗，曰增强。经ImageNet一役威震武林，豢Alpha犬一匹无人可近。一时家家筑丹炉，人人炼丹忙，门徒云集，依附者众，有一统江湖之势。有童谣为证：左手大数据，右手英伟达，每逢顶会炼丹忙。</p></blockquote><p>这里面提到的图模型加圈、神经网络加层、优化目标加正则，神经网络中的门、注意、记忆等，都是一些改进模型性能的创新思路，被各大NLP任务广泛使用并发表论文，也许就是因为被不同NLP任务的重复使用和发表，多少有些审美疲劳而缺少更深的创新思想，被有些网友和学者诟为“灌水”，似乎都不算好的想法。</p><p>那么什么才是好的想法呢？我理解这个”好“字，至少有两个层面的意义。</p><h3 id="学科发展角度的”好“"><a href="#学科发展角度的”好“" class="headerlink" title="学科发展角度的”好“"></a>学科发展角度的”好“</h3><p>学术研究本质是对未知领域的探索，是对开放问题的答案的追寻。所以从推动学科发展的角度，评判什么是好的研究想法的标准，首先就在一个“<strong>新</strong>”字。</p><p>过去有个说法，人工智能学科有个魔咒，凡是人工智能被解决（或者有解决方案）的部分，就不再被认为代表“人类智能”。计算机视觉、自然语言处理、机器学习、机器人之所以还被列为人工智能主要方向，也许正是因为它们尚未被解决，尚能代表“人类智能”的尊严。而我们要开展创新研究，就是要提出新的想法解决这些问题。这其中的”新“字，可以体现在提出新的问题和任务，探索新的解决思路，提出新的算法技术，实现新的工具系统等。</p><p>在保证”新“的基础上，研究想法好不好，那就看它<strong>对推动学科发展的助力有多大</strong>。深度学习之所以拥有如此显赫的影响力，就在于它对于人工智能自然语言处理、语音识别、计算机视觉等各重要方向都产生了革命性的影响，彻底改变了对无结构信号（语音、图像、文本）的语义表示的技术路线。</p><h3 id="研究实践角度的”好“"><a href="#研究实践角度的”好“" class="headerlink" title="研究实践角度的”好“"></a>研究实践角度的”好“</h3><p>那是不是想法只要够”新“就好呢？是不是越新越好呢？我认为应该还不是。因为，只有<strong>能做得出来的想法</strong>才有资格被分析好不好。所以，从研究实践角度，还需要考虑研究想法的<strong>可实现性</strong>和<strong>可验证性。</strong></p><p>可实现性，体现在该想法是否有足够的数学或机器学习工具支持实现。可验证性，体现在该想法是否有合适的数据集合和广泛接受的评价标准。很多民间科学家的想法之所以得不到学术界的认同，就是因为这些想法往往缺乏可实现性和可验证性，只停留在天马行空的纸面，只是些虚无缥缈的理念。</p><h2 id="好的研究想法从哪里来"><a href="#好的研究想法从哪里来" class="headerlink" title="好的研究想法从哪里来"></a>好的研究想法从哪里来</h2><p>想法好还是不好，并不是非黑即白的二分问题，而是像光谱一样呈连续分布，因时而异，因人而宜。计算机科技领域的发展既有积累的过程，也有跃迁的奇点，积累量变才会产生质变，吃第三个馒头饱了，也是因为前面两个馒头打底。</p><p>现在的学术研究已经成为高度专业化的职业，有庞大的研究者群体。”Publish or Perish“，是从事学术职业（如教授、研究员、研究生）的人必须做好平衡的事情，不能要求研究者的每份工作都是“诺贝尔奖”或“图灵奖”级的才值得发表。只要对研究领域的发展有所助力，就值得发表出来，帮助同行前进。鲁迅说：天才并不是自生自长在深林荒野里的怪物，是由可以使天才生长的民众产生，长育出来的，所以没有这种民众，就没有天才。这个庞大研究者群体正是天才成长的群众基础。同时，学术新人也是在开展创新研究训练中，不断磨砺寻找好想法能力，鲁迅也说：即使天才，在生下来的时候的第一声啼哭，也和平常的儿童的一样，决不会就是一首好诗。</p><p>那么，好的研究想法从哪里来呢？我总结，首先要有区分研究想法好与不好的能力，这需要<strong>深入全面了解所在研究方向的历史与现状</strong>，具体就是对学科文献的全面掌握。人是最善于学习的动物，完全可以将既有文献中不同时期研究工作的想法作为学习对象，通过了解它们提出后对学科发展的影响——具体体现在论文引用、学术评价情况等各方面——建立对研究想法好与不好的评价模型。我们很难条分缕析完美地列出区分好与不好想法的所有特征向量，但人脑强大的学习能力，只要给予足够的输入数据，就可以在神经网络中自动学习建立判别的模型，鉴古知今，见微知著，这也许就是常说的学术洞察力。</p><p>做过一些研究的同学会有感受，仅阅读自己研究方向的文献，新想法还是不会特别多。这是因为，读到的都是该研究问题已经完成时的想法，它们本身无法启发新的想法。如何产生新的想法呢？我总结有三种可行的基本途径：</p><p><strong>实践法</strong>。即在研究任务上实现已有最好的算法，通过分析实验结果，例如发现这些算法计算复杂度特别高、训练收敛特别慢，或者发现该算法的错误样例呈现明显的规律，都可以启发你改进已有算法的思路。现在很多自然语言处理任务的Leaderboard上的最新算法，就是通过分析错误样例来有针对性改进算法的 [1]。</p><p><strong>类比法</strong>。即将研究问题与其他任务建立类比联系，调研其他相似任务上最新的有效思想、算法或工具，通过合理的转换迁移，运用到当前的研究问题上来。例如，当初注意力机制在神经网络机器翻译中大获成功，当时主要是在词级别建立注意力，后来我们课题组的林衍凯和沈世奇提出建立句子级别的注意力解决关系抽取的远程监督训练数据的标注噪音问题 [2]，这就是一种类比的做法。</p><p><strong>组合法</strong>。即将新的研究问题分解为若干已被较好解决的子问题，通过有机地组合这些子问题上的最好做法，建立对新的研究问题的解决方案。例如，我们提出的融合知识图谱的预训练语言模型，就是将BERT和TransE等已有算法融合起来建立的新模型 [3]。</p><p>正如武侠中的最高境界是无招胜有招，好的研究想法并不拘泥于以上的路径，很多时候是在研究者对研究问题深刻认知的基础上，综合丰富的研究阅历和聪明才智产生”顿悟“的结果。这对初学者而言恐怕还很难一窥门径，需要从基本功做起，经过大量科研实践训练后，才能有登堂入室之感。</p><p>在科研实践过程中，除了通过大量文献阅读了解历史，通过深入思考总结产生洞察力外，还有一项必不可少的工作，那就是主动开放的学术交流和合作意识。不同研究领域思想和成果交流碰撞，既为创新思想提供了新的来源，也为”类比“和”顿悟“提供了机会。了解一下历史就可以知晓，人工智能的提出，就是数学、计算机科学、控制论、信息论、脑科学等学科交叉融合的产物。而当红的深度学习的起源，1980年代的Parallel Distributed Processing （PDP），也是计算机科学、脑认知科学、心理学、生物学等领域研究者通力合作的产物。下面是1986年出版的名著《Parallel Distributed Processing: Explorations in the Microstructure of Cognition》第一卷的封面。</p><p><img src="https://pic2.zhimg.com/80/v2-a8d3f6e553f9f279cdafea5a3e218701_hd.jpg" alt="img"></p><p>作者在前言中是这么讲他们的合作过程的，在最初长达六个月的时间里，它们每周见面交流两次讨论研究进展。</p><blockquote><p>We expected the project to take about <strong>six months</strong>. We began in January 1982 by bringing a number of our colleagues together to form a discussion group on these topics. <strong>During the first six months we met twice weekly</strong> and laid the foundation for most of the work presented in these volumes.</p></blockquote><p>而书中提供的PDP研究组的成员名单，40年后的今天仍让我惊叹其高度的跨机构、跨学科的交叉特点。所以，特别建议同学们在科研训练中，在专注研究问题的前提下，保持主动的学术交流意识，无论是听讲座报告，参加学术会议，还是选修课程，都有意识地扩宽学术交流的广度，不仅与小同行打成一片，更有看似八竿子打不着的研究领域的学术伙伴。随着研究经历的丰富，会越来越强烈地感受到，越是大跨度交叉的学术报告，越让你受到更大的启发，产生更多让自己兴奋的研究想法。</p><p><img src="https://pic4.zhimg.com/80/v2-404a752001300a69baabd40fb3d78b99_hd.jpg" alt="img"></p><h2 id="初学者应该怎么做"><a href="#初学者应该怎么做" class="headerlink" title="初学者应该怎么做"></a>初学者应该怎么做</h2><p>与阅读论文、撰写论文、设计实验等环节相比，如何产生好的研究想法，是一个不太有章可循的环节，很难总结出固定的范式可供遵循。像小马过河，需要通过大量训练实践，来积累自己的研究经验。不过，对于初学者而言，仍然有几个简单可行的原则可以参考。</p><p><strong>一篇论文的可发表价值，取决于它与已有最直接相关工作间的Delta</strong>。我们大部分研究工作都是站在前人工作的基础上推进的。牛顿说：如果说我看得比别人更远些，那是因为我站在巨人的肩膀上。在我看来，评判一篇论文研究想法的价值，就是看它站在了哪个或哪些巨人的肩膀上，以及在此基础上又向上走了多远。反过来，在准备开始一项研究工作之前，在形成研究想法的时候，也许要首先明确准备站在哪个巨人的肩膀上，以及计划通过什么方式走得更远。与已有最直接相关工作之间的Delta，决定了这个研究想法的价值有多大。</p><p><strong>兼顾摘果子和啃骨头</strong>。人们一般把比较容易想到的研究想法，叫做Low Hanging Fruit（低垂果实）。低垂果实容易摘，但同时摘的人也多，选择摘果子就容易受到想法撞车的困扰。例如，2018年以BERT为首的预训练语言模型取得重大突破，2019年中就出现大量改进工作，其中以跨模态预训练模型为例，短短几个月里<a href="http://link.zhihu.com/?target=http%3A//arxiv.org">http://arxiv.org</a>上挂出了超过六个来自不同团队的图像与文本融合的预训练模型 [4]。设身处地去想，进行跨模态预训练模型研究，就是一个比较容易想到的方向，你一定需要有预判能力，知道世界上肯定会有很多团队也同时开展这方面研究，这时你如果选择入场，就一定要做得更深入更有特色，有自己独特的贡献才行。相对而言，那些困难的问题，愿意碰的人就少，潜下心来啃硬骨头，也是不错的选择，当然同时就会面临做不出来的风险，或者做出来也得不到太多关注的风险。同学需要根据自身特点、经验和需求，兼顾摘果子和啃骨头两种类型的研究想法。</p><p><img src="https://pic2.zhimg.com/80/v2-d71aaf2b86116e3ea1e891bf9230a2c4_hd.jpg" alt="img"></p><p><strong>注意多项研究工作的主题连贯性</strong>。同学的研究训练往往持续数年，需要注意前后多项研究工作的主题连贯性，保证内在逻辑统一。需要考虑，在个人简历上，在出国申请Personal Statement中，或者在各类评奖展示中，能够将这些研究成果汇总在一起，讲出自己开展这些研究工作的总目标、总设想。客观上讲，人工智能领域研究节奏很快，技术更新换代快，所以成果发表也倾向于小型化、短平快。我有商学院、社科的朋友，他们一项研究工作往往需要持续一年甚至数年以上；高性能计算、计算机网络方向的研究周期也相对较长。人工智能这种小步快跑的特点，决定了很多同学即使本科毕业时，也会有多篇论文发表，更不用说硕士生、博士生。在这种情况下，就格外需要在研究选题时，注意前后工作的连贯性和照应关系。几项研究工作放在一起，到底是互相割裂说不上话，还是在为一个统一的大目标而努力，格外反映研究的大局意识和布局能力。例如，下图是我们课题组涂存超博士2018年毕业时博士论文《面向社会计算的网络表示学习》的章节设置，整体来看就比《社会计算的若干重要问题研究》等没有内在关联的写法要更让人信服一些。当然，对于初学者而言，一开始就想清楚五年的研究计划，根本不可能。但想，还是不去想，结果还是不同的。</p><p><img src="https://pic4.zhimg.com/80/v2-9fbee2d16f9c05fa1cb1ec86a27d265c_hd.jpg" alt="img"></p><p><strong>注意总结和把握研究动态和趋势，因时而动</strong>。2019年在知乎上有这样一个问题：”2019年在NLP领域，资源有限的个人/团队能做哪些有价值有希望的工作？“ 我当时的回答如下：</p><blockquote><p>我感觉，产业界开始集团化搞的问题，说明其中主要的开放性难题已经被解决得差不多了，如语言识别、人脸识别等，在过去20年里面都陆续被广泛商业应用。看最近的BERT、GPT-2，我理解更多的是将深度学习对大规模数据拟合的能力发挥到极致，在深度学习技术路线基本成熟的前提下，大公司有强大计算能力支持，自然可以数据用得更多，模型做得更大，效果拟合更好。<br>成熟高新技术进入商用竞争，就大致会符合摩尔定律的发展规律。现在BERT等训练看似遥不可及，但随着计算能力等因素的发展普及，说不定再过几年，人人都能轻易训练BERT和GPT-2，大家又会在同一个起跑线上，把目光转移到下一个挑战性难题上。<br>所以不如提前考虑，哪些问题是纯数据驱动技术无法解决的。NLP和AI中的困难任务，如常识和知识推理，复杂语境和跨模态理解，可解释智能，都还没有可行的解决方案，我个人也不看好数据驱动方法能够彻底解决。更高层次的联想、创造、顿悟等认知能力，更是连边还没碰到。这些正是有远见的研究者们应该开始关注的方向。</p></blockquote><p>需要看到，不同时期的研究动态和趋势不同。把握这些动态和趋势，就能够做出研究社区感兴趣的成果。不然的话，即使研究成果没有变化，只是简单早几年或晚几年投稿，结果也会大不相同。例如，2013年word2vec发表，在2014-2016年之间开展词表示学习研究，就相对比较容易得到ACL、EMNLP等会议的录用；但到了2017-2018年，ACL等会议上的词表示学习的相关工作就比较少见了。</p>]]></content>
    
    
    <categories>
      
      <category>1.科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文写法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1.科研/4 - 模型设计</title>
    <link href="undefined2020/02/24/1.%E7%A7%91%E7%A0%94/4%20-%20%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1/"/>
    <url>2020/02/24/1.%E7%A7%91%E7%A0%94/4%20-%20%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1/</url>
    
    <content type="html"><![CDATA[<h1 id="模型设计"><a href="#模型设计" class="headerlink" title="模型设计"></a>模型设计</h1><h4 id="确定进入-领域后，如何快速学会第一个技能？"><a href="#确定进入-领域后，如何快速学会第一个技能？" class="headerlink" title="确定进入____领域后，如何快速学会第一个技能？"></a><strong>确定进入_<strong>___</strong>领域后，如何快速学会第一个技能？</strong></h4><p>仔细研究一般现有的主要工具，流派和方法，先入门。<br>我的建议是：找到一个开源项目，比如机器翻译或者深度学习的项目。理解开源项目的任务，编译通过该项目发布的示范程序，得到与项目示范程序一致的结果。然后再深入理解开源项目示范程序的算法。自己编程实现一下这个示范程序的算法。再按照项目提供的标准测试集测试自己实现的程序。如果输出的结果与项目中出现的结果不一致，就要仔细查验自己的程序，反复修改，直到结果与示范程序基本一致。如果还是不行，就大胆给项目的作者写信请教。在此基础上，再看看自己能否进一步完善算法或者实现，取得比示范程序更好的结果。</p><h4 id="如何改进别人的模型"><a href="#如何改进别人的模型" class="headerlink" title="如何改进别人的模型"></a><strong>如何改进别人的模型</strong></h4><ul><li><p>反复阅读本领域最新发表的文章，多阅读本领域牛人发表的文章。在深入了解已有工作的基础上，探讨还有没有一些地方可以推翻、改进、综合、迁移。注意做实验的时候，不要贪多，每次实验只需要验证一个想法。</p></li><li><p>每次实验之后必须要进行分析存在的错误，找出原因。</p></li></ul><ul><li>对成功的实验，进一步探讨如何改进算法。注意实验数据必须是业界公认的数据。</li><li>与已有的算法进行比较，体会能够得出比较一般性的结论。如果有，则去写一篇文章，否则，应该换一个新的选题。</li></ul><h4 id="如何发明自己的模型"><a href="#如何发明自己的模型" class="headerlink" title="如何发明自己的模型"></a>如何发明自己的模型</h4><ol><li><p>定义你的任务，例如summarization</p></li><li><p>定义数据集，最好使用现成的数据集，因为他们已经有baselines</p></li><li><p>建立baseline，他可以是一个非常简单的一元线性回归，然后在你的训练数据集上计算你的评价标准，看看模型是过拟合还是欠拟合</p></li><li><p>遍历现有模型，选好模型，在baseline上改进.</p></li><li><p>自己创造新模型（Opitional）</p><ol><li><p>首先，你需要做好以上说的几个步骤.</p></li><li><p>然后你需要知道已经存在的模型上有哪些问题。然后你就可以设计出自己的模型。如果你想要这样做的话，<strong>你真的需要和你的导师和其他研究者保持沟通，除非你自己就是研究者并且已经获得了博士文凭</strong>。</p></li><li><p>你需要实现你的模型，然后根据你的新点子去对它快速<strong>迭代</strong>。（也许在某个位置新加一层？然后看看他起不起作用？），<strong>这种迭代的思想平滑了难度曲线，是版本控制的思想，类似于先搭起框架再往里边慢慢实现不同功能的API接口。</strong></p></li><li><p>那么在迭代的过程中，拥有足够多的的软件工程技能来配置高效的实验框架，从而能收集到这些结果就很重要。<strong>软工能力难顶，只能见招拆招了</strong></p></li><li><p>建议从一个和你的真实想法比起来相对容易很多的模型做起。先把简单模型建立起来。然后逐步尝试更复杂的模型。</p></li></ol></li><li><p>对于开始提出的终极任务：summarization。</p><ol><li>一开始你可能尝试一些非常简单的模型。比如对自然段中的所有单词向量求平均，然后用贪心搜索一个接一个地生成单词，或者用贪心搜索对维基百科中的现有文章寻找一些片段，然后把合适的片段复制过去。</li><li>然后升级你的目标，尝试某些真正让你生产整段总结的方法。</li></ol></li></ol><p><img src="http://q0u9fsub0.bkt.clouddn.com///20191202/pO3C6CoWauYN.png?imageslim" alt="mark"></p><p>CS224N课程大作业的一些关于模型选择的idea（包括了在kaggle上打比赛）</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1420_37_463.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>1.科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文写法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1.科研/7 - 论文结构</title>
    <link href="undefined2020/02/24/1.%E7%A7%91%E7%A0%94/7%20-%20%E8%AE%BA%E6%96%87%E7%BB%93%E6%9E%84/"/>
    <url>2020/02/24/1.%E7%A7%91%E7%A0%94/7%20-%20%E8%AE%BA%E6%96%87%E7%BB%93%E6%9E%84/</url>
    
    <content type="html"><![CDATA[<h1 id="7-论文结构"><a href="#7-论文结构" class="headerlink" title="7.论文结构"></a>7.论文结构</h1><p>学术研究是一项系统工程</p><p>在这个系统工程中，论文的作用则是，向学术界同行清晰准确地描述成果的创新点、技术思路、算法细节和验证结果。明白这一点，才能正确的对待论文写作：一项乏善可陈的工作，很难通过写作变得众星捧月；一项充满创新的成果，却有可能因为糟糕的写作而无法向审稿人准确传递重要价值所在，延误成果发表。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1301_14_107.png" alt=""></p><hr><p><strong>一篇NLP论文的典型结构</strong></p><p>NLP学术会议（甚至包括期刊）论文已经形成比较固定的结构。绝大部分论文由以下六大部分构成：摘要（Abstract）、介绍（Introduction）、相关工作（Related Work）、方法（Method）、实验（Experiment）、结论（Conclusion）。少数论文会根据创新成果形式不同而略有不同，例如提出新数据集的论文，可能会把Method部分调整为Dataset的标注与分析，但不影响论文整体构成。每个部分作用不同：</p><ul><li>摘要：用100-200词简介研究任务与挑战、解决思路与方法、实验效果与结论。</li><li>介绍：用1页左右篇幅，比摘要更详细地介绍研究任务、已有方法、主要挑战、解决思路、具体方法、实验结果。</li><li>相关工作：用0.5-1页左右篇幅介绍研究任务的相关工作，说明本文工作与已有工作的异同。</li><li>方法：用2-3页篇幅介绍本文提出的方法模型细节。</li><li>实验：用2-3页篇幅介绍验证本文方法有效性的实验设置、数据集合、实验结果、分析讨论等。</li><li>结论：简单总结本文主要工作，展望未来研究方向。</li></ul><p>如前所说，论文的作用是向学术界同行清晰准确地描述成果的创新点、技术思路、算法细节和验证结果。由于学术界的<strong>同行评审</strong>制度，贯穿全文的线索和目标就是要论证这份工作的<strong>创新价值</strong>，每个部分都要各司其职为这个目标而服务。为了实现这个目标，需要作者特别注意以下几点：</p><p><strong>（1）学会换位思考。</strong>要始终站在审稿人或读者的角度审视论文，思考如何更清晰地表达。这是初学者最容易忽视的问题：作为研究成果的亲历者，论文作者掌握所有细节，如果不多加留意，写作中就会出现新概念没有被明确定义就被使用等情况，很多描述和分析缺少逻辑衔接。对作者而言，这些省去的东西并不影响他对这些文字的理解；但对并不了解这份工作的读者而言，这无疑是一场噩梦，因为他们并没有作者脑中的那套背景信息。因此，写作时要时时留神，读者读这句时能否理解，所需要的背景知识前文是否已经介绍。</p><p><strong>（2）注意逻辑严谨</strong>。严谨是学术论文的底色，从引用格式、公式符号到谋章造句，虽不至于美国法学期刊的Bluebook那么变态，都力求风格统一，行文严谨。引用、公式、拼写等方面都容易学，初学者更需要注意行文严谨，力求全文从章节、段落、句子等不同级别都逻辑严密，争取做到没有一句话没来由，没有一句话没呼应：</p><ul><li>章节层面，Introduciton提到已有方法面临的几个挑战，就要对应本文提出的几个创新思路，对应Method中的几个具体算法，对应Experiment中的几个实验验证。</li><li>段落和句子层面，段间要注意照应，是并列、递进、转折还是总分关系，需要谋划妥当，要有相应句子或副词衔接。段内各句，有总有分，中心思想句和围绕论述句分工协作。</li></ul><p>除了整体结构上的建议外，每个部分也各有定式，下面按各部分提供一些写作建议，同时用我们最近发表的一篇ACL 2018论文 [2] 作为例子。</p><p><img src="https://qph.fs.quoracdn.net/main-qimg-bbaa8a2bb6839bae078ad00526fd55e8.webp" alt="img"></p><hr><p><strong>Abstract和Introduction怎么写</strong></p><p>Abstract可以看做对Introduction的提要，所以我们先介绍Introduction的写法，然后再说如何写Abstract。Introduction是对整个工作的全面介绍，是决定一篇论文能否被录用的关键。一般Introduction这么写：起手介绍<strong>研究任务</strong>和意义；随后简介面向这个任务的<strong>已有方法</strong>；接着说明已有方法面临的<strong>关键挑战</strong>；针对这些挑战，本文提出什么<strong>创新思路</strong>和具体方法；最后介绍<strong>实验结果</strong>证明本文提出方法的有效性。这几个部分各挡一面，同时又有严密的内在逻辑。每个部分也各有章法，下面分别介绍对各部分的建议：</p><p><strong>（1）研究任务</strong>。介绍本文的研究任务及其在该研究领域的重要价值和意义。如果是领域公认的重要任务的话，则可以不用详细论述其研究价值/意义；如果是新提出的研究任务，则需要花费比较多篇幅论证该任务的价值。如下所示论文[2]的第1段集中说明阅读理解研究任务。</p><p><img src="https://pic3.zhimg.com/v2-fbc75968b4f84069cf96d9f93502a7ca_b.jpg" alt="img"></p><p><strong>（2）已有方法</strong>。从研究任务递进一步，介绍这个任务的已有代表方法。如下所示论文[2]的第2段，开始介绍DS-QA。需要注意，这个已有方法需要是目前最好、最具代表性的，也是本文工作准备改进的。所谓站在巨人的肩膀上，一篇值得发表的论文需要找到那个最高的巨人。</p><p><img src="https://pic2.zhimg.com/v2-e452aa66a7d8ef7d443c2530ee7f30fd_b.jpg" alt="img"></p><p><strong>（3）面临挑战</strong>。已有方法一定仍然存在某些不足或挑战，才需要进一步研究改进。因此，需要总结已有方法面临的挑战。这是Introduction的关键部分，起着承上启下的作用。初学者特别注意，这部分涉及对已有工作的评价，务必保证精准客观。要知道，当论文投稿至NLP国际会议后，是通过同行评审决定是否录用发表，评审人一般是小同行，有很大概率是已有工作的作者。所以这部分论述一定要做到客观公正，让这些工作作者本人也能信服。</p><p>如下所示论文[2]的第3、4段，先介绍DS-QA的noisy labeling挑战，并且通过举例直观呈现。面对这个挑战，已有一些相关工作，还需说明他们各自有什么不足和挑战，为引出本文创新思路做好铺垫。</p><p><img src="https://pic3.zhimg.com/v2-18c8049ebac37a3fc1cefce247598c5a_b.jpg" alt="img"></p><p><strong>（4）创新思路</strong>。水来土掩，兵来将挡，既然已有方法有这些不足和挑战，就需要有新的创新思路和方法。这部分需要注意与上面的”挑战“部分严丝合缝，密切呼应，让读者清楚领会到这些创新思路与方法的确能够解决或缓解这些挑战问题。</p><p>如下所示论文[2]的第5段，就是介绍创新思路和方法。可以看到，一般”面临挑战“和”创新思路“部分还配图示，更直观地展示本文要解决的挑战问题和创新思路。例如论文[2]这张丑丑的图，比较直观地展示了创新方法包括Selector和Reader两个模块和作用。也可以随便看我们的其他论文[3]，大部分论文都会在Introduction中提供图示。</p><p><img src="https://pic4.zhimg.com/v2-2123386d4237163c56104af1ae38090f_b.jpg" alt="img"></p><p><strong>（5）实验结论</strong>。除了在”创新思路“部分图文两开花地说明本文创新工作外，还要通过合理的实验验证方法的有效性。一般要得到”our method achieves significant and consistent improvement as compared to other baselines“的结论，从而验证本文工作的创新性。</p><p><img src="https://pic3.zhimg.com/v2-f378a4cd00c8d1ee2b2024b27d85cb92_b.jpg" alt="img"></p><p>有些论文最后还会体贴的总结本文的主要贡献，一般说”In summary, the key contributions are x-fold: (1)…(2)…(3)…“。这样做的好处是，可以帮助审稿人总结本文的创新点放在审稿意见中，节省不少工作量。但需要注意，这些创新点要简洁明了，不能是前文的简单重复，也不能overclaim。如果要说”首次“提出或发现，一般也要前置”to the best of our knowledge“。此外还有论文最后一段会介绍接下来几个Section结构，个人感觉对一篇8页论文可能并不需要。</p><p>对于Abstract，可以看做对Introduction的简介，最简单的做法是，以上每部分都精简为1-2句话组成Abstract皆可。如下是论文[2]的Abstract内容，可以看出与Introduction的对应关系。</p><p><img src="https://pic2.zhimg.com/v2-bddf467858b05eed8d4c447bdc101831_b.jpg" alt="img"></p><hr><p><strong>Method怎么写</strong></p><p>这部分要详细介绍本文创新方法的具体细节，由于涉及非常艰涩的细节，要采用”总-分“结构来介绍。</p><p>这部分起手”总“的部分要介绍本文任务的符号定义，以及本文方法的框架组成，或者按步骤来介绍或者按模块来写，让读者对本文方法有全景式的理解。如下所示论文[2]的Methodology”总“的部分，就先介绍一些符号，然后分别介绍了Selector和Reader两个模块的主要功能。</p><p><img src="https://pic1.zhimg.com/v2-60fe75241b9ccf1217382a58f7abd19c_b.jpg" alt="img"></p><p>然后进入”分“的部分，则需对应”总“中的框架，分别介绍各关键模块/步骤。例如，论文[2]的Methodology”分“的部分，就包括3.1 Paragraph Selector、3.2 Paragraph Reader、3.3 Learning and Prediction。读者在”总“的部分已经对方法有全景式的了解，有的放矢，就比较容易理解每个模块的具体细节。而每个”分“的部分中，又可以进一步采用”总-分“结构进行介绍，例如3.1小节做完总体介绍后，又会按照Paragraph Encoding和Question Encoding分别介绍。为了更清晰地体现”总-分“结构，可以将各“分”的部分命名并加粗。</p><p><img src="https://pic2.zhimg.com/v2-a977707522216ffaf6f40905fce1c33d_b.jpg" alt="img"></p><p>初学者特别注意，（1）Introduction中对创新思路与方法的介绍，不要在Method中简单重复，否则会让认真通读全文的审稿人颇感厌烦。要做到前后照应，有所递进，前略后详，不妨使用“as mentioned in Section 1”来做关联。（2）Method部分往往包含大量公式，需要保证公式风格和符号使用前后统一，新符号使用均需显式解释。</p><hr><p><strong>Experiment怎么写</strong></p><p>这部分要详细介绍与实验相关的具体细节。一般先介绍实验数据、评测标准和比较方法等基本信息。以论文[2]为例，实验部分首先介绍实验数据与评测标准（4.1 Datasets and Evaluation Metrics）、实验比较的已有代表方法（4.2 Baselines）、实验方法的参数设置（4.3 Experimental Settings）等基本信息。</p><p>在介绍完实验基本信息后，主要开展两种实验：</p><p><strong>（1）主实验</strong>。目的是证明本文方法与已有方法相比的有效性。一般需要选取业界公认的数据集合或已有工作采用的实验验证方式，提升实验的可信性。对于学术论文而言，并不需要比该任务上最好的方法相比，只要证明采用本文创新方法与不采用本文方法相比更有效即可，也就是说，实验中尽量控制其他变量，只聚焦于本文关注的挑战问题即可。当然，如果能够因为本文创新思路，得到该任务上的最好效果，会更有吸引力，但不必总是强求。</p><p>一般实验结果用图表展示，然后在正文进行观察分析。例如，论文[2]的主实验部分先介绍不同Selector和Reader对实验效果的影响（4.4 Effect of Different Paragraph Selectors、4.5 Effect of Different Paragraph Readers），接着介绍主实验结果和观察分析（4.6 Overall Results）。其中表格中会把最好效果加粗显示，一般应大部分位于本文提出的方法；为了更加清晰明了，观察分析结论可用（1）（2）（3）列出，其中第1条一般要得出主要结论，即本文方法要显著优于已有方法。</p><p><img src="https://pic3.zhimg.com/v2-015ca9ae9ca2d0717c0398dc26abd6b6_b.jpg" alt="img"></p><p>主实验结果</p><p><img src="https://pic4.zhimg.com/v2-6766e2505e9c73ed3b2627080199fc2b_b.jpg" alt="img"></p><p>主实验分析</p><p><strong>（2）辅助实验</strong>。目的是展示本文创新方法的优势和特点。例如，不同超参数对本文方法的影响（Hyper-Parameter Effect），不同模块对本文方法效果的贡献（Ablation Test），不同数据划分对本文方法的影响（如Few-shot Learning相关工作比较常见），本文方法的主要错误类型（Error Analysis），本文方法能够改进效果的典型样例（Case Study）等。这些实验需要根据论文创新工作特点而有针对性的设计，一切要为体现本文的创新价值而服务。</p><p>例如，论文[2]的辅助实验包括4.7 Paragraph Selector Performance Analysis、4.8 Performance with different numbers of paragraphs、4.9 Potential improvement、4.10 Case study等，从各方面呈现本文提出方法的特点。</p><p>Experiment部分的特点是要图文并茂，注重通过多个表格和图示来呈现本文方法的优势和特点，需要注意图表风格统一。初学者特别注意，要做到仅凭图表下方的说明文字就可以理解每张图表内容，不要让读者还要到跑到正文寻找相关说明。因为，很多有经验的审稿人在看完Introduction后，会直接跳到Experiment图表中寻找对比效果。</p><hr><p><strong>Related Work怎么写</strong></p><p>这部分主要是介绍本文任务和方法的相关工作，目标是通过对已有工作的梳理，凸显本文工作的创新价值。对已有工作的梳理，不应是对每个工作的简单介绍，而应当注意汇总、分类、分析，或者按照时间发展顺序，或者按照技术路线划分，例如论文[2]就是按照时间脉络介绍。</p><p>在对相关工作的介绍中，要注意暗合本文创新思路要解决的挑战，不应是单纯的介绍，而是夹叙夹议，时刻注意与本文工作的照应。在Related Work的最后，应该落脚到本文工作与已有工作相比，有什么新的思路，解决了什么挑战问题。</p><p>初学者特别注意，Introduction和Related Work部分是特别需要导师或其他有经验学者帮助把关的。一是，不能遗漏重要相关工作，这点需要论文作者对相关领域工作保持跟踪；二是，与Introduction要求类似，对已有工作的评述务必精准客观。</p><p>Related Work一般放在Introduction之后，或者Conclusion之前，这一般取决于论文工作的特点。对于那些与已有工作联系紧密、创新精微的工作，一般建议放在Introduction之后，方便读者全面了解本文工作与已有工作的关系，然后开始在Method介绍本文方法。而对于有些框架性创新工作，如果主要是对已有方法的组合，一般建议Related Work放在Method、Experiment之后即可。这点并无成法，完全根据行文方便来定。</p><p><img src="https://pic1.zhimg.com/v2-2b2283b5f5e1f44ad6f14a4abf1eb920_b.jpg" alt="img"></p><hr><p><strong>Conclusion怎么写</strong></p><p>在论文最后会有总结展望，一般用一段来再次总结和强调本文的创新思路和实验结果，然后说明未来建议的研究方向和开放问题。这部分相对来讲比较固定。稍微留意的是，在准备论文最后阶段，如果发现论文有哪些应当做还没来得及做的，可以写作本文的未来工作。至少可以向审稿人表明你也想到这个问题了，赢得一点同情分。</p><p><img src="https://pic2.zhimg.com/v2-94326afefce671e98b92087a792ed3ad_b.jpg" alt="img"></p><hr><p><strong>其他建议</strong></p><p>要想写出一篇合格的NLP论文，首先是<strong>态度问题</strong>，只有态度重视，才有可能不厌其烦地反复修改，才会“不择手段”地寻找各种办法来尽力改进论文（找学长找外教借助Grammarly工具等）。其次是<strong>动手问题</strong>，只有写下来，才可能不断改，只要改就能不断进步。最后是<strong>经验问题</strong>，要写得精彩可能需要天赋，而要写得合格，只要坚持写，不断根据评阅人和其他人的意见进行思考和修改，就可以进步。总之，坚持就是胜利。</p><p>实际上，我觉得论文写作，是对思维模式的训练。也许未来你并不会从事学术研究，但通过论文写作锻炼的凝练工作创新价值的能力、清晰传递复杂信息的表达能力，对未来工作中无论是工作沟通、成果展示等，都有重要帮助。所以还希望大家都能重视这个科研道路上难得的锻炼机会。加油！</p>]]></content>
    
    
    <categories>
      
      <category>1.科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文写法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1.科研/1 - A学科调研资料库</title>
    <link href="undefined2020/02/24/1.%E7%A7%91%E7%A0%94/1%20-%20A%E5%AD%A6%E7%A7%91%E8%B0%83%E7%A0%94%E8%B5%84%E6%96%99%E5%BA%93/"/>
    <url>2020/02/24/1.%E7%A7%91%E7%A0%94/1%20-%20A%E5%AD%A6%E7%A7%91%E8%B0%83%E7%A0%94%E8%B5%84%E6%96%99%E5%BA%93/</url>
    
    <content type="html"><![CDATA[<h1 id="1-阅读列表-文献库"><a href="#1-阅读列表-文献库" class="headerlink" title="1.阅读列表+文献库"></a>1.阅读列表+文献库</h1><p>想当GrandMaster，<strong>以下提到的所有材料都绕不过去</strong>。在还没入行时间紧迫的情况下，须有所取舍。具体取舍规则办法见2 - 计划.md</p><h2 id="阅读列表"><a href="#阅读列表" class="headerlink" title="阅读列表"></a>阅读列表</h2><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>CS224N - 自然语言处理</td><td></td></tr><tr><td>CS229 - 机器学习（核心课）</td><td></td></tr><tr><td>CS229A - 机器学习应用课，数学少，应用多</td><td></td></tr><tr><td>CS231N - 计算机视觉</td><td></td></tr><tr><td>CS230 - 专注深度学习，只包含一点点机器学习（which最难的那一部分）</td><td></td></tr><tr><td>CS221 - AI导论</td><td></td></tr><tr><td>CS228 - 概率图模型</td><td></td></tr><tr><td><a href="https://www.zhihu.com/org/ji-qi-zhi-xin-65/activities" target="_blank" rel="noopener">机器之心</a></td><td></td></tr><tr><td><a href="https://zhuanlan.zhihu.com/paperweekly" target="_blank" rel="noopener">PaperWeekly</a></td><td></td></tr><tr><td><a href="https://mp.weixin.qq.com/s/B2ocJ1Y2evLwVkG5FrUz5A" target="_blank" rel="noopener">NeurIPS 2019公布获奖论文</a></td><td></td></tr><tr><td><a href="https://mp.weixin.qq.com/s/SxOKHRbTJdzlBNfHos-TOA" target="_blank" rel="noopener">深度学习所需数学知识</a></td><td></td></tr><tr><td><a href="https://mp.weixin.qq.com/s/RlgX2GErySe5rAUAvveLdQ" target="_blank" rel="noopener">EE转CS成功案例</a></td><td></td></tr><tr><td><a href="https://mp.weixin.qq.com/s/F4zQHesEGWLcBLGwYrmg3w" target="_blank" rel="noopener">Jeff Dean谈2020机器学习趋势</a></td><td></td></tr><tr><td><a href="https://mp.weixin.qq.com/s/P0MXhMBjt41WflAMGtFr-g" target="_blank" rel="noopener">从Word2Vec到BERT </a>   done</td><td></td></tr><tr><td><a href="https://mp.weixin.qq.com/s/uaBRgRp0Yue4MtC2B-8VJA" target="_blank" rel="noopener">kaggle竞赛宝典</a></td><td></td></tr><tr><td><a href="https://mp.weixin.qq.com/s/bSsmaOzQYzJ_7RjnzS804g" target="_blank" rel="noopener">NIPS2019</a></td><td></td></tr><tr><td><a href="https://mp.weixin.qq.com/s/-rrj3jusoFAqt6gjuP9y6w" target="_blank" rel="noopener">斯坦福2019全球AI报告</a></td><td></td></tr><tr><td><a href="https://mp.weixin.qq.com/s/XMcX1FQ3yDIAgvwsfEJ3yQ" target="_blank" rel="noopener">2020学术会议list</a></td><td></td></tr><tr><td><a href="https://mp.weixin.qq.com/s/E04x_tqWPaQ4CSWfGVbnTw" target="_blank" rel="noopener">因果推演</a></td><td></td></tr><tr><td><a href="https://zhuanlan.zhihu.com/p/77357304" target="_blank" rel="noopener">ACL2019知识图谱总结</a></td><td></td></tr><tr><td>概率论与数理统计</td><td></td></tr><tr><td>线性代数</td><td></td></tr><tr><td>高等数学</td><td></td></tr><tr><td><strong>优化理论</strong></td><td></td></tr><tr><td><strong>Bubeck：《Convex Optimization: Algorithms and Complexity》</strong></td><td></td></tr><tr><td>Bottou、Curtis和Nocedal：《Optimization Methods for Large-Scale Machine Learning》</td><td></td></tr><tr><td>知识图谱</td><td></td></tr><tr><td>Ian Goodfellow&amp;Yoshua Bengio&amp;Aaron Courville：DeepLearning</td><td></td></tr><tr><td><strong>nlp基石之书：speech+and+language+processing</strong></td><td></td></tr><tr><td>Manning：Introduction to Information Retrieval</td><td></td></tr><tr><td>Neural Network Methods for Natural Language Processing</td><td></td></tr><tr><td><strong>PRML：模式识别与机器学习</strong></td><td></td></tr><tr><td>周志华：《机器学习》</td><td></td></tr><tr><td><strong>李航：《统计学习方法》</strong></td><td></td></tr><tr><td>人工智能：一种现代的方法</td><td></td></tr><tr><td>数学之美</td><td></td></tr><tr><td>网络、群体与市场（中文版）</td><td></td></tr><tr><td>EMNLP2018_如何写NLP代码</td><td></td></tr><tr><td>刘洋_写论文的技术细节</td><td></td></tr><tr><td></td><td></td></tr><tr><td>理想国</td><td></td></tr><tr><td>苏格拉底自辩篇</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h2 id="对阅读列表的阐述-刘洋"><a href="#对阅读列表的阐述-刘洋" class="headerlink" title="对阅读列表的阐述 - 刘洋"></a>对阅读列表的阐述 - 刘洋</h2><p><strong>1、了解NLP的最基本知识</strong>：Jurafsky和Martin的Speech and Language Processing是领域内的经典教材，里面包含了NLP的基础知识、语言学扫盲知识、基本任务以及解决思路。阅读此书会接触到很多NLP的最基本任务和知识，比如tagging, 各种parsing，coreference, semantic role labeling等等等等。这对于全局地了解NLP领域有着极其重要的意义。书里面的知识并不需要烂熟于心，但是刷上一两遍，起码对于NLP任务有基本认识，下次遇到了知道去哪里找还是非常有意义的。另外 Chris Manning 的 introduction to information retrieval 也是一本可以扫一下盲的书，当然我认为依然不需要记住所有细节，但轮廓需要了解。IR里面的很多基本算法跟NLP有不少的重合。说说我自己曾经走过的弯路。Stanford NLP的qualification考试的一部分就是选一些jurafsky 和 manning书里面的一些chapter来读，然后老师来问相关问题。开始我一直对里面的东西懒得看，所以qualification考试一拖再拖。但博士最后一年没办法拖的时候，才发现如果早知道这些东西，博士早年可以少走很多弯路。</p><p>为什么了解NLP基础知识的重要，我给大家举几个例子。<br>最近跟同学一起做语言模型 language modeling相关的事情，很多同学用LSTM或者transformers做language model随手就能实现，但是实现一个 bigram 或者 trigram的language model（LM）却因为里面的OOV的平滑问题卡了大半天（熟悉的同学可能知道，需要拉普拉斯平滑或者更sophisticated的Kneser-Ney平滑）。为什么bigram 或者 trigram的LM很重要呢？去做一个语言模型的问题，实现深度模型之前，第一步其实就要去写一个 bigram 或者 trigram的LM。为什么呢？ 因为这些N-gram模型实现简单，并且robust。通过这样简单的实现，可以告诉你这个数据集的LM模型的下限。这样我们心里会有数，神经网络模型至少不应该比这个模型差的。神经网络模型因为其超参数、梯度爆炸等问题，有时候我们不太容易决定是真的模型不行、参数没调好还是代码有bug。那么通过N-gram LM的给出的下限，我们就可以直观地知道神经网络是有bug还是没调好参数。</p><p>第二个例子就是涉及发文章了，不知道有没有同学想过，BERT里面训练LM的随机替换为什么就使结果变好，随机替换是什么鬼，怎么结果就好了。其实在BERT之前，斯坦福的吴恩达组的Ziang Xie的 Data Noising as Smoothing in Neural Network Language Models ICLR2017（<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1703.02573.pdf">https://arxiv.org/pdf/1703.02573.pdf</a>） 就首次提出了此方法，而且给出了理论解释。这种random替换其实本质上属于language modeling里面基于interpolation的平滑方式， 而基于interpolation的LM平滑，就躺在jurafsky那本书的第3.4.3节。</p><p>\2. <strong>了解早年经典的NLP模型以及论文</strong>：相比简单粗暴的神经网络模型，早年的NLP算法确实比较繁琐复杂，但里面确实有很多早年学者在硬件条件艰苦情况下的智慧结晶。熟悉了这些模型，可以在现在神经网络里面融会贯通。去年在人民大学做seminar。Seminar有大概30-40位同学参加。Seminar中，我问了一个问题，有谁知道机器翻译中的IBM模型大概是干嘛的，举手的同学大概有五分之一。我再问，谁能来手写（或者大概手写）一下IBM model1，一个人都没有。仅仅从基于IBM模型的Hierarchical Phrase-based MT, 近几年就有很多篇引用量很高的文章是基于里面的思想的。例子数不胜数： </p><p>1) chris dyer 组的<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1601.01085">Incorporating structural alignment biases into an attentional neural translation model</a> (NAACL16) 提出用双向attention做neural机器翻译的约束项，意思是如果在英语翻译法语生成的target中的一个法语词attend到了一个source中的英语词，那么反过来，法语翻译英文 target中相同这个英语词应该也attend到source中的这个英语词。其实这个思想就是完完全全相似 Percy Liang 曾经的成名作之一，早在NAACL06年 Alignment by Agreement，大家通过题目的意思就可以猜到文章的内容，正向翻译与反向翻译中的 对齐(alignment) 要 一致(agree)。如今做neural MT的同学，有多少同学读过Percy的这篇大作呢 （大家知道Percy最多的应该是Squad吧）。</p><p>2) 处理对话系统的无聊回复，用反向概率p(source|target)做reranking现在应该已经是标配。再比如Rico Sennrich的成名作之一将Monolingual data 跟seq2seq 模型结合。其实这连个思想在phrase-base MT 里面早就被广发的使用。Neural之前的MT，需要对一个大的N-best list用MERT做 reranking， 反向概率 p(source|target) 以及语言模型概率 p(target)是reranking中feature的标配。</p><p>3) Harvard NLP组, Sam Wiseman 和Alex 发表的EMNLP16 best paper runner-up, Sequence-to-Sequence Learning as Beam-Search Optimization, 基本上传承了Daume´ III and Daniel Marcu 2005年的 LaSO模型，将其思想adapt到neural里面。</p><p>如果再准本溯源，诞生于neural MT的attention，不就是IBM模型的神经网络版本嘛。</p><p>\3. <strong>了解机器学习的基本模型：</strong>神经网络的简单暴力并且有效。但是从科研的角度讲，熟悉基本的机器学习算法是必修课。比如吴恩达的 machine learning就是必要之选。记得前段时间我面试一个小伙子，一看就是很聪明的同学，而且很短的时间就有fvg一篇NAACL在投。我就问小伙子，EM算法是什么，小伙子说没有听说过EM，而且自己的科研也用不到EM。我认为这其实是一个挺大的误区。当我想起我自己，曾经就吃过很多类似的亏。因为早期数学基础偏弱，也没有决心恶补一下数学，所以早年每次看到跟variational inference相关的算法就头大，这种偏科持续了很久，限制了科研的广度。相比粗暴的神经网络，CRF等模型的inference确实相对复杂（当年我自己也看了很多次才彻底搞明白）。但搞懂这些，是一个NLP researcher的基本素养。Pattern Recognition and Machine Learning那本书，尤其是某些小节确实比较难（又暴露了数学基础差的事实），即便是只是为了过一遍，也需要很强的耐力才能看完，更不用说完全看懂了。我自己也曾经半途而废很多次，如今依然有很多章节是不太懂的。但是其中的很多基础chapter，我认为还是很值得一读的。其实可以组成那种两三个人的学习小组，不需要有太雄伟的目标，用个一年哪怕两年的时间，把几个重要的chapter 过一遍。</p><p>NLP相对是应用科学，并不是特别的数学。但是我们天天用的算法的基本数学逻辑我认为还是需要搞懂，比如dropout, 比如天天用到的优化(SGD, momentum, adaboost, adagrad)，比如各种 batch, layer normalization。这样其实可以省去很多浪费的时间，磨刀不误砍柴工。这些年来，在帮同学调bug的过程中，我至少遇见过3-5个同学 training 的时候开dropout, test 的时候没有对每个cell用 (1-dropout)去 scale （大家不要笑，这是真的）。然后画出dropout曲线就是 dropout 值越大，结果越差。在讨论的时候，同学一脸茫然并且不清楚test时候需要scale。其实本质就是并不了解dropout背后的数学原理。</p><p><strong>4. 多看NLP其他子领域的论文</strong>：NLP有很多子领域，MT，信息抽取，parsing，tagging，情感分析，MRC等等。多多熟悉其他子领域的进展是必要的。其实不同子领域所运用的模型不会相差太大。但是最开始看不熟悉领域的问题可能会有一点难，原因是对问题的formalization不是很了解。这可能就需要多花一些时间，多找懂的同学去问。其实了解不同问题的formalization也是对领域知识最好的扩充。</p><p><strong>5. 了解 CV和data mining领域的基本重大进展</strong>：当熟悉了上面所说的点之后（当然可能至少也需要一年的时间）。熟悉CV领域的基本任务、基本算法我认为对于打开科研视野很重要。但是不可否认，因为领域不用，写作风格、术语表达相差很大，又因为缺乏背景知识（文章中会省略一些基础知识，默认大家都懂。但是跨领域的人可能不懂），第一次想读懂跨领域的文章其实并不容易。我就出现过竟然在讨论班上直接把faster-RCNN讲错了的情况，以为自己看懂了，然后就讲错了（至今昱先天天还在因为这个事情调侃我）。不过重要的是，NLP领域里面一些重要的文章其实或多或少借鉴了CV里面的思想，当然也同样出现CV借鉴NLP的情况。NLP神经网络可视化、可解释性的研究，时间上还是落后于CV里面对CNN的可视化。所以很多工作大量借鉴了CV里面的类似工作。NLP运用GAN其实也是借鉴CV的。其实两个领域很多是很相通的。比如，如果不考虑question query, vision里面detection中的 region proposal（在一个大的图片背景下找一个特定区域）, 大家想是不是跟MRC里面的 span extraction （在一大堆文字里面找一个span）有异曲同工之妙。更不用说image caption generation与sequence-to-sequence模型了，本质上几乎没什么太大的区别。强化学习在生成领域generation，发完了MT(Ranzato et al., ICLR2016)再发 image caption generation, 再回到summarization. Actor-critic 模型也是类似的，还是很多做generation diversity的文章。因为跨领域不好懂，所以第一次推荐看tutorial, 如果有 sudo code 的tutorial那就更好了。另外看看扫盲课的视频，比如Stanford CS231n也是个好办法。另外，一个NLP组里面有一个很懂CV的人也很重要（拜谢昱先）， and vise versa。<br>graph embedding近两年崛起于data mining领域。目测会在（或者已经在）NLP的不少任务得到广泛应用。想到几年前，deep walk借鉴了word2vec, 开始在data mining领域发迹，然后似乎又要轮转回NLP了。</p><p><strong>6.如何快速了解某个领域研究进展</strong></p><p>最后简单说一下快速了解某领域研究进展的经验。你会发现，搜索引擎是查阅文献的重要工具，尤其是谷歌提供的Google Scholar，由于其庞大的索引量，将是我们披荆斩棘的利器。</p><p>当需要了解某个领域，如果能找到一篇该领域的最新研究综述，就省劲多了。最方便的方法还是在Google Scholar中搜索“领域名称 + survey / review / tutorial / 综述”来查找。也有一些出版社专门出版各领域的综述文章，例如NOW Publisher出版的Foundations and Trends系列，Morgan &amp; Claypool Publisher出版的Synthesis Lectures on Human Language Technologies系列等。它们发表了很多热门方向的综述，如文档摘要、情感分析和意见挖掘、学习排序、语言模型等。</p><p>如果方向太新还没有相关综述，一般还可以查找该方向发表的最新论文，阅读它们的“相关工作”章节，顺着列出的参考文献，就基本能够了解相关研究脉络了。当然，还有很多其他办法，例如去<a href="https://link.zhihu.com/?target=http%3A//videolectures.net">http://videolectures.net</a>上看著名学者在各大学术会议或暑期学校上做的tutorial报告，去直接咨询这个领域的研究者，等等。</p><h2 id="文献库在Endnote中构建完成"><a href="#文献库在Endnote中构建完成" class="headerlink" title="文献库在Endnote中构建完成"></a>文献库在Endnote中构建完成</h2>]]></content>
    
    
    <categories>
      
      <category>1.科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文写法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1.科研/2 - 计划</title>
    <link href="undefined2020/02/24/1.%E7%A7%91%E7%A0%94/2%20-%20%E8%AE%A1%E5%88%92/"/>
    <url>2020/02/24/1.%E7%A7%91%E7%A0%94/2%20-%20%E8%AE%A1%E5%88%92/</url>
    
    <content type="html"><![CDATA[<h1 id="计划"><a href="#计划" class="headerlink" title="计划"></a>计划</h1><p>比赛 -  改进模型 - 得奖 - 阅读论文 - 写作论文</p><p>比赛得奖同时也是进入行业的必要条件</p><p>进入行业是了解行业和理解科研的必要条件</p>]]></content>
    
    
    <categories>
      
      <category>1.科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文写法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1.科研/1 - B学科调研结论</title>
    <link href="undefined2020/02/24/1.%E7%A7%91%E7%A0%94/1%20-%20B%E5%AD%A6%E7%A7%91%E8%B0%83%E7%A0%94%E7%BB%93%E8%AE%BA/"/>
    <url>2020/02/24/1.%E7%A7%91%E7%A0%94/1%20-%20B%E5%AD%A6%E7%A7%91%E8%B0%83%E7%A0%94%E7%BB%93%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="1-学科调研结论"><a href="#1-学科调研结论" class="headerlink" title="1.学科调研结论"></a>1.学科调研结论</h1><h3 id="简要结论"><a href="#简要结论" class="headerlink" title="简要结论"></a><strong>简要结论</strong></h3><p>经过对上文阅读列表+论文库的调研，得出如下结论，可以开始选题。</p><ul><li>近年来,我感兴趣的关于_<strong>____</strong>的研究方向,全球呈现_<strong><strong>_趋势,其中较多的论文来自\</strong></strong>国家\地区,发表相关论文的研究机构有_<strong>__</strong>.蓝海领域比较新，容易出成果，我选择的这个方向是否是蓝海领域_<strong>____</strong>.</li><li>为了充分了解这个领域目前的发展状况，需要如下几个方面的调研：方法方面，是否有一套比较清晰的数学体系和机器学习体系_<strong><strong>；数据方面，有没有一个大家公认的标准训练集和测试集__</strong></strong>；研究团队，是否有著名团队和人士参加_<strong>__</strong>。如果以上几个方面的调研结论不是太清晰，作为初学者可能不要轻易进入。</li><li>全球的研究人员主要从_<strong>___</strong>等领域对课题进行研究,同时我们也注意到_<strong>__</strong>等领域的研究可能会给我们带来不一样的视角和灵感</li><li>相关课题的研究成果目前主要发表在_<strong>___</strong>等期刊上,在相关研究领域中,_____等几位学者有较多的论文产出.</li><li>影响力较高的几篇论文分别来自于_<strong>__</strong>(国家/地区)的_<strong><em>(机构)的___</em></strong>学者</li><li>近半年来_<strong><strong>__</strong></strong>方向引起了较多科研人员的关注</li><li>选择_<strong>___</strong>综述文章作为快速了解这个课题的切入点</li><li>最新的研究进展指出,该研究方向_<strong>_____</strong></li></ul><h3 id="BackGround："><a href="#BackGround：" class="headerlink" title="BackGround："></a><strong>BackGround：</strong></h3><p>自然语言处理（简称NLP），是研究计算机处理人类语言的一门技术，包括：</p><p>1.<strong>句法语义分析</strong>：对于给定的句子，进行分词、词性标记、命名实体识别和链接、句法分析、语义角色识别和多义词消歧。</p><p>2.<strong>信息抽取</strong>：从给定文本中抽取重要的信息，比如，时间、地点、人物、事件、原因、结果、数字、日期、货币、专有名词等等。通俗说来，就是要了解谁在什么时候、什么原因、对谁、做了什么事、有什么结果。涉及到实体识别、时间抽取、因果关系抽取等关键技术。</p><p>3.<strong>文本挖掘</strong>（或者文本数据挖掘）：包括文本聚类、分类、信息抽取、摘要、情感分析以及对挖掘的信息和知识的可视化、交互式的表达界面。目前主流的技术都是基于统计机器学习的。</p><p>4.<strong>机器翻译</strong>：把输入的源语言文本通过自动翻译获得另外一种语言的文本。根据输入媒介不同，可以细分为文本翻译、语音翻译、手语翻译、图形翻译等。机器翻译从最早的基于规则的方法到二十年前的基于统计的方法，再到今天的基于神经网络（编码-解码）的方法，逐渐形成了一套比较严谨的方法体系。</p><p>5.<strong>信息检索</strong>：对大规模的文档进行索引。可简单对文档中的词汇，赋之以不同的权重来建立索引，也可利用1，2，3的技术来建立更加深层的索引 。在查询的时候，对输入的查询表达式比如一个检索词或者一个句子进行分析，然后在索引里面查找匹配的候选文档，再根据一个排序机制把候选文档排序，最后输出排序得分最高的文档。</p><p>6.<strong>问答系统</strong>： 对一个自然语言表达的问题，由问答系统给出一个精准的答案。需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别，形成逻辑表达式，然后到知识库中查找可能的候选答案并通过一个排序机制找出最佳的答案。</p><p>7.<strong>对话系统</strong>：系统通过一系列的对话，跟用户进行聊天、回答、完成某一项任务。涉及到用户意图理解、通用聊天引擎、问答引擎、对话管理等技术。此外，为了体现上下文相关，要具备多轮对话能力。同时，为了体现个性化，要开发用户画像以及基于用户画像的个性化回复。</p>]]></content>
    
    
    <categories>
      
      <category>1.科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文写法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1.科研/0 - 总体思路</title>
    <link href="undefined2020/02/24/1.%E7%A7%91%E7%A0%94/0%20-%20%E6%80%BB%E4%BD%93%E6%80%9D%E8%B7%AF/"/>
    <url>2020/02/24/1.%E7%A7%91%E7%A0%94/0%20-%20%E6%80%BB%E4%BD%93%E6%80%9D%E8%B7%AF/</url>
    
    <content type="html"><![CDATA[<h1 id="0-总体思路"><a href="#0-总体思路" class="headerlink" title="0 - 总体思路"></a>0 - 总体思路</h1><ul><li>基础 ： Speech and Language Processing（完成）</li><li>早年经典NLP论文：遍历论文库（进行中）</li><li>了解机器学习的基本模型：<ul><li>CS229（完成）</li><li>Pattern Recognition and Machine Learning</li></ul></li><li>了解NLP其他子领域(MT,信息抽取,parsing,tagging,情感分析,MRC等)（进行中）</li><li>了解CV和数据挖掘领域的进展</li></ul><ol><li>学科调研：构建阅读列表和论文库；读近五年survey，近三年顶会，感兴趣方向热门论文和经典书单；输出互引DAG图，输出专有名词词典，输出调研文档。</li></ol><p>   *<em>构建文献库时遇到的困难：flood。目前资料：知识图谱80 + 深度学习100 + 生物医学CRF70 + ACL660 = 910篇论文，以及7本大部头。这还不包括所谓的5年survery100 + 三年顶会3000共约3100篇论文。 *</em></p><p>   <strong>我需要的：迅速了解整个学科发展大致现状，选择自己感兴趣的+有前途做的人少的领域迅速构建论文库精读切入</strong></p><p>   <strong>初步解决方案：控制工作量，看优质survey，大部头只挑一本看，其余当工具书，迅速对学科整体建模。积极寻求所有论文的互引网络图，然后只阅读高引论文 + 感兴趣细分领域的施引文献，根据CheBio论文list启发，面向“方法”构建相似论文库。</strong></p><p>   <strong>当下任务：阅读speech and language processing</strong></p><ol start="2"><li><p>计划：工作示意图 + excel详表。</p></li><li><p>选题：刚入门不要有开创道路的想法，第一篇论文可新颖度略低。</p><p>最好能找到一起讨论的同学/小老板（科研伙伴），在前辈的带领下快速从一个小方向切入进去。</p></li><li><p>模型设计：模型设计和下边的代码、实验、调参、写文都可以从模仿起步。把和你工作最相关的文章 好好读几遍，从结构到段落到句子都可以模型。当年王鸿伟的第一篇论文就是吃透并模仿了好几篇参考文献而已，得到了老师的表扬。</p></li><li><p>代码实现</p></li><li><p>实验环节：设计实验，做实验，调参，记录结果</p></li><li><p>了解科研论文的组成结构。[Done]</p></li><li><p>开始写文，确定发文目标:列出顶会清单。查看其字数和格式限制，论文一旦超长很难缩短。</p></li><li><p>逻辑第一，这种逻辑是贯穿全文的，段落层面的逻辑、句子层面的逻辑、甚至一句话里的逻辑，都是非常关键的。一篇好的论文要循循善诱，有理有据，让人读起来不要废太多脑子，就觉得你说的很有道理。这里面要着重注意各种句子层面的关系：转折、因果、递进等。一句话可以有无数种表达方式，你要做的就是在脑子里把各种方式过滤一遍，选择最流畅的那一种。</p></li><li><p>简单构造：不要试图在一篇论文中提出太多的概念。专注于简单的构造，并围绕它构建您的论点。</p></li><li><p>使其有趣，学术论文不必很无聊。如果您想让自己的知识传播开来，请花些时间以使读者易于理解的方式来构造您的叙述。介绍您的概念，告诉它为什么很有趣，这个想法，它如何工作以及如何解决问题。</p></li><li><p>请一位导师</p></li><li><p>注意格式：始终以标准格式的单词以及双倍间距书写。</p></li><li><p>版本控制：每天另存为新草稿。您可以使用以下命名约定“ yyyymmdd-xxx work-draft”。</p></li><li><p>摘要最后写。当您完成研究论文时，您将更好地掌握所谈论的内容。在最后写摘要比在开始写摘要要容易得多。用您的文字来质疑自己的假设和推论。这样，您的写作可以使您的内容变得更好。</p></li><li><p>查重，语法检测</p></li><li><p>与已经发布的人联系，了解发布需要什么。与您信任的审稿人分享您的论文。特别是如果这是您的第一篇论文，您可能会花更多时间在不重要的论文方面。优秀的审稿人可以帮助您使您的论文格式更好。</p></li><li><p>善用工具</p><table><thead><tr><th>工具名称</th><th>描述</th><th>备注</th></tr></thead><tbody><tr><td>Paperpile</td><td>在线研究工作流</td><td><a href="https://paperpile.com/app" target="_blank" rel="noopener">https://paperpile.com/app</a></td></tr><tr><td>sci-hub</td><td>黑文献</td><td><a href="http://tool.yovisun.com/scihub/" target="_blank" rel="noopener">http://tool.yovisun.com/scihub/</a>    ww1.sci-hub.tv</td></tr><tr><td>Grammerly</td><td>语法矫正</td><td></td></tr><tr><td>Turnitin</td><td>查重</td><td></td></tr><tr><td>Inkscape</td><td>工作示意图</td><td></td></tr><tr><td>Power Point</td><td>工作示意图</td><td></td></tr><tr><td>Adobe illustrator</td><td>工作示意图</td><td></td></tr><tr><td>Readcube</td><td>Ref Control</td><td></td></tr><tr><td>Endnote</td><td>Ref Control</td><td>找投稿官网;自动匹配8个推荐投稿地;可按照期刊自动调整style;Endnote online免费，可绑定web of science账号</td></tr><tr><td>Zotero</td><td>Ref Control</td><td><a href="https://www.zotero.org" target="_blank" rel="noopener">https://www.zotero.org</a></td></tr><tr><td>Mendeley</td><td>Ref Control</td><td><a href="https://www.mendeley.com/" target="_blank" rel="noopener">https://www.mendeley.com/</a></td></tr><tr><td>Google Scholar</td><td>查阅论文</td><td><a href="https://scholar.google.co.in/" target="_blank" rel="noopener">https://scholar.google.co.in/</a></td></tr><tr><td>LaTeX</td><td>插入图片、数学公式等</td><td><a href="https://www.latex-project.org/" target="_blank" rel="noopener">https://www.latex-project.org/</a></td></tr><tr><td>TypeSet</td><td>多人协作，各期刊格式管理</td><td><a href="https://typ.st/2st78LG" target="_blank" rel="noopener">https://typ.st/2st78LG</a></td></tr><tr><td>ShareLaTexX</td><td>多人协作</td><td><a href="https://www.sharelatex.com/" target="_blank" rel="noopener">https://www.sharelatex.com/</a></td></tr><tr><td>OverLeaf</td><td>各期刊模板和格式</td><td></td></tr><tr><td>web of science</td><td>学科跟踪服务</td><td>使用邮箱创建学科跟踪服务，新动态可自动发送</td></tr><tr><td>mjl.clarivate</td><td>查看被期刊收录情况</td><td><a href="https://mjl.clarivate.com" target="_blank" rel="noopener">https://mjl.clarivate.com</a></td></tr><tr><td>publons/publons academy</td><td>寻找审稿人/同行评议</td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table></li><li><p>注意事项</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>保证有效工作时间8h/day。</td><td>工作低效的标志：老看时间，察觉到时间的流逝。这时不可硬看，通过散步/睡觉/健身及时调整。</td></tr><tr><td>张弛有度</td><td>保证work-life-balance。每天必须要高质量地玩，天天紧绷很快就疲了，拉低生命体验本末倒置，且走不远。</td></tr><tr><td>冷静思考，摒弃急躁和蛮力</td><td>科研是优雅的脑力劳动，是艺术创作。不是体育比赛，不是谁力气大谁能忍受痛苦谁就厉害。遇到困难(which一定会)心情焦躁的时候时往回拉一拉，想想Andrew Ng,Manning,Percy Liang,Einstein这些偶像会怎么做。</td></tr><tr><td>长期solo，注意maintain精神状态</td><td>多健身，尝试回到绿茵场。心怀善念，别忘了初心是make the world a better place。</td></tr><tr><td></td><td></td></tr></tbody></table></li><li><p>Times</p><p>ACL2019 : 7.28</p><p>EMNLP : 11.3</p><p>NAACL : 6.2</p></li><li></li></ol>]]></content>
    
    
    <categories>
      
      <category>1.科研</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/快捷键</title>
    <link href="undefined2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E5%BF%AB%E6%8D%B7%E9%94%AE/"/>
    <url>2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E5%BF%AB%E6%8D%B7%E9%94%AE/</url>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h1 id="PyCharm"><a href="#PyCharm" class="headerlink" title="PyCharm"></a>PyCharm</h1><p>CTRL + -  = 折叠本行 </p><p>CTRL + + = 打开本行</p><p>CTRL + SHIFT + -  = 全部折叠</p><p>CTRL + SHIFT + + = 全部打开</p><h1 id="Chrome"><a href="#Chrome" class="headerlink" title="Chrome"></a>Chrome</h1><p>CTRL + SHIFT + N = 打开匿名窗口</p><p>CTRL + SHIFT + I =  检查</p><p>CTRL + W  = 关闭当前标签页</p><p>CTRL + fn  + PgUp/PgDn = 上翻/下翻当前标签页</p><h1 id="IDEA"><a href="#IDEA" class="headerlink" title="IDEA"></a>IDEA</h1><p>CTRL + SHIFT + F12 =  编程窗口最大化</p><p>CTRL + SHIFT+ N = 查找文件</p><p>SHIFT + F6 = 全局替换</p><p>CTRL + ALT + L = 格式化代码</p><p>CTRL + ALT + V =快速赋值</p><p>Cookie[] cookeis数组遍历便捷写法  = cookies.for = for(Cookie cookie : cookies)</p><p>SHIFT + F6  = 文件重命名</p><p>CTRL + SHIFT + C = 拷贝文件路径</p><p>CTRL + D = 拷贝一行</p><p>SHIFT + ENTER = 换行</p><p>CTRL + P = 查看参数</p><p>CTRL + N = 查找代码行例:QutsitonMapper.java:31  可直接定位到该文件31行</p><p>CRTL + E 切换到最近页面</p><p>数据库consolo:CTRL+ENTER:执行当前mysql    </p><p>CTRL+SHIFT+上箭头 = 本行上移</p><p>ALT+1=显示项目目录</p><p>CTRL + ALT + O = 删除无用import</p><p>CTRL + F12 =展示所有方法</p><p>CTRL + ALT + 左/右  = 前进或后退</p><p>CTRL+F6 =  替换函数参数位置，如果有依赖会自动一起替换</p><p>CTRL+A = 选中当页所有</p><p>CTRL+SHIFT+U = 大小写转换    </p><p>ALT+INSERT  = 自动生成getter setter等</p><h1 id="HTML"><a href="#HTML" class="headerlink" title="HTML"></a>HTML</h1><p>CTRL + W = 逐层扩散选中</p><p>span + TABLE = <span>|</span></p><h1 id="TYPORA"><a href="#TYPORA" class="headerlink" title="TYPORA"></a>TYPORA</h1><p>CTRL + 1 = 一级标题</p><p>三个* = 分割线</p><p>三个` = 代码块</p><p>无序列表：以*开头S</p><p>有序列表：1.开头</p><h1 id="GIT"><a href="#GIT" class="headerlink" title="GIT"></a>GIT</h1><p>SHIFT + : + x  = 退出并保存</p><p>Q = 终止当前对话</p><p>git checkout [序列号] [文件地址] =文件版本回滚</p><h1 id="CMD"><a href="#CMD" class="headerlink" title="CMD"></a>CMD</h1><p>​    Keyboard shortcuts</p><h4 id="Tab-manipulation"><a href="#Tab-manipulation" class="headerlink" title="Tab manipulation"></a>Tab manipulation</h4><ul><li><p>Ctrl + ` : <strong>Global</strong> Summon from taskbar</p></li><li><p><code>Win + Alt + p</code> : Preferences (Or right click on title bar)</p></li><li><p><code>Ctrl + t</code> : New tab dialog (maybe you want to open cmd as admin?)</p></li><li><p><code>Ctrl + w</code> : Close tab</p></li><li><pre><code>Shift + Alt + number</code></pre><p>Fast new tab:</p><ol><li><code>1.</code> CMD</li><li><code>2.</code> PowerShell</li></ol></li><li><p><code>Alt + Enter</code> : Fullscreen</p></li></ul><h4 id="Shell"><a href="#Shell" class="headerlink" title="Shell"></a>Shell</h4><ul><li><code>Ctrl + Alt + u</code> : Traverse up in directory structure (lovely feature!)</li><li><code>End, Home, Ctrl</code> : Traverse text as usual on Windows</li><li><code>Ctrl + r</code> : History search</li><li><code>Shift + mouse</code> : Select and copy text from buffer</li><li><code>Right click / Ctrl + Shift + v</code> : Paste text</li></ul><h4 id="VIM"><a href="#VIM" class="headerlink" title="VIM"></a>VIM</h4><hr><p>CTRL+O = 返回上一个文件</p><h3 id="Linux"><a href="#Linux" class="headerlink" title="Linux:"></a>Linux:</h3><hr><p>chmod -R 777 /路径  ：让锁定文件可读可写</p><p>ps -ef   查看当前进程</p><p>ps  -p 6176 -o etime查看运行时间</p><p>netstat -ntlp:查看所有端口占用情况</p><h3 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h3><p>build-target# ./bin/start-cluster.sh :拉起flink web 注意目录是bin之前那个target文件夹</p>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>快捷键</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2.比赛/指导_三个集成学习模型</title>
    <link href="undefined2020/02/24/2.%E6%AF%94%E8%B5%9B/%E6%8C%87%E5%AF%BC_%E4%B8%89%E4%B8%AA%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/"/>
    <url>2020/02/24/2.%E6%AF%94%E8%B5%9B/%E6%8C%87%E5%AF%BC_%E4%B8%89%E4%B8%AA%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="三个常用的集成学习模型"><a href="#三个常用的集成学习模型" class="headerlink" title="三个常用的集成学习模型"></a>三个常用的集成学习模型</h1><h4 id="常见集成学习模型一览图"><a href="#常见集成学习模型一览图" class="headerlink" title="常见集成学习模型一览图"></a>常见集成学习模型一览图</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200525_1241_36_530.png" alt=""></p><h4 id="集成学习的优点"><a href="#集成学习的优点" class="headerlink" title="集成学习的优点"></a>集成学习的优点</h4><p>采用多个分类器对数据集预测,提高整体分类器的泛化能力</p><h4 id="三种常见的集成学框架"><a href="#三种常见的集成学框架" class="headerlink" title="三种常见的集成学框架"></a>三种常见的集成学框架</h4><ul><li>bagging 装袋</li><li>boosting 提升</li><li>stacking 堆栈</li></ul><h2 id="bagging–装袋"><a href="#bagging–装袋" class="headerlink" title="bagging–装袋"></a>bagging–装袋</h2><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1436_45_170.png" alt=""></p><p>子训练集一般是各不相同的</p><p>基模型一般采用SVM或者朴素贝叶斯(大家一般采用同一种模型)</p><p>测试集扔给基模型们,然后各个基模型投票表决,简单多数为最终结果</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1437_03_239.png" alt=""></p><h2 id="Boosting提升"><a href="#Boosting提升" class="headerlink" title="Boosting提升"></a>Boosting提升</h2><p>第一次训练得到返回结果,然后给每一个结果分配权值,分类正确的权值降低,错误的权值上升</p><p>分类错误权值升高,在第二次训练时被重点关照</p><p>测试–测试集扔给各个样本,最后根据投票权值分配投票权,最终得到分配结果</p><h2 id="Stacking–堆叠"><a href="#Stacking–堆叠" class="headerlink" title="Stacking–堆叠"></a>Stacking–堆叠</h2><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1437_12_514.png" alt=""></p><ul><li>训练集分出n个基模型</li><li>集成方法:<ul><li>基础模型比如有100个,每个输出三维向量,一共就输出300维的向量</li><li>这个向量在堆叠模型那里训练</li><li>测试集也有三百维,最后生成测试数据让模型训练,得到最终结果</li></ul></li></ul><p>集成模型的偏差与方差</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1437_30_634.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>2.比赛</category>
      
    </categories>
    
    
    <tags>
      
      <tag>集成学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2.比赛/指导_数据探索&amp;预处理_CrowdFlower</title>
    <link href="undefined2020/02/24/2.%E6%AF%94%E8%B5%9B/%E6%8C%87%E5%AF%BC_%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2&amp;%E9%A2%84%E5%A4%84%E7%90%86_CrowdFlower/"/>
    <url>2020/02/24/2.%E6%AF%94%E8%B5%9B/%E6%8C%87%E5%AF%BC_%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2&amp;%E9%A2%84%E5%A4%84%E7%90%86_CrowdFlower/</url>
    
    <content type="html"><![CDATA[<h1 id="以CrowdFlower比赛为例讲解数据探索与预处理"><a href="#以CrowdFlower比赛为例讲解数据探索与预处理" class="headerlink" title="以CrowdFlower比赛为例讲解数据探索与预处理"></a>以CrowdFlower比赛为例讲解数据探索与预处理</h1><h4 id="比赛目标"><a href="#比赛目标" class="headerlink" title="比赛目标"></a>比赛目标</h4><p>​    衡量搜索结果的相关性</p><ul><li>比赛数据集<ul><li>CrowdFlower平台丰富的查询结果配对创建的</li><li>为了评估搜索相关性,CrowdFlower已经将261个搜索词与产品列表放在一起,要求人群对每个搜索结果评分,1,2,3,4分别表示搜索结果从完全不相关到完全相关</li></ul></li></ul><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><ul><li>train.csv训练集数据<ul><li>id 产品id</li><li>query 搜索词语</li><li>product_title 产品标题</li><li>product_description 产品描述文本</li><li>median_relevance 三位评分员的相关性评分中位数</li><li>relevance_variance 评分员的相关性评分方差</li></ul></li><li>test.csv<ul><li>id 产品id</li><li>query 搜索词语</li><li>product_description 产品描述文本</li></ul></li><li>目标变量<ul><li>median_relevance</li></ul></li></ul><h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1435_22_447.png" alt=""></p><ul><li>首先本数据以文字为主,文字只能输入进分类树模型,所以首先要把文字转换成数字</li><li>Dropping HTML标签</li><li>Word Replacement:然后要把拼写错误的单词替换掉</li><li>stemming:词干化:把过去时,进行时等等还原成词干 </li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1436_13_722.png" alt=""></p><ul><li>Counting Features :频率特征</li><li>DIstance features:距离特征</li><li>TF - IDF features:对频率特征的改进,如果频率太高可以通过取对数等方式进行改进</li><li>Query ID:查询的编号</li></ul><h3 id="3-1数据探索"><a href="#3-1数据探索" class="headerlink" title="3.1数据探索"></a>3.1数据探索</h3><ul><li><p>查看一眼文件头</p></li><li><p>查看列信息</p></li><li><p>查看有多少行</p></li><li><p>查看感兴趣的列信息去重后有多少项</p></li><li><p>查看感兴趣的信息去重后前一百项是什么</p></li><li><p>查看两个文件中,感兴趣的同一列有多少个是相同的,多少是不同的</p></li><li><p>对感兴趣的某一列信息,根据重复数画出柱状图</p></li></ul><pre><code class="python"># -*- coding: utf-8 -*-import pandas as pdimport numpy as npimport nltktrain = pd.read_csv(&quot;filePath&quot;)test = pd.read_csv(&quot;filePath&quot;)train.head()test.head()#查看文件头train.columnstest.columns#查看文件的列信息len(train)len(test)#查看文件有多少行train[&#39;query&#39;].unique()[0:100]#查看train这个文件中query的去重的前一百个数据train[&#39;product_title&#39;].unique()[0:10]#查看train这个文件中product_title的去重前十个元素len(train[&#39;query&#39;].unique())    len(test[&#39;query&#39;].unique())#查看train和test两个文件,去重之后一共有多少个数据len(train[&#39;product_title&#39;].unique())len(test[&#39;product_title&#39;].unique())#查看两个文件去重product_title的长度len(np.setdiff1d(test[&#39;product_title&#39;].unique(),train[&#39;product_title&#39;]))#查看两个文件中的product_title有多少个是不一样的len(np.intersect1d(test[&#39;product_title&#39;].unique(),train[&#39;product_title&#39;]))#查看两个文件中的product_title有多少个是一样的query = train[&#39;query&#39;].map(nltk.tokenize.word_tokenize)from nltk.corpus import stopwordsstopset = set(stopwords.words(&#39;english&#39;))def key_plot(data,col,top_num = 10):    s = data[col].map(nltk.tokenize.word_tokenize)    fdisk = nltk.FreqDist(words.lower() for x in s                          for words in x if words.lower()not in stopset)    #让数据集token化,统一小写,把stopwords(in at之类)去掉    top = pd.DataFrame(fdisk.most_common(top_num),columns=[&#39;query&#39;,&#39;time&#39;])    top = top.set_index(&#39;query&#39;)    top.plot(kind=&#39;bar&#39;) #按照query从大到小顺序排列key_plot(train,&#39;query&#39;)key_plot(test,&#39;query&#39;)#然后画图</code></pre><h3 id="3-2预处理"><a href="#3-2预处理" class="headerlink" title="3.2预处理"></a>3.2预处理</h3><ul><li><p>剔除HTML标签</p><ul><li>通过beautifulSoup4库提取HTML的文本信息</li></ul></li><li><p>单词替换</p><ul><li><p>拼写错误检查</p></li><li><p>同义词替换</p></li><li><p>其他替换</p></li><li><p>如下图</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1438_18_617.png" alt="">词干化</p></li><li><p>stemming:</p><ul><li>选择是porter还是snowball</li></ul></li></ul></li><li><p>数据清洗</p><ul><li>clean text:</li><li>drop html</li><li>repalce dic</li></ul></li></ul><pre><code class="python">#!/usr/bin/env python3# -*- coding: utf-8 -*-&quot;&quot;&quot;Created on Thu Oct 31 14:34:00 2019@author: bq____file____    nlp_utils.py____description___    The file provides functions to perform  NLP task,e.g.,TF-IDF and POS tagging__author__    BqLion copy from Chenglong Chen For Study use&quot;&quot;&quot;import reimport sysimport nltkfrom bs4 import BeautifulSoupfrom replacer import CsvWordReplacerfrom nltk import pos_tagfrom sklearn.feature_extraction.text import TfidVectorizer,CountVectorizersys.path.append(&quot;../&quot;)from param_config import config######stop words####stopwords  = nltk.corpus.stopwords.word(&quot;english&quot;)stopwords = set(stopwords)############stemming###########if config.stemmer_type == &quot;porter&quot;:    english_stemmer = nltk.stem.PorterStemmer()elif config.stemmer_type == &quot;snowball&quot;:    english_stemmer = nltk.stem.SnowballStemmer(&#39;english&#39;)def stem_tokens(tokens,stemmer):    stemmed = []    for token in tokens:        stemmed.append(stemmer.stem(token))    return stemmed##############POS tag###########token_pattern = r&quot;(?u)\b\w\w+\b&quot;# \b :bound # \w :any char# \+ :once or more def pos_tag_text(line,            token_pattern=token_pattern,            exclude_stopword=config.cooccurrence_word_exclude_stopword,            encode_digit=False):    token_pattern = re.compile(token_pattern,flags = re.UNICODE | re.LOCATE)    for name in [&quot;query&quot;,&quot;product_title&quot;,&quot;product_description&quot;]:        l = line[name]        ##tokenize        tokens = [x.lower()for x in token_pattern.findall(l)]        ##stem        tokens = stem_tokens(tokens,english_stemmer)        if exclude_stopword:            tokens = [x for x in tokens if x not in stopwords]        tags = pos_tag(tokens)        tags_list = [t for w,t in tags]        tags_str = &quot; &quot;.join(tags_list)        #print tags_str        line[name] = tags_str        return line###########TF-IDF#########class StemmedTfidfVectorizer(TfidVectorizer):    def build_analyzer(self):        analyzer = super(TfidVectorizer,self).build_analyzer()        return lambda doc:(english_stemmer.stem(w)for w in analyzer(doc))token_pattern = r&quot;(?u)\b\w\w+\b&quot;tfidf__norm = &quot;l2&quot;tfidf__max_df = 0.75tfidf__min_df = 3def getTFV(token_pattern = token_pattern,           norm = tfidf__norm,           max_df = tfidf__max_df,           min_df = tfidf__min_df,           ngram_range = (1, 1),           vocabulary = None,           stop_words = &#39;english&#39;):    tfv = StemmedTfidfVectorizer(min_df=min_df, max_df=max_df, max_features=None,                                  strip_accents=&#39;unicode&#39;, analyzer=&#39;word&#39;, token_pattern=token_pattern,                                 ngram_range=ngram_range, use_idf=1, smooth_idf=1, sublinear_tf=1,                                 stop_words = stop_words, norm=norm, vocabulary=vocabulary)    return tfv########### BOW ###########class StemmedCountVectorizer(CountVectorizer):    def build_analyzer(self):        analyzer = super(CountVectorizer, self).build_analyzer()        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))token_pattern = r&quot;(?u)\b\w\w+\b&quot;#token_pattern = r&#39;\w{1,}&#39;#token_pattern = r&quot;\w+&quot;#token_pattern = r&quot;[\w&#39;]+&quot;bow__max_df = 0.75bow__min_df = 3def getBOW(token_pattern = token_pattern,           max_df = bow__max_df,           min_df = bow__min_df,           ngram_range = (1, 1),           vocabulary = None,           stop_words = &#39;english&#39;):    bow = StemmedCountVectorizer(min_df=min_df, max_df=max_df, max_features=None,                                  strip_accents=&#39;unicode&#39;, analyzer=&#39;word&#39;, token_pattern=token_pattern,                                 ngram_range=ngram_range,                                 stop_words = stop_words, vocabulary=vocabulary)    return bow       ################## Text Clean #################### synonym replacerreplacer = CsvWordReplacer(&#39;%s/synonyms.csv&#39; % config.data_folder)## other replace dict## such dict is found by exploring the training datareplace_dict = {    &quot;nutri system&quot;: &quot;nutrisystem&quot;,    &quot;soda stream&quot;: &quot;sodastream&quot;,    &quot;playstation&#39;s&quot;: &quot;ps&quot;,    &quot;playstations&quot;: &quot;ps&quot;,    &quot;playstation&quot;: &quot;ps&quot;,    &quot;(ps 2)&quot;: &quot;ps2&quot;,    &quot;(ps 3)&quot;: &quot;ps3&quot;,    &quot;(ps 4)&quot;: &quot;ps4&quot;,    &quot;ps 2&quot;: &quot;ps2&quot;,    &quot;ps 3&quot;: &quot;ps3&quot;,    &quot;ps 4&quot;: &quot;ps4&quot;,    &quot;coffeemaker&quot;: &quot;coffee maker&quot;,    &quot;k-cups&quot;: &quot;k cup&quot;,    &quot;k-cup&quot;: &quot;k cup&quot;,    &quot;4-ounce&quot;: &quot;4 ounce&quot;,    &quot;8-ounce&quot;: &quot;8 ounce&quot;,    &quot;12-ounce&quot;: &quot;12 ounce&quot;,    &quot;ounce&quot;: &quot;oz&quot;,    &quot;button-down&quot;: &quot;button down&quot;,    &quot;doctor who&quot;: &quot;dr who&quot;,    &quot;2-drawer&quot;: &quot;2 drawer&quot;,    &quot;3-drawer&quot;: &quot;3 drawer&quot;,    &quot;in-drawer&quot;: &quot;in drawer&quot;,    &quot;hardisk&quot;: &quot;hard drive&quot;,    &quot;hard disk&quot;: &quot;hard drive&quot;,    &quot;harley-davidson&quot;: &quot;harley davidson&quot;,    &quot;harleydavidson&quot;: &quot;harley davidson&quot;,    &quot;e-reader&quot;: &quot;ereader&quot;,    &quot;levi strauss&quot;: &quot;levi&quot;,    &quot;levis&quot;: &quot;levi&quot;,    &quot;mac book&quot;: &quot;macbook&quot;,    &quot;micro-usb&quot;: &quot;micro usb&quot;,    &quot;screen protector for samsung&quot;: &quot;screen protector samsung&quot;,    &quot;video games&quot;: &quot;videogames&quot;,    &quot;game pad&quot;: &quot;gamepad&quot;,    &quot;western digital&quot;: &quot;wd&quot;,    &quot;eau de toilette&quot;: &quot;perfume&quot;,}     def clean_text(line,drop_html_flag=False):    names = [&quot;query&quot;,&quot;product_title&quot;,&quot;product_description&quot;]    for name in names:        l = line[name]        if drop_html_flag:            l = drop_html(l)        l = l.lower()        ## replace gb        for vol in[16,32,64,128,500]:            l = re.sub(&quot;%d gb&quot;%vol,&quot;%dgb&quot;%vol,l)            l = re.sub(&quot;%d g&quot;%vol,&quot;%dgb&quot;%vol,l)            l = re.sub(&quot;%dg&quot;%vol,&quot;%dgb&quot;%vol,l)        ## replace tb        for vol in [2]:            l = re.sub(&quot;%d tb&quot;%vol,&quot;%dtb&quot;%vol,l)        ## replace other words        for k,v in repalce_dict.item():            l = re.sub(k,v,l)            l = l.split(&quot; &quot;)        ## replace synonyms        l = replacer.replace(l)        l = &quot; &quot;.join(l)        line[name] = l    return line    ##########    ##drop html tag    ##########    def drop_html(html):        return BeautifulSoup(html).get(text(separator = &quot; &quot;))</code></pre>]]></content>
    
    
    <categories>
      
      <category>2.比赛</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据预处理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2.比赛/指导_常见算法与工具</title>
    <link href="undefined2020/02/24/2.%E6%AF%94%E8%B5%9B/%E6%8C%87%E5%AF%BC_%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95%E4%B8%8E%E5%B7%A5%E5%85%B7/"/>
    <url>2020/02/24/2.%E6%AF%94%E8%B5%9B/%E6%8C%87%E5%AF%BC_%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95%E4%B8%8E%E5%B7%A5%E5%85%B7/</url>
    
    <content type="html"><![CDATA[<h1 id="比赛常用算法与工具"><a href="#比赛常用算法与工具" class="headerlink" title="比赛常用算法与工具"></a>比赛常用算法与工具</h1><h3 id="1-1-本文提纲"><a href="#1-1-本文提纲" class="headerlink" title="1.1 本文提纲"></a>1.1 本文提纲</h3><ul><li>机器学习应用领域</li><li>机器学习常见算法</li><li>常用工具</li><li>建模与问题解决流程<ul><li>数据处理</li><li>特征工程</li><li>模型选择</li><li>寻找最佳超参数:交叉验证</li><li>模型分析与模型融合</li></ul></li><li>kaggle wiki</li><li>简单案例讲解</li></ul><h3 id="1-2机器学习常见算法"><a href="#1-2机器学习常见算法" class="headerlink" title="1.2机器学习常见算法"></a>1.2机器学习常见算法</h3><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1432_54_439.png" alt=""></p><h3 id="1-3机器学习常见工具"><a href="#1-3机器学习常见工具" class="headerlink" title="1.3机器学习常见工具"></a>1.3机器学习常见工具</h3><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1433_04_073.png" alt=""></p><ul><li>scikit - learn :速度不快,但是全面,封装的好,只需要造出来基本参数就可以自动去跑</li><li>gensim - 自然语言处理会用</li><li>NUmPy - 科学计算(封装到其他工具里了)</li><li>matplotlib - 绘图</li><li>pandas - 数据清洗,产出特征,缺省值,填充等</li><li>xgboost - 基于boost的库,分类和回归都可以完成</li><li>Natural Language Toolkit多用于英文的自然语言处理,中文用的很少</li><li>Jieba - 多用于中文语言处理</li><li>TensorFlow - 深度学习库,对显存的占用较高,速度不算太快</li><li>Caffe -深度学习库, 图像用的很多</li><li>Keras - 深度学习库,接口简单,本视频deep learning部分用Keras</li></ul><h3 id="1-4解决问题流程"><a href="#1-4解决问题流程" class="headerlink" title="1.4解决问题流程"></a>1.4解决问题流程</h3><ul><li>了解场景和目标</li><li>了解评估准则</li><li>认识数据</li><li>数据预处理(清洗,调权)</li><li>特征工程</li><li>模型调参</li><li>模型状态分析</li><li>模型融合</li></ul><h3 id="1-5-个重要的问题"><a href="#1-5-个重要的问题" class="headerlink" title="1.5 个重要的问题"></a>1.5 个重要的问题</h3><ul><li><p>拿到数据后怎么了解数据(可视化)</p><ul><li><p>肉眼看数据,尤其是维度较高的情况下是看不出什么东西的,而我们对图像的理解程度比数据高多了</p></li><li><p>例如数据可视化工具Seaborn,例如数据散列分布图和柱状图,这个可以使用Seanborn的pairplot完成</p></li><li><p>各个维度之间可以排列组合,然后看出哪些维度比较有区分度,如下图就11维和14维区分度较高</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1434_01_156.png" alt=""></p></li></ul></li><li><p>选择对贴切的机器学习算法</p><ul><li><p>数据大概看一眼,确定一些特征维度之后,可以用机器学习算法做一个baseline的系统出来</p></li><li><p>根据图谱选择</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1434_20_615.png" alt=""></p></li><li><p>确定算法的超参数</p><ul><li>例子:线性回归:多项式回归<ul><li>多项式的次数很高,就会过拟合<ul><li>过拟合就是模型太努力地去记住训练样本的分布状况,解决办法是增大样本量,或增强正则化作用</li></ul></li><li>次数很低,就会欠拟合<ul><li>解决办法是可以降低样本量,或减少正则化作用</li></ul></li></ul></li></ul></li></ul></li></ul><ul><li><p>定位模型状态,是过拟合还是欠拟合,以及解决办法</p></li><li><p>大量级的数据特征分析,和可视化</p></li><li><p>各种损失函数loss function的优缺点以及取舍</p></li></ul><h3 id="1-6通用机器学习流程与问题解决架构模板"><a href="#1-6通用机器学习流程与问题解决架构模板" class="headerlink" title="1.6通用机器学习流程与问题解决架构模板"></a>1.6通用机器学习流程与问题解决架构模板</h3><ul><li><p>数据转换</p><ul><li><p>在应用机器学习模型之前,所有的数据都必须转换为表格形式,如下图所示,这个过程最耗时,也最困难</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1434_30_463.png" alt=""></p></li><li><p>转换完成后,可以将数据灌入机器学习模型中,表格数据是机器学习中对常见的数据表示形式</p></li><li><p>一般用x表示数据,y表示标签</p></li></ul></li><li><p>标签的种类</p><ul><li>单列 - 分类问题,一个样本只属于一个类,而且一共有两个类</li><li>单列 - 实数制,回归问题,只预测一个值</li><li>多列 - 二进制,分类问题,一个样本只有一个类,总共有多个类</li><li>多列 - 实数制,回归问题,多个值的预测</li><li>多个标签 - 分类问题,一个样本可以属于几个类</li></ul></li><li><p>评估指标</p><ul><li>对任何类型的机器学习,我们都一定要知道如何评估结果<ul><li>例如不均衡的二进制分布问题,我们通常选择受试者工作特征曲线线下面积ROC AUC</li><li>对于多标签的分类问题,通常选择分类交叉熵或多类对数损失</li><li>对于回归问题,会选择均方差</li></ul></li></ul></li><li><p>库</p><ul><li>安装各种库</li><li>pandas,scikit - learn</li><li>xgboost</li><li>keras</li></ul></li><li><p>机器学习处理框架</p></li></ul><ul><li><p>第一步是识别问题,可以通过观察标签解决,一定要知道这个问题是二元分类,还是多种类或多标签分类,还是一个回归问题,当识别了问题,就可以把问题分类训练集和测试集两个部分,如下图所示</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1434_39_647.png" alt=""></p></li></ul><ul><li><p>数据预处理</p><ul><li>数据清洗<ul><li>不可信的样本丢掉</li><li>缺省值极多的字段考虑不用</li></ul></li><li>数据采样<ul><li>上/下采样</li><li>保证样本均衡</li></ul></li><li>工具<ul><li>hive sql</li><li>sparl sql</li><li>pandas</li></ul></li></ul></li><li><p>特征工程</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1434_50_075.png" alt=""></p></li><li><p>特征处理</p><ul><li>数值型</li><li>类别形</li><li>时间类</li><li>文本型</li><li>统计型</li><li>组合特征</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>2.比赛</category>
      
    </categories>
    
    
    <tags>
      
      <tag>比赛</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS229_8.应用机器学习的建议</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_8.%E5%BA%94%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BB%BA%E8%AE%AE/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_8.%E5%BA%94%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BB%BA%E8%AE%AE/</url>
    
    <content type="html"><![CDATA[<h1 id="8-应用机器学习的建议"><a href="#8-应用机器学习的建议" class="headerlink" title="8.应用机器学习的建议"></a>8.应用机器学习的建议</h1><h4 id="8-1如何提高一个算法-机器学习模型的性能"><a href="#8-1如何提高一个算法-机器学习模型的性能" class="headerlink" title="8.1如何提高一个算法/机器学习模型的性能"></a>8.1如何提高一个算法/机器学习模型的性能</h4><p>问题:预测房价的机器学习模型性能遇见瓶颈,如何提升性能?</p><ul><li><p>很多人是凭感觉去解决问题,他们有如下猜想并随便选一个去做</p></li><li><p>随便某个猜想都是耗时耗力巨大的项目,人们常常选择的是一条不归路</p></li><li><p>如何尽量排除无效的道路?</p></li></ul><ol><li>获得更多的训练样本——解决高方差   </li><li>尝试减少特征的数量——解决高方差</li><li>尝试获得更多的特征——解决高偏差</li><li>尝试增加多项式特征——解决高偏差</li><li>尝试减少正则化程度λ——解决高偏差</li><li>尝试增加正则化程度λ——解决高方差</li></ol><h3 id="8-2如何评估一个模型表现"><a href="#8-2如何评估一个模型表现" class="headerlink" title="8.2如何评估一个模型表现"></a>8.2如何评估一个模型表现</h3><ul><li><p>模型的代价函数非常小未必是好事,因为可能存在过拟合的现象</p><p>为了检验模型是否过拟合,可以采用交叉验证的方法</p><p>可以把数据集分为训练集和测试集,然后重复洗牌交叉验证</p></li><li><p>高偏差与高方差</p><ul><li><p>高偏差是欠拟合</p></li><li><p>高方差是过拟合</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1448_17_292.png" alt=""></p></li></ul></li><li><p>增加模型的多项式次数导致的过/欠拟合问题</p><ul><li>训练集上:<ul><li>增加模型多项式的次数会导致代价函数单调递减</li></ul></li><li>交叉验证集上:<ul><li>增加模型多项式的次数会导致代价函数先减后增</li><li>先减是改善了欠拟合的问题</li><li>后增就是过拟合越来越严重</li></ul></li><li>结论:<ul><li>交叉验证是评估过/欠拟合的有效手段</li></ul></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>评估模型表现</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS229_5.过拟合和正则化</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_5.%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_5.%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AD%A3%E5%88%99%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h3 id="1-过拟合和欠拟合问题"><a href="#1-过拟合和欠拟合问题" class="headerlink" title="1.过拟合和欠拟合问题"></a>1.过拟合和欠拟合问题</h3><p>两张插图说明白</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1444_50_140.png" alt=""></p><p><img src="http://q0u9fsub0.bkt.clouddn.com///20191112/wVpA8KOfcEdf.png?imageslim" alt="mark"></p><p>解决办法:</p><p>过拟合就丢弃不能正确预测的特征,欠拟合就加特征</p><h3 id="2-代价函数的惩罚项"><a href="#2-代价函数的惩罚项" class="headerlink" title="2.代价函数的惩罚项"></a>2.代价函数的惩罚项</h3><p>由下图可以看出,过拟合由于高维项造成</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1445_01_605.png" alt=""></p><p><img src="http://q0u9fsub0.bkt.clouddn.com///20191112/tmeD1LOk8k1Q.png?imageslim" alt="mark"></p><ul><li><p>给高维项设置惩罚项</p><ul><li><p>代价函数里给三次项和四次项增加一些重量,梯度下降的时候系统就会更急切想把三次项和四次项降下来</p></li><li><p>增加了惩罚项的代价函数<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1445_12_545.png" alt=""></p></li></ul></li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1445_57_673.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS229_6.神经网络(表述)</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_6.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(%E8%A1%A8%E8%BF%B0)/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_6.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(%E8%A1%A8%E8%BF%B0)/</url>
    
    <content type="html"><![CDATA[<p>#</p><h1 id="6-神经网络-表述"><a href="#6-神经网络-表述" class="headerlink" title="6.神经网络(表述)"></a>6.神经网络(表述)</h1><h3 id="6-1问题的由来"><a href="#6-1问题的由来" class="headerlink" title="6.1问题的由来"></a>6.1问题的由来</h3><ul><li>特征爆炸无法处理<ul><li>例如,智能识别一个图片是不是汽车的问题</li><li>假设使用50*50小黑白照片</li><li>那么就有2500个像素灰度作为特征</li><li>2500个特征两两组合就是3000000项,这的多项式处理不了</li><li>这时候就需要神经网络</li></ul></li></ul><h3 id="6-2直观理解和逻辑运算符的构建"><a href="#6-2直观理解和逻辑运算符的构建" class="headerlink" title="6.2直观理解和逻辑运算符的构建"></a>6.2直观理解和逻辑运算符的构建</h3><ul><li><p>直观理解</p><ul><li>神经网络能够通过自身学习得到一些列特征,这些特征是使用数据的原始特征经过拆分组合和一系列逻辑运算得到的</li><li>这样的一系列特征比普通的逻辑回归的表层特征要深刻的多</li><li>最后做决定的神经元依据上一层传进来的特征,这些特征已经经过多层传导,内化为自己的特征了</li></ul></li><li><p>神经元可以通过增加一个固定权重神经元构建逻辑运算符(与,非,或,抑或等)</p><ul><li>如下图</li></ul></li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1446_26_856.png" alt=""></p><h3 id="6-3神经网络的形态"><a href="#6-3神经网络的形态" class="headerlink" title="6.3神经网络的形态"></a>6.3神经网络的形态</h3><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1446_40_056.png" alt=""></p><h4 id="6-4神经网络基本概念的定义from-CS224N"><a href="#6-4神经网络基本概念的定义from-CS224N" class="headerlink" title="6.4神经网络基本概念的定义from CS224N"></a>6.4神经网络基本概念的定义from CS224N</h4><p>如图，神经网络有输入，有偏置单元，有激活函数，以及对后续神经网络的输出。</p><p>输入x，w是乘以输入的权重，b是偏置值，f是sigmod函数。</p><p>输入乘以权重加上偏置值后，进入sigmod函数，输出一个分类值。</p><p>有了神经网络，让我们再回到之前的单窗口分类器。代替之前直接将softmax应用在词向量，现在我们在词向量和输入之间有了中间隐藏层，这正是我们真正获得准确性和表现力的开端。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1418_52_588.png" alt=""></p><p>如上图，x是输入，是有20维的窗口，输出是8维，所以W参数矩阵时8 * 20的规模。</p><p>然后s最终的分类结果是一个单独的数字，所以U矩阵是 8*1的规模</p><p><strong>最大间距损失函数</strong></p><p>最大间距损失函数比softmax的交叉熵误差更具有鲁棒性。softmax得出的决策边界如下图所示，再数据集加入一些靠近边界的数据时，本边界会显得鲁棒性不足</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1419_00_261.png" alt=""></p><p>最大间距损失函数和svm的边界类似，它在计算边界时得出的是距离两类数据都最远的边界。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1419_09_277.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS229_7.神经网络(学习)</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_7.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(%E5%AD%A6%E4%B9%A0)/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_7.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(%E5%AD%A6%E4%B9%A0)/</url>
    
    <content type="html"><![CDATA[<h1 id="7-神经网络-学习"><a href="#7-神经网络-学习" class="headerlink" title="7.神经网络(学习)"></a>7.神经网络(学习)</h1><h3 id="7-1神经网络的代价函数"><a href="#7-1神经网络的代价函数" class="headerlink" title="7.1神经网络的代价函数"></a>7.1神经网络的代价函数</h3><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1447_04_343.png" alt=""></p><ul><li>第一项是在K个输出项上(比如四分类就是K=4),累加逻辑回归的代价函数</li><li>第二项就是更高维的罚函数</li></ul><p>代价函数的目的:找到使得J(θ)最小化的θ</p><h3 id="7-2单个训练数据如何通过神经网络"><a href="#7-2单个训练数据如何通过神经网络" class="headerlink" title="7.2单个训练数据如何通过神经网络"></a>7.2单个训练数据如何通过神经网络</h3><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1447_17_262.png" alt="">    </p><ul><li><p>z(2) = θ(1)a(1) </p><ul><li>z(2)是第二层神经元的输入项,共五项,是a(1)的三项经过θ(1)矩阵拆分后的结果</li></ul></li><li><p>a(2) = g[z(2)]</p><ul><li>a(2)是第二层神经元的输出项,共五项,是z(2)五项经过第二层神经元的sigmod函数变换后的结果</li></ul></li></ul><p>后边的几层同理</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS229_2.正规方程</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_2.%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_2.%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h1 id="CS229—正规方程"><a href="#CS229—正规方程" class="headerlink" title="CS229—正规方程"></a>CS229—正规方程</h1><p>1.定义</p><p>​    正规方程(Normal Equation)是通过对训练数据/训练答案/待求参数之间的矩阵变换得到答案</p><ul><li>正规方程求解参数矩阵的公式:<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1442_35_105.png" alt=""></li></ul><p>2.正规方程法和梯度下降法的对比</p><ul><li><p>梯度下降的特点</p><ul><li>要设置学习率α</li><li>多次迭代</li><li>工作地非常均衡</li><li>适合特征变量很多的时候用</li><li>要求参数归一化</li></ul></li><li><p>正规方程法</p><ul><li>不用设置学习率</li><li>不用迭代</li><li>需要计算</li><li>适合特征变量不超过1000时用</li><li>不要求参数归一化,如下图</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1442_51_944.png" alt=""></p><ul><li>有些较复杂问题不能使用正规方程法,只能使用梯度下降</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>正规方程</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS229_10.聚类Clustering</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_10.%E8%81%9A%E7%B1%BBClustering/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_10.%E8%81%9A%E7%B1%BBClustering/</url>
    
    <content type="html"><![CDATA[<h1 id="10-聚类Clustering"><a href="#10-聚类Clustering" class="headerlink" title="10.聚类Clustering"></a>10.聚类Clustering</h1><h2 id="10-1无监督学习简介"><a href="#10-1无监督学习简介" class="headerlink" title="10.1无监督学习简介"></a>10.1无监督学习简介</h2><p>无监督学习是让计算机学习无标签数据,而不是之前的标签数据</p><p>如图就是聚类问题<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1451_06_446.png" alt=""></p><p>左上第一张图是市场分割,就是根据数据库里沉淀的客户信息来对客户分类,做知识图谱和推荐系统</p><h2 id="10-2-K-mean-K均值算法"><a href="#10-2-K-mean-K均值算法" class="headerlink" title="10.2 K-mean (K均值算法)"></a>10.2 K-mean (K均值算法)</h2><p>K均值是最普及的一个聚类算法,算法接受一个数据集,将数据聚类为不同的组</p><ul><li><p>K-均值是一个迭代算法</p><ul><li><p>设想我们将数据聚集成n组,选择k个随机的点为聚类中心</p></li><li><p>对数据集中的每个数据,按照距离k个中心点的距离,将其与距离最近的点关联起来,与这个点聚为一类</p></li><li><p>然后开始迭代:</p><ul><li>每次计算一个组的平均值,将中心点移动到平均值的位置</li><li>如此反复几次中心点的位置就不再变化</li><li>如下图所示<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1451_27_402.png" alt=""></li></ul></li></ul></li></ul><h2 id="10-3优化目标"><a href="#10-3优化目标" class="headerlink" title="10.3优化目标"></a>10.3优化目标</h2><p>K-均值的优化目标就是最小化所有的数据点与其关联的聚类中心点之间的距离之和</p><p>因此K均值的代价函数为:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1451_42_387.png" alt=""></p><h2 id="10-4随机初始化"><a href="#10-4随机初始化" class="headerlink" title="10.4随机初始化"></a>10.4随机初始化</h2><p>在运行k均值算法之前,需要随机初始化所有的聚类中心点</p><p>1.选择K &lt; m ,即聚类中心点的个数要小于所有训练集实例的数量</p><p>2.随机选择K个训练实例,然后令K个聚类中心等于他们</p><p>K-均值算法的一个问题在于,他可能停留在某个局部最小值处,这取决于初始化的位置(类似于梯度下降算法的初始位置选取),为了解决此问题,可以多次运行K-均值算法,每次都随机初始化,然后比较各自结果,选取最小的值,当然如果K值较大结果不会有明显改善</p><h2 id="10-5选择聚类数"><a href="#10-5选择聚类数" class="headerlink" title="10.5选择聚类数"></a>10.5选择聚类数</h2><p>聚类的数量一般是人工选取,这时候需要思考运用K-均值算法聚类的动机是什么,最好能服务于该目标的聚类数</p><p>聚类数量选择的时候有个肘部法则,例如K值设置为1,2,3,4,5的时候发现3的效果最好,更多或者更少都不行,像人的肘关节那样有个拐点,通常这个肘部就是最优的点</p><p>比如T恤制造可以选择S,M,L三个尺码也可以选择 S,M,L,XL,XXL五个尺码一样,K选3还是5取决于具体的业务场景</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1451_56_417.png" alt=""></p><h2 id="10-6聚类的衡量指标"><a href="#10-6聚类的衡量指标" class="headerlink" title="10.6聚类的衡量指标"></a>10.6聚类的衡量指标</h2><ol><li><p>均一性:P</p><p>可以认为是正确率(每个聚簇中正确分类的样本数占该聚簇总样本数的比例和)</p></li><li><p>完整性:r</p><p>类似于召回率，同类别样本被归类到相同簇中，则满足完整性;每个聚簇中正确分类的样本数占该 类型的总样本数比例的和</p></li><li><p>V-measure:均一性和完整性的加权</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1452_07_748.png" alt=""></p></li></ol>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>聚类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS229_4.逻辑回归</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_4.%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_4.%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h1 id="4-逻辑回归-logistics-regression"><a href="#4-逻辑回归-logistics-regression" class="headerlink" title="4.逻辑回归(logistics regression)"></a>4.逻辑回归(logistics regression)</h1><h3 id="1-逻辑回归模型"><a href="#1-逻辑回归模型" class="headerlink" title="1.逻辑回归模型"></a>1.逻辑回归模型</h3><ul><li><p>分类问题输出一个结果”正确”或者”错误”</p></li><li><p>逻辑回归模型:</p></li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1443_44_904.png" alt=""></p><ul><li>当hθ(x) 在区间[0.5,1]时,预测是”正确”,越贴近1越确信</li><li>当hθ(x) 在区间[0,0.5)时,预测是”错误”,越贴近0越确信</li></ul><h3 id="2-逻辑回归的代价函数"><a href="#2-逻辑回归的代价函数" class="headerlink" title="2.逻辑回归的代价函数"></a>2.逻辑回归的代价函数</h3><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1443_56_769.png" alt=""></p><ul><li>简单点一句话讲:<ul><li>答案是1,你预测了1,无误差</li><li>答案是1,你预测的数字离0越近误差就越大,完全是0误差就是无限大(不可能情况)</li><li>答案是0的情况与答案是1的情况同理</li></ul></li></ul><h3 id="3-对逻辑回归的代价函数使用梯度下降算法-以得到最优参数-最优解"><a href="#3-对逻辑回归的代价函数使用梯度下降算法-以得到最优参数-最优解" class="headerlink" title="3.对逻辑回归的代价函数使用梯度下降算法,以得到最优参数/最优解"></a>3.对逻辑回归的代价函数使用梯度下降算法,以得到最优参数/最优解</h3><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1444_07_614.png" alt=""></p><h3 id="4-高级优化"><a href="#4-高级优化" class="headerlink" title="4.高级优化"></a>4.高级优化</h3><ul><li><p>一些建议</p><ul><li><p>不需要写代码实现代价函数的迭代</p><ul><li>从技术来讲,其实我们不需要自己手动写程序来计算刚才提到的梯度下降算法的代价函数以及其迭代过程,就像我们不需要自己手动写求平方根和创建数组一样,这些问题早都有非常成熟的库来调用</li></ul></li><li><p>更高级的算法</p><ul><li>除了梯度下降算法,还有<strong>共轭梯度法 BFGS</strong> (<strong>变尺度法</strong>) 和<strong>L-BFGS</strong> (<strong>限制变尺度法</strong>)这种更高级算法可调用</li><li>吴恩达本人已经使用如上提到的算法很多年了,也才是最近才搞清楚他们的内部实现细节</li></ul></li></ul></li></ul><h3 id="5-一对多分类问题"><a href="#5-一对多分类问题" class="headerlink" title="5.一对多分类问题"></a>5.一对多分类问题</h3><ul><li>逻辑回归是二选一分类问题,只用输出0或者1</li><li>还有一对多分类问题,输出结果可能是1,2,3,4,5<ul><li>对应的业务场景就是预测收到的邮件要自动打上什么样的标签?(重要,垃圾,家人,朋友,公司等)</li></ul></li></ul><ul><li>一对多分类问题的解决思想<ul><li>一对多假如有三个标签A,B,C,则采用一对余的思想来分类</li><li>具体就是把A打上”正确”标签,BC都是错误标签</li><li>B,C也是同理操作</li><li>然后就可以应用上文所说的二分法工具来做了(逻辑回归)</li></ul></li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1444_21_852.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>逻辑回归</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224n_9 - RNN,LSTM,GRU</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_9%20-%20RNN,LSTM,GRU/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_9%20-%20RNN,LSTM,GRU/</url>
    
    <content type="html"><![CDATA[<h4 id="助教分享：建立一个更好的语言模型"><a href="#助教分享：建立一个更好的语言模型" class="headerlink" title="助教分享：建立一个更好的语言模型"></a>助教分享：建立一个更好的语言模型</h4><p>语言模型是NLP中最经典的任务，这里有三种方式可以让其变得更好，第一是更好的输入表达，第二是更好的正则化或者预处理，第三是有更好的模型。</p><p>第一种是改进输入，例如Glove是一个词层面的表示，事实上可以把单词进一步编码为子词阶段，可以采用语素解码方式，可以采用BPE，最终也可以采用字母级的嵌入，他的作用是大大减少所用到的词汇量，让模型预测变得更简单，如下图展示的论文就是对输入端的改进。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191209_1558_58_194.png" alt=""></p><p>第二种办法是采用正则化技巧，来改善过拟合的问题，有一堆论文在讲如何做正则化，今天聚焦的点是在预处理阶段，通过和计算机视觉类似的替换技巧来构建更合理的语料库，比如把频繁和罕见的词语的评率都往平均值上拉一拉，就会使得曲线更显平滑，平滑的分布能让系统获得更好的语言模型和更好的实验结果。</p><p>第三是更好的模型，etc。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS229_1.绪论</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_1.%E7%BB%AA%E8%AE%BA/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_1.%E7%BB%AA%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="CS229–绪论"><a href="#CS229–绪论" class="headerlink" title="CS229–绪论"></a>CS229–绪论</h1><h3 id="1-监督学习和无监督学习"><a href="#1-监督学习和无监督学习" class="headerlink" title="1.监督学习和无监督学习"></a>1.监督学习和无监督学习</h3><ul><li><h3 id="监督学习定义"><a href="#监督学习定义" class="headerlink" title="监督学习定义"></a>监督学习定义</h3><ul><li>部分数据集已经有答案.比如房价信息集,知道了正确的房价,需要预测出更多的房价<ul><li>回归问题,预测连续值的输出(房价)</li><li>聚类问题,预测离散值的输出(肿瘤是良性还是恶性?)</li></ul></li></ul></li><li><p>无监督学习</p><ul><li>无监督学习没有答案,自动聚类:比如google搜索BP油井泄露,会把CNN,ABC,BBC新闻放一起</li></ul></li></ul><h3 id="2-线性回归算法"><a href="#2-线性回归算法" class="headerlink" title="2.线性回归算法"></a>2.线性回归算法</h3><ul><li><p>监督学习的典型过程</p><ul><li><p>Training set –[喂食给]—&gt;Learning Algorithm –[生成]–&gt;hypothesis(假设函数)</p></li><li><p>size of house–[输入]—-&gt;hypothesis(函数)—-[输出]—-&gt;Estimate price</p></li></ul></li></ul><ul><li><p>线性回归</p><ul><li>预测房价的假设函数:h(x) = θ0 + θ1x</li><li>根据训练集生成一元一次方程,输入房间大小得到房价预测</li></ul></li></ul><h3 id="3-代价函数"><a href="#3-代价函数" class="headerlink" title="3.代价函数"></a>3.代价函数</h3><ul><li><p>代价函数有利于弄清楚如何把最有可能的直线与我们的数据相拟合</p></li><li><p>最小化代价函数就可得到最好的预测函数</p></li><li><p>如何选择假设函数hypothesis “h(x) = θ0 + θ1x” 中的两个参数θ0和θ1,使预测结果与答案误差最小?</p><ul><li><p>用最小二乘法:找到预测直线中,y与实际的y0之差平方和最小的那个直线</p></li><li><p>代价函数:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1442_21_679.png" alt=""></p></li><li><p>上图只是平方和代价函数,还有很多其他类型的代价函数.</p></li><li><p>代价函数是关于假设函数的参数 θ0/θ1的函数</p></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224n_6 - 依存分析</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_6%20-%20%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_6%20-%20%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h1 id="Lecture6-依存分析"><a href="#Lecture6-依存分析" class="headerlink" title="Lecture6 依存分析"></a>Lecture6 依存分析</h1><p>本节课综述</p><ul><li>这节课主要讲句法 + 语法 + 依存分析</li><li>基于转移的依存关系分析语法</li><li>神经网络内容</li><li>TensorFlow + 实际的nlp内容例如构建神经依存关系分析器</li></ul><h4 id="1-依存分析的概念和一个例子"><a href="#1-依存分析的概念和一个例子" class="headerlink" title="1.依存分析的概念和一个例子"></a>1.依存分析的概念和一个例子</h4><p>句子或句子的一部分有一种结构，人们可以用特定的方式将他们组合起来，我们可以从非常简单的并不构成句子的东西入手。</p><ul><li>一个冠词 + 名字 通常被语言学家称为名词短语</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1421_08_337.png" alt=""></p><ul><li><p>有一些规则可以用来扩展他们：比如在冠词和名词之间加一个形容词</p><p>the large cat</p><p>a beautiful dog等</p></li><li><p>你也可以在名词后边添加介词短语</p><p>the large cat in a crate</p><p>the barking dog on the table等</p></li></ul><p>传统上，语言学家和自然语言处理器想做的是描述人类语言结构，人们过去有两个关键工具来做到这点，一种是上下文无关文法的方法，这种方法经常被语言学家引用为短语结构文法。我们现在要做的就是写出这些上下文无关文法的规则。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1421_18_978.png" alt=""></p><p>还有一种了解语言结构的不同角度，就是依存句法结构。它通过找到句子当中每一个词所依赖的部分来描述句子结构。如果一个词修饰另一个词，或者是另外一个词的论证，那么他就是那个词的依赖。例如在barking dog中barking就是dog的依赖，因为barking 修饰dog。在large barking dog中，large也是dog的依赖.同理， a large bakring dog by the door中，by the door也是dog的依赖。所以我们可以画出一个句子中的依存结构<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1421_37_671.png" alt=""></p><p>space可以修饰study，代表科学家从宇宙空间中学习鲨鱼的知识。 （科学家从宇宙空间中学习关于鲨鱼的知识）</p><p>space也可以修饰whale，代表鲨鱼是从宇宙空间中来的。(科学家研究从宇宙空间中来的鲨鱼）</p><h4 id="2-解析依存关系的方法"><a href="#2-解析依存关系的方法" class="headerlink" title="2.解析依存关系的方法"></a>2.解析依存关系的方法<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1421_47_879.png" alt=""></h4>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224n_7 - Tensorflow</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_7%20-%20Tensorflow/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_7%20-%20Tensorflow/</url>
    
    <content type="html"><![CDATA[<h1 id="Lecture7-TensorFlow"><a href="#Lecture7-TensorFlow" class="headerlink" title="Lecture7 TensorFlow"></a>Lecture7 TensorFlow</h1><p>为什么要使用深度学习框架？</p><ul><li>有助于扩展机器学习代码</li><li>自动计算梯度（通常梯度计算并不重要，这些框架可以自动处理梯度，使得我们把重点放在更高层次的数学上）</li><li>在很多领域使机器学习更标准化</li><li>这些框架提供了GPU接口</li></ul><h4 id="1-TensorFlow是什么？"><a href="#1-TensorFlow是什么？" class="headerlink" title="1.TensorFlow是什么？"></a>1.TensorFlow是什么？</h4><p>是一个谷歌开发的深度学习框架，使用流式图进行数值计算的开源软件库。TensorFlow是用于表示机器学习算法的接口，以及用于执行这种算法的实现。</p><p>TensorFlow的一个big idea就是数值计算表示为计算图来进行。 图的节点是操作，每个节点都有一个输入和输出，节点之间的边表示他们之间流动的张量，在实践中最好的方法就是认为，张量是一个n维数组。</p><p>使用流式图作为深度学习框架主干的优点在于它允许你使用小而简单的操作建立复杂的模型，当我们这样做的时候，会使得梯度计算变得极其简单。</p><p>此框架可以自动求导，而且图方法的每一个操作都是可以在其所在点被评估的一个函数。变量将成为输出其当前值的有状态节点。</p><p>占位符是那些在执行期间才会接收值的节点，如果在你的网络中有依赖外部数据的一些输入，你并不想在建立图时依赖任何实际的值。你并不想在建立图时依赖任何的实际的值，那么这些就是我们在训练时添加到计算中的充当占位符的值。对于占位符，我们不给定任何初始的值，仅仅分配一个数据类型，我们的图仍然知道该计算什么，尽管当前并没有存储的值。 </p><p>还有一种节点是数学操作节点，比如矩阵乘法，加法，激活函数、</p><h4 id="2-建立图和会话"><a href="#2-建立图和会话" class="headerlink" title="2.建立图和会话"></a>2.建立图和会话</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1422_00_819.png" alt=""></p><p>新建一个变量b，将其初始化大小是100维的“0”的向量。</p><p>新建一个W，服从[-1,1]均匀分布，shape是（784,100）的TensorFlow变量。</p><p>我们给输入创建一个占位符x，没有被初始化任何值。它仅仅接受32位浮点数的数据类型，接受一个（100,784）shape。</p><p>实际建立流式图，我们用h表示对x和W进行矩阵乘法并加上偏置b，然后进行Relu运行的结果。</p><p>h的数学表达式：</p><p><img src="http://q0u9fsub0.bkt.clouddn.com///20191204/4b2z8nKEsqKk.png?imageslim" alt="mark">)<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1422_10_895.png" alt=""></p><p>我们已经定义了图，下一个问题是我们如何执行它。在TensorFlow中运行图形的方式是将其部署在一个称为会话的地方（session）。</p><p>会话是特定的执行上下文（如CPU和GPU）的绑定。</p><p>我们将会建立一个会话对象，并且调用两个参数fetches和feeds，fetches是返回节点输出的图形节点列表，feeds是一个从图节点到我们想要在模型中运行的实际值的字典映射，所以这就是我们实际填写在前面提到的占位符的地方。</p><p> <img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1422_21_099.png" alt=""></p><p>代码增添了如图所示三行。</p><pre><code>sess = tf.Session()</code></pre><p>表示新建一个session，他会采取默认的环境，最有可能的是一个CPU，当然你也可以添加一个你想要的运行设备作为参数。 </p><pre><code class="python">sess.run(tf.initialize_all_variables())</code></pre><p>初始化所有的变量。</p><p>这是概念密集型流程，称为懒惰评估。这意味着你的图片的评估只会在运行时发生。我们可以在TensorFlow中给运行时间添加一个解释。这就是session（会话）。</p><p>一旦我们建立好了会话，我们就可以准备好实际调用。</p><pre><code class="python">sess.run(h,{x:np.random.random(100,784)})</code></pre><p>第三行：输入h中的x，x是随机初始化的矩阵</p><p>到目前为止，我们首先使用变量和占位符建立了一个图，然后我们将图部署到会话上，也就是我们的执行环境，接下来我们看看如何训练模型。</p><p>一个问题：变量和占位符的不同？为什么要使用占位符？</p><p>通常情况下 ，变量是我们感兴趣的参数，你几乎可以将他们视为直接对应关系。</p><p>上文代码中的x是数据，而不是我们感兴趣的需要调优的参数，对数据使用占位符。</p><p>此外，在刚开始的时候，对我们模型中的参数进行初始化试非常重要的。我们如果想要把我们的模型传给别人，他没有理由包含任何真实的数据，数据应该是任意的。模型中的参数才是你的模型的依据，这些参数是的你的模型更有趣，模型的计算实际上就是计算这些参数。</p><p>​        </p><h4 id="3-训练模型"><a href="#3-训练模型" class="headerlink" title="3.训练模型"></a>3.训练模型</h4><h4 id="3-1定义损失"><a href="#3-1定义损失" class="headerlink" title="3.1定义损失"></a>3.1定义损失</h4><p>对于优化而言，我们如何定义损失？我们将使用占位符作为标签，使用label和prediction建立一个损失节点。</p><p>如下代码表示我们在网路结束时的预测值，它是对神经网络顶层进行softmax函数计算，输出一个概率向量，这是一个回归过程。 </p><p>第一个标志是你的神经网络前馈阶段在哪里结束？他就是在网络预测出值的时候结束。</p><pre><code class="pyhton">prediction = tf.nn.softmax(...) </code></pre><p>如下代码表示创建一个label变量，它是我们真实标记的占位符，我们的模型根据他来训练。</p><pre><code class="python">label = tf.placeholder(tf.float32,[100,10])</code></pre><p>如下代码是创建交叉熵节点，它是真实标签乘以Tensorflow中对prediction取对数的乘积，然后再按列求和。也就是计算预测的结果准不准。</p><pre><code class="python">cross_entropy = -tf.reduce_sum(label * tf.log(prediction),axis = 1)</code></pre><p>TensorFlow会把我们写的加减乘除自动转换为TensorFlow中的操作。</p><h4 id="3-2计算梯度"><a href="#3-2计算梯度" class="headerlink" title="3.2计算梯度"></a>3.2计算梯度</h4><p>我们在计算梯度之前，首先要定义一个优化器对象。在tf中有一个通用的抽象列叫做优化器，在这个类中的每一个子类都是一个针对特定学习算法的优化器，我们目前使用的学习算法是梯度下降算法，</p><p>如下代码调用的.minimize方法，参数就是我们想要最小化的节点。</p><p>minimize方法其实做了两件事，第一是计算了交叉熵的梯度（他和我们在图中定义的所有的变量相关），第二是他会给这些变量根据梯度进行更新。</p><p>我们在底层实际是如何计算梯度的呢？每一个图节点都有一个附加的梯度操作，都有相对于输入预先构建的输出梯度，当我们在计算交叉熵相对于所有参数的梯度的时候，通过图使用链式法则利用反向传播计算是非常简单的。</p><p>这就是把机器学习框架表示为计算图表的主要优势。这样子程序也很容易回退，在图形中向后遍历，并且在每个节点上将误差信号乘以节点的预定义梯度。所有的这些都是自动实现的。</p><pre><code class="python">tf.train.GradientDescentOptimizer(lr).minimize(cross_entropy)</code></pre><h4 id="3-3学习计划"><a href="#3-3学习计划" class="headerlink" title="3.3学习计划"></a>3.3学习计划</h4><p>我们有了梯度，我们能对梯度进行更新，我们需要创建一个迭代的学习计划，比如这里我们需要迭代一千次，我们在数据集上使用next_batch方法。</p><pre><code class="python">for i in range(1000):    batch_x,batch_label = data.next_batch()</code></pre><p>我们将会得到我们输入的一个批次的数据，以及对应的标签值。</p><p>然后在train_step变量上调用session.run方法，当我们调用他的时候，已经对图中所有的变量应用了梯度。我们需要建立字典，将值传给我们之前定义好的两个占位符。这里的x和label是图中的节点，字典中的关键字是图中的节点，对应的项是numpy数据。</p><pre><code class="python">sess.run(train_step,feed_dict = {x:batch_x,label:batch_label}</code></pre><p>TensorFlow提供了很好的numpy接口，tf会把图中节点的数据转换为张量，所以我们可以把numpy数组插入到字典中，比如batch_x和batch_label，我们执行sess.run之后会得到输出，如果定义了变量比如output等于sess.run,那么他们也是相应节点的nump数组，也就是这些节点评估的值。通过train_step将会返回给你梯度的值。</p><h4 id="4-变量共享"><a href="#4-变量共享" class="headerlink" title="4.变量共享"></a>4.变量共享</h4><p>如果想要建立一个大的模型，经常需要共享大的变量，你可以能想在一个地方初始化他们，例如，我们可能想要多次实例化我的图，或者更有趣的是，我们想要在GPU集群上训练。也就是说通常情况下我们希望能在不同的设备上训练模型。</p><p>实现这种共享的简单思路是，在代码的顶端创建这个变量的字典。把一些字符串的字典放到他们所代表的变量中。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1422_33_648.png" alt=""></p><p>但是有一些原因证明这其实不是一个好的主意，这主要的原因是它破坏了封装性，在TensorFlow中构建图的代码应该始终具有相关使用节点和操作的所有有关信息。在代码文件中应该能找到神经元的名字，变量的形状。如果只是在代码的顶部有这些大的field，你其实会丢失这些信息。</p><p>TensorFlow设计了有关变量作用域的解决方案:variable_scope。</p><p>提供了一个简单的名称空间方案来避免冲突，另一个相关的函数是get_variable。get_variable会为你创建一个变量，如果具备特定名词的变量存在，他会访问该变量，让我们来看看它是如何起作用的。</p><pre><code class="pyhton">tf.variable_scope() #provides simple name_spacing to avoid clashes</code></pre><p>open一个foo的新变量作用域，调用get_variable方法，创建变量名，名字为v。 </p><pre><code class="python">with tf.varibale_scope(&quot;foo&quot;）:        v = tf.get_varibale(&quot;v&quot;,shape = [1])# v.name == &quot;foo/v:0&quot;</code></pre><p>这样当我们调用foo，就好像进入了一个目录，让我们关闭这个变量作用域并reopen，并使用参数reuse = True。现在如果调用get_varibale，name为v，我实际上得到的是同一个变量，我将会访问该变量，其实我访问的是和之前创建的对象相同的变量。</p><p> 如果我再一次关闭变量作用域，然后重新打开，使用参数reuse = False，如果我尝试再次运行，程序会崩溃。因为你已经设置他不复用任何变量，所以当它试图创建新的变量，但是已经存在一个同名变量了，在下个作业里，变量作用域的使用将会是明显的。</p><pre><code class="python">小结;1.建立一个    1.1前馈+预测    1.2优化器（梯度+训练步骤设置）2.初始化一个会话3.训练session.run（train_step,feed_dict）</code></pre><h4 id="live-demo环节"><a href="#live-demo环节" class="headerlink" title="live demo环节"></a>live demo环节</h4><p>直接在anaconda中做。</p><h4 id="助教论文分享：Visual-Dialog（可视对话）"><a href="#助教论文分享：Visual-Dialog（可视对话）" class="headerlink" title="助教论文分享：Visual Dialog（可视对话）"></a>助教论文分享：Visual Dialog（可视对话）</h4><p>许多人相信下一代人工智能系统将会能够根据视觉内容以自然语言与人类进行有意义的对话。在看这篇论文之前，我们先看看相关工作。</p><p>许多人对自然语言处理和计算机视觉融合方向做了许多工作。比如给图像加字幕或者描述，如下图片。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1422_44_145.png" alt=""></p><p>还有一类工作是视觉-语义对齐，这个模型并不是给出每张图片的描述，而是给出图片中每个独立组件的描述。在搜集图片中的组件时，需要给图片画出许多边界框，并单独描述每一个独立的组件。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1422_53_931.png" alt=""></p><p>还有一类工作与我们今天分享的论文很相关，就是视觉问答，模型基于视觉内容回答给定的问题。在此种情况下，如图所示，回答是二分类的，yes or no或者非常简洁的其他二分类回答（数字，圆或者方之类的）。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1423_01_032.png" alt=""></p><p>我们要介绍的论文，《visual dialog》中，模型可以基于视觉内容，和人进行有意义的对话。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1423_10_721.png" alt=""></p><p>如下图所示，这个模型使用卷积神经网络将图片进行编码，同时使用两个循环神经网络将问题和对话历史进行编码，然后将三个编码合并为一个向量。为他接上一层全连接层和一个解码器，用来生成基于向量表示的答案。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1423_39_943.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224n_8 - RNN和语言模式</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_8%20-%20RNN%E5%92%8C%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%BC%8F/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_8%20-%20RNN%E5%92%8C%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<h1 id="Lecture8-RNN和语言模式"><a href="#Lecture8-RNN和语言模式" class="headerlink" title="Lecture8 RNN和语言模式"></a>Lecture8 RNN和语言模式</h1><p>本节讲简单的循环神经网络模型，这个模型家族是大多数人现在在实际训练环境中使用的。</p><p> 概览：</p><p>1.传统的语言模型</p><p>2.RNNs</p><p>3.语言模型建模来驱动循环神经网络（RNN）</p><p>4.重要的训练时的问题和技巧</p><p>​    梯度消失问题</p><p>​    梯度爆炸问题</p><p>5.用于队列处理的RNN</p><p>6.双向深度RNN</p><h4 id="1-传统的语言模型和现在的词向量模型对比"><a href="#1-传统的语言模型和现在的词向量模型对比" class="headerlink" title="1. 传统的语言模型和现在的词向量模型对比"></a>1. 传统的语言模型和现在的词向量模型对比</h4><p>传统语言模型中，理想情况下预测一个语序是根据前n-1个词出现的条件概率下，第n个词出现的概率。实际中这么做不可行，一个是语序有无限多个，而且计算每一个w(n)都要把之前所有的词都遍历一遍这个成本太大了。<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1423_54_634.png" alt=""></p><p>如上条件概率公式也就是表示在词语w1出现的条件下，w2出现的概率是由在整个语料库中的count(w1,w2)/count(w1)决定的。</p><p>如果我想提高精度，将预测w1的工作加上了w2和w3.那么我就需要统计出语料库中，所有三元组三三组合出现的概率，假设物料库有10万个词，那么这个统计和存储的工作量就是10万的三次方。要求有140G的内存仅仅用来存放1260亿的记号语料库计算所有的计数。所以从内存的角度看，这种方法非常低效。</p><p>我们发明了很多种不同的途径来规避这些问题。例如由于4元统计模型出现频率过低根本不使用4-gram。那么我会尝试使用3元统计模型，或者再没有充足的计数，则会再次退而求其次，在上下文大小范围内使用较少的字组预估概率。但总的来说没如果你希望至少获取存储的3元或者4元统计模型的话，相关的内存需求会非常非常大。</p><p>通过深度学习模型和传统nlp模型进行大量比较，自然会注意到上述现象。而这些模型大都围绕着特定类别的字组进行计数。通常模型越强大，内存的需求会增长地更快。</p><h4 id="2-RNN"><a href="#2-RNN" class="headerlink" title="2.RNN"></a>2.RNN</h4><p>我们目前回避这种内存需求爆炸的问题的方法是，RNN，循环神经网络。</p><p>循环神经网络在不同时间步长之间绑定相应的权重，在你通过这条路径时，其实你在使用同一个线性和非线性的层，至少从理论层面上来说，我们可以根据所有前序字组来进行预测。如此以来，内存需求只会取决于字词的数量规模，而不受我们想依据的序列长度的影响。也就是十万就是十万，没有十万的平方或者立方。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191208_0918_21_529.png" alt=""></p><p>假设我们有一个词汇向量表x,假设这些词汇向量均是固定的，我们在后续环节中放宽此假设并将其去除。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191208_0927_58_435.png" alt=""></p><p>紧接着，计算每个时间步长的隐状态，这个步骤中我们只有两个线性层和矩阵向量结果：W（hh）和W（hx）。我们会把这个两个矩阵相加。</p><p>其中x[t]是在第t个时间步长中出现的字组向量。 </p><p>如上这个公式将时间步长t-1和t处的词汇向量串联起来。同时也将这两个矩阵串联起来。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191208_0928_11_626.png" alt=""></p><p>我们可以将h(t)用作特征向量或者作为softmax函数分类层的输入，来获取概率输出（比如所有字组的概率）。</p><p>几个矩阵W的含义：</p><p>我们通常使用上标来识别现存的矩阵，W（hh）表示本W是输入h（t-1）后计算出的隐层h；W（hx）表示刚计算的h层有一个h_x,h_x将x映射到已有的相同向量空间。 </p><p>x[t]是第t个时间步长出现的字组向量。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191208_0933_51_156.png" alt=""></p><p>如下就是在特异指标j的约束下，下一个特定字组的概率。所有前序字组本质上都是该大规模输出向量的第j个元素。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191208_0934_34_205.png" alt=""></p><h4 id="3-探究梯度消失-爆炸问题"><a href="#3-探究梯度消失-爆炸问题" class="headerlink" title="3.探究梯度消失/爆炸问题"></a>3.探究梯度消失/爆炸问题</h4><p>训练上文讲的循环神经网络事实上是一件非常困难的事情，这也是后边的课程我们会引入更优质的神经网络的原因。</p><p>下文是对困难的描述</p><p>基本上我们是让每个时间步长上相同的矩阵相乘，这可以看作是一个在所有时间步长上反复方法某些特定模式的过程。在理想情况下，我们很乐意看到多个时间步长中的输入值可以在后续的时间步长中继续修正我们一直努力预测的目标字组。尝试对W求导，如果你只有两三个字组序列的话，这将会是一项非常行之有效的训练，在正向传播过程中，我们想成了每个时间步长上相同的矩阵，而在反向传播期间我们也同样要这样做，我们必须时刻记住增量、空中讯号、以及各种梯度方向的全局元素。本质上讲，每个时间步长上的全局元素都会通过这个网络向后流动。所以当我们处理交叉熵损失函数时 ，通常会利用导数，我们需要回过头重新传播增量。第一个接近输出值的时间步长就会有效执行更新操作。并且字组向量也有可能得到合理的更新。</p><p>随着时间的推移，你的讯号有可能变得过于强烈或者过于微弱。这就是所谓的梯度消失问题。随着时间的推移，你尝试在时间步长t处发送空中讯号，但由于此前已经存在很多时间步长，这样你就会遭遇梯度消失问题。</p><ul><li><p>简化的RNN模型来研究爆炸/消失问题。h(t)剥离西格玛，yt剥离softmax</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191208_1019_48_100.png" alt=""></p></li></ul><ul><li><p>所有时间步长中的误差W是分步误差的加和，如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191208_1028_39_394.png" alt=""></p></li></ul><ul><li><p>接下来我们要核算的是总误差的时间标记t中的元素，我们只需要核算单个时间步长以及单个时间步长上的单向误差。而现在即使计算也需要我们有一个非常大的链式求导法则。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191208_1031_02_913.png" alt=""></p><ul><li><p>主要分析这个求导法则中的第三项</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191208_1859_39_756.png" alt=""></p></li><li><p>记住ht的定义</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191208_1900_20_207.png" alt=""></p></li><li><p>基本上每个方向的前序时间步长的ht的偏导数都已经给出，为了计算ht的所有偏导数，又要使用链式求导法则。如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191208_1905_20_004.png" alt=""></p></li><li><p>这就意味着向量相对于另一个向量的偏导数是存在的。其实假如我们可以灵活运用反向传播的定义，那就没必要再进行实际计算。实际上，在将计算与流程图以及增量信息有效结合之前，我们并不需要计算清楚这些雅克比矩阵，但是为了分析其中的数学问题，我们还是需要计算所有的导数。</p><p>每一个方向的ht的偏导数都是一个雅克比矩阵</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191208_1907_42_256.png" alt=""></p></li></ul></li></ul><p>  这么多雅克比行列式最终得到的结果也相当大，为了简化它，我们可以设定一个上限。相对于hj我们还要设定h的导数。事实上，在这一系列简单定义的基础上，所有的h都可以通过这种方式来计算。我们还可以设定矩阵的范数上限，这个上限就是这些方程的乘积。也就是Wt，我们可以将其算作一个已知的对角线，而非元素级结果。一般情况下，我们会将元素归置到一个较大矩阵的对角线上，在0路径中一切都是非对角线分布的。接下来我们将两个范数相乘，并将beta_W和beta_h定义为上限.那么每一个数值和单个标量，都能达到自身的最大值。</p><p>  <img src="http://bqlab-pic.test.upcdn.net/pic/20191208_1918_58_863.png" alt=""></p><p>  已知W我们可轻松算出W的任意一种范数。我们只需要一个矩阵就可以计算出一个矩阵范数，当把这些范数全部列出来后，我们就发现这些雅克比行列式的上限实际上被用作这些元素的乘积。就其上限beta而言，假如我们定义了所有元素，那就等于拥有了计算t-k幂的乘积beta。所以序列越长t值越大。它真的取决于测试的t值究竟是变大还是变小。例如当前矩阵范数就是上述范数，那就意味着你已经控制了该范数。在开始训练之前，你将等待矩阵W,初始化为一些小的随机值，如果将其初始化为一个范数大于1的矩阵，那么在反向传播过程中，时间序列就会变得更长，由于你采用了大于1的值，最终你会得到一个爆发的梯度。例如数值为100，但现有的范数只有两个，那么在该梯度的上限就在这两个范数到100之间。反之亦然，假如在开始的时候，你将矩阵W初始化为一群小的随机值，那么W的范数就会小于1，那么最终ht发送到hk的最终梯度可能会缩小为一个十分小的数值，差不多是第100个范数的一半。总体来讲，这一过程不会有任何误差。同时这一过程也不会有误差讯号，随着时间推移，梯度消失或者爆炸也会越来越严重。</p><p>  假如现有的梯度面临激增或者突然减少，是否意味着较远的数组最较近的数组有较大影响呢？答案是，当梯度激增或者突减时，在短时间内你便无法获得数字。这些数字并非字面意义上的数字，因为他们的数值实在太大了，我们根本无法计算。因此上述问题并非实际问题，我们必须设法回归。事实证明，梯度爆炸问题的确包含一些很强势的非法入侵，相比这些梯度消失问题，解决这些非法入侵问题其实要简单的多。</p><p>  <img src="http://bqlab-pic.test.upcdn.net/pic/20191208_1931_18_003.png" alt=""></p><p>  在语言模型或者问答模型的情况下，一些时间序列间隔较大的语句，通常在预测下一个词是什么的时候不被考虑进去，这是梯度消失问题的一种。我们通过如下一个语言建模的例子来了解梯度消失问题。</p><p>  Jane walk into the room,John walked in too,It was late in the day,Jane said hi to ___.</p><p>  人类会觉得几乎概率是100%的下一个单词应该是John，但是现在每个单词都有词向量，你需要将他们全部输入到隐状态上，然后进行计算。接下来会希望模型可以筛选出一个特定的模型，在该模式中，一个人遇见另一个人，然后两个人互相问好，然后介绍姓名等等等等。但是实际上你无法命令模型将中间这个错误讯号收回，你需要手动修改隐状态，将John这个答案设置为较高的概率。中间的这个it was late in the day是一个打乱了时间序列的句子。</p><h4 id="4-python笔记本演示简单的梯度消失问题"><a href="#4-python笔记本演示简单的梯度消失问题" class="headerlink" title="4.python笔记本演示简单的梯度消失问题"></a>4.python笔记本演示简单的梯度消失问题</h4><p>展示的时候用的是一个简单的两层神经网络，并没有用完整的周期性实体网络。在这个网络中会看到从顶部开始的误差，会随着你在网络中的搜索，梯度范数已经会变得越来越小了。如下的方程能解释这个现象，这两个方程在代码中也得到体现。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191209_1311_03_191.png" alt=""></p><p>代码对数学公式的实现格式非常规整，值得好好学习。</p><pre><code>小结:1.传统语言模型是使用条件概率公式，遍历前n-1个词汇，预测第n个词汇出现的概率。这种方式cost太大2.后来改进，根据1-gram，2-gram，3-gram模型预测下一个词出现概率。本质还是使用条件概率公式，这种方式内存开销仍然太大。3.为了解决开销问题，引入RNN网络，RNN定义由上文方程式和图片清晰定义。4.RNN又有新的麻烦，就是梯度爆炸or消失问题。因为RNN基本上就是在时间序列上连续传导特定的模式，随着时间的积累这种模式一定会变得过于强烈或者过于微弱。</code></pre>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224n_2,- 注释之softmax</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_2,-%20%E6%B3%A8%E9%87%8A%E4%B9%8Bsoftmax/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_2,-%20%E6%B3%A8%E9%87%8A%E4%B9%8Bsoftmax/</url>
    
    <content type="html"><![CDATA[<h1 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h1><p><strong>定义：</strong></p><p>假如有一个数组V，Vi表示V中的第i个元素，这个元素的softmax值如下，也就是该元素的指数值和所有元素指数值之和的比值。softmax通常希望特征对概率的影响是乘性的。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191215_1105_33_496.png" alt=""></p><p><strong>softmax VS k个二元分类器</strong>：<br>如果你在开发一个音乐分类的应用，需要对k种类型的音乐进行识别，那么是选择使用 softmax 分类器呢，还是使用 logistic 回归算法建立 k 个独立的二元分类器呢？<br>这一选择取决于你的类别之间是否互斥，例如，如果你有四个类别的音乐，分别为：古典音乐、乡村音乐、摇滚乐和爵士乐，那么你可以假设每个训练样本只会被打上一个标签（即：一首歌只能属于这四种音乐类型的其中一种），此时你应该使用类别数 k = 4 的softmax回归。（如果在你的数据集中，有的歌曲不属于以上四类的其中任何一类，那么你可以添加一个“其他类”，并将类别数 k 设为5。）<br>如果你的四个类别如下：人声音乐、舞曲、影视原声、流行歌曲，那么这些类别之间并不是互斥的。例如：一首歌曲可以来源于影视原声，同时也包含人声 。这种情况下，使用4个二分类的 logistic 回归分类器更为合适。这样，对于每个新的音乐作品 ，我们的算法可以分别判断它是否属于各个类别。<br>现在我们来看一个计算视觉领域的例子，你的任务是将图像分到三个不同类别中。(i) 假设这三个类别分别是：室内场景、户外城区场景、户外荒野场景。你会使用sofmax回归还是 3个logistic 回归分类器呢？ (ii) 现在假设这三个类别分别是室内场景、黑白图片、包含人物的图片，你又会选择 softmax 回归还是多个 logistic 回归分类器呢？<br>在第一个例子中，三个类别是互斥的，因此更适于选择softmax回归分类器 。而在第二个例子中，建立三个独立的 logistic回归分类器更加合适。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224n_5 - 项目建议</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_5%20-%20%E9%A1%B9%E7%9B%AE%E5%BB%BA%E8%AE%AE/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_5%20-%20%E9%A1%B9%E7%9B%AE%E5%BB%BA%E8%AE%AE/</url>
    
    <content type="html"><![CDATA[<h1 id="Lecture5-项目建议"><a href="#Lecture5-项目建议" class="headerlink" title="Lecture5 项目建议"></a>Lecture5 项目建议</h1><h4 id="1-一层神经网络过渡到多层神经网络"><a href="#1-一层神经网络过渡到多层神经网络" class="headerlink" title="1.一层神经网络过渡到多层神经网络"></a>1.一层神经网络过渡到多层神经网络</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1420_08_008.png" alt=""></p><h4 id="2-项目建议"><a href="#2-项目建议" class="headerlink" title="2.项目建议"></a>2.项目建议</h4><p>老师推荐的文章和会议</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1420_24_988.png" alt=""></p><p>1.定义你的任务：</p><p>例如：summarization</p><p>2.定义数据集：</p><p>最好使用现成的数据集，因为他们已经有baselines</p><p>3.建立baseline</p><p>他可以是一个非常简单的一元线性回归，然后在你的训练数据集上计算你的评价标准，看看模型是过拟合还是欠拟合</p><h4 id="4-选做题：自己发明新的模型"><a href="#4-选做题：自己发明新的模型" class="headerlink" title="4.选做题：自己发明新的模型"></a>4.选做题：自己发明新的模型</h4><ol><li><p>首先，你需要做好以上说的几个步骤.</p></li><li><p>然后你需要知道已经存在的模型上有哪些问题。然后你就可以设计出自己的模型。如果你想要这样做的话，<strong>你真的需要和你的导师和其他研究者保持沟通，除非你自己就是研究者并且已经获得了博士文凭</strong>。</p></li><li><p>你需要实现你的模型，然后根据你的新点子去对它快速迭代。（也许在某个位置新加一层？然后看看他起不起作用？）</p></li><li><p>那么在迭代的过程中，拥有足够多的的软件工程技能来配置高效的实验框架，从而能收集到这些结果就很重要。</p></li><li><p>建议从一个和你的真实想法比起来相对容易很多的模型做起。先把简单模型建立起来。然后逐步尝试更复杂的模型。</p></li><li><p>对于终极任务：summarization任务。</p><ol><li>一开始你可能尝试一些非常简单的模型。比如对自然段中的所有单词向量求平均，然后用贪心搜索一个接一个地生成单词，或者用贪心搜索对维基百科中的现有文章寻找一些片段，然后把合适的片段复制过去。</li><li>然后升级你的目标，尝试某些真正让你生产整段总结的方法。</li></ol></li></ol><p>一些项目的idea（包括了在kaggle上打比赛！）</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1420_37_463.png" alt=""></p><p>下次课程会学一些有趣的基础语言学任务，比如句法分析；然后会学习TensorFlow的应用。</p><h4 id="论文分享："><a href="#论文分享：" class="headerlink" title="论文分享："></a>论文分享：</h4><p>词袋模型</p><p>文本分类是nlp领域一个非常重要的话题， 给定一串文本，我们就可能问，它带有的是积极还是消极情绪。是垃圾邮件还是正常邮件。如图就是一个情感分析的例子。</p><p>取得一个句子的特征的一种简单的办法就是对句子中所有的单词向量做平均。这篇论文中的模型也是类似这样的方式。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1420_49_271.png" alt=""></p><p>对他们求平均，但是会丢失他们之间的顺序。然后得到的是这个低纬度的文本向量，代表这个句子，这就揉碎了的低纬度的文本向量就是词袋（word  bag）</p><h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>反向传播和词袋模型在其他文档中有总结。</p><p>本节课最有含金量的就是老师对想要发文章的同学的做项目的建议。</p><p>首先，纯理论突破现在不可能。所以必须得要做项目，项目的选题要与老师或者其他研究者讨论，避免过大或者过小，除非你自己是一个phd和独立研究者。但是现在我没有人去讨论，但是回避六个月白忙的风险的动作是要做的，解决思路有两个</p><ul><li>发邮件、线下拜访中国的nlp领域的老师，求指点</li><li>直接打kaggle。问题具体，选题明确。</li></ul><p>做能发文章的项目要先确定baseline和数据集。然后遍历先有的模型，然后提出一些修改意见（idea），然后在数据集上跑，看看是不是确实有提高，如果有，就完事儿了嗷。做出修改意见并且修改别人的模型的前提是得有充足的工程能力，这个工程能力我现在他妈绝对不具备。难顶，只能硬上。</p><p>另一个要注意的事情是老师反复<strong>强调迭代 + 先从容易的模型开始实现</strong>。我认为这是把大问题难问题拆分成小问题容易问题的思路。这种思路非常关键，小熊也提过很多次。我需要做计划，把大问题拆分成小问题。当然具体是一个多大的问题，发论文一共有多少步，哪些步骤最难大概耗时多久如何拆分成容易问题都是还需要探索的地方。</p><p>所以在做总计划的时候也要秉承把困难计划先简答实现然后再根据新发现的信息对其迅速迭代的思路。</p><p>之所以这样做因为what else？</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224n_4 - wordWindow和神经网络</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_4%20-%20wordWindow%E5%92%8C%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_4%20-%20wordWindow%E5%92%8C%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="Lecture4-wordWindow和神经网络"><a href="#Lecture4-wordWindow和神经网络" class="headerlink" title="Lecture4  wordWindow和神经网络"></a>Lecture4  wordWindow和神经网络</h1><h4 id="1-分类背景知识"><a href="#1-分类背景知识" class="headerlink" title="1.分类背景知识"></a>1.分类背景知识</h4><p>对分类的直觉感受是什么？在机器学习领域，在还没有达到深度学习领域的情况下，我们通常将分类理解为简单的逻辑回归，也就是定义一个简单的决策边界。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1417_17_117.png" alt=""></p><h4 id="2-窗口分类"><a href="#2-窗口分类" class="headerlink" title="2.窗口分类"></a>2.窗口分类</h4><p>在一般的机器学习中，我们假设输入是固定的。输入X都是固定的，我们只训练<strong>参数</strong>W，也就是softmax的权值，然后计算给定输入X时输出Y的概率。<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1417_27_826.png" alt=""></p><h4 id="3-交叉熵"><a href="#3-交叉熵" class="headerlink" title="3.交叉熵"></a>3.交叉熵</h4><p>我们假设正确类别的概率为1，其余的概率为0.举个例子，假设共有5个类别，正确类别是中间的第三个，那么第三个的概率为1，其他都是0.我们把理想的概率定义为p，softmax计算的概率为q，这里给出了交叉熵的定义，就是对所有类别的求和<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1417_43_174.png" alt=""></p><p>正则化项：里边包含的参数θ如果是标准逻辑回归中的矩阵W，实际上目标函数加入这个正则化项的目的就是是为了鼓励模型中的所有权值尽可能地小。</p><p>可以假设你想要一个贝叶斯模型，你可以有一个先验的高斯分布，理想情况下这些参数的值都很小，但是如果没有这个正则化项，通常情况下你得到的模型参数会爆炸，他会越来越过拟合。如果没有正则化项，我们会专注于如何拟合我们的模型。</p><p>通常的机器学习优化就是只优化模型的参数W，也就是softmax分类器的参数，因为参数众多，所以每次的更新和梯度会特别小。通常我们只有两三个类别，但是词向量的维度是上百。例如我们有三个类别，和100维的词向量，这样我们就有300个参数。在深度学习中，我们有相当惊人的词向量，而且实际上我们不仅仅是学习softmax分类器，还有词向量，我们可以反向传播到词向量。我们每次更新词向量的时候，必须意识到词向量通常都巨大。例如你的词有300个维度，你的词典中有10000个单词，突然你有如此巨大的参数集，在这种情况下，你很有可能会过拟合。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1417_53_374.png" alt=""></p><p><strong>小数据集训练容易过拟合</strong></p><p>我们想对单个词做积极还是消极的分类，我们的训练测试集中包含了单词TV，telly，television。假设这三个单词是在一个评价刚上映电影的语境中（说一个上线的电影适合在电视上放），那么这种评价通常是消极的。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1418_09_632.png" alt=""></p><p>在开始的时候，这三个词汇在向量空间中很接近，我们用word2vec或者glove向量学习，我们在非常大的语料库上训练这些词向量，他学习到这三个单词经常出现在相似的上下文中，所以他们在向量空间中很接近，现在我们要在更小的情感数据集中训练，只包含了TV和telly，不包括television，当我们重新训练这两个向量的时候会发生什么？</p><p>结果是他们发生了变化，把情感分类的结果显示出来，你会看到telly和tv移动到向量空间的其他位置，而television还停留在原来的位置。这就是向量在小数据集上的过拟合问题。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1418_22_424.png" alt=""></p><p>我们实际上会把这个词汇错误分类，因为他们三个词汇其实是一个意思。</p><p>what does it mean？</p><p>这里的关键信息是，如果你只有小训练数据集，不要训练词向量。（深入模型在小数据上容易过拟合）</p><h4 id="4-窗口分类的例子：命名实体识别4分类问题"><a href="#4-窗口分类的例子：命名实体识别4分类问题" class="headerlink" title="4.窗口分类的例子：命名实体识别4分类问题"></a>4.窗口分类的例子：命名实体识别4分类问题</h4><p>问题：将语料库中的单词分为如下四类：地点，人名，组织，其他</p><p>思路：训练一个softmax模型，然后用一个窗口把他们前后的单词链接起来</p><p>目的：给中心词Paris分类</p><p>中心词巴黎周围的步长是2，这就构成了一个五维的窗口向量，每一维都是一个单独的词向量，比如【I love Paris very much】。我们能想到的最简单的窗口分类器就是将这五个词向量拼接放入softmax分类器中，五个词构成了softmax分类器的分母，分子是一个单词paris。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1418_34_641.png" alt=""></p><p>参数解释：</p><p>x是窗口，W矩阵是参数。</p><p>代价函数J(θ)中，F（yi）是参数W和X的交叉熵。</p><p>如何更新词向量X？X是五个词构成的窗口，X深入在softmax中。直接回答就是我们可以多次求导。</p><p>y尖 为softmax概率向量，也就是对所有四个分类的归一化的得分或概率，比如（0.9,0.05,0.05,0）.那么分类结果就是第一个（比如是地点）</p><p>其实我们是想更新pairs这个词的向量，而不是整个窗口，窗口只是一个中间步骤，所以其实我们做的是更新和计算关于词向量的每个元素的导数，最终他会变得很简单（类似于梯度下降）。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1418_42_262.png" alt=""></p><p>softmax只会给我们线性决策边界，如左图。而神经网络可以给我们复杂的决策边界，通常会work的更好，如右图。</p><pre><code>定义了窗口分类和交叉熵，用一个实例实现了窗口分类，其实就是对softmax的应用。窗口分类只能给出直线边界，没有神经网络的复杂边界好用，本节课还提到了神经网络的基本概念，为了解耦把神经网络的概念剪切到CS229的相关文章里边去了。</code></pre>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224n_16 - 深度学习在NLP中的限制</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_16%20-%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9C%A8NLP%E4%B8%AD%E7%9A%84%E9%99%90%E5%88%B6/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_16%20-%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9C%A8NLP%E4%B8%AD%E7%9A%84%E9%99%90%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="Lecture16-深度学习在NLP中的限制"><a href="#Lecture16-深度学习在NLP中的限制" class="headerlink" title="Lecture16 - 深度学习在NLP中的限制"></a>Lecture16 - 深度学习在NLP中的限制</h1><p>对于语言，如果你想要一个合适的语言系统，你可能需要一些对于输入的情感理解，在逻辑上推理某些事实，在数据库中检索一些事实，或者基于数据库中的逻辑原因，再做一些内存检索。还有一些时候，你需要对谈论的内容做一些奖励。在现实世界中这些过程是由很多不同的成分组成的。我们想要一个更好的理解语言的系统，理想情况下这个系统应该包括很多内容，以一种更科学的方式呈现。</p><p>现在联合多任务学习仍然非常困难，迄今为止，人们谈论多任务学习时，他们假设有一个源任务和一个目标任务，他们希望神经网络在源任务的预训练可以加强另一个目标任务的表现。就我而言，理想的情况是让他们共同训练，而不是分开训练不同的解码器。例如不同语言中，不同的分类问题。理想情况下，我们只有一个很大的不同种类的数据集，我们想根据输入来预测，他们有完全一样的解码器，但是很多时候人们做多任务学习的时候他们只是共享低层的参数，共同训练他们，但是不能共享高层的参数。</p><p>也就是说，在自然语言处理的过程中，我们通常只是共享了词向量，我们没有共享其他高层的比如LSTM层，这种层能解决更多的任务，其实<strong>计算机视觉在这方面有挺大的进步，他们有预训练的卷积神经网络，在Imagenet这样大的数据集上训练的网络</strong>，可以运用在许多其他的任务中，并且有很好的表现。计算机视觉中，只用改变深度卷积神经网络的前几层，就可以得到很好的准确率。并且在不同的视觉任务中迁移学习。在NLP中我们还不能做到这种令人信服的事情。</p><p>很多时候，你只能看到一些多任务学习的例子，这些任务有一些关联，所有互相有帮助，比如词性标注有助于句法分析，但是你很难看到彼此任务不相关联，任务对彼此无帮助但是对互相的结果有影响的论文。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_2111_11_257.png" alt=""></p><p>如下的论文试图结果这个多任务处理艰难的问题，他的模型像个怪物。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_2123_46_118.png" alt=""></p><p>从下往上看，使用了n-gram代替了普通的词向量表示，这些词向量又被输入到一系列的LSTM中，上边的大块都是用了LSTM表示，一个LSTM的输出作为下一个LSTM的输入被传递出去。第一个LSTM被用作词性标注，下一个LSTM用作分块工作，对每一块的起始和结束做分类，再下一个LSTM会做依存语法分析，然后把词性标注等等底层链接作为每个高层任务的输入。</p><p>有时候会要求同时理解两个句子，在两个冷却塔尖的中间使用了一个池化的结构。池化结构和卷积神经网络中讲到的类似，池化操作后我们可以对句子的关联性和蕴含性做分类，最后我们可以对整个的联合结构用一个损失函数训练。</p><p>第一层用于词性标注，在每一个时刻，有一个单层的LSTM,把它输入到softmax中，在此同时生成一个向量标签，这个操作可以让我们考虑到词性标注的一些不确定性。你可以把这里当做是一个层，这个层接受softmax的输出，凸组合公式如下图右下角所示，在这里每一个标签都与一个向量组合，把所有向量的加权相加，权重取决于模型在某一时刻得到的那个标签的确定性。<br>举个例子，如果有三个不同的标签（动词名词形容词），这三个标签每一个都有一个向量，是一个50维的随机向量，模型可以把它训练的很好。这时比如动词的可能性是0.9，名词和形容词都是0.05。把这三个数都乘以相应的向量，这就得到最后的标签向量。这就是词性标注LSTM的输出。</p><p>下一层分块模型，会接收和前面一样的输入，也就是词向量X，词性标注LSTM的隐藏状态h，以及标签向量y，这些都是输入，把他们拼接起来，再输入到另一个LSTM中，接下来会做类似的事情，输出一个隐藏状态，将隐藏状态输入到softmax中，得到一个分块的标签向量。理论上来说，你可以把这个操作做很多次。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_2132_26_804.png" alt=""></p><p>根据历史经验，词性标注，分块，和依存句法分析，这三个功能让他们分别有自己的LSTM层对性能优化最有利。</p><p>下一层，依存句法分析<br>我们想要获得一个树结构，因此依存句法分析，在很多情况下要求集束搜索，但这个模型十分简单，使用一个双向的LSTM,有四个输入，词向量，分块的隐藏状态，词性标注和分块的标签向量。每一时刻都有这四个输入，这里是双向的LSTM的结构，我们把它当成一个分类问题。运行一个 二次分类问题，判断某个词语是依存这个词语还是依存那个词语，所有通通运行一遍，取每一个的最大值，这就生成了我们希望得到的树。虽然有概率出现回环生成不合法的树，但是现实情况是99%的情况都生成的是合法的树，加上一些简单的剪枝规则，删除一些边，能让这1%的不合法的树也变得合法。</p><p>奇怪的是，不再要求使用束搜索，只是前馈计算，就能在大多数情况下获得很优秀的树。（可以做很多优化，可以加入集束搜索，通过几个节点或者类似的东西，也可以做适当的SQL，因为通常有连续的向量）</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_2145_54_515.png" alt=""></p><p>最后一层可以训练多个句子，处理语义相关性等任务，这里所做的就是，用一个简单的时间最大池化，这个LSTM的最后一个隐藏状态，会在每一个时刻生成一个特征向量，在所有时刻隐藏状态的特征的每一个维度中，找到最大的值并选择它，这就是我们把它叫做最大池化的原因。这些看起来都很复杂，但是都是之前仔细讲过的内容。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_2152_02_006.png" alt=""></p><p>这里有一个额外的想法可以用，叫做连续正则化，在每个mini-batch，先让模型专注于不同的任务，当层次越来越高的时候，对低层的权重加入正则化项，使他们不要有太多的改变，是用这里的正则化项delta实现的。这在训练中是一个新方法，可以使鲁棒性更强，最终的系统在五个任务中的四个都达到了当前最好的结果，直觉上讲，在第一个mini-batch结束的时候，模型关注的只是词性标注，得到权重theta，定义了标签向量，LSTM权重等等,然后训练下一个，高层次任务也就是分块的时候，不要让他们改变太多，这些权重在词性标注模块已经被调节的很好了，在向更高层训练时，基本上是同样的想法， 不要对底层权重改变太多，但如果高层任务非常想改变权重，也是可以做的。有人将SNLI和蕴含性分类作为预训练步骤，应用在问答系统中，这些都是可以尝试的方法。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_2247_36_017.png" alt=""></p><p>另一个NLP领域的障碍，就是在使用词向量的时候有许多冗余，我们使用word2vec和Glove作为预训练词向量，如果我们训练出一个输出，比如机器翻译或者语言模型，在softmax中其实是另外一组权重，在softmax的输出中，每个单词都用一个向量表示，意思就是，在上面这里有一个很大的softmax，是词汇表的大小乘以LSTM的隐藏状态的维度，在输入时，每个单词也有对应的词向量，也是同样的大小的v乘以词向量的维度。这里有一篇很赞的论文，实际上是上这门课的两个同学的论文，也就是把这两组向量联系起来，也就是将这两组向量变成一样的向量，每个向量的softmax权重和输入的词向量是完全相等的。经过共同训练，只需要反向传播，使用同样的导数，但是他们现在是相等的，如果不自己求导，是很容易实现的。使用tensorflow等工具就可以了，否则会有一些困难。他们也有一些关于softmax的很好的理论，例如热度，看了论文就知道了。这是一个很简单的思想，但是又很大的帮助。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_2300_18_923.png" alt=""></p><h3 id="读这篇论文"><a href="#读这篇论文" class="headerlink" title="读这篇论文"></a>读这篇论文</h3><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_2304_28_736.png" alt=""></p><p>以前我们花大量时间做特诊工程，现在可以自动化了，我们又花大量时间设计深度学习神经网络，如下这篇论文提出了我们可以使用AI模型去做架构，代替人类创造与设计网络架构。controller是一个循环神经网络，循环神经网络输出的是网络架构的超参数。应该由多少隐藏层，在每个时刻，循环神经网络会输出这些特征。每当他输出时，他会尝试训练一个新的简单的网络。用这个架构在一个简单任务上可以得到一个准确率。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_2313_25_221.png" alt=""></p><p>【CS224N完结撒花】</p><h1 id="记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记别跟论坛项目一样做完了之后就把半成品笔记往那一扔很多概念在热乎的时候不搞清楚可能以后就更懒得对着乱乱的笔记搞清楚了。笔记应该是愿意反复看，反复修改调整结构迭代版本的东西"><a href="#记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记别跟论坛项目一样做完了之后就把半成品笔记往那一扔很多概念在热乎的时候不搞清楚可能以后就更懒得对着乱乱的笔记搞清楚了。笔记应该是愿意反复看，反复修改调整结构迭代版本的东西" class="headerlink" title="记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记别跟论坛项目一样做完了之后就把半成品笔记往那一扔很多概念在热乎的时候不搞清楚可能以后就更懒得对着乱乱的笔记搞清楚了。笔记应该是愿意反复看，反复修改调整结构迭代版本的东西"></a>记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记别跟论坛项目一样做完了之后就把半成品笔记往那一扔很多概念在热乎的时候不搞清楚可能以后就更懒得对着乱乱的笔记搞清楚了。笔记应该是愿意反复看，反复修改调整结构迭代版本的东西</h1>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224n_15 - NLP的问题和可能性架构</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_15%20-%20NLP%E7%9A%84%E9%97%AE%E9%A2%98%E5%92%8C%E5%8F%AF%E8%83%BD%E6%80%A7%E6%9E%B6%E6%9E%84/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_15%20-%20NLP%E7%9A%84%E9%97%AE%E9%A2%98%E5%92%8C%E5%8F%AF%E8%83%BD%E6%80%A7%E6%9E%B6%E6%9E%84/</url>
    
    <content type="html"><![CDATA[<h1 id="Lecture15-NLP的问题和可能性架构"><a href="#Lecture15-NLP的问题和可能性架构" class="headerlink" title="Lecture15 - NLP的问题和可能性架构"></a>Lecture15 - NLP的问题和可能性架构</h1><h3 id="1-NLP问题"><a href="#1-NLP问题" class="headerlink" title="1.NLP问题"></a>1.NLP问题</h3><p>视觉研究以前在深度学习中占主导地位，但是三巨头Yann LeCun,Geoff Hinton,Yoshua Bengio都把他们的研究方向转向NLP(也就是说深度学习三巨头的研究方向有风向标的意义)</p><p>Bengio在采访中表示，实验论证深度学习技术应用与新应用的可能性，这些新应用包括了计算机视觉，对话系统，虚拟助手，语音识别，NLP,机器翻译以及其他应用。</p><p>这世界上有太多不同的语言现象和相对应的任务要完成，所以解决NLP问题不能像解决计算机视觉那样，某个人构建出一个足够复杂的深度神经网络就vans了.NLP问题们并不同源，不能说解决了其中一个，就可以宣布全部解决。</p><p>在传统NLP问题中丢失了的东西：</p><p>将现在和过去对比是一件很有趣的事情，在70和80年代的那批从事NLP研究的人员里，他们有着非常崇高的目标，他们想要达到人类级别的语言理解能力，但是他们最终达到的是非常骨感的现实。</p><p>不论上述对比的结果如何，我们现在所处的环境，我们能比他们当时更好地实现目标，但是，达到目标的途径却开始变得不那么明朗。作为实践，我们可以在这个语料数据上运行一个LSTM,然后将这个模型取得很好的性能表现，然后感觉这似乎就已经是极限了。</p><p>老师采用Peter Norvig的博士论文作为例子，Norvig是谷歌的研究主管，距离他发表这篇博士论文到现在已经过去32年了，他的论文的题目叫做，一种关于文本理解的推论的统一理论。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_1955_43_230.png" alt=""></p><p>他的论文实际上在讲NLP和语言理解的内容，但是在他的论文里，只有很少的篇幅在分析自然语言，他举得例子都是很小儿科的语句，比如A给了B一个自行车，C想要它，于是B又给了C都是这样的话。</p><p>80年代的人处理自然语言的观点和现在截然相反，当时的人们都认为唯一的方法是，拥有可用于推理的知识库，他在论文的第四页写到，一个合适的知识库，是做出正确推理的先决条件，这不过是说，我们需要在事实之间建立推理关系，以此来理解较复杂的指代问题。当时他非常依赖知识库，但是后来20年里大家发现其实有很多自然语言是可以仅仅通过文本的组成而不需要任何知识库就可以进行处理的。</p><p>在他的论文里，他列举了6种推理形式试图去建立一个包含这些种类的自然语言理解理论。然后能实现在一篇简单的童话故事中推理。</p><p>第一种推理形式是阐述，就是处理如何连接各个单词实体，这里的例子是john拿出一个存钱罐，因为他需要去买礼物，也就是需要对这些单词实体做上下文的阐述。</p><p>第二是指代消解，也就是处理好指针和内存的关系，这种方法之前已经讲过，这个现在已经能被很好地处理。</p><p>第三种是解释语言中的隐喻和抽象，比如Laker队kill了Yeekoins，这里的kill并不是杀死，而是把yeekinos队打败了。Norvig另有一篇论文去讲述人们如何去衍生这些抽象解释。第三种我们一直以来没有考虑太多，但实际上这是很重要的一部分，具体化是可以让你解决如何将一个大致上的描述，转变为更具体的形态。比如有人驾车出游，那么在自动驾驶技术成熟之前，你一定能推测出有人在开车。这就是具体化，能够让你在有一定的常识的基础上，做出更加合理的推论。</p><p>在Norvig的论文中，他们当年没有一个良好的语法分析器供他们使用，他们当时手动把句子拆开，我们现在的parser发展已经很不错了，这就是progress。</p><p>在自然语言处理的方面还有很多事情需要完成，一方面，在过去的几年中，NLP领域和其他各种领域一样，都度过了一段激动人心的时期，很多系统取得了非常好的效果，值得注意的是双向LSTM正在占领自然语言处理领域，因为你可以在任何任务上尝试他们。你可以把双向LSTM用作注意力机制，他也可以成为你的天然语言分析器。</p><p>另一个令人激动的事情是这些神经方法，赋予了语言生成这一新生任务，生成对话，机翻，等等，这些任务在2000-2010之间还停滞不前，但是现在，我们有了这些令人惊叹的语言模型，我们有了各种的方式去配置他们，我们可以进行很棒的语言生成，这些领域最近都活跃了起来。</p><p>从科学上来讲，这是很有趣的一段时间，因为大多数的NLP任务都可以假设我们需要建立独有的语言表示体系并且来使用他们进行进一步分析，所以我们需要语法表示，我们想要使用语义框架去表示语言中的事件与关联，同时还需要建立详尽的有地方特色的知识表示方式。</p><p>当下的工作能显示出很多现在的情况，我们能建立端到端的深度学习系统，但是对于那些地方语言特色的表示，这些系统运行的并不好。另一方面，我们在很多事情上并没有触及到问题的本质，其中之一就是我们现在还在停留在使用很原始的方法来构建和得到记忆或者知识，的确现在已经有了很多关于LSTM的工作，但其实这里边的记忆都是非常短时的记忆，它并不是我们人类在脑子里的模型，能够存储生活中很多年积累的经验那种模型，我们人类还能在需要的时候将这些经验灵活地调动和组织起来，而LSTM能做的就只是线性扫描：最后100个单词发生了什么事诸如此类。</p><p>另外还显得不足的方面是，我们并没有什么模型能够使我们让目标或者计划公式化，比如在一组人类的有意义的对话里边，你有一些计划和目标需要达成，但是在自然语言处理中，还不能对这个目标或者计划公式化（我理解为不能建模）。</p><p>还有一个有待提高的领域是语际关系，一旦我们需要处理单个的句子，句子的结构通常很清晰，我们可以直接加工，但是我们一旦尝试解释句子或者从句之间的关系，或者说出他们之间这样的原因，我们就不能很好地处理。</p><p>我们想要实现借助常识实现阐述，以目前的能力，是没有办法造出这样的深度学习系统的。</p><h3 id="2-树形结构"><a href="#2-树形结构" class="headerlink" title="2.树形结构"></a>2.树形结构</h3><p>如下，通过树形结构把文章拆分为小块，红色为保守派观点，灰色是中性表达，蓝色是自由派观点，通过各种组合最后得到的还是自由派的观点。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_2041_50_770.png" alt=""></p><p>另一个例子也是使用树形结构去分析文章的意识形态是偏保守还是自由</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_2042_35_003.png" alt=""></p><p>如下是树形循环神经网络，它在理论上很吸引人，特别是如果你没有一亿个单词作为训练集加以训练，从经验上来看，他是很有竞争力的。但是他的缺陷就是非常慢。大多数这种网络都需要外部分析器。</p><p>树形结构的运算表现非常之差的原因是他们没有适应GPU上的批处理运算，而批处理运算恰恰是让深度学习模型能够高效训练的核心原因。如果你有一个序列模型，一个序列模型只能有一种结构，假如你的结构就是从左往右这样，你开始循环计算每一个单词的隐藏属性，这种结构你能在序列模型中使用一堆句子（最好长度相仿），然后在序列模型中运行并锁定步长，这样做效率真的很高。但是如果你在TreesRNN网络中训练，又使用这种特定的结构，他每一句的结构都不同，这就削弱了你进行批处理计算的能力。因为你每句句子都要构造出不同的结构单元。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224n_14 - 问答系统</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_14%20-%20%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_14%20-%20%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="Lecture14-问答系统"><a href="#Lecture14-问答系统" class="headerlink" title="Lecture14 - 问答系统"></a>Lecture14 - 问答系统</h1><h3 id="1-实现问答系统的联合模型：动态记忆网络"><a href="#1-实现问答系统的联合模型：动态记忆网络" class="headerlink" title="1.实现问答系统的联合模型：动态记忆网络"></a>1.实现问答系统的联合模型：动态记忆网络</h3><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_1049_47_393.png" alt=""></p><p>从下边开始看，输入都是词向量，比如word2vec或者glove之类。基本会有一个循环的神经序列模型，比如GRU,他为每个输入单词计算一个隐藏层。</p><p>在右下角的问题模块中也有处理问题的GRU,用于计算输入的问题的向量。</p><p>以右下角输入的问题为例，“足球在哪里？”我们会认为足球被提及的事实的这个问题储存在上一次GRU隐藏层的某个地方，我们称之为Q，我们使用Q从本质引发所有潜在输入的注意力机制，每当有一个特定的句子被强烈关注，我们将把这个句子作为另一个GRU的输入，那就是情节记忆模块。基本上只要有上图蓝色的线存在，那基本就是一种循环的神经网络序列模型。所以基本上，一个问题促发一个注意力机制。遍历所有输入的所有隐藏层，现在基本上说这个事实似乎与这个问题有关。基本上说，一个问题引发一个注意力机制，然后遍历所有输入的所有隐藏层，也就是说在输入模块，我们认为最后一次提到“足球”肯定和现在问的问题是相关的，于是GRU的隐藏层捕捉到了这一点，然后经过上边的记忆模块中的GRU（fliter）过滤，现在只聚合与当前问题相关的句子。 图上方记忆模块的第三条线，这个是GRU的最后隐藏层。对于下一次迭代，发现“joho put down the football”，然后不知道john在哪，所以我们现在已经存储在这个向量m中，为了回答现在的问题，他似乎是相关的，模型会再一次自己学习所有这些东西，然后会注意到所有提及john或者足球的每一个事实。在这种情况下，这些都是现实的数字，然后很容易发现“john去了走廊”和“john去了卧室“的句子，然后提到这两个句子的注意力值，然后在右上角的回答模块输出回答。 </p><p>问题：上图上方的情节记忆模块的两个不同轨道是什么？实际上，这些轨道是输入的完全镜像。因此在情节记忆模块中总是有很多时间步骤，因为输入中有时间步骤，但是模型可以用分类器来决定或者只是靠固定次数的输入。介于2到5之间，每次它结束时，他都会注意输入中的各种句子，取决于问题是什么以及它迄今为止在前一时间步骤或者一个情节中的相关事实方面聚集在一起。所以在这里，我们再次回顾一下第一次的输入，储存关于john和足球的情节，那么第二次，我们现在也要关注john的事实，所有再由直觉，第一句，我问足球在哪里？在这种情况下，john移动了卧室这一句似乎与提问并没有任何关系，所以你必须做传递推理，这种传递推理能力，只有你在信息中有很多次传递时才能得到。</p><h3 id="2-组件剖析"><a href="#2-组件剖析" class="headerlink" title="2.组件剖析"></a>2.组件剖析</h3><p>为了定义模块，我们得将整个体系结构进行端到端的约束。我们基本上只有一个标准GRU,你只需要给它一个输入序列，一个问题和一个答案，现在从这个答案开始我们基本上会在一开始就会犯错，然后就会有很高的交叉熵误差，那么在这里我们基本上可以通过所有这些向量反向传播，通过所有的方式进入单词向量，我们可以训练整个系统，这真的是深度学习的力量。</p><p><strong>input模块：</strong></p><p>输入模块是标准的GRU，我们实际上会使每个句子的最后一个隐藏状态明确地被访问，我们已经在第二次迭代中做了一个改进，把它换成了双向GRU。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_1223_27_097.png" alt=""></p><p><strong>question模块：</strong></p><p>问题模块同样是一个标准GRU,这里我们有称之为v_t的单词向量，q_t是GTU模型接受vt和上一个q_(t-1)之后的输出。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_1225_26_291.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_1225_48_410.png" alt=""></p><p><strong>情节记忆模块的第一条线</strong></p><p>这个模块带有注意力机制，示意图如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_1226_48_307.png" alt=""></p><p>隐藏状态如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_1227_07_348.png" alt=""></p><p>首先定义g的关注机制，g只是一个标量数字（代表我是否应该注意第i句话？）这里h的上标表示迭代次数。这里的主要想法是我们在标准GRU之上有一个全局gate，“g”。这个全局gate会说整句话都非常重要或者整句话都不重要，当然并不是所有的隐藏状态都有自己的大门。也就是说，g（i，t）代表的第i个句子和第t个情节，通过整个输入是0，我们要做的就是基本上完全复制h向量，没有计算和更新。比如上文举得例子，比如找的是足球，足球和john有关，其他人比如mary就和足球无关。然后和mary有联系的话语就被标记成0.</p><p>那么如何计算g？</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_1244_54_642.png" alt=""></p><p>其实非常简单直接，首先用句子部分之间的一些简单相似性来计算这个向量z，这又是每句话结束时候的隐藏层，计算组z之后进行简单装配和e和sum就OK了。</p><p>关于计算z的详解：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_1245_37_692.png" alt=""></p><p>第一项si * q是如何标识产出</p><p>第二项si * m(t - 1)是 在记忆中的句子、问题与记忆状态下的每个句子之间的乘法交互作用。</p><p>第三和第四项是简单的向量相似度量度。</p><p>举例：我在si处输入，在这里捕捉关于john的fact。但是其实我们问题是关于足球，所以在第一次迭代中，m（0）也刚刚初始化为这个问题，不会添加任何东西吗，因为我们还没有完成整个输入。基本上在隐藏状态里，我们希望能捕捉到一些约翰走到走廊的一些事实，所以|si - q|这一项，也就是si和q之间的相似性不会太高，然后我们把|si - m|这个长特征向量插入这g（it）个双层神经网络，所以g(it)的值会非常小，但是在第二次迭代中，基本上链接了john，足球，和走廊。现在在m隐藏层下有一些关于john和足球的fact，现在这种相似性可以通过注意力机制来获得，然后下次给这个句子分配多一些注意力。</p><p><strong>情节记忆模块的第二和第三条线</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_1250_01_915.png" alt=""></p><p>第二第三条线也是GRU,但是是一个不会有很多时间步骤的GRU，基本是一个从一个记忆状态到下一个记忆状态，并在最右侧聚集的GRU。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224n_12 - 语音处理的端到端模型</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_12%20-%20%E8%AF%AD%E9%9F%B3%E5%A4%84%E7%90%86%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E6%A8%A1%E5%9E%8B/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_12%20-%20%E8%AF%AD%E9%9F%B3%E5%A4%84%E7%90%86%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="Lecture12-语音处理的端到端模型"><a href="#Lecture12-语音处理的端到端模型" class="headerlink" title="Lecture12 - 语音处理的端到端模型"></a>Lecture12 - 语音处理的端到端模型</h1><ol><li>介绍传统的语音识别系统</li><li>引出语音识别的端到端模型，并给出描述（CTC,LAS）</li><li>端到端模型的改进版</li><li>语言模型如何影响语音识别的</li><li>语音识别模型中的解码工作的改进</li></ol><h3 id="1-传统语音识别系统"><a href="#1-传统语音识别系统" class="headerlink" title="1.传统语音识别系统"></a>1.传统语音识别系统</h3><p>自动语音识别的基本定义是把语音信号自动转换为文字呈现，语音识别系统的经典实现方法是使用生成模型（generate model），后来被简单的神经网络模型取代。</p><p>如下图最右，你从语言模型（n-gram）中生成了一个由单词组成的特定序列。然后到右二，每个单词都有一个读音模型，也就是每个单词都有自己的指定发音方法（音标），读音模型可以将文本序列转换为读音token序列，然后将这些模型传递给给右三声学模型（acoustic models），声学模型基本上给出一个token听起来是什么样，一般由高斯混合模型来构建，然后这个声学模型会输出一组一组的声音features，一般这些features是由信号处理专家定义的，就像是被捕捉到的声音的评率成分的特征一样（又被称为频谱图或者钟形滤波器组频图）。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1349_12_368.png" alt=""></p><p><strong>如下图，应用神经网络，同架构，performance提升较大</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1353_03_768.png" alt=""></p><h3 id="2-端到端模型"><a href="#2-端到端模型" class="headerlink" title="2.端到端模型"></a>2.端到端模型</h3><p>虽然每个组件都应用了神经网络导致性能有提升，但是这里有个问题，就是虽然每个组件的误差都不同，所以组合起来效果会不太好。一个解决的思路是把整体系统当成一个大模型来统一训练。</p><p>引出端到端模型：直接从语音输入得到文字输出</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1402_53_072.png" alt=""></p><p><strong>端到端模型实例1：CTC模型（Connectionist temporal classification）</strong></p><p>在这个实例中，X是频谱图，Y是相应的输出文本。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1405_50_030.png" alt=""></p><p>下图是CTC模型核心架构（双向的RNN），图中箭头指的地方是基于输入数据的任意时间节点。输入的每个时间帧上都有一个这个模型。softmax模型作用在你比较感兴趣的词汇表上。被箭头指着的上边竖着的条条，是预测值。每一个预测值都会产生一个不同token类在这个时间上的对数概率。这个对数概率我们称它为score，在这里，得分s(k,t)就是log(Pr(k,t|X))，也就是数据X在时间为t的类别k上的对数概率。</p><p>对竖着的条条(预测值)的进一步阐述：<br>仅看softmax函数，假如第一个脚标是关于字符a的概率，第二个脚标是关于字符b的概率，第27个角标是关于1的概率，最后一个角标是空字符的概率。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1406_28_030.png" alt=""></p><p>CTC做的就是如果你仅仅看由RNN在每个时间节点上产生的softmax函数的话，慢慢的你会对根据单个softmax函数找到文本概率更感兴趣。我们可以选择一条通过整个softmax空间的路径，然后找到对应每个时间节点的字符。如下图，cc aa t，或者 cccccc aaaaa t预测同一个词可以有不同的路径和重复率，但是每个token都用空格符&lt;b&gt;隔开。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1418_58_941.png" alt=""></p><p><strong>端到端模型实例2：seq2seq with attention for speech</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1425_05_004.png" alt=""></p><p>给出了输入数据x（语音频率）和前i个输出y，预测y（i+1）的概率。</p><p>如上图，中间白色块是这个模型的解码器，也就是接受下部输入数据作为编码。他能馈入你生成的字符路径，因为他是RNN,你可以持续馈入字符并且字符长度不是问题。所以你在RNN的字符路径上馈入一个字符，然后你就能生成下一个token并且输出。 </p><p>需要实现的是让模型具有读取在时序空间上的不同部分数据的能力，因为输入数据真的很长，如果你看翻译结果的话，你会发现翻译的句子越长翻译效果越差，因为模型在定位需要翻译的内容的时候会有困难，尤其是输入的数据时音频数据流时。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1430_46_313.png" alt=""></p><p>音频数据比文本数据大多了，可能每秒有100组数据，比如你想存储10秒的音频输入，大概就有1000个token。所以这是个很恼人的问题，想要让模型工作的好的话一定要引入注意向量（attention）。</p><p>注意向量如下所示</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1434_08_958.png" alt=""></p><p><strong>端到端模型实例3:LAS模型（listen attend and spell）</strong></p><p>首先，下图是个LAS模型的编码器，图中ht和ht+1就是时间为t和时间为t+1时候生成的向量，在每一个时间节点上都使用解码器生成下一个字母，所以你做的就是提取解码器的状态向量，在RNN底层是解码器。现在来比较一下状态向量，每一个时间节点上的隐藏向量，从语义上讲，有这样一个查询，也就是状态s，将正在看到的ht放到了一个他可能出现的地方，你使用这个请求并将比较每个ht，你可以做一些简单的事情比如说点乘，（向量不必维数相等）。你也可以做一些更复杂的事情，比如将想要比较的隐藏向量和查询的向量串联成同一个向量，然后将他们输入到神经网络中生成一个数值。这里有一个et函数，<strong>et  = f([ht,s])</strong>,函数f将时间为t时的隐藏状态向量ht和RNN中的状态向量级联到一起作为一个解码状态，然后生成数字et。现在每一个时间节点上你都对编码器这么做，然后你就在时间上在编码空间上有了这么一个趋势，得到这个趋势et后（一串数字），然后把这一串数字丢给softmax，然后就可以得到一个注意力向量。如下的图片也清楚地展示了随着时间的变化，注意力向量变化的一个趋势。注意力向量告诉你在该时间节点上你看的位置应该是哪。然后接着向下一个时间节点，移动计算出新的注意力向量，依次计算。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1501_28_029.png" alt=""></p><p>现在已经有了注意力向量，现在就可以使用在时间节点上的概率来将隐藏状态混合到一起。然后得到文本值表示。在这里，你会取得所有与注意力向量的值有关的隐藏状态，将他们乘起来再加起来。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1507_03_732.png" alt=""></p><p>然后就会得到一个内容向量。这个内容向量就是一个会引导你做出预测的向量。有了它就可以把r和n的状态链接到一起，然后再传入到一个神经网络中就可以得到该时间点上的预测。输出的Y就是下一个token的概率</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1506_45_828.png" alt="">    </p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224n_1 - 导论</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_1%20-%20%E5%AF%BC%E8%AE%BA/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_1%20-%20%E5%AF%BC%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="Lecture1-导论"><a href="#Lecture1-导论" class="headerlink" title="Lecture1-导论"></a>Lecture1-导论</h1><h4 id="1-本课程研究对象"><a href="#1-本课程研究对象" class="headerlink" title="1.本课程研究对象"></a>1.本课程研究对象</h4><p>NLP是计算机科学和语言学的交叉学科。</p><p>语言是人类特有的属性，人类通过语言思考、交流、行动。</p><p>nlp要解决的核心问题是让计算机了解人类语言，然后完成有意义的任务（知识图谱，语义识别等）</p><p>nlp的步骤</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1315_43_291.png" alt=""></p><ul><li>预处理：首先speech和text都会被拆分</li><li>构词法分析：Morphological analysis，拆开ing和ex等前缀后缀，还原不同时态的词根。</li><li>句子结构分析：Syntactic analysis，比如主语是我，宾语是你，谓语是艹。</li><li>语义解释：Semantic interpretation，得出句子的含义。仅知道每个单词的含义是不够的，许多单词在一块构成了上下文，许多含义是依靠上下文传递的。这就引出了语用学和语篇处理等研究领域。</li></ul><p>cs224n这门课主要讨论<strong>句子结构分析</strong>和<strong>语义解释</strong></p><h4 id="2-人类语言的特殊之处"><a href="#2-人类语言的特殊之处" class="headerlink" title="2.人类语言的特殊之处"></a>2.人类语言的特殊之处</h4><p>做信号处理，数据挖掘之类的事情时人类往往只用视觉系统识别数据，然后处理数据。</p><p>但几乎所有的人类语言都是某人想要传递某个信息，他为此构建了一条信息，并试图传递给另一个人。语言有明确的指向性。</p><p>人类的语言系统是一套离散的符号分类信号系统，如下图</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1316_33_231.png" alt=""></p><p>人类语言在一些富有表现力的场景下会有一些小区别</p><pre><code>I loooooooooove it.            Whoooooooooooooooha</code></pre><p> 据推测，这些特征都是基于EE信息理论，因为有了符号，所以能可靠地进行远距离信息传输。</p><p>语言就是符号，符号不基于逻辑。</p><p>人类在交流时想表达的内容包括离散的符号，但具体沟通方式是使用了一种连续的载体。而且人类可以用不同的连续载体来传递完全相同的信息。比如声音/手势/文章。这就很有趣了，因为表达载体是连续的，而语言这一套符号系统是离散的，而大脑处理这些符号又是连续的。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1316_40_260.png" alt=""></p><p>哲学史和科学史上的主导观点是，将语言的符号体系映射到我们大脑中去，大脑可抽象为一组符号处理器。深度学习领域产生的一个有意思的想法是应该把大脑想象成具有连续的激活模式，上图显示的就是我们从连续到离散再到连续。语言通常都有巨大的词汇库（几万词），像英语还有巨大的科学词汇库（十几万词），在加上无穷无尽的构词法，可以说有些语言的词汇是无限的。</p><h4 id="3-深度学习介绍"><a href="#3-深度学习介绍" class="headerlink" title="3.深度学习介绍"></a>3.深度学习介绍</h4><p>首先深度学习是机器学习的一个分支，基本思想是怎样让电脑自动学习，而不是传统那样人工写代码，明确告诉计算机你想做什么。对于大多数传统的机器学习，他们都围绕着决策树和逻辑回归，朴素贝叶斯，SVM等，这些本质都是人类仔细审视一个特定的问题，找出这个问题的关键要素，设计出该问题相关的重要特征要素，然后手工写代码，识别这些特征，这些特征可以是单词是否大小写，单词是否有连字符等，这个系统最终可能会有数百万的人工设定的特征，其实2015年之前谷歌搜索就是这么做的，他们的具体做法就是工程师通过实验演示，如果添加了这些额外的特征，谷歌搜索效果就会好一点点，如果大家一致同意就加入这个特征。这种方式当时被称为机器学习，但是机器到底学习了什么？</p><p>机器只是在做数值优化，当收集好了特征，就会建立一个线性分类器，给每个特征量前加一个参数权重，而机器学习系统的工作就是调校这些数字来优化表现。电脑非常擅长做数值优化，如果有100个特征，每个特征之前要放一个实数，使得表现最优，人类对此只有一个模糊的概念，但是数值优化是否是计算机在机器学习中唯一该做的事情呢？</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1316_47_910.png" alt=""></p><p> 如图，前边大部分工作都是人类主导的特征工程，后边的数值优化算法交给计算机。特征工程是决胜的关键，数值优化算法大家用的都差不多。</p><p>深度学习是表征学习的一个分支，表征学习的理念是，我们可以只向电脑提供来自世界的原始信号（视觉/语言）电脑就可以得到好的中介表征，来很好的完成任务。某种意义上来讲，他是自己在定义特征，和过去人类自己定义特征一样。深度学习的真实含义是指，你将得到多层的习得表征。<strong>机器学习自己要定义特征，表征学习只把表面信息丢进去，特征由模型自己定义</strong>。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1316_58_968.png" alt=""></p><p>神经网络其实是<strong>栈逻辑回归</strong>，或者更广义的堆栈线性模型。</p><p>为什么深度学习如此激动人心？因为一般我们手动设计的特征往往太过具体，且不完整，需要花很长时间来验证和设计，而最终只能达到一定的表现水平。</p><p>但习得的特征适应性强，训练得快。他们可以不断学习，不断提升表现水平，比之前的作品要好。深度学习提供了非常灵活通用的学习架构，可以表示各种类型的信息。</p><p>从2010年开始，深度学习的表现就优于传统机器学习，最近六七年发展突飞猛进。从某种意义来说，之说以发展如此之快，因为现实使用的很多深度学习关键技术，其实是在8090年代发明的，只是当年他们并没有真正发挥作用，现在我们有了海量数据，计算机也有了足够的算力，这两个条件8090年代是不具备的。因此经过了几十年的计算能力的提升，现在可以搭建有效的系统了。</p><p>深度学习也被证明非常适合并行向量处理，可以在GPU中以非常廉价的方式实现（tensorflow，pytorch）。</p><h4 id="4-本课程教学目标"><a href="#4-本课程教学目标" class="headerlink" title="4.本课程教学目标"></a>4.本课程教学目标</h4><p>首先是理解并有能力运用有效的现代方法进行深度学习，我们将涵盖所有的基础知识，特别强调的是NLP中使用的主要方法，例如RNN循环神经网络，注意力机制等。</p><p>对人类语言要有总体的了解，以及目前人们理解和产生语言中遇到的困难。</p><p>第三点是这两个内容的交集，具有为重要NLP问题构建系统架构的能力。（会在作业中遇见这类问题）</p><h4 id="5-为什么NLP很难"><a href="#5-为什么NLP很难" class="headerlink" title="5.为什么NLP很难"></a>5.为什么NLP很难</h4><p>自然语言是模糊的，而编程语言在设计之初就是非常明确的。比如自然语言else在翻译中只是和看上去最合理的那个if合成一对，而编程语言中if和else严格按照最近的为一对。</p><p>自然语言作为一种高效的沟通工具，其中高效的关键就是大量使用省略。写程序会明确写出需要它运行的一切，但自然语言就像Stack Overflow上的代码段一样，听众需要自己去补全缺失的上下文代码。一句话来讲，自然语言需要“大家都懂的”。</p><p>自然语言通信靠的是用嘴说用耳朵听，这和5G通信信道比起来是非常慢的信道，但是它能有效运作的关键是沟通者只用说出最少量的信息，听众会根据自身的世界性常识和交流情境来自动补全剩下的部分。</p><p>所以说切换语言和国家之后，感到沟通困难的原因就是共识变少，要我们自己靠常识补全的部分非常不一样，<strong>“说话是冰山一角，没说的常识是冰山”</strong>。英语世界不会明白“天行健，君子以自强不息”，同样，只会说two car pengpeng one car die的老移民必然不明白boundary是什么。</p><p><strong>一个引申</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1317_26_340.png" alt=""></p><p>我们就是如此不可思议的复杂动物，漂浮在虚空之中，想要链接他人而总不得不盲目地向黑暗中蹦出词语，    每一种解读，拼读，语音语调，以及说话的时机都蕴含了丰富的信息，语境和言外之意。而每一个听众都会    以自己的方式来解读这些信号。自然语言并不是一种语言的正式系统，而是一种华丽的混沌。你永远不知道    某个词对某个人意味着什么，你所能做的，就是试图更好地猜测你的词语如何影响别人。你还是有机会找到    那些合适的词语，让对方按照你的意图来理解它的含义。而其他的一切毫无意义。</p><h4 id="6-Deep-NLP-深度学习-NLP"><a href="#6-Deep-NLP-深度学习-NLP" class="headerlink" title="6.Deep NLP = 深度学习 + NLP"></a>6.Deep NLP = 深度学习 + NLP</h4><p>深度学习近些年在如下三个方面有巨大突破：</p><ul><li><p>Level：speech,word,syntax,semantics</p><p>词汇学，语义学，句法学等</p></li><li><p>tools:  part-of-speech,entities,parsing</p><p>比如为标注词性，命名实体识别，parsing</p></li><li><p>applications:   machine translation,sentiment analysis,dialogue agent,question answering</p><p>applications:   machine translation,sentiment analysis,dialogue agent,question answering</p><p>机翻，情感分析，聊天助手等</p></li></ul><p>有意思的是，深度学习运用一套模式，解决各个领域的问题。<br>深度学习和语言学都是以词义为起点，这是一个非常核心的想法，具体做法是词语向量表示。一般会用最少25维的向量，通常是300维的向量。有了高维向量，说明我们将单词放在了高维向量空间中，这些空间就变成了非常棒的语义空间，具有相似含义的词汇将在矢量空间中形成聚集块，不止如此，我们会发现，向量空间中存在方向，它们会披露关于成分和意思的信息。人类的问题是不擅长解读高维空间，人类总是不得不将高维映射成二维或者三维才能理解。如下图所示，看到的词云其实是从高维映射下来的，相似的名词比如国家就在相近的地方。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1317_35_665.png" alt=""></p><h4 id="7-从语言学的几个层面看DL与传统方法的异同"><a href="#7-从语言学的几个层面看DL与传统方法的异同" class="headerlink" title="7.从语言学的几个层面看DL与传统方法的异同"></a>7.从语言学的几个层面看DL与传统方法的异同</h4><ul><li>传统语言学和DL在处理构词法上的区别：</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1317_44_699.png" alt=""></p><p>传统语言处理句子就是分析构词法。DL是通过把单词向量表示，在处理构词法的时候采用了树结构去拼装。</p><p><strong>助教分享</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1318_13_449.png" alt=""></p><p>她强调向量的重要性，所有这些神奇的事情背后都运用的是向量，我们用这个来表示所有的语言形式，无论是声音，单词，句子，他们都变成了含有真实值的向量。其实向量要比人意识到的还要微妙许多。你可能觉得这么庞大的数字向量里没有结构，我们也可以将这个向量变形，变成一个矩阵或者更高阶的阵列（张量），其实向量是一个非常灵活的数据结构，具有强大的表达能力，这也是深度学习的真正强项。</p><p>第一课陈丹琦的分享ppt中其实涵盖了很多后边才讲的内容，比如词性标注，比如词语的向量表示，比如输入层–隐藏层–softmax输出层等等。</p><pre><code>小结：NLP是语言学和计算机科学的交叉学科，既然是交叉学科，我现在假设各位DL大佬对DL的了解要远远多于对语言学的了解，而语言学排除了可能存在的迂腐研究方法之后仍然是一门有悠久历史和大量沉淀的值得尊重的学科，如果能发掘出这个学科内部不足为外人道的真知灼见，也许有助于在NLP上做出突破。找准自己的研究方向超重要，nlp作为一个新兴领域刚出现时肯定无人即精通语言学又精通DL，那个时候如果自己有深厚的积淀并选择了正确的导师果断进入行业内展开研究，多少能搭上一点历史进程的便车。现在nlp问题也火了好几年，语音处理已经做得非常不错了，机器翻译也有长足的进步，各分支领域大牛已经出现。自己不是天才，到处都是天资更高基础更扎实的业内人士，不选准方向就很难进入学界，就算进入了也可能是炮灰毕不了业，或者勉强毕业但方向大错误，一辈子也只是个平庸之人。想想自己的经历，若能找一个力学/土木/采购+计算机交叉的方向来做，这是沉没成本最低的路径，当然这个路径不一定存在或者非常坑。若经过【调研】发现上文说的交叉方向不靠谱，那就想办法选一个风口，跟上时代的大势去做研究。**如何摸到时代的脉搏？一个思路是对当前cs界的所有知名学者的个人页面爬数据，把他们的当前研究兴趣这一栏做聚类分析**，这个工作做完应该能对当下技术的走向有些眉目，当然，这个过程中也得有自己的独立思考。【2019.12.15批注】：上边这一段是11.20刚开始看NLP这门课的时候写的，我艹，这门课居然看了25天，进度太鸡儿慢了，做事一定要有可行的计划。CS224N第一次出现在计划里的时候是还在武大实验室的时候，我还记得当时的计划是一个星期看完这门课，外加很多其他的算法+后端的任务。结果老是完不成。其实就好好做做可行的计划，可能看完NLP也不需要花25天这么久。【2019.12.15批注】：上边这一段要寻找力学/土木/采购和计算机交叉学科的想法非常错误，因为我本身在力学和另外这两个领域都学的一塌糊涂，不要欺骗自己。数据结构的基本类型我能说的头头是道因为我真的好好学了，剪力图和弯矩图现在还是一塌糊涂，本科怎么混毕业的不用多说了8，心里千万不可没b数。NLP问题解决好的话就能对人类千万年历史积淀下来的浩如烟海的文字记录中提取出大量的宝贵知识，在此问题解决之前不会有任何一个人有精力或技术去了解古今中外所有典籍中的内容并做出提炼归纳总结。这是非常honor的工作。“本课程的教学目标是能让你使用现代方法进行深度学习，同时对人类语言也要有所了解。在这两者都具备的情况下，要能为nlp问题构建一个系统（炸翻世界的牛逼系统），这个项目不必写项目计划书【调研项目计划书】，因为导师已经帮大家设计好了整个项目&quot;。斯坦福的大作业居然是要写一个能用的nlp处理框架……这里大胆猜测，李沐的parameter server，mxnet等系统，最初的灵感和自信就是来源于这种课程的大作业吧【李沐的ps系统以及2+1篇论文调研，看看他到底怎么就两个月发文的】。斯坦福的大作业让我想到冷奕在面试亚麻的时候也把自己计算机课上的大作业拿出来说事儿，这个大作业是真的知识输出，我是不是也应该在kaggle上整个比赛，不说拿名次发论文了，可以当成我的NLP截课大作业啊，毕竟知识输出和输入一样重要，只输出文档还是不行。斯坦福课程的学习强度和创造性level很高，如果没有良好的积极性肯定不能完成学业。要发掘自己学习的热情和积极性，否则就算申上phd也难以胜任科研工作。学术是一种志业，科研是一种lifestyle，lifestyle是对生活的理解和自己想活成的样子，这种lifestyle应该是愉快和始终如一的。</code></pre>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
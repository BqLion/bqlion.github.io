<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>3.课程/正则表达式</title>
    <link href="undefined2020/05/25/3.%E8%AF%BE%E7%A8%8B/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    <url>2020/05/25/3.%E8%AF%BE%E7%A8%8B/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>正则表达式</tag>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2.比赛/Toxic_Abhi_Bert_Senti</title>
    <link href="undefined2020/05/25/2.%E6%AF%94%E8%B5%9B/Toxic_Abhi_Bert_Senti/"/>
    <url>2020/05/25/2.%E6%AF%94%E8%B5%9B/Toxic_Abhi_Bert_Senti/</url>
    
    <content type="html"><![CDATA[<h1 id="MultiLinguToxic-Abhishek-Thakur-s-Guide"><a href="#MultiLinguToxic-Abhishek-Thakur-s-Guide" class="headerlink" title="MultiLinguToxic : Abhishek Thakur`s Guide"></a>MultiLinguToxic : Abhishek Thakur`s Guide</h1><h3 id="数据探索"><a href="#数据探索" class="headerlink" title="数据探索"></a>数据探索</h3><h4 id="基本文件"><a href="#基本文件" class="headerlink" title="基本文件"></a>基本文件</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200331_1144_50_792.png" alt=""></p><p>如上是本比赛需要的数据，标橙的是主要数据。其中第一行和第二行的两个train数据来自之前jigsaw举办的比赛<br>两个128长度的seq for BERT也许不是我们需要的东西。</p><h4 id="数据列分析"><a href="#数据列分析" class="headerlink" title="数据列分析"></a>数据列分析</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200331_1148_30_471.png" alt=""></p><p>如上是数据不同的列，分别表示了id，真实评论，语言类型，是否是toxic的标签。</p><h4 id="对train-csv的一瞥"><a href="#对train-csv的一瞥" class="headerlink" title="对train.csv的一瞥"></a>对train.csv的一瞥</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200331_1151_18_648.png" alt=""></p><p>可以看到，数据的分列情况就是id+评论+是否toxic+是否严重toxic+是否淫秽。</p><h4 id="bert-sentiment"><a href="#bert-sentiment" class="headerlink" title="bert-sentiment"></a>bert-sentiment</h4><p>bert-sentiment用的是IMDb数据集，在github上star,Abhi出了三集视频专门介绍这个project。本次比赛可以直接调用。</p>]]></content>
    
    
    <categories>
      
      <category>2.比赛</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Toxic</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2.比赛/指导_特征提取与特征选择</title>
    <link href="undefined2020/05/25/2.%E6%AF%94%E8%B5%9B/%E6%8C%87%E5%AF%BC_%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B8%8E%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
    <url>2020/05/25/2.%E6%AF%94%E8%B5%9B/%E6%8C%87%E5%AF%BC_%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B8%8E%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/</url>
    
    <content type="html"><![CDATA[<h1 id="特征提取与特征选择"><a href="#特征提取与特征选择" class="headerlink" title="特征提取与特征选择"></a>特征提取与特征选择</h1><p>这两者的效果是一样的，都是试图去减少特征数据集中的属性（特征）的数目，但是两者采用的方法不同，</p><p>特征提取的方法是通过属性之间的关系，通过组合不同的属性得到新的属性，改变了特征空间</p><p>特征选择的方法是选择出特征集的子集，是一种包含的关系，没有改变特征空间</p><h4 id="1-2特征提取的主要方法"><a href="#1-2特征提取的主要方法" class="headerlink" title="1.2特征提取的主要方法"></a>1.2特征提取的主要方法</h4><p>PCA,LDA,SVD等</p><h4 id="1-3特征选择的主要方法"><a href="#1-3特征选择的主要方法" class="headerlink" title="1.3特征选择的主要方法"></a>1.3特征选择的主要方法</h4><p>1.fliter方法</p><p>对每个特征打分，即给出每一维的特征赋予权重，该权重代表了特征的重要性，然后根据权重排序。</p><p>主要的方法：卡方，信息增益，相关系数</p><p>2.Wrapper方法</p><p>主要思想：将子集的选择看成一个搜索寻优问题，生成不同的组合，对组合进行评论</p><p>可看做一个优化问题，有很多优化算法可供使用</p><p>尤其是启发式算法GA，PSO,DE,ABC等等</p><p>主要方法：递归特征消除方法</p><p>3.Embedded方法</p><p>主要思想：在模型既定的情况下，学习出对提高模型准确性最好的属性，即在确定模型的过程中，挑选出哪些对模型训练有重要意义的属性</p><p>主要方法：正则化，如岭回归就是在回归的基础上加上了正则化项</p><h4 id="1-4总结"><a href="#1-4总结" class="headerlink" title="1.4总结"></a>1.4总结</h4><p>1.特征提取是从杂乱无章的世界中，去到更高的世界俯瞰原有世界，这时会发现很多杂乱无章的物理现象背后暗含的道理是相通的，这时会想用一个更普世的观点和理论去解释原有的世界</p><p>2.特征选择是待在原有的世界里，只想对现有的维度取其精华去其糟粕，这就是特征选择，只是对现有进行筛选</p><p>3.特征提取和特征选择统称为降维</p><h1 id="2-文本提取特征"><a href="#2-文本提取特征" class="headerlink" title="2.文本提取特征"></a>2.文本提取特征</h1><h4 id="2-1-TF-IDF"><a href="#2-1-TF-IDF" class="headerlink" title="2.1    TF-IDF"></a>2.1    TF-IDF</h4><p>算法介绍：</p><p>词频-逆向文件频率，可以体现一个词在语料库中重要的程度</p><p>词频TF表示词语t在文档d中出现的次数，文件频率DF是语料库中包含词语的文档个数</p><p>如果只使用词频来衡量重要性，很容易放大“is”“a”之类词语的重要性</p><p>所以需要加上文件频率，文件频率越低，说明本词语携带的特殊信息越多越有标识度</p><p>公式</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1440_30_231.png" alt=""></p><p>TF-IDF实施步骤</p><p>1.获取原始文本内容信息</p><p>2.转换成小写，按空格将文章分成独立的词语组成的list</p><p>3.去除噪音，如： \ =  , : ;  : \n等等</p><p>4.去除stop-word</p><p>5.提取词干 ： goes,go,went,going 都换成go</p><p>6.wordcount，统计每个词出现的次数，去掉出现次数过少的词语，比如一百篇文档出现了1-2次的词语一般无用</p><p>7.训练idf模型</p><p>8.对输入的每篇测试文章计算tf-idf向量，利用tfidf向量求文章之间的相似度（欧拉距离，余弦相似度，jaccard系数等）</p><h3 id="2-2词袋模型"><a href="#2-2词袋模型" class="headerlink" title="2.2词袋模型"></a>2.2词袋模型</h3><ul><li>词袋模型的问题是丢失了语序</li><li>对于词频率特别高的词，会频繁出现但是意义不大</li></ul><h3 id="2-3词向量wordToVector"><a href="#2-3词向量wordToVector" class="headerlink" title="2.3词向量wordToVector"></a>2.3词向量wordToVector</h3><p>将词语映射到高维空间中，意思相近的词语在空间中的距离也近</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1441_00_995.png" alt=""></p><h3 id="3-kaggle项目实战"><a href="#3-kaggle项目实战" class="headerlink" title="3.kaggle项目实战"></a>3.kaggle项目实战</h3><h4 id="–sentiment-analysis-on-movie-reviews"><a href="#–sentiment-analysis-on-movie-reviews" class="headerlink" title="–sentiment analysis on movie reviews"></a>–sentiment analysis on movie reviews</h4><h4 id="3-1明确任务，数据探索"><a href="#3-1明确任务，数据探索" class="headerlink" title="3.1明确任务，数据探索"></a>3.1明确任务，数据探索</h4><p>任务：给出了电影评论短语，已按照情感程度打分（0-4），我需要对未打分的短语打分</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1441_11_885.png" alt=""></p><pre><code class="python">data_train = pd.read_csv(&#39;./train.csv&#39;,sep = &#39;\t&#39;)data_test = pd.read_csv(&#39;./test.csv&#39;,sep = &#39;\t&#39;)</code></pre><p>首先读入数据，sep=’\t’是指数据按照制表符分割</p><pre><code class="python">data_train.head()</code></pre><p>显示前五行数据</p><pre><code class="python">data_train.shape</code></pre><p>显示数据有多少行，多少列</p><h4 id="3-2构建语料库"><a href="#3-2构建语料库" class="headerlink" title="3.2构建语料库"></a>3.2构建语料库</h4><ul><li>将文本转换成计算机看得懂的向量，这一过程叫文本的特征工程</li><li>有很多将词转换成向量的方法，下面用的是word2vec，还有词袋模型和tf-idf模型</li><li>不管采用什么模型，都要把训练集和测试集所有文本内容组合在一起，构建一个语料库</li></ul><ul><li><p>首先将train和test数据集中的文字列合并</p><pre><code class="python">train_sentences = data_train[&#39;Phrase&#39;]test_sentences = data_test[&#39;Phrase&#39;]sentences = pd.concat([train_sentences,test_sentences])sentences.shape</code></pre></li><li><p>然后提出打分那一列</p><pre><code class="python">label = data_train[&#39;Sentiment&#39;]label.shape</code></pre></li><li><p>把stopwords删除</p><pre><code class="python">stop_words = open(&#39;/home/bq/IdeaProjects/python/SentimentOnMovie/data/stop_words.txt&#39;,encoding = &#39;utf-8&#39;).read().splitlines()stop_words</code></pre></li><li><p>用词袋模型构建语料库</p><pre><code class="python">co = CountVectorizer(     analyzer=&#39;word&#39;,     ngram_range = (1,4),     stop_words = stop_words,     max_features = 150000)co.fit(sentences)</code></pre></li><li><p>2比1的分割训练集，分割为训练集和验证集，交叉验证</p><pre><code class="python">x_train,x_test,y_train,y_test = train_test_split(train_sentences,label,random_state=1234)x_train = co.transform(x_train)x_test = co.transform(x_test)x_train[1]</code></pre></li><li><p>逻辑回归</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1441_26_989.png" alt=""></p></li><li><p>忽略版本问题出现的警告</p><pre><code class="python">import warningswarnings.filterwarnings(&#39;ignore&#39;)</code></pre></li><li><p>分析结果，做预测</p><pre><code class="python">from sklearn.linear_model import LogisticRegressionlg1 = LogisticRegression()lg1.fit(x_train,y_train)print(&#39;log regression from sklearn,the prescion is:&#39;,lg1.score(x_test,y_test))</code></pre></li></ul><ul><li><p>把词袋模型换成tf-idf模型,用逻辑回归做分类器</p><pre><code class="python">from sklearn.feature_extraction.text import TfidfVectorizertf = TfidfVectorizer(    analyzer=&#39;word&#39;,    ngram_range=(1,4),    max_features = 150000)tf.fit(sentences)lg2  = LogisticRegression()lg2.fit(x_train,y_train)print(&#39;using TDIDF model to do feature engnieer,using classifer of logi regresiion&#39;,lg2.score(x_test,y_test))</code></pre><p>逻辑回归加上正则化项，结果变好</p><pre><code class="python">######tf-idf to do feature engineer,using the normalization of log#####lg3 = LogisticRegression(C = 3,dual = True)lg3.fit(x_train,y_train)print(&#39;TF-IDF doing feature engineer,using normalizaion&#39;,lg3.score(x_test,y_test))</code></pre><p>sklearn有网格化参数搜索功能，可以在C从1到10，dual从正确到错误上遍历搜索</p><pre><code class="python">from sklearn.model_selection import GridSearchCVparam_grid = {&#39;C&#39;:range(1,10),              &#39;dual&#39;:[True,False]              }lgGS = LogisticRegression()grid = GridSearchCV(lgGS,param_grid = param_grid,cv = 3,n_jobs=-1)grid.fit(x_train,y_train)</code></pre></li></ul><p>输出上文计算的最优参数</p><pre><code class="python">  grid.best_params_  lg_final = grid.best_estimator_  print(&#39;after sklearn-search,using best params,the result is :&#39;,lg_final.score(x_test,y_test))</code></pre><p>最终输出结果</p><pre><code class="python">data_test.head()text_X = tf.transform(data_test[&#39;Phrase&#39;])predictions = lg_final.predict(text_X)predictionspredictions.shape data_test.loc[:,&#39;Sentiment&#39;] = predictionsdata_test.head()final_data = data_test.loc[:,[&#39;PhraseId&#39;,&#39;Sentiment&#39;]]final_data.head()final_data.to_csv(&#39;final_data.csv&#39;,index = None)</code></pre><p>竞赛在复赛一般要考虑模型 融合，前期可能不需要，但是后期不融合可能走不远</p><p>投票法，stacking，blending等</p><p>特征提取有四大部分</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1438_37_938.png" alt=""></p><p>​    </p><ul><li><p>Counting Features –词数的统计</p></li><li><p>DIstance Features –数据集query,desc,counting之间的的距离  :  </p></li><li><p>TF - IDF Features –  词频–逆文本频率指数(一个词在本文频率越高,在语料库中出现在文件中频率越低,说明越有标识度)</p><ul><li><p>例如”母牛”在本文出现三次,本文一百个单词,则在本文词频0.03</p></li><li><p>“母牛”在语料库中出现在1000份文件中,语料库共有1000,0000文件,逆向文件频率就是</p><p>lg(1000,0000 / 1000)=4</p></li><li><p>“母牛的”TF-IDF = 0.03*4 = 0.12    </p></li></ul></li><li><p>Query ID  –  给primary key换成独热编码</p></li></ul><h3 id="1-Query-ID特征"><a href="#1-Query-ID特征" class="headerlink" title="1.Query ID特征:"></a>1.Query ID特征:</h3><p>把primary key转换为one hot encoding</p><h3 id="2-Counting特征"><a href="#2-Counting特征" class="headerlink" title="2.Counting特征"></a>2.Counting特征</h3><p>-基本Counting特征</p><ul><li><p>Count of n-gram</p><ul><li>count of ngram(query,n),ngram(title,n),and ngram(description,n)</li><li>Count &amp; Ratio of Unique n-gram</li><li>Description Missing Indicator</li></ul></li><li><p>Count &amp; Ratio of a<code>s n-gram in b</code>s n-gram </p></li><li><p>Statisticcs of position of a<code>s n-gram in b</code>s n-gram</p></li><li><p>Statisics of normalized position of a<code>s n-gram in b</code>s n-gram</p></li></ul><h3 id="3-距离特征"><a href="#3-距离特征" class="headerlink" title="3.距离特征"></a>3.距离特征</h3><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1438_54_504.png" alt=""></p><ul><li><p>数据集的距离</p><ul><li><p>Jaccard coefficient:雅克比距离</p><p>计算法则:AB两个数据集相交的数据个数 除以  AB两个数据集相并的个数</p><p>例如A = [1,2,3,4,] B = [1,2,5,6]</p><p>那么雅克比距离 =  2/6  =三分之一</p></li><li><p>Dice 距离:</p><p>计算法则:两倍的交集除以两数据集元素个数</p><p>例如例如A = [1,2,3,4,] B = [1,2,5,6]</p><p>dice距离为4除以8 = 二分之一</p></li></ul></li></ul><ul><li>D(ngram(query,n),ngram(title,n))            查询与产品名称的距离</li></ul><ul><li>D(ngram(query,n),ngram(description,n))           查询与产品描述的距离</li></ul><ul><li>D(ngram(title,n),ngram(description,n))            产品描述与产品名称的距离</li></ul><h3 id="4-tf-idf特征"><a href="#4-tf-idf特征" class="headerlink" title="4.tf-idf特征"></a>4.tf-idf特征</h3><p>基本TF-IDF特征</p><p>-TF-IDF 特征</p><p>-Basic cosine 相似度</p><p>-statistical cosine 相似度</p><p>-SVD reduce相似度</p><p>-Basic cosine similarity based on SVD reduce features</p><p>-Statistical cosine similarity based on SVD reduce features</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1439_05_660.png" alt=""></p><p>​    </p>]]></content>
    
    
    <categories>
      
      <category>2.比赛</category>
      
    </categories>
    
    
    <tags>
      
      <tag>特征提取</tag>
      
      <tag>特征选择</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/技术_Storm</title>
    <link href="undefined2020/05/25/0.%E6%A6%82%E5%BF%B5/%E6%8A%80%E6%9C%AF_Storm/"/>
    <url>2020/05/25/0.%E6%A6%82%E5%BF%B5/%E6%8A%80%E6%9C%AF_Storm/</url>
    
    <content type="html"><![CDATA[<h1 id="Storm"><a href="#Storm" class="headerlink" title="Storm"></a>Storm</h1><p>storm是一个Apache顶级开源项目,用于流计算</p><p>流式计算</p><ul><li>实时获取/传输/计算/展示数据</li><li>代表技术:Flume实时获取,KAFKA实时存储,Storm实时计算,Redis实时缓存,mysql实时存储</li></ul><p>批计算</p><ul><li>批量获取/传输数据</li><li>代表技术:sqoop导入数据,HDFS存储数据,MapReduce计算数据,Hive计算数据</li></ul><p>storm和hadoop的区别:</p><ul><li>storm用于流计算,数据用网络传进来,处理过的数据放在内存中</li><li>hadoop用于批计算,处理的数据保存在文件存放系统中</li></ul><p>storm和hadoop的相同点:</p><ul><li>编程模型类似<ul><li>storm                     hadoop</li><li>Nimbus                JobTracker</li><li>Supervisor         TaskTracker</li><li>Worker                  child</li><li>topology              job</li><li>spout/bolt           mapper/reducer</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hadoop</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_GRU</title>
    <link href="undefined2020/05/19/0.%E6%A6%82%E5%BF%B5/NLP_GRU/"/>
    <url>2020/05/19/0.%E6%A6%82%E5%BF%B5/NLP_GRU/</url>
    
    <content type="html"><![CDATA[<h1 id="Gated-RNN-GRU"><a href="#Gated-RNN-GRU" class="headerlink" title="Gated RNN : GRU"></a>Gated RNN : GRU</h1><p>LSTM和GRU这两个重要的深度学习模型，几乎所有能看到的所有深度学习论文都是以这两个模型为基础。</p><h4 id="GRU模型"><a href="#GRU模型" class="headerlink" title="GRU模型"></a>GRU模型</h4><p>GRU模型的主要思想是，我们希望能够保存一些记忆，能够捕获一些较长的依赖，需要有一个模型去学习何时以及如何去做，同时还得允许错误消息流转。基于输入，这些流转会向不同的方向以不同的强度传递。</p><p><strong>对GRU的介绍从标准的RNN开始</strong></p><p>循环神经网络RNN基本上直接计算下一步的隐层，同样的，之前的隐状态会回归给向量，我们不是去计算GRU数值，而是先要计算其中各个门，这些门和h(t)一样，就是一些连续的隐状态长度一致的向量。这些门包括更新门和重置门。</p><p>计算更新门时主要是基于当前输入的词向量和隐藏状态</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191209_1611_59_836.png" alt=""></p><p>计算重置门方法类似但是使用的是不同的权重</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191209_1612_26_600.png" alt=""></p><p>通过一个直观例子来认识<strong>重置门</strong>的作用：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191209_1614_04_663.png" alt=""></p><p>详细分析上图的公式和重置门的作用：假如这是一个语义的情感分析任务，前边都是对一个电影长篇累牍的剧情讨论，最后一句话是it`s a really boring movie.那么其实只要最后这个boring就OK了，前边的都可以舍弃。所以这里重置门就可以置为0，把计算累计的东西都清理掉，只是记录词向量Xt而已。</p><p>直观例子认识<strong>更新门</strong>的作用：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191209_1621_08_398.png" alt=""></p><p>因为重置门和更新门都是经过了西格玛函数，最终输出的结果在0到1之间。如上图的公式，如果更新门Zt是1，那么最终的记忆内容ht就是直接复制h(t-1)，反之就是抛弃h(t-1)，继承之前所有的。直观例子还是语义情感分析任务，你上来就说这个电影我太喜欢了，然后后文是一堆对他细节的讨论。这时只用设置更新门确保不要丢失第一句话就好了。</p><p><strong>图示GRU模型</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191209_1636_02_643.png" alt=""></p><p>更新门和重置门在内存步进更新时起的作用。</p><p>电路设计视角;</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191209_1639_28_543.png" alt=""></p><h4 id=""><a href="#" class="headerlink" title=""></a></h4><p><strong>门控制单元GRU的阐述</strong></p><p>在朴素的翻译函数中，我们在序列中执行矩阵乘法，我们每一步乘以一个矩阵U，这样代表着我们正在学习这其中对我们的决策会起到多大的影响。我们通过在整个序列的中间节点做反向传播，这也就是说，整个GRU单元的思路中，我们希望能够获得更多直接的证据证明前期步骤对后续步骤有影响，但是不用做长序列的矩阵乘法。我们同时希望能够在短距离链接中如下图下边的示意，能够有反向传播的跨度为2的反馈，这样我们就能学到这些长期的依赖关系。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1433_29_452.png" alt=""></p><p><strong>GRU不会遭受梯度消失的阐述</strong></p><p>不会梯度消失的秘密在下图这个加号这里。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1525_59_925.png" alt=""></p><p>注意老师在这里也提到了门控单元对设计实用系统的重要性，电路设计系统设计又是一个深坑</p>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>GRU</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_LSTM</title>
    <link href="undefined2020/05/19/0.%E6%A6%82%E5%BF%B5/NLP_LSTM/"/>
    <url>2020/05/19/0.%E6%A6%82%E5%BF%B5/NLP_LSTM/</url>
    
    <content type="html"><![CDATA[<h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p>LSTM和GRU这两个重要的深度学习模型，几乎所有能看到的所有深度学习论文都是以这两个模型为基础。</p><h4 id="朴素RNN与展开"><a href="#朴素RNN与展开" class="headerlink" title="朴素RNN与展开"></a><strong>朴素RNN</strong>与展开</h4><p>是一种处理序列数据的神经网络，某单词根据上下文变化而有不同含义这种任务就适合交给RNN来做。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1429_27_198.png" alt=""></p><p>RNN在时序上展开</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1429_35_535.png" alt=""></p><h4 id="LSTM与RNN结构对比"><a href="#LSTM与RNN结构对比" class="headerlink" title="LSTM与RNN结构对比"></a><strong>LSTM与RNN结构对比</strong></h4><p>长短期记忆（Long short-term memory, LSTM）是一种特殊的RNN，主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。简单来说，就是相比普通的RNN，LSTM能够在更长的序列中有更好的表现。</p><p>LSTM结构和普通RNN的对比图如下，LSTM有两个传输状态，分别是cell state <strong>ct</strong> 和 hidden state <strong>ht</strong>，一般ct改变的很慢，通常是上一个ct加上一些数值。不同的时序下ht改变很快。</p><p>而RNN只有一个传输状态也就是<strong>ht</strong>，对应于LSTM中的ct。 </p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1453_46_649.png" alt=""></p><h4 id="LSTM的一胞三门两终态"><a href="#LSTM的一胞三门两终态" class="headerlink" title="LSTM的一胞三门两终态"></a><strong>LSTM的一胞三门两终态</strong></h4><ul><li><p><strong>输入门</strong>，基本上决定了关心当前向量的强度，或者说当前的cell或者当前输入的词能起多少作用</p><p>公式：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1523_51_723.png" alt=""></p><p>示意图：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1523_06_419.png" alt=""></p></li></ul><ul><li><p><strong>遗忘门</strong>，是一种分离机制，告诉cell有些时候需要忘记信息或者保留信息。这种情况看起来可能有一些不直观，通常定义遗忘门时0代表忘记过去。</p><p>公式：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1527_37_613.png" alt=""></p><p>示意图：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1528_00_542.png" alt=""></p></li></ul><ul><li><p><strong>输出门</strong>，用于分离那些与预测相关的内容和那些在该时点上需要保留在RNN中的内容。因此可以说，在当前时点上，这个特定的细胞单元并不重要，不过随后会变得很重要，比如现在不会输出最终的softmax但是还是希望保留这一信息，这就是另一个分离机制，会去学习何时输出。</p><p>公式：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1528_52_937.png" alt=""></p><p>示意图：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1529_07_262.png" alt=""></p></li></ul><ul><li><p><strong>新的记忆cell</strong>，跟GRU中的作用类似。在数学形式上和上</p><p>公式：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1529_39_278.png" alt=""></p><p>示意图：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1530_44_309.png" alt=""></p></li><li><p><strong>最终记忆cell</strong>，ct,分离了输入门和遗忘门。就是遗忘门乘以上一次的记忆细胞，加上输入门乘以新的记忆细胞。这里会再次决定，多大程度上希望保存或者忘记过去的内容。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1532_14_473.png" alt=""></p></li></ul><ul><li><p><strong>最终隐状态</strong>，如图是输出乘以<strong>tan(最终记忆cell）</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1532_56_409.png" alt=""></p></li></ul><pre><code class="python"># 遗忘门，输入门，输出门，以及新的记忆细胞都是单层神经网络。# 其中记忆细胞z是将结果通过tanh激活函数将数值映射在-1 到 1之间，使用tanh而不是sigmod是因为其职责是输入信号而非门控信号。# 其中输入门zi、遗忘门zf、输出门zo这三个门是由拼接向量乘以相应的权重矩阵后再通过一个sigmoid激活函数转换为0到1之间的数值。</code></pre><h4 id="LSTM内部的三个阶段："><a href="#LSTM内部的三个阶段：" class="headerlink" title="LSTM内部的三个阶段："></a><strong>LSTM内部的三个阶段：</strong></h4><ol><li><p>忘记阶段</p><p>这个阶段主要是对上一个节点传进来的输入进行选择性忘记，具体操作是通过计算得到的zf来作为忘记门控来控制上一个c(t-1)哪些需要记哪些要忘。</p></li><li><p>选择记忆阶段</p><p>对输入xt进行选择性记忆，重要的多记不重要少记。门控信号使用zi输入门控制。</p><p>另一个输入内容是z。也是用zi信息门控制。</p><p>上面两步得到的结果相加，即可得到传输给下一个状态的c0</p></li><li><p>输出阶段</p><p>对选择记忆阶段输出的c0通过tanh函数放缩，然后输出门z0控制哪些会被作为当前的状态输出。</p><p>与普通RNN类似，输出yt往往由最终状态ht得到。</p></li></ol><p><strong>把门控制单元部署到LSTM上的解释</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1439_41_349.png" alt=""></p><p>解析上图第二行，标绿的<strong>神经网络的循环单元</strong>：<br>h(t)，循环单元，等于当前的向量输入与W矩阵相乘，加上之前的状态乘以U矩阵，然后再加上一个偏差b，然后将他们放进tan函数中去，这是标准的RNN迭代。我们像常规的RNN一样去更新迭代。</p><p>解析上图第一行<strong>核心公式</strong>;<br>对于准确确定我们计算的函数，我们需要自适应学习需要用多少和哪些维度去更新candidate，以及我们需要截取并使用百分之多少我们之前时间段获得的内容也就是h(t-1)。我们允许使用距离远的过去时间段的内容，并且这个远距离内容并不是使用乘法进入当前时间点，而是相加进入，而核心公式的相加第二项也就是我们刚才说的在下一个时间点之前移动这个内容，我们直接从过去获取这个内容，影响现在并进一步影响决定。这是一种递增法，你可以一直这么处理，你正在决定哪些内容应该被关注，但是如果你一旦关注了他，效果会一直伴随，因为你正在添加更多的东西上去。</p><p><strong>遗忘门</strong></p><p>最后我们可能想要修剪一下过去自适应的东西，使他不要一直存在。这个需求引入了第二个门控，遗忘门控。遗忘门控给了你一个0到1之间的向量，他像一个标准的循环单元一样计算，他可以删掉在t-1时间点的不再相关的东西，因此如下图<strong>标绿</strong>所示，遗忘门控与之前的隐藏单元之间的元素做点积。这样我们就能忘掉过去的隐藏层。而我们忘掉的部分已经嵌入进更新的候选内容中了。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1046_59_208.png" alt=""></p><p><strong>tanh-RNN</strong></p><p>如下图，没有GRU的情况下，在隐藏层里，先读取寄存器h，然后执行RNN更新，然后返回。这样做要不停地更新，非常不灵活。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1249_53_627.png" alt=""></p><p>如下图，有了GRU，便允许灵活的自适应学习。<br>首先，重置门控，然后学习一个可读的隐藏状态层的子集r，其他部分将会丢弃，这样就有了忘记的能力。<br>然后将读取的r和h点乘，继续进行标准的RNN计算，更新，重写。<br>然后选择一个可写入的子集u，将子集按照下图第四行公式更新写入。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1258_39_454.png" alt=""></p><p><strong>tricky概念阐述</strong></p><p><strong>更新门</strong>是选择读取什么，然后更新候选值；<strong>重置门</strong>负责选择哪些部分的隐藏状态需要被覆盖。</p><p>问题：如何知道哪个子集依赖特定的重置门？</p><p>重置门基于当前的输入来计算需要读取/更新隐藏状态的哪个部分以及需要读取/更新哪个以前的状态，如下图，根据输入xt确定需要读取Wr的哪个部分。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1308_26_328.png" alt=""></p><p>重置门rt还在更新候选值中发挥作用，与之前的h(t-1)隐藏层点乘，决定了要在多大程度上采纳之前的隐藏状态。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1315_55_469.png" alt="">    </p><p><strong>根据提问对整体概念再次阐述</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1319_51_030.png" alt=""></p><p>我们从左往右一直携带一个隐藏状态层，并在每一个时间序列点上，我们想尝试计算一个新的隐藏状态层through乘以矩阵W。当然有的时候不需要计算整个隐藏状态层，仅仅传过来的之前的信息也是有用的。</p><p>如果我们仅仅是在每一步做乘法，沿着一个朴素的RNN,我们就失去了长期记忆的概念。而且实质上，我们不能记住超过7步的内容。（我的理解是每一个新的乘法都会冲淡之前的乘法对整体数值概念的程度，比如1连乘七次0.1，就会基本消失掉）</p><p>如下，我们想要做的是计算一个候选值更新同时保留我们已经完成的工作。（加法具有累计性，每一个新的加法都不会太大幅度地改变总体值）</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1331_23_028.png" alt=""></p><p>具体实现：</p><p>更新门ut : ut这个向量的输出值在0到1之间。如上左右切割缩放公式，若它接近1，表示用我们这个时间计算的内容覆盖当前隐藏状态层，接近0表示使用过去的那个状态保持住这个元素向量。</p><p>计算更新门ut的方法是使用如下常规的循环单元，他括号内第一项关注当前的输入xt，括号内第二项关心近期的历史h(t-1)。最后加上第三项偏置项。最后投入西格玛函数里，输出一个0到1的结果，然后作用于上文提到的核心公式里。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1337_02_492.png" alt=""></p><p>假设一个单元对输入的词敏感，我们想要这些向量的维度记录见过的词，直到一个新词的出现，在新词出现的瞬间，我们说，好了，是时候更新了，忘记过去在这五个维度中存储的内容，现在需要存储一个新词，这就是更新门所做的内容。更新门的括号内第一项能够记录输入词，比如向量的47到52维应该给一个1的值。那意味着，它们将被存储和从计算更新，并且忽略他们曾今在过去存储中的内容。但是比如更新门发现它们正存储着一个介词，他将会将更新值设置为接近0。那意味着47到52维将继续存储上次见过的词，即使它已经是前十个词了。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1524_49_078.png" alt=""></p><p>不仅门控会更新，候选值也会同时更新。当然候选值是使用tan函数来输出的，最后的值在-1到1之间。<br>sum up idea就是如果你正在做一个候选值的更新，你总是使用之前的隐藏状态层，和新的输入词在更新门中使用相同的方式。如果你已经在输入中检测到一个新的词汇，你应该存储进47到52维，然后你就应该忽略你过去在那里有的内容，还要一些情况你需要丢掉当前的隐藏层的就是用新的隐藏层取代它。这种取代就是重置门所要做的，重置门能观察之前的隐藏状态层在当前的重要性，并且让他选择一个0-1之间的值，如果重置门的选择的值接近0，必须扔掉之前的隐藏状态层，并且根据新的输入计算。</p><p>这里有一个具体语言的类比。比如你正在记录东西，上次见过的词在47到52维之间，当你见到一个新的词，要做的正确的事情是丢掉之前47到52维的历史记录，并且根据新的计算输入更新值。但是上述操作并不是总适用。比如英语中有大量的助词词组，例如make up,make out之类的。比如你想计算出make out这个词的意思，当你先看见了make，并且把它放进了47到52维之间，如果47到52维之间真的存储了主要谓语make的意思，但是后边的out又出来了，你肯定不想丢掉make，因为这是一个词组。在这种情况下，你希望重置门能有一个接近1的值，希望即保留前边的词又import进新的词。</p><h3 id=""><a href="#" class="headerlink" title=""></a></h3><h4 id="LSTM和GRU的对比图"><a href="#LSTM和GRU的对比图" class="headerlink" title="LSTM和GRU的对比图"></a>LSTM和GRU的对比图</h4><p>这里要注意LSTM引入了cell细胞的概念，可等价于GRU中的hidden layer。也就是C&lt;==&gt;L</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1649_02_108.png" alt=""></p><p>下图LSTM细胞更新公式是LSTM的核心，和GRU中的隐藏状态层更新公式有细微的差别，在LSTM中，有两个门，一个是遗忘门，一个是输入门，他们的值都在0到1之间，所以你可以保存任何之前和新计算的值，然后还能把他们加起来。这是在权衡保留多少值，这点是和GRU不一样的。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1649_57_914.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1650_44_552.png" alt=""></p><p>候选值更新是完全相同的：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1654_35_751.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1654_43_736.png" alt=""></p><p>使用RNN风格的各个门控几乎是完全相同的，都会得到一个0到1之间的某个值，区别是LSTM加了一个新的门，输入门i</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1656_34_491.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1656_42_614.png" alt=""></p><p>GRU和LSTM还有个区别就是GRU有重置门的功能，在计算候选值的时候可能忽略部分过去的值，LSTM的做法稍微有些不同，LSTM在候选更新的时候总是使用当前输入，但是对于另一部分，不使用C(t-1)。而且在LSTM中也有ht，是通过ct推导过来的。形式和GRU略有不同。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1445_05_063.png" alt=""> </p><p>上图是LSTM的构造图，横着的三行是各种门，第四行是候选值更新，和RNN一样。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1859_02_952.png" alt=""></p><p>上图紫色部分的加权加法是整个LSTM网络拥有长期记忆能力的秘诀。一种可能是你仅仅复制前一个时间步的内容，如果出现这种情况，梯度是1，就可以把误差反向传播回去，对于每个时间步长都可以持续这么做。所以把这个和前一个时间步加起来，而不是做矩阵乘法，这是让LSTM拥有了长短期记忆的核心思想。这被证实是一个非常强大的想法。这个思想在深度学习做计算机视觉的领域也应用地很好。这种思想也叫做残差网络，一般简写为ResNet，大概来说，大概来说，我们希望能够设计100层的深度神经网络，希望能够成功的训练他们，这里的加号和ResNet的第一次近似的思想完全相同。随着你不断向后层计算，我们会使用常规的神经网络层进行一些非线性运算，但是提供一种选择，你可以把前面层的输入移过来，并把他们相加起来，重复一次又一次直到100层。</p><p>如下图片演示是从第128个时间步反向进行，可清晰看出LSTM中的信息会持续多久。</p><ul><li><p>步数128</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1940_55_049.png" alt=""></p></li><li><p>步数120：左边RNN基本丢光了</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1941_28_304.png" alt=""></p></li><li><p>步数90：LSTM还有信息记录</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1942_11_246.png" alt=""></p></li><li><p>步数60：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1942_37_957.png" alt=""></p></li><li><p>步数30</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_1942_54_429.png" alt=""></p></li></ul><h4 id="警惕可读性过差的信息"><a href="#警惕可读性过差的信息" class="headerlink" title="警惕可读性过差的信息"></a>警惕可读性过差的信息</h4><p>下图为LSTM原论文配图，严重缺少注释，结构表现不明显，放大缩小无度。和前文知乎作者的图片比起来表达力高下立判。</p><p>其实所有的模型or概念只要表达得当都很容易理解。以后再遇到类似下图这种表达力实在呵呵的东西如<strong>高级数据结构/可读性巨差的数学公式时</strong>，要明确这都是已经提出了很久的模型或者固定知识，动作要敏捷，不要在一个具体的点上耗费太多精力，科学地、分层地去看，每一层不会有很多hardcore信息。</p><p>某个点过于难顶一定是<strong>作者表达方法不行，或者结构设置不合理，或者自己极度缺乏基础知识</strong>。</p><p>遇见这种难顶的点的时候一定注意时间和passion的消耗问题，路一直都明确地在脚下，失败是因为停止行路，停止的原因不就是passion消耗殆尽吗，认识难顶概念被卡住时passion消耗最快。这时必须高度警惕，迅速调整打法，通过阅读其他大牛的技术博客或其他靠谱信息源去理解它，横看成岭侧成峰，现成的概念一定有一个角度是非常好理解的！</p><p>我们无法摆脱对前人后人工作的依赖和影响，单个科学家生命周期实在太短，人类科学是作为一个整体前进的。所以写代码的人有维持可读性的责任，论文、博客、图片同理。苏联式教育虽然能打下扎实基本功，但对可读性的追求不够讲究，这种结构性的缺陷要格外注意回避。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191209_1703_05_453.png" alt=""></p><h4 id="关于Gated-RNN的一些实践性的指导意见"><a href="#关于Gated-RNN的一些实践性的指导意见" class="headerlink" title="关于Gated RNN的一些实践性的指导意见"></a><strong>关于Gated RNN的一些实践性的指导意见</strong></h4><ol><li><p>如果你要建立一个大的循环神经网络RNN，不管是GRU还是LSTM,初始化真的非常非常重要，如果你的RNN不能正常工作，很多时候是你的初始化做的不好。通常来讲把前一个cell的隐藏状态层乘起来对循环矩阵很有用，通常可以让他正交，这样就有了使用线性代数的机会。</p></li><li><p>RNN中其实没有多少参数，为了能让它们学习到一些有用的东西，提供一个正交的初始化被证实确实是一种更好的方式。有了这种想法，你就能够在RNN中一直做乘法，通常你的初始值非常小，不可以大，大了就会毁掉神经网络。</p></li><li><p>通常我们把权重随机地初始化为0，不过在设置遗忘门的bias的时候是个例外，如果你把遗忘门的bias设置为一个大小合适的正数，比如1或者2，通常效果会更好。这表明，你应该对遥远的过去给予更多的关注。这种偏置用来保存长久的记忆。这鼓励你得到一个能有使用的有效的长期记忆的模型。如果过去的内容没用了，就丢掉一部分。但是遗忘门开始主要是遗忘。</p></li><li><p>这些RNN LSTM等等算法结合现在的动态学习率算法效果会更好。比如adam，adaDelta算法等。这笔SGD好多了。</p></li><li><p>对于各种RNN,在垂直方向使用dropout是很常见的。这通常能够改进性能。但是不能在水平方向使用，因为这样几乎每一个维度都会被dropout，你就不会得到信息流。</p></li><li><p>assembly，如果希望结果能提升2%，那么使用集成是行之有效的手段。如下使用投票型集成，紫色是使用集成后的提升，可看到性能提升显著。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191211_2041_14_615.png" alt=""></p></li></ol>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LSTM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_RNN循环神经网络</title>
    <link href="undefined2020/05/18/0.%E6%A6%82%E5%BF%B5/NLP_RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>2020/05/18/0.%E6%A6%82%E5%BF%B5/NLP_RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="循环神经网络RNN"><a href="#循环神经网络RNN" class="headerlink" title="循环神经网络RNN"></a>循环神经网络RNN</h1><p>循环神经网络<strong>擅长处理序列化的数据</strong>，比如文字序列或者时间序列。</p><p>循环神经网络比基本的前馈神经网络强在它可以跨时间序列地共享不同输入序列之间的特征。</p><p>之前提到的CNN卷积神经网络也可参数共享，而RNN循环神经网络与之不同的是每一点的输出依赖于之前的结果。</p><h4 id="状态机近似表示"><a href="#状态机近似表示" class="headerlink" title="状态机近似表示"></a>状态机近似表示</h4><p>RNN名字中的recurrent得名于如下图所示的<strong>t时刻的状态 依赖于 t - 1时刻的状态 + 输入x。</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1137_36_586.png" alt=""></p><p>​              ↑总图                   ↑拆分                                 ↑ t - 1时刻            ↑ t时刻               ↑ t + 1时刻</p><p>上图维持的状态h(t)可以看作是之前的向量序列【x1，x2，x3，x4，……，xn】的全体有损表示（损度可控，某些语言模型只有附近的文字重要就可以加大损度）。</p><h4 id="RNN基本结构图"><a href="#RNN基本结构图" class="headerlink" title="RNN基本结构图"></a>RNN基本结构图</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1145_18_442.png" alt=""></p><p>如上，x是时时输入的序列，h是维持的状态，o是状态给出的输出，y是训练集中的答案，L是答案和输出之间的损失函数（如交叉熵）。</p><p>因为上一个状态h(t-1)和下一个状态ht之间有循环连接，所以计算时要顺序计算而不能并行计算，所以训练过程比较缓慢。而且该模型需要输入与输出序列相同。</p><h4 id="RNN-的-teacher-forcing变体"><a href="#RNN-的-teacher-forcing变体" class="headerlink" title="RNN 的 teacher forcing变体"></a>RNN 的 teacher forcing变体</h4><p>丧失一定的普适性，去掉h之间的循环连接，建立前一个时间点真实答案值y与当前h的连接。这样当前值就不用依赖于前一个h的状态计算结束。这样就可以使用并行计算。当然这样做y(t-1)并不能代表h(t-1)的信息，这也就是丧失普适性的原因。如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1337_11_840.png" alt=""></p><h4 id="RNN的-encoder-decoder结构"><a href="#RNN的-encoder-decoder结构" class="headerlink" title="RNN的 encoder - decoder结构"></a>RNN的 encoder - decoder结构</h4><p>基本的RNN要求输入输出定长，但是实际应用比如语音识别和机翻，是做不到定长的，所以需要先用RNN将输入序列转换为一个向量（通常是最终态h（t）的一个函数）这个函数被称为encoder或者decoder。</p><p>结合起来就是经典的encoder - decoder结构又称sequence2sequence结构。经过这两步就解决了不定长的问题。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1351_12_857.png" alt=""></p><p>encoder和decoder的数学表示：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1418_35_164.png" alt=""></p><p>损失函数：目标就是要最小化这个交叉熵</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200519_1419_00_647.png" alt=""></p><h4 id="RNN的问题："><a href="#RNN的问题：" class="headerlink" title="RNN的问题："></a>RNN的问题：</h4><p>在时间t时有一些随着时间变化的东西（图的左边开始端），然后到了时间t+n的时候它发生了变化（图的右边结束端）。我们要研究的就是在时间t时看到的东西是如何影响到t+n时刻的。我们要测量比如现在看到的单词是如何影响到接下里的6个或者8个单词的。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1415_39_199.png" alt=""></p><p>我们要如何实现RNN？我们要做的是，在基本RNN的每个时间点，我们已经获得了一些隐藏的状态，我们用矩阵去乘他们，然后我们增加一些东西去处理输入，然后我们进入下一个时间，我们将隐藏状态再次乘以相同的矩阵，然后多次重复这个过程就会遇到梯度消失的问题。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1419_07_794.png" alt=""></p><p>这个问题其实是由如下的朴素的transition函数导致的。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1421_20_904.png" alt=""></p><p>为解决RNN的梯度消失问题，后边引入了带gate的RNN，也就是LSTM和GRU，见专门的文章。</p>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/线性代数_正交归一性</title>
    <link href="undefined2020/05/17/3.%E8%AF%BE%E7%A8%8B/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0_%E6%AD%A3%E4%BA%A4%E5%BD%92%E4%B8%80%E6%80%A7/"/>
    <url>2020/05/17/3.%E8%AF%BE%E7%A8%8B/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0_%E6%AD%A3%E4%BA%A4%E5%BD%92%E4%B8%80%E6%80%A7/</url>
    
    <content type="html"><![CDATA[<h1 id="正交归一性"><a href="#正交归一性" class="headerlink" title="正交归一性"></a>正交归一性</h1><p>如果内积空间的两个向量是互相正交的，并且两个向量的范数都是1，则称这两个向量互相具有正交归一性/正交规范性。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>线代</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/线性代数_归一化</title>
    <link href="undefined2020/05/17/3.%E8%AF%BE%E7%A8%8B/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0_%E5%BD%92%E4%B8%80%E5%8C%96/"/>
    <url>2020/05/17/3.%E8%AF%BE%E7%A8%8B/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0_%E5%BD%92%E4%B8%80%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h1><p>归一化消除了量纲，是处理数据的一种手段。通过归一化最优解的寻找过程会变得平缓，更容易正确的收敛到最优解。</p><p>下图的例子是房价预测模型，横坐标是房间数量（0 - 10），纵坐标是面积大小（0 - 1000），预测结果是等高线上的没画出的第三维。</p><p>图一未归一化，图像是长椭圆，图二归一化后是正圆（拍扁了数据），可见归一化后收敛过程更平缓。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200517_1514_26_365.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200517_1514_45_488.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>线代</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.文章/NLP_The Annotated Transformer</title>
    <link href="undefined2020/05/15/0.%E6%96%87%E7%AB%A0/NLP_The%20Annotated%20Transformer/"/>
    <url>2020/05/15/0.%E6%96%87%E7%AB%A0/NLP_The%20Annotated%20Transformer/</url>
    
    <content type="html"><![CDATA[<h1 id="The-Annotated-Transformer"><a href="#The-Annotated-Transformer" class="headerlink" title="The Annotated Transformer"></a>The Annotated Transformer</h1><p>harvard nlp 对Transformer论文的逐行注解,原文：<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">http://nlp.seas.harvard.edu/2018/04/03/attention.html</a></p><p>本文在github和谷歌drive上的网址：</p><p><a href="https://github.com/harvardnlp/annotated-transformer" target="_blank" rel="noopener">https://github.com/harvardnlp/annotated-transformer</a></p><p><a href="https://drive.google.com/file/d/1xQXSv6mtAOLXxEMi8RvaW8TW-7bvYBDF/view" target="_blank" rel="noopener">https://drive.google.com/file/d/1xQXSv6mtAOLXxEMi8RvaW8TW-7bvYBDF/view</a></p><h4 id="Import-lib"><a href="#Import-lib" class="headerlink" title="Import lib:"></a>Import lib:</h4><pre><code class="python">import numpy as npimport torchimport torch.nn as nnimport torch.nn.functional as Fimport math, copy, timefrom torch.autograd import Variableimport matplotlib.pyplot as pltimport seabornseaborn.set_context(context=&quot;talk&quot;)# %matplotlib inline</code></pre><h4 id="Background背景知识"><a href="#Background背景知识" class="headerlink" title="Background背景知识"></a>Background背景知识</h4><p>减少顺序计算的目标也构成了扩展神经GPU：<strong>ByteNet和ConvS2S</strong>的基础，它们全部使用卷积神经网络作为基本构建块，可以并行计算所有输入和输出位置的隐藏表示。</p><p>但是他们<strong>关联来自任意两个输入或者输出的操作数</strong>在位置之间的距离中随之增加，ConvS2S是线性增长，ByteNet是对数增长。这使得学习远距离之间的依赖性变得更为困难。在transformer中此操作被减少为恒定的操作次数。</p><p>这种优化的代价是以平均注意力加权位置为代价，导致有效的分辨率降低了，但是我们使用<strong>多头注意力</strong>来抵消这种debuff。</p>]]></content>
    
    
    <categories>
      
      <category>文章</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_词向量_Word_embedding详解</title>
    <link href="undefined2020/05/15/0.%E6%A6%82%E5%BF%B5/NLP_%E8%AF%8D%E5%90%91%E9%87%8F_Word_embedding%E8%AF%A6%E8%A7%A3/"/>
    <url>2020/05/15/0.%E6%A6%82%E5%BF%B5/NLP_%E8%AF%8D%E5%90%91%E9%87%8F_Word_embedding%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h1 id="Word-embedding"><a href="#Word-embedding" class="headerlink" title="Word_embedding"></a>Word_embedding</h1><p>之前词语数字化表示的几种思路和问题，以及WE的提出：</p><ul><li><p>独热码离散表示：红橙黄绿青蓝紫<strong>大卡车</strong>和红橙黄绿青蓝紫<strong>中卡车</strong>是2 * 7 = 14个对象</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200508_1453_28_279.png" alt=""></p></li><li><p>构建特征的分布式表示：红橙黄绿青蓝紫<strong>大卡车</strong>和红橙黄绿青蓝紫<strong>中卡车</strong>是7 + 2 = 9个对象</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200508_1454_02_846.png" alt=""></p></li></ul><ul><li>邻接词的分布式表示：</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200329_1534_13_274.png" alt=""></p><p>离散表示太稀疏，使用一个词附近的词分布式表示是不错的折中方案。</p><p>WE的概念图（同义相近）：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200508_1428_00_244.png" alt=""></p><p>WE的目标：把单词转换为数字（向量）。</p><p>这种转换是必要的，因为深度神经网络等技术都只能处理连续值的向量，不能处理文本和字符串。</p><p>把单词变成向量还有如下两个好处：</p><ul><li>降维：更高效的表示方法</li><li>上下文相似性：更有表达力的表示方法</li></ul><p>词向量的训练方法:</p><p>使用语料库和模型训练词向量，把嵌入矩阵embedding matrix当成模型参数的一部分，通过词语词之间的共现矩阵或者上下文关系来优化模型参数，最终得到的矩阵就是所有词的词向量。最初的词向量都是随机初始化的，后边采用标准训练和优化。</p>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Word_Embedding</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_词向量_SkipGram</title>
    <link href="undefined2020/05/14/0.%E6%A6%82%E5%BF%B5/NLP_%E8%AF%8D%E5%90%91%E9%87%8F_SkipGram/"/>
    <url>2020/05/14/0.%E6%A6%82%E5%BF%B5/NLP_%E8%AF%8D%E5%90%91%E9%87%8F_SkipGram/</url>
    
    <content type="html"><![CDATA[<h1 id="skip-gram算法详解"><a href="#skip-gram算法详解" class="headerlink" title="skip-gram算法详解"></a>skip-gram算法详解</h1><p>Skip-gram算法是word2vec模型中两大生成词向量算法中的一个。</p><p>生成词汇向量的skip-gram算法的中心思想：给定中心词汇，预测单词们出现在它周围的概率，最终确定的这些单词们，要使概率分布值最大化。</p><p>如下，中心词是banking，Skip - gram会尝试去预测它一定范围内（m window）的上下文词汇。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1410_21_598.png" alt=""></p><h4 id="Skip-gram的损失函数"><a href="#Skip-gram的损失函数" class="headerlink" title="Skip-gram的损失函数"></a><strong>Skip-gram的损失函数</strong></h4><p><strong>损失函数J`(θ)</strong>表示的是：一串很长的文本比如整个维基百科（which has足够长的词汇序列和足够多的词汇和真正的行文）然后遍历文本中的所有位置，对于每个位置，我们都会定义一个围绕输入词汇的大小为2m的窗口，窗口包括前后各m个单词，把每个词汇和周围的窗口的<strong>p(周围每一个词|中心词)</strong>都连乘起来。</p><p>这就是一个有监督学习，目标是使损失函数最小化，手段是通过对训练集的training动态调整模型的超参数们(窗口大小，容差系数等).</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200514_1447_30_964.png" alt=""></p><p><strong>损失函数plus</strong> : 计算机中大量连乘容易误差累计和误差消失，给上边的损失函数整体套上log，将连乘转换为带log的连加。如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200514_1448_27_511.png" alt="">要不是高中学的明白根本肝不到这里……answer the calling！</p><p><strong>调整完毕超参数，如何最优化损失函数plus</strong>?  —— 最优化每一个P（边|中）</p><p>分子是预测的那个边词与中词的乘积</p><p>分母是window内所有边词和中词的乘积的加和</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1411_03_305.png" alt=""></p><p>现在明确了代价函数，就有了明确的训练方向，so超参数和向量的训练细节在下文讲解。</p><p><strong>模型训练</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1411_41_142.png" alt=""></p><p>如上图最左，有一个中心词汇，他是一个独热向量wt。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200514_1543_38_220.png" alt=""></p><p><strong>还有一个所有中心词汇的词向量组成的矩阵W</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200514_1544_27_971.png" alt=""></p><p>将独热向量乘以矩阵，提取了矩阵的那一列，实现了词向量从稀疏的独热码变成了稠密的词向量vc。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200514_1547_35_972.png" alt=""></p><p>存在第二个矩阵——存储上下文词汇表示的上下文矩阵uo。对于上下文的每个位置（这里只列举了三个），我们列出中心词汇vc和上下文表示uo的点积</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200514_1547_53_911.png" alt=""></p><p>经过中心向量和上下文向量的点积，然后通过softmax方法转换成概率分布，其实softmax的数学含义就是大者更大，小者更小。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200514_1549_15_234.png" alt=""></p><p>然后将softmax的输出与上下文真正的独热码对比，看是否预测正确词，错了就惩罚现有参数，对了强化现有参数，实现了<strong>监督学习</strong>，而对现有参数（词向量们，超参数们）的动态调整，本质上<strong>是神经网络的反向传播算法的做法</strong>。</p><p><strong>参数如何动态调整？</strong></p><p>我们会将模型中所有的参数（词向量们，超参数们）放进一个大向量θ里。这个向量里有V个单词，每个单词是d维的向量，每个单词按照中心词汇和上下文词汇分别出现一次。所以向量的总长度是2dV。θ也就是词汇表。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200514_1554_36_528.png" alt=""></p><h4 id="最小化目标函数的简单数学推演"><a href="#最小化目标函数的简单数学推演" class="headerlink" title="最小化目标函数的简单数学推演"></a>最小化目标函数的简单数学推演</h4><p>何求这个代价函数的最值？——对Vc（中心向量）求导，如下：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200514_1601_17_279.png" alt=""></p><p>外边套了个log，分数就可以改成减法，分成减号的左右两项：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200514_1605_16_164.png" alt=""></p><p>自然指数的对数是可以消去的，所以左边项化为：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200514_1604_17_177.png" alt=""></p><p>右边项：复合函数注意链式求导法则</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200514_1606_57_996.png" alt=""></p><p> 最终：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200514_1613_56_501.png" alt=""></p><p>U0是实际输出的上下文词汇，Ux是期望向量。</p><h3 id="6-Skip-gram模型的优化"><a href="#6-Skip-gram模型的优化" class="headerlink" title="6.Skip-gram模型的优化"></a>6.Skip-gram模型的优化</h3><p>采用随机梯度下降法（SGD）优化，在语料库中移动窗口时， 我们会在每个窗口更新损失函数，损失函数中的分母不用遍历所有的语料库，随机抽取一部分遍历就好，节省了大量的资源，效果也不差。。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1414_44_200.png" alt=""></p><p>算法的计算过程：</p><p>分子比较容易计算，只是两个百维向量相乘。分母则比较困难，因为在每个窗口，都要遍历一次完整的语料库，uw是要遍历整个物料库的向量，也许词汇表中有20000个词汇，也就是说分母要做20000次内积运算，这非常低效。</p><p>但实际上不需要教模型这么多，在每个窗口中，deep learning和learning和zebra并不同时出现，也不和aardvark一起出现，不和其他20000个单词同时出现.所以使用SGD的skip-gram模型主要思想其实是一个小技巧，我们只对可以配对的单词pair训练一些二元逻辑回归，因此我们保留了想要优化和最大化中心词和外围词的内积的想法。 相对于遍历所有单词，实际上支取一些随机单词并指明， 这些从语料库其余部分取出的随机词是不同时出现的，这就得到了使用SGD的skip-gram算法的原始目标函数，通常这被称为一个软件包Word2Vec，初次讲解该方法的论文标题就叫做单词和短语的分布式表示以及他们的语义合成性。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1414_53_123.png" alt=""></p><p>基本上需要再遍历每个窗口，因此这里的T需要遍历语料库中的每个窗口。加号左边第一项只是中心词和外围词同时出现的概率的对数（两词内积又被钟形函数压缩，再被求对数），第二项是负的，表明我们将从语料库中随机抽取几个单词，对每个单词，我们会尽量减少他们共同出现的概率（这里又引出关于目标函数的意义：所谓目标函数，就是要以本函数值最大化为目标，正数项要尽可能大，负数项要尽可能小）。</p><p>so，我们对语料库中随机抽取的单词做重采样，而不是遍历所有不同的单词，然后认为aardvark没有和learning一起出现等等。我们仅取出五个十个或者其他数目的随机词，然后最小化他们的概率，并且通常我们把它当做一个超参数，一个需要评估重要性的参数，我们将为目标函数的第二部分，为每个窗口，取k个负样本 ，然后最小化这些随机词出现在中心词附近的概率，接着从简单的均匀分布或者一元模型分布（unigram distribution）中采样获取，观测这些词大致出现的频率，然后基于频率去采样，但是我们也取其指数为3/4的值，这是一种简单的术语，如果你使用这个模型有一段时间了，也许你觉得它应该经常采用一些稀有的词，不然他将会经常采样THE和A以及其他stop word，在我们的语料库中对aardvark和zebra进行采样，因此取这个指数为3/4的值。</p><p>通常在遍历整个语料库时，可以对每个窗口做一次更新，但也可以是先走过五个窗口，收集好梯度更新，然后再对随机梯度下降的小批量数据进行进一步的操作。</p><p>Jt(θ),在模型中θ通常作为一个用于所有变量的参数，因此在skip-gram模型的例子中，它基本上就是所有的u向量和v向量。然后当我们调用时，我们就调用theta，它可能有神经网络，层等其他参数，J是我们的损失函数，T是遍历语料库的第T个时间步或者T个窗口，最后我们优化的整体目标函数，实际是这些目标函数的总和，再一次声明，我们不想对整个语料库做一次大的梯度更新，我们不想经过所有窗口，收集梯度更新然后做一次大的梯度更新，因为这样通常效果不怎么好。通常我们想通过中心词周围的词的加和来预测中心词。</p><p>word2vec将语义近似的词语放在向量空间中的近处。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1415_03_036.png" alt=""></p><p>训练词向量时，我们会优化目标函数，并且求梯度。词向量们会聚类，图上图中左侧，“周二周三”和“工作日周末日”聚集在一起，图中左上“数字项”聚集在一起，图中右上“名字项”聚集在一起等等。</p>]]></content>
    
    
    
    <tags>
      
      <tag>词向量</tag>
      
      <tag>SkipGram</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.文章/NLP_word2vec应用</title>
    <link href="undefined2020/05/13/0.%E6%96%87%E7%AB%A0/NLP_word2vec%E5%BA%94%E7%94%A8/"/>
    <url>2020/05/13/0.%E6%96%87%E7%AB%A0/NLP_word2vec%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="word2vec应用"><a href="#word2vec应用" class="headerlink" title="word2vec应用"></a>word2vec应用</h1><h4 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h4><pre><code class="python">import pandas as pd# Read data from files train = pd.read_csv( &quot;labeledTrainData.tsv&quot;, header=0,  delimiter=&quot;\t&quot;, quoting=3 )test = pd.read_csv( &quot;testData.tsv&quot;, header=0, delimiter=&quot;\t&quot;, quoting=3 )unlabeled_train = pd.read_csv( &quot;unlabeledTrainData.tsv&quot;, header=0,  delimiter=&quot;\t&quot;, quoting=3 )# Verify the number of reviews that were read (100,000 in total)print (&quot;Read %d labeled train reviews, %d labeled test reviews, &quot; \ &quot;and %d unlabeled reviews\n&quot; % (train[&quot;review&quot;].size,   test[&quot;review&quot;].size, unlabeled_train[&quot;review&quot;].size ))</code></pre><h4 id="清洗数据"><a href="#清洗数据" class="headerlink" title="清洗数据"></a>清洗数据</h4><p>和词袋模型不同，word2vec不去除停止词。因为word2vec需要尽可能宽泛的语料来输出更高质量的向量。</p><p>review_to_wordlist函数如名所示，返回的是包含了清洗好的word的list</p><pre><code class="python"># Import various modules for string cleaningfrom bs4 import BeautifulSoupimport refrom nltk.corpus import stopwords# 注意下一行中参数，stopword这一个参数被设置为false。def review_to_wordlist( review, remove_stopwords=False ):    # Function to convert a document to a sequence of words,    # optionally removing stop words.  Returns a list of words.    #    # 1. Remove HTML    review_text = BeautifulSoup(review).get_text()    #      # 2. Remove non-letters    review_text = re.sub(&quot;[^a-zA-Z]&quot;,&quot; &quot;, review_text)    #    # 3. Convert words to lower case and split them    words = review_text.lower().split()    #    # 4. Optionally remove stop words (false by default)    if remove_stopwords:        stops = set(stopwords.words(&quot;english&quot;))        words = [w for w in words if not w in stops]    #    # 5. Return a list of words    return(words)</code></pre><h4 id="将如上函数的输出拼接为句子"><a href="#将如上函数的输出拼接为句子" class="headerlink" title="将如上函数的输出拼接为句子"></a>将如上函数的输出拼接为句子</h4><pre><code class="python"># Download the punkt tokenizer for sentence splittingimport nltk.data# nltk.download()   # Load the punkt tokenizertokenizer = nltk.data.load(&#39;tokenizers/punkt/english.pickle&#39;)# Define a function to split a review into parsed sentencesdef review_to_sentences( review, tokenizer, remove_stopwords=False ):    # Function to split a review into parsed sentences. Returns a     # list of sentences, where each sentence is a list of words    #    # 1. Use the NLTK tokenizer to split the paragraph into sentences    raw_sentences = tokenizer.tokenize(review.strip())    #    # 2. Loop over each sentence    sentences = []    for raw_sentence in raw_sentences:        # If a sentence is empty, skip it        if len(raw_sentence) &gt; 0:            # Otherwise, call review_to_wordlist to get a list of words            sentences.append( review_to_wordlist( raw_sentence, \              remove_stopwords ))    #    # Return the list of sentences (each sentence is a list of words,    # so this returns a list of lists    return sentences</code></pre><h4 id="将句子拼接为语料库"><a href="#将句子拼接为语料库" class="headerlink" title="将句子拼接为语料库"></a>将句子拼接为语料库</h4><pre><code class="python">sentences = []  # Initialize an empty list of sentencesprint (&quot;Parsing sentences from training set&quot;)for review in train[&quot;review&quot;]:    sentences += review_to_sentences(review, tokenizer)print (&quot;Parsing sentences from unlabeled set&quot;)for review in unlabeled_train[&quot;review&quot;]:    sentences += review_to_sentences(review, tokenizer)</code></pre><h4 id="检查语料库"><a href="#检查语料库" class="headerlink" title="检查语料库"></a>检查语料库</h4><pre><code class="python">print (len(sentences))print (sentences[0])print (sentences[1])</code></pre><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>word2vec模型参数设置：</p><ul><li>架构：分别是skip-gram和continuous bag of word。</li><li>训练算法：Hierarchical softmax(default)和negative sampling</li><li>常用词的下采样：谷歌文档建议是在0.00001到0.001之间，本例在0.001效果最好</li><li>词向量的维度：并不是越多越好，而且维度和训练时间正相关。一般在10到1000之间，本例用300</li><li><strong>Context / window size</strong> : 训练算法应该考虑多少个上下文词汇？本例子用Hierarchical softmax算法，10个就够用了。一般在临界点之前，越多效果越好。</li><li><strong>Worker threads</strong>: Number of parallel processes to run. This is computer-specific, but between 4 and 6 should work on most systems.我的电脑是4核的，应该设置为4.</li><li><strong>Minimum word count</strong>: 只有大于这个出现率大于这个频率的词汇才会被录入词汇表（有些太不常见的词汇可以忽略掉以控制词汇表规模）。一般合理的数值在10到100之间。本例因为电影都重复出现了30次，所以设置为40.</li></ul><p>开始训练</p><pre><code class="python"># Import the built-in logging module and configure it so that Word2Vec # creates nice output messagesimport logginglogging.basicConfig(format=&#39;%(asctime)s : %(levelname)s : %(message)s&#39;,\    level=logging.INFO)# Set values for various parametersnum_features = 300    # Word vector dimensionality                      min_word_count = 40   # Minimum word count                        num_workers = 4       # Number of threads to run in parallelcontext = 10          # Context window size                                                                                    downsampling = 1e-3   # Downsample setting for frequent words# Initialize and train the model (this will take some time)from gensim.models import word2vecprint (&quot;Training model...&quot;)model = word2vec.Word2Vec(sentences, workers=num_workers, \            size=num_features, min_count = min_word_count, \            window = context, sample = downsampling)# If you don&#39;t plan to train the model any further, calling # init_sims will make the model much more memory-efficient.model.init_sims(replace=True)# It can be helpful to create a meaningful model name and # save the model for later use. You can load it later using Word2Vec.load()model_name = &quot;300features_40minwords_10context&quot;model.save(model_name)</code></pre>]]></content>
    
    
    <categories>
      
      <category>文章</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Word2vec</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_神经网络和反向传播算法</title>
    <link href="undefined2020/05/12/0.%E6%A6%82%E5%BF%B5/NLP_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/"/>
    <url>2020/05/12/0.%E6%A6%82%E5%BF%B5/NLP_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><p>神经网络深入学习的网站：neuralnetworksanddeeplearning.com </p><p>神经网络就是一个函数，本例中有764个输入，有10个输出 </p><h4 id="总览图"><a href="#总览图" class="headerlink" title="总览图:"></a>总览图:</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200512_1420_29_240.png" alt=""></p><h4 id="第一层-764-："><a href="#第一层-764-：" class="headerlink" title="第一层(764)："></a>第一层(764)：</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200512_1421_14_257.png" alt=""></p><p>764个输入源于28 * 28的正方形格子，每个小格子都是0到1之间的值，越接近1就越白，越接近0就越黑。</p><h4 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200512_1427_10_637.png" alt=""></p><p>隐藏层是干什么的？ 大体上说，隐藏层用于发掘模式。如上所示，隐藏层一识别小线段，隐藏层二识别小线段构成层的圈或者大线段</p><p><strong>从这里看出深度神经网络其实就是模式识别的高级应用，所以要恶补pattern recognition。</strong></p><h4 id="如何训练神经网络？"><a href="#如何训练神经网络？" class="headerlink" title="如何训练神经网络？"></a>如何训练神经网络？</h4><p> 训练神经网络就是把训练集中的数据和label在神经网络中的(784 * 16 + 16 * 16 + 16 * 10) + (16+16+10) =13002    个参数通过动态调整来拟合。</p><p>从数学上来说，动态调整13002个参数以获得min(代价函数)的做法，本质上就是寻找函数的最小值，如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200512_1435_08_891.png" alt=""></p><p>w1和a1： </p><p>a1就是输入的784个点中第一个点的亮暗情况</p><p>w1就是接受这第一个点a1刺激的第一层的第一个神经元对第二层的如图所示的神经元的兴奋程度（越兴奋，第二层的这个接受的神经元越倾向于也兴奋）</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200512_1441_49_301.png" alt=""></p><p> 成本函数/代价函数/cost function</p><p> 上文提到，训练神经网络的目的就是为了最小化成本函数。那么成本函数是什么？</p><p>成本函数是神经网络的所有预测结果和训练集中所有正确结果之间的各误差平方再sum。</p><p> 于是once again，优化代价函数的具体做法就是对这13002个参数组成的函数做梯度下降法（with随机初始化）</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200512_1449_59_022.png" alt=""></p><h4 id="BP反向传播算法"><a href="#BP反向传播算法" class="headerlink" title="BP反向传播算法"></a>BP反向传播算法</h4><p>反向传播算法示例：</p><p>1.找出输出层问题中当下最需要调整的值</p><p> 如下的神经网络在训练过程中，13002个参数正在调整。正确结果是2，预测结果1/3/6并不正确，所以要调整参本例当务之急是增大2的预测值，而不是例如“减小8的预测值”，因为2非常错误，8的预测值接近0不那么错误。 </p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200512_1531_40_013.png" alt=""></p><p>2.如何调整输出值</p><p>接上文，当下最需要调整的是输出单元2的输出值，输出单元2的输出构成如下图所示，由上一层中若干个输入值和权重值经过挤压函数决定。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200512_1544_02_132.png" alt=""></p><p>调整输出值无外乎三条路可走，一是增大偏置值b，二是增大权重w，三是增大输入a。</p><p>偏置和输入都是固定值很好理解，如何最有效的通过调整w增大输出结果？</p><p>——应该增加连接最亮的那个点的权重值，因为显然改变亮的w比改变暗淡的w的效果要好上几倍</p><p> 如此，我们知道了如果想要提高输出层2号节点的输出信号，该让倒数第二层的神经元们如何改变。 </p><h4 id="反向传播的实现"><a href="#反向传播的实现" class="headerlink" title="反向传播的实现"></a>反向传播的实现</h4><p>承上，知道了提高输出层的2号节点该如何做，那么加上<strong>输出层0,1,3,4,5,6,7,8,9节点</strong>的诉求，倒数第二层该如何改变？如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200512_1553_24_709.png" alt=""></p><h4 id="改变参数的性价比"><a href="#改变参数的性价比" class="headerlink" title="改变参数的性价比"></a>改变参数的性价比</h4><p>BP算法中，不该只看每个参数该增大还是减小，还应该看改变哪个参数的性价比最高。</p><p>从生物学的角度讲，权重的最大增长也就是连接变的更强的部分，会发生在已经最活跃的神经元和想要更多激发的神经元之间。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200512_1547_47_429.png" alt=""></p><p>每次输入一个train data，反向传播的详细改变（从左往右逐列进行）</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200512_1618_47_643.png" alt=""></p><h4 id="在反向传播过程中的一个小技巧：mini-batch"><a href="#在反向传播过程中的一个小技巧：mini-batch" class="headerlink" title="在反向传播过程中的一个小技巧：mini batch"></a>在反向传播过程中的一个小技巧：mini batch</h4><p>如果在梯度下降中把所有的样本都用上，那么花的时间就太长了。所以可以把训练样本都打乱，然后分成若干组mini batch，例如本来train set是10000条数据，每个mini batch都是100个。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200512_1623_28_286.png" alt=""></p><p>这样计算的梯度不是代价函数真实的梯度，毕竟真实的梯度是所有的训练集算出来的，这也不是下降最快的方向，但是这样计算量会下降非常多。</p><p>如下图对比，左边是用上所有的train data，下降的精确但是非常慢，右边是mini batch也成为<strong>随机梯度下降</strong>，下降的很快但是未必精准。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200512_1625_46_277.jpg" alt=""></p><h1 id="反向传播的老版笔记"><a href="#反向传播的老版笔记" class="headerlink" title="反向传播的老版笔记"></a>反向传播的老版笔记</h1><ul><li><p>定义</p><ul><li>反向传播算法是利用链式法则递归计算表达式梯度的方法</li></ul></li><li><p>理解</p><ul><li><p>反向传播就是先正向走一遍,得到各个节点的计算结果</p></li><li><p>从最后一个节点开始,设置梯度是1</p></li><li><p>反向计算各个上层节点对他的局部梯度</p></li><li><p>[dz/dx ]  = [dz/dy]   *  [dy/dx]</p><p>[最终结果与本层的梯度]  =上层梯度*本层局部梯度</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1447_31_252.png" alt=""></p></li></ul></li></ul><ul><li><p>目的</p><ul><li>首先随机初始化[代价函数]中各个权重值</li><li>正向计算</li><li>反向传播算法,检查各个权重值的梯度</li><li>使用梯度下降或其他高级优化算法,改变权重值来最小化代价函数</li></ul></li><li><p>过程</p><ul><li>首先x输入是2维的， w是三维的，多了一个偏倚项。上边的过程图是对这个函数计算过程的最详细描述。</li></ul></li><li><p>前向传播和反向传播之间的关系是，前向传播就是计算你在测试时需要的结果，计算你的函数的最终输出。</p><p>当你计算这个单词位置的概率，你做的就是前向传播。</p><p>对于反向传播，就是要用数据集训练模型，要更新模型的时候计算所需的梯度。</p></li></ul><h4 id="cs231n举例"><a href="#cs231n举例" class="headerlink" title="cs231n举例"></a>cs231n举例</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1447_53_986.png" alt=""></p><p>如图所示，f对x，f对y的导数，分别就是f对q的导数乘以q对x的导数，和f对q的导数乘以q对y的导数（链式求导法则）。</p><p>然后逐层求导，乘以链式法则，完事儿。</p>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>神经网络</tag>
      
      <tag>反向传播算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_随机森林</title>
    <link href="undefined2020/05/12/0.%E6%A6%82%E5%BF%B5/NLP_%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"/>
    <url>2020/05/12/0.%E6%A6%82%E5%BF%B5/NLP_%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/</url>
    
    <content type="html"><![CDATA[<h1 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h1><p>随机森林就是用随机的方法建立一堆决策树。每棵树之间是没有关联的，当一个待分类样本输入森林中时，将此样本输入进每一个决策树然后看哪种分类结果最多，就最终输出哪种分类结果。</p><p>随机森林的构造过程：</p><ul><li>假设有N个样本，有放回的随机选择N个样本，然后按照这N个样本训练一个决策树，这N个样本就是决策树根节点的样本</li><li>当每个样本有M个属性时，就需要在决策树中分裂节点以做进一步的判断。于是从M中随机选出m个(m &lt;&lt; M)个属性，然后从m个属性中采取某种策略（比如min信息熵）来选择一个属性来作为该节点的分裂属性。</li><li>然后就是重复上边的步骤。当下一次分裂选出来的属性还是父节点的属性时，说明已经穷尽属性，到达叶子节点。</li></ul><p>按照如上三个步骤，可以构造大量决策树，例如movie sentiment tutor step1中的随机森林就是构造了100颗决策树。</p><p>注意：<strong>随机森林中的决策树并没有为了回避过拟合问题而剪枝</strong>，因为随机采样已经避免了过拟合。</p><p>此算法优点：</p><ol><li>数据上表现好，两个随机性的引入避免了过拟合问题</li><li>抗噪声</li><li>能处理高维度数据，而且不用做特征选择。</li><li>对数据集的适应能力强，既能处理连续性数据，也能处理离散型数据。</li><li>可以生成一个Pij矩阵，用于度量样本之间的相似性。</li><li>在创造随机森林的过程中，对generlization error采用的是无偏估计</li><li>训练速度快</li><li>在训练过程中，能检测到feature间的互相影响。</li></ol>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随机森林</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_决策树</title>
    <link href="undefined2020/05/12/0.%E6%A6%82%E5%BF%B5/NLP_%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <url>2020/05/12/0.%E6%A6%82%E5%BF%B5/NLP_%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    
    <content type="html"><![CDATA[<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><p>决策树是一种树形结构分类器。每个节点都有一个表示属性的判断，每个分支都代表了一个判断结果的输出。</p><p>常见的决策树生成算法有ID3,C4.5,C5.0,CART等。</p><p>示例：如下图片是正确的分类集（train set）</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200512_1127_57_754.png" alt=""></p><p>如下是分类树的判断过程（演示了分类树其实可以有不同构型）</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200512_1134_06_590.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200512_1134_23_636.png" alt=""></p><p>如上分类树的根节点都是判断分数是否大于A（90），如果是就是好学生。然后第二个节点就出现了不同，分别是判断分数是否小于70 OR 出勤率是否大于0.8。</p><p>这个分类器节点的构造其实很灵活，怎样构造最好的分类器呢？<strong>ID3,ID4.5算法</strong>都是倾向于迭代选择熵值最小化的那个分类器。但是<strong>ID3,ID4.5算法</strong>都有严重的过拟合问题。</p><p><strong>CART</strong>算法也使用了类似熵的指数–GINI指数，CART算法迭代的目标也是向着GINI指数最小化的方向进行，不过CART算法为了解决过拟合的问题，使用了剪枝的策略，也就是将树中过长的枝叶剪去。</p>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>决策树</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_词向量_词袋BOW</title>
    <link href="undefined2020/05/12/0.%E6%A6%82%E5%BF%B5/NLP_%E8%AF%8D%E5%90%91%E9%87%8F_%E8%AF%8D%E8%A2%8BBOW/"/>
    <url>2020/05/12/0.%E6%A6%82%E5%BF%B5/NLP_%E8%AF%8D%E5%90%91%E9%87%8F_%E8%AF%8D%E8%A2%8BBOW/</url>
    
    <content type="html"><![CDATA[<h1 id="词袋算法详解"><a href="#词袋算法详解" class="headerlink" title="词袋算法详解"></a>词袋算法详解</h1><p>词袋算法是word2vec中构建两大构建词向量算法中的一个。</p><h4 id="kaggle-tutor实战解读："><a href="#kaggle-tutor实战解读：" class="headerlink" title="kaggle tutor实战解读："></a>kaggle tutor实战解读：</h4><p>具体操作如下：</p><ul><li>清洗整个语料库by美丽汤</li><li>去掉停止词和非英语单词，去掉数组，大小写统一by正则表达式</li><li>将剩下的单词组成词典</li><li>词典按照词频降序排列</li><li>然后指定词袋的feature个数，以五千为例就是前5000个常用单词作为每一个词语/句子的向量表示。<ul><li>每个向量都是5000维，第一维就是最常用的单词，第二维就是第二常用的单词，第一维是1说明被本向量表示的句子中最常用的单词出现了一次，第一维是2说明被表示的句子中最常用的单词出现了两次。</li></ul></li><li>然后可以将例如25000条语料依次分别按照5000维的向量表示，集合到shape(25000 * 5000)的矩阵中</li></ul><p>然后此矩阵带上label就可以用来训练model了</p><h4 id="CS224N解读"><a href="#CS224N解读" class="headerlink" title="CS224N解读"></a>CS224N解读</h4><p>词袋的训练模型如下。</p><ol><li><p>输入层：上下文的独热码，假设向量空间V维，上下文单词个数是C。</p></li><li><p>所有onehot分别乘以共享的W（V*N）初始化权重矩阵，V是向量维度，N自己设置。</p></li><li><p>因为是onehot向量，只有一个值为1，其他都是0，所以乘以权重矩阵后仍然是向量，然后将所有这些向量相加，得到隐藏层向量hi。</p><p>x1–&gt;h1,x2–&gt;h2,x3–&gt;h3，…………，xn–&gt;hn.</p><p>sum{h1 + h2 + h3 + …… + hn} = hi</p></li><li><p>hi向量乘以W`(N<em>V)输出权重矩阵，得到向量（1\</em>V）,激活函数比如softmax处理后得到V维概率分布，其中最大的index所指示的单词就是预测的中间词，然后与训练集中打上了标签独热码作比较，误差越小越好，根据误差动态更新权重矩阵。</p></li></ol><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191215_1015_20_195.png" alt=""></p><p><strong>例子</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191215_1038_40_778.png" alt=""></p><p>coffee前后的词是输入对象，coffee作为训练集的正确label项，在结果处等待与训练结果对比，以动态调整W。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191215_1040_13_293.png" alt=""></p><p>达到迭代次数后，训练出来的look up table应该就是W矩阵，任何一个单词的独热码乘以它都应该得到自己的word embedding。</p>]]></content>
    
    
    
    <tags>
      
      <tag>词向量</tag>
      
      <tag>词袋</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.文章/NLP_词袋模型应用</title>
    <link href="undefined2020/05/08/0.%E6%96%87%E7%AB%A0/NLP_%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/"/>
    <url>2020/05/08/0.%E6%96%87%E7%AB%A0/NLP_%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="词袋模型的应用"><a href="#词袋模型的应用" class="headerlink" title="词袋模型的应用"></a>词袋模型的应用</h1><p>1.引入训练数据</p><pre><code class="python">import pandas as pdtrain = pd.read_csv(&quot;labeledTrainData.tsv&quot;, header=0,delimiter=&quot;\t&quot;, quoting=3)</code></pre><p>2.确定引入数据是否正确 – 看一下shape和列标签</p><pre><code class="python">&gt;&gt;&gt; train.shape(25000, 3)&gt;&gt;&gt; train.columns.valuesarray([id, sentiment, review], dtype=object)</code></pre><p>3.glipise</p><pre><code class="python">print train[&quot;review&quot;][0]</code></pre><p>4.用beautiful soap清洗掉HTML标签</p><pre><code class="python"># Import BeautifulSoup into your workspacefrom bs4 import BeautifulSoup             # Initialize the BeautifulSoup object on a single movie review     example1 = BeautifulSoup(train[&quot;review&quot;][0])  # Print the raw review and then the output of get_text(), for # comparisonprint train[&quot;review&quot;][0]print example1.get_text()</code></pre><p>对比图如下，beautifulsoup确实清理了HTML标签</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200510_1531_48_363.png" alt=""></p><p>5.正则表达式清洗所有非英文单词</p><pre><code class="python">import re# Use regular expressions to do a find-and-replaceletters_only = re.sub(&quot;[^a-zA-Z]&quot;,           # The pattern to search for                      &quot; &quot;,                   # The pattern to replace it with                      example1.get_text() )  # The text to searchprint letters_only</code></pre><p>如下截图，正则表达式把所有的标点和数字都替换成了空格。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200510_1536_13_005.png" alt=""></p><p>6.大小写统一，句子分割成单词</p><pre><code class="python">lower_case = letters_only.lower()        # Convert to lower casewords = lower_case.split()               # Split into words</code></pre><p>效果如下截图，已经区分了大小写，并分隔开为各个单词</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200510_1539_03_534.png" alt=""></p><p>7.用 nltk 去除停止词</p><pre><code class="python">import nltknltk.download()  # Download text data sets, including stop wordsfrom nltk.corpus import stopwords # Import the stop word listprint stopwords.words(&quot;english&quot;) # Remove stop words from &quot;words&quot;words = [w for w in words if not w in stopwords.words(&quot;english&quot;)]print words&gt;&gt;&gt;····[u&#39;stuff&#39;, u&#39;going&#39;, u&#39;moment&#39;, u&#39;mj&#39;, u&#39;ve&#39;, u&#39;started&#39;, u&#39;listening&#39;, u&#39;music&#39;, u&#39;watching&#39;, u&#39;odd&#39;, u&#39;documentary&#39;, u&#39;watched&#39;, u&#39;wiz&#39;, u&#39;watched&#39;, u&#39;moonwalker&#39;, u&#39;maybe&#39;, u&#39;want&#39;, u&#39;get&#39;, u&#39;certain&#39;, u&#39;insight&#39;, u&#39;guy&#39;, u&#39;thought&#39;, u&#39;really&#39;, u&#39;cool&#39;, u&#39;eighties&#39;, u&#39;maybe&#39;, u&#39;make&#39;, u&#39;mind&#39;, u&#39;whether&#39;, u&#39;guilty&#39;, u&#39;innocent&#39;, u&#39;moonwalker&#39;, u&#39;part&#39;, u&#39;biography&#39;, u&#39;part&#39;, u&#39;feature&#39;, u&#39;film&#39;, u&#39;remember&#39;, u&#39;going&#39;, u&#39;see&#39;, u&#39;cinema&#39;, u&#39;originally&#39;, u&#39;released&#39;, u&#39;subtle&#39;, u&#39;messages&#39;, u&#39;mj&#39;, u&#39;feeling&#39;, u&#39;towards&#39;, u&#39;press&#39;, u&#39;also&#39;, u&#39;obvious&#39;, u&#39;message&#39;, u&#39;drugs&#39;, u&#39;bad&#39;, u&#39;m&#39;, u&#39;kay&#39;,.....]</code></pre><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200511_1329_57_787.png" alt=""></p><p>8.至此，完成了对一条数据的清洗工作。把上述功能整理到下边的函数中，后边被其他数据复用</p><pre><code class="python">def review_to_words( raw_review ):    # Function to convert a raw review to a string of words    # The input is a single string (a raw movie review), and     # the output is a single string (a preprocessed movie review)    #    # 1. Remove HTML    review_text = BeautifulSoup(raw_review).get_text()     #    # 2. Remove non-letters            letters_only = re.sub(&quot;[^a-zA-Z]&quot;, &quot; &quot;, review_text)     #    # 3. Convert to lower case, split into individual words    words = letters_only.lower().split()                                 #    # 4. In Python, searching a set is much faster than searching    #   a list, so convert the stop words to a set    stops = set(stopwords.words(&quot;english&quot;))                      #     # 5. Remove stop words    meaningful_words = [w for w in words if not w in stops]       #    # 6. Join the words back into one string separated by space,     # and return the result.    return( &quot; &quot;.join( meaningful_words ))   </code></pre><p>9.清洗剩下的数据</p><pre><code class="python"># testclean_review = review_to_words( train[&quot;review&quot;][0] )print clean_review# 查看一共要清理多少数据num_reviews = train[&quot;review&quot;].size# 带进度条的数据清洗print &quot;Cleaning and parsing the training set movie reviews...\n&quot;clean_train_reviews = []for i in xrange( 0, num_reviews ):    # If the index is evenly divisible by 1000, print a message    if( (i+1)%1000 == 0 ):        print &quot;Review %d of %d\n&quot; % ( i+1, num_reviews )                                                                        clean_train_reviews.append( review_to_words( train[&quot;review&quot;][i] ))</code></pre><p>10.用词袋模型构建征</p><p>简单例子演：</p><ul><li>输入如下两个句子：</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200510_1510_20_882.png" alt=""></p><ul><li>两个句子组成的词汇表如下：</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200510_1511_31_799.png" alt=""></p><ul><li>两个句子用词汇表表示的向量如下：</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200510_1512_15_583.png" alt=""></p><p>如下是复杂例子，步骤严格相同。把输入的25000个句子打散，组成词汇表（按照词频降序排列），然后每一句都按照词汇表的前五千个词汇向量表示（max_features = 5000）</p><pre><code class="python">print &quot;Creating the bag of words...\n&quot;from sklearn.feature_extraction.text import CountVectorizer# Initialize the &quot;CountVectorizer&quot; object, which is scikit-learn&#39;s# bag of words tool.  vectorizer = CountVectorizer(analyzer = &quot;word&quot;,   \                             tokenizer = None,    \                             preprocessor = None, \                             stop_words = None,   \                             max_features = 5000) # fit_transform() does two functions: First, it fits the model# and learns the vocabulary; second, it transforms our training data# into feature vectors. The input to fit_transform should be a list of # strings.train_data_features = vectorizer.fit_transform(clean_train_reviews)# Numpy arrays are easy to work with, so convert the result to an # arraytrain_data_features = train_data_features.toarray()</code></pre><p>查看特征数组第一行的结果：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200511_1419_47_515.png" alt=""></p><p>11.观察词典 - feature表的形状，观察词汇表，观察各个词汇出现的频次</p><pre><code class="python">&gt;&gt;&gt; print (train_data_features.shape)(25000, 5000)# Take a look at the words in the vocabularyvocab = vectorizer.get_feature_names()print (vocab)import numpy as np# Sum up the counts of each vocabulary worddist = np.sum(train_data_features, axis=0)# For each, print the vocabulary word and the number of times it # appears in the training setfor tag, count in zip(vocab, dist):    print (count, tag)</code></pre><p>词汇表</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200511_1423_46_822.png" alt=""></p><p> 词汇表中各词在训练集中使用频率</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200511_1424_03_671.png" alt=""></p><h3 id="至此，已经把训练集中的所有单词都打散和清洗完毕，构建了词汇表，并将所有词汇都根据词汇表中最常用五千词的特征表示为五千维的向量。"><a href="#至此，已经把训练集中的所有单词都打散和清洗完毕，构建了词汇表，并将所有词汇都根据词汇表中最常用五千词的特征表示为五千维的向量。" class="headerlink" title="至此，已经把训练集中的所有单词都打散和清洗完毕，构建了词汇表，并将所有词汇都根据词汇表中最常用五千词的特征表示为五千维的向量。"></a>至此，已经把训练集中的所有单词都打散和清洗完毕，构建了词汇表，并将所有词汇都根据词汇表中最常用五千词的特征表示为五千维的向量。</h3><p>12.使用随机森林有监督学习向量化的train set，得到训练好的模型“forest”</p><pre><code class="python">print (&quot;Training the random forest...&quot;)from sklearn.ensemble import RandomForestClassifier# Initialize a Random Forest classifier with 100 treesforest = RandomForestClassifier(n_estimators = 100) # 设置随机森林的区分器是100个# Fit the forest to the training set, using the bag of words as # features and the sentiment labels as the response variable## This may take a few minutes to runforest = forest.fit( train_data_features, train[&quot;sentiment&quot;] )#                     我是向量化的评论         我是情绪的正确结果</code></pre><p>13.提交结果</p><p>和上边的步骤一样，把test集用pd.read读入，清洗数据，构建词汇表，转换成array，然后套进forest，得到输出。</p><pre><code class="python"># Read the test datatest = pd.read_csv(&quot;testData.tsv&quot;, header=0, delimiter=&quot;\t&quot;, \                   quoting=3 )# Verify that there are 25,000 rows and 2 columnsprint (test.shape)# Create an empty list and append the clean reviews one by onenum_reviews = len(test[&quot;review&quot;])clean_test_reviews = [] print (&quot;Cleaning and parsing the test set movie reviews...\n&quot;)for i in range(0,num_reviews):    if( (i+1) % 1000 == 0 ):        print (&quot;Review %d of %d\n&quot; % (i+1, num_reviews))    clean_review = review_to_words( test[&quot;review&quot;][i] )    clean_test_reviews.append( clean_review )# Get a bag of words for the test set, and convert to a numpy arraytest_data_features = vectorizer.transform(clean_test_reviews)test_data_features = test_data_features.toarray()# Use the random forest to make sentiment label predictionsresult = forest.predict(test_data_features)# Copy the results to a pandas dataframe with an &quot;id&quot; column and# a &quot;sentiment&quot; columnoutput = pd.DataFrame( data={&quot;id&quot;:test[&quot;id&quot;], &quot;sentiment&quot;:result} )# Use pandas to write the comma-separated output fileoutput.to_csv( &quot;Bag_of_Words_model.csv&quot;, index=False, quoting=3 )</code></pre><pre><code>for i in range( 0, num_reviews ):    clean_train_reviews.append( review_to_words( train[&quot;review&quot;][i] ) )</code></pre>]]></content>
    
    
    <categories>
      
      <category>文章</category>
      
    </categories>
    
    
    <tags>
      
      <tag>词袋模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.文章/NLP_pre-trained-LM发展史WE2Bert</title>
    <link href="undefined2020/05/07/0.%E6%96%87%E7%AB%A0/NLP_pre-trained-LM%E5%8F%91%E5%B1%95%E5%8F%B2WE2Bert/"/>
    <url>2020/05/07/0.%E6%96%87%E7%AB%A0/NLP_pre-trained-LM%E5%8F%91%E5%B1%95%E5%8F%B2WE2Bert/</url>
    
    <content type="html"><![CDATA[<h1 id="从Word-embedding到Bert：A-brief-history-of-nlp-pre-train-model"><a href="#从Word-embedding到Bert：A-brief-history-of-nlp-pre-train-model" class="headerlink" title="从Word embedding到Bert：A brief history of nlp pre-train model"></a>从Word embedding到Bert：A brief history of nlp pre-train model</h1><h2 id="Word-embedding"><a href="#Word-embedding" class="headerlink" title="Word embedding"></a>Word embedding</h2><p><strong>LM</strong>做<strong>word embedding</strong> –&gt; 使用的工具是<strong>word2vec</strong> –&gt; <strong>word2vec</strong>的训练方法：<strong>CBOW</strong> 和 <strong>SKIP - GRAM</strong></p><p><strong>word embedding</strong>的问题：无法解决一词多义。</p><p>​                                                两种不同的上下文信息经过<strong>word2vec</strong>都会预测相同的单词bank，因为同一个单词占                                                用的是同一行的参数空间。所以WE无法区分一词多义。</p><h2 id="ELMO"><a href="#ELMO" class="headerlink" title="ELMO"></a>ELMO</h2><p>为解决WE一词多义问题，提出ELMO，其思路是事先用LM学好 一个单词的word embedding，后边根据上下文动态调整。WE是静态，ELMO是动态，所以ELMO可以解决一词多义问题。</p><p>ELMO工作方式分为两段，分别是<strong>利用语言模型进行预训练</strong>和做下游任务时<strong>将预训练网络中提取对应单词的网络的各层的word embedding作为新特征补充</strong>到下游任务中。</p><p>阶段一 ：<strong>预训练</strong></p><p>如图，网络结构是<strong>双层双向LSTM</strong>,训练任务是<strong>根据上下文正确预测中间的单词</strong>。下图左边用于输入context-before右边输入context-after。<br>通过大量语料做语言模型任务就能训练好这个网络。<br>训练好后，输入一个新的句子“Sen”，可得到关于它的三个embedding：<br>                                                                                底层的<strong>单词的word embedding</strong>，<br>                                                                                第一层双向LSTM中对应单词位置的embedding包含<strong>句法信息</strong><br>                                                                                第二层双向LSTM中对应单词位置的embedding包含<strong>语义信息</strong></p><p>也就是说ELMO不仅训练了单词的word embedding，还训练了双层双向的LSTM网络结构（这个包含了句法和语义信息的两层embedding是解决一词多义问题的重要推手）。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200507_1519_00_500.png" alt=""></p><p>阶段二 ：<strong>将阶段一训练的embedding作为新特征补充到下游任务中</strong></p><p>比如下游任务是QA，对于问句X，先把X丢进阶段一训练好的ELMO网络中，这样就可以得到关于X的三个embedding，然后给这三个embedding分别学习一个权重值，然后三个embedding加权求和成为一个embedding。然后将这个embedding作为X在自己任务中的那个网络的输入。以此，实现了对下游任务的特征补充。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200507_1519_22_092.png" alt=""></p><p>ELMO的问题：</p><p>1.LSTM抽取特征的能力要远远弱于Transformer</p><p>2.双向融合特征的拼接方式导致了特征融合能力偏弱</p><h2 id="GPT-Generative-pre-training"><a href="#GPT-Generative-pre-training" class="headerlink" title="GPT - Generative pre training"></a>GPT - Generative pre training</h2><p>GPT也是采用两段过程，第一阶段是语言模型预训练，第二阶段是使用fine - tuning解决下游任务。</p><p>这个模型和ELMO类似，只是把特征提取器从RNN换成了更强的Transformer，但是此模型的缺点是在训练的时候只考虑了上文而未考虑下文，败笔。</p><p>预训练：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200507_1522_49_144.png" alt=""></p><p>使用：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200507_1526_37_530.png" alt=""></p><p>GPT的缺点：单向语言模型。</p><h2 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h2><p>Bert的结构和GPT以及ELMO大同小异，都是两阶段模型：<strong>语言模型预训练</strong> &amp; <strong>Fine-tuning模式解决下游任务</strong>。</p><p>Bert比GPT强在使用了双向语言模型、语言模型的数据规模比较大。</p><p>两阶段工作图示：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200507_1530_10_632.png" alt=""></p><p>Bert改造四种下游任务：</p><ul><li>句子关系类（POS tagging，NER）：和GPT类似，每局加上一个起始和终结符号，句子间加上分隔符。输出的时候把第一个起始符号对应的Transformer最后一层位置上面串接一个softmax分类层即可。</li><li>分类问题（sentiment，文本分类）：和GPT一样，每一句增加起始和结束符号。把第一个起始符号对应的Transformer最后一层位置上面串接一个softmax分类层即可。</li><li>序列标注文图：输入部分和单句分类是一样的，只需要输出部分Transformer最后一层每个单词对应位置都进行分类即可。</li><li>生成类任务：bert暂无现成方案，但是理论上改装也不难，例如机翻或者QA任务，只要附着在S2S上，编解码器是深度Transformer即可。</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200507_1531_21_288.png" alt=""></p><h2 id="Bert-Word2vec-ELMO-GPT之间的关系"><a href="#Bert-Word2vec-ELMO-GPT之间的关系" class="headerlink" title="Bert,Word2vec,ELMO,GPT之间的关系"></a>Bert,Word2vec,ELMO,GPT之间的关系</h2><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200507_1539_47_580.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>文章</category>
      
    </categories>
    
    
    <tags>
      
      <tag>WordEmbedding</tag>
      
      <tag>Bert</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2.比赛/Toxic_思路</title>
    <link href="undefined2020/05/07/2.%E6%AF%94%E8%B5%9B/Toxic_%E6%80%9D%E8%B7%AF/"/>
    <url>2020/05/07/2.%E6%AF%94%E8%B5%9B/Toxic_%E6%80%9D%E8%B7%AF/</url>
    
    <content type="html"><![CDATA[<h1 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h1><h4 id="ToDoList"><a href="#ToDoList" class="headerlink" title="ToDoList"></a>ToDoList</h4><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>bpe</td><td>跨语言编码</td></tr><tr><td>XLM-R + ERNIE</td><td>kaggle最热跨语言模型 + 注入特定知识toxic</td></tr><tr><td>transformer</td><td><strong>特征抽取器(模型)要向匹配问题领域的特点去修改</strong></td></tr><tr><td>lstm</td><td>transformer组件</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h4 id="选择正确的特征抽取器"><a href="#选择正确的特征抽取器" class="headerlink" title="选择正确的特征抽取器"></a>选择正确的特征抽取器</h4><p>解决情感分类问题，从模型角度讲，最重要的是特征抽取器的能力。以前是研发人员设计抽取哪些特征，现在都是端到端的抽取，也就是<strong>特征抽取器Transformer自动抽取</strong>。根据今日阅读知乎和github上的观点，不要使用RNN和CNN，应该采用更加先进的Transformer。</p>]]></content>
    
    
    <categories>
      
      <category>2.比赛</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Toxic</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224n_9 - 机翻</title>
    <link href="undefined2020/05/07/3.%E8%AF%BE%E7%A8%8B/CS224n_9%20-%20%E6%9C%BA%E7%BF%BB/"/>
    <url>2020/05/07/3.%E8%AF%BE%E7%A8%8B/CS224n_9%20-%20%E6%9C%BA%E7%BF%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="机翻"><a href="#机翻" class="headerlink" title="机翻"></a>机翻</h1><p>对于机器翻译这种任务，你可以抛弃那些考虑规则的想法。情感分析类任务也许可以建立词汇的正面或者负面表格，然后根据这些词汇的词性进一步延伸出其他规则或系统，但是翻译类问题baseline一般都是基于统计的，因为没有人可以遍历一门语言翻译为另一门语言时所有的规则和异常语言。</p><p>我们总是倾向于使用相当庞大的语料库，我们通常称为平行语料库。在平行语料库中，很多相同的段落平行地以多种语言表达。最经典的平行语料库应该就是圣经了。</p><p><strong>基于统计的机翻模型介绍</strong></p><p>源语言是法语，目标语言是英语，利用贝叶斯准则的概率公式,也就是后验概率 = 显先验概率*似然，然后除以边缘概率。这里的边缘概率可以是源语言。其中p(f|e)是翻译模型，p(e)是语言模型，语言模型也就是我们试图计算更长序列概率的那个模型。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191209_1407_40_006.png" alt=""></p><p>如下是机器翻译模型的工作流程：首先翻译模型会把一个法语句子基于统计列出一系列可能的英语选项。然后使用语言模型，在这些选项里找一个简单流畅的句子。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191209_1413_18_341.png" alt=""></p><p>其中的一个模块，翻译模型中，第一件要做的事情是匹配两种语言的单词，匹配所面临的困难使用法语英语互译来展示。其实英法是相似度很高的语言，换成其他相似度低的语言则难度会进一步上升。</p><p><strong>选词</strong></p><p>首先是有些单词在法语里有，英语里不存在，翻译的时候要把这种词汇取出掉，比如le japon在英语里对应的就是japan，没有冠词，如图一。然后还有多对一或者一对多的情况，如图二。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191209_1418_10_267.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191209_1419_07_809.png" alt=""></p><p><strong>语序对齐</strong>：A语言的正序是B语言的倒装。如下是德语和英语的例子。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191209_1421_48_691.png" alt=""></p><p><strong>搜索最优options</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191209_1422_49_424.png" alt=""></p><p>老师对深度学习的阐述：</p><p>深度学习他最喜欢的一个属性是，不仅仅是机翻问题，甚至上升到NLP或者AI，深度学习模型试图构建一个端到端的可训练模型，你只需要关心最终的目标函数即可。所有的东西都在一个模型里共同学习，从某种角度看，我们现在说的机器学习模型和这个比起来是相反的，你有匹配模型，你需要先优化它，然后可能需要重新排序模型，然后会有分布在不同系统的不同模型，你不能将所有的模型得到一致的训练。</p><p>神经网络机翻通常是指，构造一个大型神经网络，我们可以在上边以端到端的方式对这个网络进行训练和优化。如下图所示，我们使用一个大型神经网络，他接受输入文本并将其编码为神经网络内部传递的向量，然后通过解码器解码，输出是文本。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1059_01_286.png" alt=""></p><p>早期机翻模型如下，结构相对简单，由认知学家和心理学家们设计</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1101_06_914.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1101_45_119.png" alt=""></p><p>这些模型的设计应该遵循一定的规则，特别是昨天学的LSTM内部的各种门和电路设计感觉是同一种思路，找到这门系统设计的学科并从头到尾过一遍，</p><p><strong>神经网络曾经是一个非常沉寂的边缘细分学科，以前都是心理学家和认知科学家们在这里耕耘，直到算力和网络技术发展成熟，大量CS科学家和资本入场，这里才变得热闹起来。做科研也需要赶时髦，因为如果当下一个科研方向很热门，说明在这个历史时期科学正在往这个方向发展。本身科学家做研究就需要互相引用他人的成果，使用他们的公式和仪器，站在巨人肩膀上做事，scientist,as a part of science,必须顺应时代的潮流，甚至要能预判时代的走向打一个提前量，这样对自己的职业发展最有利，也对推动世界进步最有利。</strong></p><p><strong>深度学习这一块提升性能最突飞猛进的几年已经过去了，国内对AI和DL这一块关注度也在下降。找一个benchmark评估下各细分方向/行业的繁荣度指标。学完NLP读完survey后好好考考虑下是继续NLP还是转到因果推理上去。毕竟因果推理已经不止第一次听说了，就像我在大三的时候知道了kaggle和python和CS229却没有抓住现在他妈的还在补课一样。有了机会和idea就应该好好分析尽量抓住。</strong></p><h3 id="1-现代机翻模型"><a href="#1-现代机翻模型" class="headerlink" title="1.现代机翻模型"></a>1.现代机翻模型</h3><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1126_02_497.png" alt=""></p><p>如上示意图，一个循环神经网络作为编码器，另一个循环神经网络作为解码器。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1128_15_920.png" alt=""></p><p>如上，使用了多层神经网络的机翻模型。 </p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224u_Sentiment_Analysis_有监督的情感分析.md</title>
    <link href="undefined2020/04/28/3.%E8%AF%BE%E7%A8%8B/CS224u_Sentiment_Analysis_%E6%9C%89%E7%9B%91%E7%9D%A3%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90.md/"/>
    <url>2020/04/28/3.%E8%AF%BE%E7%A8%8B/CS224u_Sentiment_Analysis_%E6%9C%89%E7%9B%91%E7%9D%A3%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90.md/</url>
    
    <content type="html"><![CDATA[<h1 id="Sentiment-Analysis-有监督的情感分析-md"><a href="#Sentiment-Analysis-有监督的情感分析-md" class="headerlink" title="Sentiment_Analysis_有监督的情感分析.md"></a>Sentiment_Analysis_有监督的情感分析.md</h1><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200428_2253_57_336.png" alt=""></p><p>如上所示，phi函数就是上篇介绍的feature function，提供的功能就是把tree转换成字典。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200428_2254_14_407.png" alt=""></p><p>如上所示，fit model函数提供将数据x和y fit进指定模型（逻辑回归）中的功能。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200428_2254_30_253.png" alt=""></p><p>然后使用上节介绍的sst.experiment函数（三个参数）分别是路径，feature函数和fit函数。</p><p>这样设计的好处就是实现了类的分离，如果需要改写feature函数或者fit函数，直接在本体里边修改就好。</p><h4 id="超参数的搜索与确定"><a href="#超参数的搜索与确定" class="headerlink" title="超参数的搜索与确定"></a>超参数的搜索与确定</h4><p><strong>超参数的基本原理</strong>：<br>超参数是模型的优化过程之外的“settings”。有如下几个例子：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200428_2254_40_886.png" alt=""></p><p><strong>超参数搜索</strong>实例</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200428_2254_50_180.png" alt=""></p><p>如上，In[4] 的 fit_softmax_with_crossvalidation函数，就是搜索超参数的函数。basemod是逻辑回归，给他加上了搜索空间：c,罚函数，等，然后调用系统工具箱里的utils.fit_classifier_with_crossvalidation就可以得到搜索结果。当然这个搜索的过程耗时较长。</p><p>然后把得到的函数放进experiment函数中开始实验。因为得到的函数是在超参数空间中的最优解，所以效果肯定比默认函数要好。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/Chrome OS初始化操作</title>
    <link href="undefined2020/04/28/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/Chrome%20OS%E5%88%9D%E5%A7%8B%E5%8C%96%E6%93%8D%E4%BD%9C/"/>
    <url>2020/04/28/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/Chrome%20OS%E5%88%9D%E5%A7%8B%E5%8C%96%E6%93%8D%E4%BD%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="Chrome-OS初始化操作"><a href="#Chrome-OS初始化操作" class="headerlink" title="Chrome OS初始化操作"></a>Chrome OS初始化操作</h1><p>0.进入开发者模式，esc + f３ + 电源，then ctrl + D </p><p>1.参见“共享代理文章”，用手机和电脑开启代理共享，chromebook连上wifi，登陆谷歌账号，不断重试，连接google play（这一步非常耗时）。</p><p>2.安装包管理工具crew<br>  Ctrl + Alt +t<br>  curl -Ls <a href="https://raw.github.com/skycocker/chromebrew/master/install.sh" target="_blank" rel="noopener">https://raw.github.com/skycocker/chromebrew/master/install.sh</a> | bash</p><p>3.安装ssr<br>  在主流的apk网站上下载并安装ssr，登陆amytelecom，将信息复制到ssr中，删除无关的免费节点。</p><p>4.安装马克飞象，印象笔记</p><p>5.setting中安装linux，打开linux命令行输入sudo apt install vim-gtk3安装gvim</p><p>6.用gvim打开rsa_pub，在linux中连接github，将post项目拉下来</p>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>chromebook</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/chromebook代理共享</title>
    <link href="undefined2020/04/23/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/chromebook%E4%BB%A3%E7%90%86%E5%85%B1%E4%BA%AB/"/>
    <url>2020/04/23/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/chromebook%E4%BB%A3%E7%90%86%E5%85%B1%E4%BA%AB/</url>
    
    <content type="html"><![CDATA[<p>1.下载ssr</p><p>2.google免费ssr节点，复制粘贴。连接ssr，右键ssr图标-选项设置-允许来自局域网的连接。</p><p>3.电脑连接手机热点，电脑打                          开热点共享，chromebook连接电脑发射的热点</p><p>4.电脑win+R,cmd,ipconfig,得到如下所示ip信息</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200423_1941_35_146.png" alt=""></p><p>5.chromebook点击所连接wifi热点，点击热点详情-连接类型-手动配置代理-输入上图ip，端口号是1080（详见第2条中允许来自局域网连接的按钮周围的端口信息）。</p><p>6.登录，done。</p>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>chromebook</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224u_Sentiment_Analysis_SST</title>
    <link href="undefined2020/04/21/3.%E8%AF%BE%E7%A8%8B/CS224u_Sentiment_Analysis_SST/"/>
    <url>2020/04/21/3.%E8%AF%BE%E7%A8%8B/CS224u_Sentiment_Analysis_SST/</url>
    
    <content type="html"><![CDATA[<h1 id="Sentiment-Analysis-SST"><a href="#Sentiment-Analysis-SST" class="headerlink" title="Sentiment_Analysis SST"></a>Sentiment_Analysis SST</h1><h4 id="SST项目基本信息"><a href="#SST项目基本信息" class="headerlink" title="SST项目基本信息"></a>SST项目基本信息</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1111_40_329.png" alt=""></p><p><a href="https://nlp.stanford.edu/sentiment/" target="_blank" rel="noopener">SST斯坦福链接</a></p><h4 id="SST工作展示"><a href="#SST工作展示" class="headerlink" title="SST工作展示"></a>SST工作展示</h4><p>蓝色是正面情绪，黄色是负面，灰色是中性情绪。叶子节点的merge原理暂时未知。<br>They said it would be great这句话下边都是正面情绪，直到跟节点的they才恢复为中性情绪，因为they said<br>其实不能反映出作者的 bias。</p><p>而后边接上的they were wrong明显是负面，然后merge后总体就是负面情绪了。这个总体的判断要比直接把一句话拆开按照各个单词的正负性简单sum要靠谱很多。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1123_19_945.png" alt=""></p><p>verse vesa</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1130_52_522.png" alt=""></p><h4 id="Readers讲解"><a href="#Readers讲解" class="headerlink" title="Readers讲解"></a>Readers讲解</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1145_53_021.png" alt=""></p><p>In [5]、In [6]中的<strong>train_reader</strong>参数详解</p><table><thead><tr><th></th><th>meaning</th></tr></thead><tbody><tr><td>class_func=sst.binary_class_func</td><td>二分类问题</td></tr><tr><td>class_func=sst.ternary_class_func</td><td>三分类问题</td></tr><tr><td>class_func=None</td><td>五分类问题</td></tr></tbody></table><h4 id="核心类-：-nltk-tree-Tree"><a href="#核心类-：-nltk-tree-Tree" class="headerlink" title="核心类 ： nltk.tree.Tree"></a>核心类 ： nltk.tree.Tree</h4><p>输入一个tree：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1155_58_935.png" alt=""></p><p>循环打印各个subtree：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1156_26_139.png" alt=""></p><p>输出tree的lable和subtree：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1156_42_630.png" alt=""></p><h4 id="feature-function示例（词袋）"><a href="#feature-function示例（词袋）" class="headerlink" title="feature function示例（词袋）"></a>feature function示例（词袋）</h4><p>feature function只要输入的是tree，输出的是词典即可。</p><h4 id=""><a href="#" class="headerlink" title=""></a><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1203_50_804.png" alt=""></h4><p>unigrams_phi函数输入的是一个tree，输出的是dictionary。</p><p>tree.leaves()输出了tree里的各个元素（并没有利用tree结构）。</p><p>Counter清点了各个元素的数量，输出dictionary。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1202_20_877.png" alt=""></p><p>手动构建tree</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1204_52_532.png" alt=""></p><p>输入tree，输出的是词典。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1206_41_521.png" alt=""></p><h4 id="Model-wrappers详解"><a href="#Model-wrappers详解" class="headerlink" title="Model wrappers详解"></a>Model wrappers详解</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1212_14_383.png" alt=""></p><p>Model wrappers做什么：</p><p>输入一些supvised training data,输出的是训练好的模型（fitted model）</p><p>也许model wrapper做的事情（仅仅是fit model）比较乏味，但是使用wrapper后就可以在这个预处理的过程中做很多事情而不用改变interface。</p><h4 id="sst-experiment"><a href="#sst-experiment" class="headerlink" title="sst.experiment"></a>sst.experiment</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1509_41_075.png" alt=""></p><p>运行试验的时候，就会发现experiment类就像瑞士军刀一样。</p><p>你只需要设置如下三个参数，SSH_HOME,feature function，model_wrapper就好了。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1615_51_211.png" alt=""></p><p>然后experiment就会运行，给出的结果如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1617_23_029.png" alt=""></p><p>SST的一个完整示例，在逻辑回归模型上测试词袋模型。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1621_24_339.png" alt=""></p><p><strong>SST under the hood</strong> ： sklearn.feature_extraction.DictVectorizer</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1624_05_691.png" alt=""></p><p>sst框架实际上用的是sklearn.feature_extraction.DictVectorizer来<strong>combine data with feature function</strong></p><p>例如我的train feature是如下两个词典，数字是count。也就是说，如果你写了一个feature function用来将一个tree mapping到一个dictionary上，上面那段代码做的事情就是applying the feature function to all examples,creating a list like it。</p><p>但是sklearn这个框架和各种ML框架一样，处理的是矩阵，而不是词典，所以需要转换格式。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1637_01_166.png" alt=""></p><p>如下，就是对DictVectorizer初始化以及把字典格式的train_features转换为矩阵格式，也就是X_train</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_2310_08_461.png" alt=""></p><p>输出转换成功的矩阵，这里矩阵的列就是各个元素，行是字典的顺序编号</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_2311_04_629.png" alt=""></p><h4 id="Homework"><a href="#Homework" class="headerlink" title="Homework:"></a>Homework:</h4><p>on jupyter notebook</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1633_49_395.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.文章/python_env_kag1_yml</title>
    <link href="undefined2020/04/19/0.%E6%96%87%E7%AB%A0/python_env_kag1_yml/"/>
    <url>2020/04/19/0.%E6%96%87%E7%AB%A0/python_env_kag1_yml/</url>
    
    <content type="html"><![CDATA[<h1 id="environment-yml"><a href="#environment-yml" class="headerlink" title="environment_yml"></a>environment_yml</h1><pre><code class="yml">name: kag1channels:  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch  - https://mirrors.ustc.edu.cn/anaconda/pkgs/main  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge  - defaultsdependencies:  - absl-py=0.9.0=py37_0  - astor=0.7.1=py_0  - attrs=19.3.0=py_0  - boto=2.49.0=py_0  - boto3=1.12.11=py_0  - botocore=1.15.11=py_0  - bz2file=0.98=py_0  - ca-certificates=2020.4.5.1=hecc5488_0  - catalogue=1.0.0=py_0  - category_encoders=1.2.6=py_0  - certifi=2020.4.5.1=py37hc8dfbb8_0  - cffi=1.13.2=py37hb32ad35_0  - chardet=3.0.4=py37_1003  - click=7.0=py_0  - cryptography=2.8=py37hb32ad35_1  - cycler=0.10.0=py_2  - cymem=2.0.3=py37h6538335_0  - cython-blis=0.4.1=py37hfa6e2cd_0  - docutils=0.15.2=py37_0  - flask=1.1.2=pyh9f0ad1d_0  - freetype=2.10.0=h563cfd7_1  - gast=0.2.2=py_0  - gensim=3.8.1=py37h6538335_1  - google-pasta=0.1.8=py_0  - grpcio=1.23.0=py37h3db2c7e_0  - h5py=2.9.0=nompi_h3cb27cb_1101  - hdf5=1.10.4=nompi_hcc15c50_1106  - idna=2.9=py_1  - importlib_metadata=1.5.0=py37_0  - itsdangerous=1.1.0=py_0  - jinja2=2.11.1=py_0  - jmespath=0.9.4=py_0  - joblib=0.14.1=py_0  - jpeg=9c=hfa6e2cd_1001  - jsonschema=3.2.0=py37_0  - keras-applications=1.0.8=py_1  - keras-preprocessing=1.1.0=py_0  - kiwisolver=1.1.0=py37he980bc4_0  - libblas=3.8.0=15_mkl  - libcblas=3.8.0=15_mkl  - libgpuarray=0.7.6=hfa6e2cd_1003  - liblapack=3.8.0=15_mkl  - libpng=1.6.37=h7602738_0  - libprotobuf=3.11.4=h1a1b453_0  - libtiff=4.0.9=h36446d0_1002  - mako=1.1.0=py_0  - markdown=3.2.1=py_0  - markupsafe=1.1.1=py37hfa6e2cd_0  - matplotlib-base=3.1.3=py37h2981e6d_0  - murmurhash=1.0.0=py37h6538335_0  - ninja=1.10.0=h1ad3211_0  - nltk=3.4.4=py_0  - numpy=1.18.1=py37hc71023c_0  - olefile=0.46=py_0  - openssl=1.1.1f=hfa6e2cd_0  - opt_einsum=3.1.0=py_0  - pandas=1.0.1=py37he350917_0  - patsy=0.5.1=py_0  - pillow=6.1.0=py37h9a613e6_0  - pip=20.0.2=py_2  - plac=0.9.6=py_1  - preshed=3.0.2=py37h6538335_1  - protobuf=3.11.4=py37he025d50_0  - pycparser=2.19=py_2  - pygpu=0.7.6=py37hc8d92b1_1000  - pyopenssl=19.1.0=py_1  - pyparsing=2.4.6=py_0  - pyreadline=2.1=py37_1001  - pyrsistent=0.15.6=py37hfa6e2cd_0  - pysocks=1.7.1=py37_0  - python=3.7.3=h510b542_1  - python-dateutil=2.8.1=py_0  - python_abi=3.7=1_cp37m  - pytz=2019.3=py_0  - pyyaml=5.3=py37hfa6e2cd_0  - regex=2020.2.20=py37hfa6e2cd_0  - requests=2.23.0=py37_0  - s3transfer=0.3.3=py37_0  - sacremoses=0.0.38=py_0  - scikit-learn=0.22.1=py37h7208079_1  - seaborn=0.10.0=py_1  - setuptools=45.2.0=py37_0  - six=1.14.0=py37_0  - smart_open=1.9.0=py_0  - spacy=2.2.3=py37he980bc4_0  - sqlite=3.30.1=hfa6e2cd_0  - srsly=1.0.0=py37h6538335_0  - statsmodels=0.11.1=py37hfa6e2cd_0  - tensorboard=1.15.0=py37_0  - tensorflow-hub=0.7.0=pyhe1b5a44_0  - termcolor=1.1.0=py_2  - theano=1.0.4=py37h6538335_1001  - thinc=7.3.0=py37he980bc4_0  - tk=8.6.10=hfa6e2cd_0  - tornado=6.0.3=py37hfa6e2cd_4  - tqdm=4.43.0=py_0  - urllib3=1.25.7=py37_0  - wasabi=0.6.0=py_0  - werkzeug=0.16.1=py_0  - wheel=0.34.2=py_1  - win_inet_pton=1.1.0=py37_0  - wincertstore=0.2=py37_1003  - wordcloud=1.6.0=py37hfa6e2cd_0  - wrapt=1.12.0=py37hfa6e2cd_0  - yaml=0.2.2=hfa6e2cd_1  - zipp=3.0.0=py_0  - zlib=1.2.11=h2fa13f4_1004  - m2w64-gcc-libgfortran=5.3.0=6  - m2w64-gcc-libs=5.3.0=7  - m2w64-gcc-libs-core=5.3.0=7  - m2w64-gmp=6.1.0=2  - m2w64-libwinpthread-git=5.0.0.4634.697f757=2  - msys2-conda-epoch=20160418=1  - pytorch=1.2.0=py3.7_cuda100_cudnn7_1  - torchvision=0.4.0=py37_cu100  - blas=1.0=mkl  - _tflow_select=2.2.0=eigen  - cudatoolkit=10.0.130=0  - icc_rt=2019.0.0=h0cc432a_1  - intel-openmp=2020.0=166  - mkl=2020.0=166  - scipy=1.2.1=py37h29ff71c_0  - tensorflow=1.15.0=eigen_py37h9f89a44_0  - tensorflow-base=1.15.0=eigen_py37h07d2309_0  - tensorflow-estimator=1.15.1=pyh2649769_0  - vc=14.1=h0510ff6_4  - vs2015_runtime=14.16.27012=hf0eaf9b_1  - vs2015_win-64=14.0.25420=h55c1224_11prefix: F:\Softwares\Anaconda3\envs\kag1</code></pre><p>\</p>]]></content>
    
    
    <categories>
      
      <category>文章</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.文章/python_环境迁移</title>
    <link href="undefined2020/04/19/0.%E6%96%87%E7%AB%A0/python_%E7%8E%AF%E5%A2%83%E8%BF%81%E7%A7%BB/"/>
    <url>2020/04/19/0.%E6%96%87%E7%AB%A0/python_%E7%8E%AF%E5%A2%83%E8%BF%81%E7%A7%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="python环境迁移"><a href="#python环境迁移" class="headerlink" title="python环境迁移"></a>python环境迁移</h1><p><strong>copy from：WenShan`s blog</strong></p><p>Conda 是著名的包管理器和虚拟环境管理器。</p><p>在配置完项目环境，并编写和测试代码后，您可能希望将其移至另一台计算机。</p><p>Conda 提供了多种保存和移动环境的方法。</p><h2 id="Clone"><a href="#Clone" class="headerlink" title="Clone"></a>Clone</h2><p>在本地，conda 可以方便地创建环境的快照或者备份：</p><pre><code>1 conda create --name snapshot --clone myenv </code></pre><h2 id="Spec-List"><a href="#Spec-List" class="headerlink" title="Spec List"></a>Spec List</h2><p>如果需要在具有 <strong>相同操作系统</strong> 的计算机之间复制环境，则可以生成 <code>spec list</code>。</p><p><strong>生成 <code>spec list</code> 文件：</strong></p><pre><code>1 conda list --explicit &gt; spec-list.txt </code></pre><p><strong>重现环境：</strong></p><pre><code>1 conda create  --name python-course --file spec-list.txt </code></pre><h2 id="Environment-yml"><a href="#Environment-yml" class="headerlink" title="Environment.yml"></a>Environment.yml</h2><p>也可以使用 <code>-export</code> 选项生成一个 <code>environment.yml</code> 文件，以在 <strong>不同的平台和操作系统之间</strong> 复现项目环境。 <code>spec list</code> 文件和 <code>environment.yml</code> 文件之间的区别在于： <code>environment.yml</code> 文件不针对特定操作系统，并且使用YAML格式。 <code>environment.yml</code> 仅列出了软件包名称，由 conda 基于软件包的名称构建环境。 另一个区别是 <code>-export</code> 还包括使用pip安装的软件包，而 <code>spec list</code> 则没有。</p><p><strong>导出 <code>environment.yml</code> 文件：</strong></p><pre><code>1 conda env export &gt; environment.yml </code></pre><blockquote><p>注意：如果当前路径已经有了 environment.yml 文件，conda 会重写这个文件</p></blockquote><p><strong>重现环境：</strong></p><pre><code>1 conda env create -f environment.yml </code></pre><h2 id="Conda-Pack"><a href="#Conda-Pack" class="headerlink" title="Conda Pack"></a>Conda Pack</h2><p><code>Conda-pack</code> 是一个命令行工具，用于打包 conda 环境，其中包括该环境中安装的软件包的所有二进制文件。 当您想在有限或没有网络访问的系统中重现环境时，此功能很有用。上面的方法均从其各自的存储库下载软件包以创建环境。而此方法不需要。<strong>注意，conda-pack 指定平台和操作系统，目标计算机必须具有与源计算机相同的平台和操作系统。</strong></p><p>要安装 conda-pack，请确保您位于 root 或 base 环境中，以便 conda-pack 在子环境中可用。Conda-pack 可通过 conda-forge 或者 PyPI 安装。</p><p><strong>conda-forge:</strong></p><pre><code>1 conda install -c conda-forge conda-pack </code></pre><p><strong>PyPI:</strong></p><pre><code>1 pip install conda-pack </code></pre><p><strong>打包一个环境：</strong></p><pre><code># Pack environment my_env into my_env.tar.gzconda pack -n my_env# Pack environment my_env into out_name.tar.gzconda pack -n my_env -o out_name.tar.gz# Pack environment located at an explicit path into my_env.tar.gzconda pack -p /explicit/path/to/my_env</code></pre><p><strong>重现环境：</strong></p><pre><code># Unpack environment into directory `my_env`mkdir -p my_envtar -xzf my_env.tar.gz -C my_env# Use Python without activating or fixing the prefixes. Most Python# libraries will work fine, but things that require prefix cleanups# will fail../my_env/bin/python# Activate the environment. This adds `my_env/bin` to your pathsource my_env/bin/activate# Run Python from in the environment(my_env) $ python# Cleanup prefixes from in the active environment.# Note that this command can also be run without activating the environment# as long as some version of Python is already installed on the machine.(my_env) $ conda-unpack</code></pre><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Conda 提供了多种复制项目环境的方法。 创建环境的克隆可以提供定制的基本环境或该环境的快照。<code>spec list</code> 和 <code>conda-pack</code> 可创建特定于平台和操作系统的环境副本。 其中 <code>spec list</code> 使用网络来下载环境中特定的软件包，而 <code>conda-pack</code> 可以打包包括软件包二进制文件在内的整个环境，这在带宽不足或没有网络的情况下很有用。 Conda导出 <code>environment.yml</code> 的方式非常适合在不同平台和操作系统之间重新创建环境。</p><p>更多详情请见 <a href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#create-env-from-file" target="_blank" rel="noopener">docs.conda.io</a> 和 <a href="https://conda.github.io/conda-pack/" target="_blank" rel="noopener">conda-pack project page</a>.</p>]]></content>
    
    
    <categories>
      
      <category>文章</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Python环境迁移</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224u_Sentiment_Analysis_材料和预处理</title>
    <link href="undefined2020/04/18/3.%E8%AF%BE%E7%A8%8B/CS224u_Sentiment_Analysis_%E6%9D%90%E6%96%99%E5%92%8C%E9%A2%84%E5%A4%84%E7%90%86/"/>
    <url>2020/04/18/3.%E8%AF%BE%E7%A8%8B/CS224u_Sentiment_Analysis_%E6%9D%90%E6%96%99%E5%92%8C%E9%A2%84%E5%A4%84%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h1 id="Sentiment-Analysis"><a href="#Sentiment-Analysis" class="headerlink" title="Sentiment Analysis"></a>Sentiment Analysis</h1><h3 id="阅读材料和数据预处理"><a href="#阅读材料和数据预处理" class="headerlink" title="阅读材料和数据预处理"></a>阅读材料和数据预处理</h3><h4 id="核心阅读材料"><a href="#核心阅读材料" class="headerlink" title="核心阅读材料"></a>核心阅读材料</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/3dGRneJbr5.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200420_2220_31_292.png" alt=""></p><p>情感分析各子领域合适的切入点。他们入选的原因是<strong>位置合适</strong>以及<strong>有公开数据集</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200420_2255_57_400.png" alt=""></p><p>常用情感分析<strong>数据库</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200420_2257_46_870.png" alt=""></p><p>lexica</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/nhHRwoK9ZU.png" alt=""></p><h3 id="preprocessing"><a href="#preprocessing" class="headerlink" title="preprocessing"></a>preprocessing</h3><p><strong>preprocessing</strong> ： tokenizing</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/8QE9Dv0qwL.png" alt=""></p><p>preprocessing : 词干化stemming</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/YUIDg86RTY.png" alt=""></p><p>porter stemming演示</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/pT41Ong7Ie.png" alt=""></p><p>兰开斯特stemming演示</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/BmfmhXeeg7.png" alt=""></p><p>wordNet stemming演示</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/BxFjaM2dhQ.png" alt=""></p><p><strong>preprocessing</strong>:  POS tagging</p><p>有些英文单词的sentiment只能通过pos来确定。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200420_2304_40_721.png" alt=""></p><p>需要注意的是即使有些单词的pos tagging一模一样仍然会有相反的sentiment</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200420_2307_36_210.png" alt=""></p><p>preprocessing : <strong>简单情感标记</strong>和推荐阅读</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1037_27_849.png" alt=""></p><p>简单负面情绪标记示例</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1036_52_478.png" alt=""></p><p>推荐阅读</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1037_37_712.png" alt=""></p><p>负面情绪标记带来的性能提升</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200421_1044_06_021.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_batchsize</title>
    <link href="undefined2020/04/08/0.%E6%A6%82%E5%BF%B5/NLP_batchsize/"/>
    <url>2020/04/08/0.%E6%A6%82%E5%BF%B5/NLP_batchsize/</url>
    
    <content type="html"><![CDATA[<h1 id="BatchSize"><a href="#BatchSize" class="headerlink" title="BatchSize"></a>BatchSize</h1><p>机器学习训练中，把数据集分割成若干个batch，每个batch的大小叫做batchsize。</p><p>对batchsize、epoch、iteration的举例：</p><pre><code class="txt">mnist 数据集有张图片作为训练数据，张图片作为测试数据。假设现在选择 Batch_Size = 对模型进行训练。迭代次。每个 Epoch 要训练的图片数量：(训练集上的所有图像)训练集具有的 Batch 个数： 每个 Epoch 需要完成的 Batch 个数：每个 Epoch 具有的 Iteration 个数：（完成一个Batch训练，相当于参数迭代一次）每个 Epoch 中发生模型权重更新的次数：训练 10 个Epoch后，模型权重更新的次数： 不同Epoch的训练，其实用的是同一个训练集的数据。第1个Epoch和第10个Epoch虽然用的都是训练集的图片，但是对模型的权重更新值却是完全不同的。因为不同Epoch的模型处于代价函数空间上的不同位置，模型的训练代越靠后，越接近谷底，其代价越小。总共完成30000次迭代，相当于完成了个Epoch</code></pre><h4 id=""><a href="#" class="headerlink" title=""></a></h4>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Batchsize</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_epoch</title>
    <link href="undefined2020/04/08/0.%E6%A6%82%E5%BF%B5/NLP_epoch/"/>
    <url>2020/04/08/0.%E6%A6%82%E5%BF%B5/NLP_epoch/</url>
    
    <content type="html"><![CDATA[<h1 id="Epoch"><a href="#Epoch" class="headerlink" title="Epoch"></a>Epoch</h1><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>训练集的所有数据对模型进行了一次完整训练，称为“一代训练”（epoch）。</p><p>也就是说所有训练样本在神经网络中都进行了一次正向传播和一次反向传播。</p><h4 id="为什么要进行多于一次的epoch？"><a href="#为什么要进行多于一次的epoch？" class="headerlink" title="为什么要进行多于一次的epoch？"></a>为什么要进行多于一次的epoch？</h4><p>神经网络中传递完整的数据集一次是不够的，而且我们需要将完整的数据集在同样的神经网络中传递多次。</p><p>但是因为使用的是有限的数据集，并且使用一个迭代过程也就是梯度下降来优化学习过程。如下图所示，随着epoch的增加，神经网络的权重更新次数也在增加，曲线从欠拟合变成了过拟合。所以需要选择合适的epoch。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200408_2017_23_000.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Epoch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_池化</title>
    <link href="undefined2020/04/08/0.%E6%A6%82%E5%BF%B5/NLP_%E6%B1%A0%E5%8C%96/"/>
    <url>2020/04/08/0.%E6%A6%82%E5%BF%B5/NLP_%E6%B1%A0%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h1><h4 id="池化是什么"><a href="#池化是什么" class="headerlink" title="池化是什么"></a>池化是什么</h4><p>卷积之后往往是池化操作，which可以降低卷积输出的特征向量，避免过拟合。</p><p>以最大池化max pooling为例：整个图像被分为<strong>不重叠</strong>的若干小块，每个小块内取最大的数字，然后保持原<br>平面结构，得出output。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200408_1048_11_399.png" alt=""></p><p>​    </p><h4 id="池化的作用"><a href="#池化的作用" class="headerlink" title="池化的作用"></a>池化的作用</h4><p>感受到<strong>不变性</strong>，增大感受野。</p><p>减小下一层的输入大小，减小计算量和参数个数，降维。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200408_1057_07_135.png" alt="">如上图，第一行是卷积，第二行是池化。经过这两行操作，<strong>“横折”</strong>这个点，不论做平移旋转或者伸缩，都有相同的输出。于是</p>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>池化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_卷积</title>
    <link href="undefined2020/04/08/0.%E6%A6%82%E5%BF%B5/NLP_%E5%8D%B7%E7%A7%AF/"/>
    <url>2020/04/08/0.%E6%A6%82%E5%BF%B5/NLP_%E5%8D%B7%E7%A7%AF/</url>
    
    <content type="html"><![CDATA[<h1 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h1><p>机器学习中的卷积操作的核心就是“对应相乘”。feature和原图的各部位分别<strong>相乘相加求均值</strong>，把原图遍历一道，便得到结果。如下所示，采用不同的卷积核可得到不同的结果图。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200408_1042_38_315.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>卷积</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/U盘安装Ubuntu</title>
    <link href="undefined2020/03/31/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/U%E7%9B%98%E5%AE%89%E8%A3%85Ubuntu/"/>
    <url>2020/03/31/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/U%E7%9B%98%E5%AE%89%E8%A3%85Ubuntu/</url>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h3 id="U盘安装Ubuntu"><a href="#U盘安装Ubuntu" class="headerlink" title="U盘安装Ubuntu"></a>U盘安装Ubuntu</h3><p>windows下 - 磁盘管理 - 压缩卷，然后删除卷。</p><p>找一个大于4G的u盘</p><p><a href="http://mirrors.ustc.edu.cn/ubuntu-releases/16.04/" target="_blank" rel="noopener">http://mirrors.ustc.edu.cn/ubuntu-releases/16.04/</a> 下载ubuntu-16.04.6-desktop-amd64.iso    </p><p><a href="https://cn.ultraiso.net/xiazai.html" target="_blank" rel="noopener">https://cn.ultraiso.net/xiazai.html</a> 下载ultralSo，安装，运行</p><p>左上角 文件 - 打开iso文件；左下角本地目录 选中u盘，启动 - 写入硬盘映像 - 写入</p><p>插入U盘，重启，F1,选择直接进入U盘。</p><p>选择安装Ubuntu - 其他选项（根据需求调整分区）-  点击左下方的加号添加分区，分区方案参考如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200331_1923_31_301.png" alt=""></p><p>现在安装 - 等待  - 拔U盘 - 重启 - 进</p>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_tokenize和预处理</title>
    <link href="undefined2020/03/28/0.%E6%A6%82%E5%BF%B5/NLP_tokenize%E5%92%8C%E9%A2%84%E5%A4%84%E7%90%86/"/>
    <url>2020/03/28/0.%E6%A6%82%E5%BF%B5/NLP_tokenize%E5%92%8C%E9%A2%84%E5%A4%84%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h4 id="Tokenize"><a href="#Tokenize" class="headerlink" title="Tokenize"></a>Tokenize</h4><p>把长句子拆分成有意义的小句子。</p><pre><code class="python">import nltksentence = &quot;hello,world&quot;tokens = nltk.word_tokenize(sentence)token = [&#39;hello&#39;, &#39;,&#39; , &#39;world&#39;]</code></pre><p>有时候tokenize没那么简单，比如如下的例子是黄晓明在推特上@安吉拉卑鄙秀恩爱的语料。</p><p>在这个场景下，大家会用很多# @ 表情符和超链接等，自然语言呈现出与书本不同形态，如果还沿用老的tokenize策略，就会分出不少乱七八糟的符号，并且拆散了有意义的本符号比如“ :D ” 。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200326_1132_28_606.png" alt=""></p><p>所以需要根据不同的业务场景开发定制化的tokenize策略，根据上文的分析，将本场景下最容易出现的专有token全部用正则表达式重写，试图完整表达。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200326_1143_17_623.png" alt=""></p><p>通过re.compile定义一个小方法token_re,返回和regex匹配的模式。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200327_0948_47_992.png" alt=""></p><h4 id="词形的变化"><a href="#词形的变化" class="headerlink" title="词形的变化"></a>词形的变化</h4><p>词有两种变化，分别是</p><p>Inflection<strong>变化</strong>: walk -&gt; walking -&gt;walked<br>不影响词性</p><p>derivation<strong>引申</strong>：nation -&gt; national -&gt; nationalize<br>影响词性</p><p>应对两种变化的对策：</p><p>Stemming<strong>词干提取</strong>：把不影响词性的<strong>变化</strong>的尾巴砍掉(ing,ed等等)</p><p>walking -&gt; walk</p><p>Lemmatization<strong>词形归一</strong>：把各个类型的词的变形，归为一个形式（有些时态、格的变化完全换了词，要变回来）</p><p>went -&gt; go</p><p>使用NLTK实现stemming</p><pre><code class="python">from nltk.stem.porter import PorterStemmer&gt;&gt;&gt; porter_stemmer.stem(&#39;provision&#39;)u&#39;provis&#39;from nltk.stem.poter import SnowballStemmer&gt;&gt;&gt; snowball_stemmer.stem(&#39;pressmably&#39;)u&#39;press&#39;</code></pre><p>使用NLTK实现Lemma</p><pre><code class="python">from nltk.stem import WordNetLemmatizer&gt;&gt;&gt; wordnet_lemmatizer.lemmatize(&#39;churches&#39;)u&#39;church&#39;</code></pre><p>使用NLTK加POS tagging更好实现Lemma</p><pre><code class="python"># 没有POS tag，默认是NN，名词&gt;&gt;&gt; wordnet_lemmatizer.lemmatize(&#39;are&#39;)&#39;are&#39;&gt;&gt;&gt; wordnet_lemmatizer.lemmatize(&#39;is&#39;)&#39;is&#39;# 加上POS tag&gt;&gt;&gt; wordnet_lemmatizer.lemmatize(&#39;is&#39;,pos=&#39;v&#39;)u&#39;be&#39;&gt;&gt;&gt; wordnet_lemmatizer.lemmatize(&#39;are&#39;,pos=&#39;v&#39;)u&#39;be&#39;</code></pre><p>使用NLTK标注POS tagging</p><pre><code class="python">&gt;&gt;&gt;import nltk&gt;&gt;&gt;text = nltk.word_tokenize(&#39;what does the fox say&#39;)&gt;&gt;&gt;text[&#39;what&#39;,&#39;does&#39;,&#39;the&#39;,&#39;fox&#39;,&#39;say&#39;]&gt;&gt;&gt;nltk.pos_tag(text)[(&#39;what&#39;,&#39;WDT&#39;),(&#39;does&#39;,&#39;VBZ&#39;),(&#39;the&#39;,&#39;DT&#39;),(&#39;fox&#39;,&#39;NNS&#39;),(&#39;say&#39;,&#39;VBP&#39;)]</code></pre><h4 id="停止词"><a href="#停止词" class="headerlink" title="停止词"></a>停止词</h4><p>像是he，the这种词汇，一千个词有一千个意思，在注重文本理解的应用场景来说，歧义太多。所以可以吧stopword去掉。</p><p>使用nltk去除stopwords</p><pre><code class="python">from nltk.corpus import stopwords# 先token一把，得到一个word_list# ...# 然后filter一下filtered_words = [word for word in word_list if word not in stopwords.words(&#39;english&#39;)]</code></pre><h4 id="典型的文本预处理流程"><a href="#典型的文本预处理流程" class="headerlink" title="典型的文本预处理流程"></a>典型的文本预处理流程</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200328_0943_28_506.png" alt=""></p><p>经过了文本预处理，文本变成了单词的list，然后将list通过特征工程变成计算机可以理解的数字。</p><h4 id="NLTK在NLP上的经典应用"><a href="#NLTK在NLP上的经典应用" class="headerlink" title="NLTK在NLP上的经典应用"></a>NLTK在NLP上的经典应用</h4><p>情感分析</p><p>文本相似度</p><p>文本分类</p><p><strong>情感分析</strong>就是分析如下的评论哪些是在黑你哪些是在夸你？</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200328_1436_09_350.png" alt=""></p><p>最简单的情感分析是对词汇进行打分，比如1是like，2是good，-2是bad，-3是terrible</p><p>用nltk完成简单的情感分析</p><pre><code class="python">sentiment_dictionary = {}for line in open(&#39;data/AFINN-111.txt&#39;)    word,score = line.split(&#39;\t&#39;)  # 单词和单词的情感得分用空格分开    sentiment_dictionary(word) = int(score)total_score = sum(sentiment_dictionary.get(word,0) for word in words)# 遍历一遍待分析的句子，把对应的值相加，最终得到一个sentiment score就是最终结果</code></pre><p>当然这种方式非常naive，新词、特殊词、更深层次的词没有办法处理。更有效的办法是配上ML的情感分析。后面会讲。</p><p>​     </p><p><strong>文本相似度</strong>是NLTK的另一个应用，比如在百度搜索“七月在线”，百度就会返回和这个字符串最相似的结果。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200328_1452_16_384.png" alt=""></p><p>如何计算相似度？构建一个所有单词都在的长向量，然后把每一句话都用这个向量表示，如下是简化版：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200328_1505_47_705.png" alt=""></p><p>​    然后使用余弦定理来确定两句话的相似程度</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200328_1506_32_358.png" alt=""></p><p><strong>文本分类</strong>是另一个应用，意思是打开一句话，然后来确定这句话是属于哪个topic下的？（多分类问题）。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200328_1607_32_201.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200328_1608_08_239.png" alt=""></p><p>在nltk中实现TF-IDF</p><pre><code class="python">from nltk.text import TextCollection#首先，把所有的文档放到TextCollection中，#这个类会帮你自动断句，做统计和计算corpus = TextCollection([&#39;this is sentence one&#39;,                        &#39;this is sentence two&#39;,                        &#39;this is sentence three])print(corpus.tf_idf(&#39;this&#39;,&#39;this is sentence four&#39;))# 0.444342                                           </code></pre>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Tokenize</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>5.杂谈/悉达多的王子</title>
    <link href="undefined2020/03/27/5.%E6%9D%82%E8%B0%88/%E6%82%89%E8%BE%BE%E5%A4%9A%E7%9A%84%E7%8E%8B%E5%AD%90/"/>
    <url>2020/03/27/5.%E6%9D%82%E8%B0%88/%E6%82%89%E8%BE%BE%E5%A4%9A%E7%9A%84%E7%8E%8B%E5%AD%90/</url>
    
    <content type="html"><![CDATA[<h1 id="悉达多"><a href="#悉达多" class="headerlink" title="悉达多"></a>悉达多</h1><p>1.爱与知识并不能来带peace</p><p>悉达多感到，父母朋友之<strong>爱</strong>，长者传授的<strong>知识</strong>不能让他感到满足，不能使他的内心安宁。悉达多开始怀疑，<strong>沐浴、祭祀</strong>等仪式是否有必要？</p><p>爱是大脑的内分泌短暂失调+欲求不满的表现，知识的传授其实可以在极短的时间内完成（剩余时间是磨洋工），沐浴祭祀等繁文缛节是keep everybody busy的手段，换成现代概念几乎可以等同于各种繁复的人事缓冲池——若不是要严控失业率，从基层到决策层之间90%的传递信息者都可被优化掉。</p><p>2.众生皆苦，逃避无用</p><p>悉达多说，冥想，对肉体的决弃，斋戒和调息都是在逃避自我，是对自我所受的苦难的短暂的逃避，这种逃避和牧牛人在酒馆里喝几碗米酒是同样的。在这种短暂的麻醉下，他们不再感受到自我，不再感觉到生命的苦难。那几碗米酒让牧牛人浑然入睡，他同样找到了悉达多和乔文达在长时间的修行中逃离肉体并宅于非我之境所找到的感觉。</p><p>悉达多说的这种逃避痛苦的方式，暗示了痛苦的根源就在与“我”，感受到我的存在，便能感受到痛苦。喝了酒，进入了禅定，做爱，入睡了，在别的国家旅游用英语讲话逃离中文世界，weed等都可短暂的<strong>忘我</strong>。但是在短暂的忘我结束后，一切一如从前，自己并没有变的明智，从未得到更高的知识，并没有进入更高的境界。</p><p>3.听从inner voice，不要逃避自我</p><p>教师和教义能教你许多，但是有些事情是无法传授的。那就是“自我”。过去总想着去逃避自我征服自我，然后我从未能征服自我，只是在欺骗，逃避。的确，世间没有任何东西能像自我那样占据自己的全部思绪。<br>害怕自我，逃避自我，从而去追寻梵天，去追寻阿特曼。我欲求摧毁自我，摆脱自我，以便在自我未知的最深层发现万相的核心，也就是阿特曼，生命，神灵等绝对终极之物。正因为如此，我却一路丢了自我。<br>不要企图逃离自己，不要去思索宇宙的奥义和世间的苦难。不要去摧毁自我并试图在自我的废墟中寻找秘密。要以自我为师，从自我找到自我的秘密。</p><p>不要逃避自我，要去好好倾听inner voice。追求科学突破，追求荣誉金钱，疏远朋友，放任傲慢，这是我的初心，还是我在向俗世的评价标准出卖自己？若过的不开心，虚无感爆棚，为何不调整？为何允许自己陷入如此境地？这是不知道如何与自己相处、长期忽略inner voice的恶果。<br>以前有人强迫着自己去学习，不许恋爱，不让玩耍，或者工作职责强迫自己加班劳动疲惫不堪，现在我终于自由，可做任何想做之事，不可以逃避，倾听inner voice找到内心之宁静其实不难。其实很多事情开始前便有答案。</p><p>4.觉醒的阵痛</p><p>世间无人如他一般孤独，以前他还是父亲的儿子，是高贵的婆罗门，是宗教信仰者，但是现在他只是觉醒的悉达多而已。贵族可以属于某个上流阶层，工匠可以在行业协会中安身立命，苦修者可以进行沙门的修行。婆罗门可以和婆罗门一起生活，甚至林中最与世隔绝的隐士也并非孤独薏苡仁，仍然属于某一群体。僧人也有千万的僧侣是他的兄弟。但是悉达多应该归属何方？加入谁的生活？使用什么语言？</p><p>他应该坚定的成为他自己，这是觉醒欧虎的颤栗，是新生之后的阵痛。随后他立刻重新上路，不再朝回家的方向，不再希望回到父亲的身边，不再犹豫和回顾。</p><p>5.心中的圣地</p><p>悉达多对加摩拉说，你很像我，你与众不同，你的心中有一处宁静的圣地，你可以随时退避并在哪里成为你自己，我也会这样做，极少数人具备这种能力。</p><p>我曾经也有这种能能力，我的心中的圣地是一片密林，长期的应试教育摧毁了我的内心世界。</p>]]></content>
    
    
    <categories>
      
      <category>5.杂谈</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/技术_正则表达式</title>
    <link href="undefined2020/03/26/0.%E6%A6%82%E5%BF%B5/%E6%8A%80%E6%9C%AF_%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    <url>2020/03/26/0.%E6%A6%82%E5%BF%B5/%E6%8A%80%E6%9C%AF_%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<h1 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h1><h4 id="re-compile"><a href="#re-compile" class="headerlink" title="re.compile()"></a>re.compile()</h4><p>compile() 的定义：</p><pre><code class="python">compile(pattern,flags=0)# compile a regular expression pattern,returning a pattern object</code></pre><p>从原文档的定义可知，compile函数返回的是一个匹配对象，一般和findall(),search(),match()函数搭配使用。返回的是一个<strong>列表</strong>。</p><p>搭配使用的例子：</p><pre><code class="python">import redef main():    content = &#39;Hello, I am Jerry,from Chongqing, a montain city,nice to meet you&#39;    regex = re.compile(&#39;\W*o\w*&#39;)    x = regex.findall(content)    print(x)if __name__ == &#39;__main__&#39;：    main()# [&#39;Hello&#39;,&#39;from&#39;,&#39;Chongqing&#39;,&#39;montain&#39;,&#39;to&#39;,&#39;you&#39;]</code></pre><h2 id="正则表达式-1"><a href="#正则表达式-1" class="headerlink" title="正则表达式"></a>正则表达式</h2><p>定义:简洁表达一组字符串的表达式</p><p>优势:简洁,一行胜千言</p><p>用途:</p><ul><li>文本处理</li></ul><ul><li>表达文本信息特征</li><li>查找/替换字符串</li><li>匹配字符串的全部或者部分</li></ul><p>常用符号（百度百科版）</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1457_52_681.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1458_04_805.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1458_12_967.png" alt=""></p><h4 id="常用操作符"><a href="#常用操作符" class="headerlink" title="常用操作符"></a>常用操作符<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1458_27_941.png" alt=""></h4><p>正则表达式实例<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1458_47_258.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>正则表达式</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_CNN卷积神经网络</title>
    <link href="undefined2020/03/25/0.%E6%A6%82%E5%BF%B5/NLP_CNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>2020/03/25/0.%E6%A6%82%E5%BF%B5/NLP_CNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="CNN卷积神经网络"><a href="#CNN卷积神经网络" class="headerlink" title="CNN卷积神经网络"></a>CNN卷积神经网络</h1><p>卷积神经网络<strong>擅长处理网格结构的数据</strong>。</p><p>顾名思义，卷积神经网络是利用了数学上的卷积操作的神经网络。和之前的前馈神经网络比起来，卷积神经网络不过是将<strong>其中的某几层的矩阵乘法运算替换为卷积运算</strong>。其他的比如最大似然法则、反向传播算法等都不变。</p><h4 id="什么是卷积运算"><a href="#什么是卷积运算" class="headerlink" title="什么是卷积运算"></a>什么是卷积运算</h4><p>回顾一下矩阵乘法C = AB的表示：</p><p><img src="https://pic3.zhimg.com/80/v2-379d3631a456648dba7f6d2e14ca0efe_720w.jpg" alt="img"></p><p>卷积的定义：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200518_1559_57_035.png" alt=""></p><p>其中I是输入矩阵，K是kernels核矩阵，S就是输出的特征图，m和m是K矩阵的大小</p><p>如下图，卷积就是kernel核矩阵扫过输入矩阵产生输出矩阵的过程。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200518_1458_22_460.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200518_1550_43_618.png" alt=""></p><h4 id="卷积神经网络的两大优势"><a href="#卷积神经网络的两大优势" class="headerlink" title="卷积神经网络的两大优势"></a>卷积神经网络的两大优势</h4><p>卷积神经网络<strong>擅长处理网格结构的数据</strong>，比如一维的时间序列，二维的图像像素，三维的医学CT图。之所以擅长是因为卷积神经网络的两个优势：</p><ul><li><p><strong>稀疏连接</strong></p><p>传统的神经网络主要操作是矩阵乘法，每个输入输出元之间都需要一个独立的参数表示，这也代表着两层之间每个输出元和输入元都有连接。于是就需要很大的存储空间来存储这些参数。对于CNN来说，核矩阵的大小远小于输入的大小，对于图像数据来说，输入通常有成千上万的数据，但是检测图像中边edge的结构的核矩阵可能只有数十最多数百个像素。这极大的减小了存储所需空间和计算量。如下，上边是神经网络层间的卷积操作，下边是神经网络层间的矩阵乘法操作。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200518_1621_52_131.png" alt=""></p></li><li><p><strong>参数共享</strong></p><p>传统采用矩阵乘法的神经网络，每一个输入和输出的元素都是独立的，参数只对一个输入元素负责。但是CNN中kernel矩阵的元素对每一个输入的元素都负责从而实现了参数的共享。如下，上边是一个kernel为3的神经网络，参数共享，下边是矩阵乘法的神经网络，参数只在单个输入中使用。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200518_1624_51_756.png" alt=""></p></li></ul><h4 id="池化层pooling-layer"><a href="#池化层pooling-layer" class="headerlink" title="池化层pooling layer"></a>池化层pooling layer</h4><p>CNN中加入池化层的作用就是利用附近元素的相关统计信息来替代真实数据。这样做的目的就是使结果在输入的小量改变的干扰下保持稳定。例如一个经典的pooling操作是max pooling，也就是输入周围小方格中的最大值。</p><p>加入池化层的动机是比起特定的位置我们更关心的是某个特征是否存在。</p>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_gensim</title>
    <link href="undefined2020/03/13/0.%E6%A6%82%E5%BF%B5/NLP_gensim/"/>
    <url>2020/03/13/0.%E6%A6%82%E5%BF%B5/NLP_gensim/</url>
    
    <content type="html"><![CDATA[<h1 id="Gensim"><a href="#Gensim" class="headerlink" title="Gensim"></a>Gensim</h1><p><strong>word2vec</strong></p><p>word2vec的使用就是gensim中models/word2vec.py文件里的word2vec类中，共有如下24个参数。</p><table><thead><tr><th>参数名称</th><th>默认值</th><th>用途</th></tr></thead><tbody><tr><td>sentences</td><td>None</td><td>训练的语料，一个可迭代对象。对于从磁盘加载的大型语料最好用gensim.models.word2vec.BrownCorpus，gensim.models.word2vec.Text8Corpus ，gensim.models.word2vec.LineSentence 去生成sentences</td></tr><tr><td>size</td><td>100</td><td>生成词向量的维度</td></tr><tr><td>alpha</td><td>0.025</td><td>初始学习率</td></tr><tr><td>window</td><td>5</td><td>句子中当前和预测单词之间的最大距离，取词窗口大小</td></tr><tr><td>min_count</td><td>5</td><td>文档中总频率低于此值的单词忽略</td></tr><tr><td>max_vocab_size</td><td>None</td><td>构建词汇表最大数，词汇大于这个数按照频率排序，去除频率低的词汇</td></tr><tr><td>sample</td><td>1.00E-03</td><td>高频词进行随机下采样的阈值，范围是(0, 1e-5)</td></tr><tr><td>seed</td><td>1</td><td>向量初始化的随机数种子</td></tr><tr><td>workers</td><td>3</td><td>几个CPU进行跑</td></tr><tr><td>min_alpha</td><td>0.0001</td><td>随着学习进行，学习率线性下降到这个最小数</td></tr><tr><td>sg</td><td>0</td><td>训练时算法选择 0:skip-gram, 1: CBOW</td></tr><tr><td>hs</td><td>0</td><td>0: 当这个为0 并且negative 参数不为零，用负采样，1：层次 softmax</td></tr><tr><td>negative</td><td>5</td><td>负采样，大于0是使用负采样，当为负数值就会进行增加噪音词</td></tr><tr><td>ns_exponent</td><td>0.75</td><td>负采样指数，确定负采样抽样形式：1.0：完全按比例抽，0.0对所有词均等采样，负值对低频词更多的采样。流行的是0.75</td></tr><tr><td>cbow_mean</td><td>1</td><td>0:使用上下文单词向量的总和，1:使用均值； 只适用于cbow</td></tr><tr><td>hashfxn</td><td>hash</td><td>希函数用于随机初始化权重，以提高训练的可重复性。</td></tr><tr><td>iter</td><td>5</td><td>迭代次数，epoch</td></tr><tr><td>null_word</td><td>0</td><td>空填充数据</td></tr><tr><td>trim_rule</td><td>None</td><td>词汇修剪规则，指定某些词语是否应保留在词汇表中，默认是 词频小于 min_count则丢弃，可以是自己定义规则</td></tr><tr><td>sorted_vocab</td><td>1</td><td>1：按照降序排列，0：不排序；实现方法：gensim.models.word2vec.Word2VecVocab.sort_vocab()</td></tr><tr><td>batch_words</td><td>10000</td><td>词数量大小，大于10000 cython会进行截断</td></tr><tr><td>compute_loss</td><td>FALSE</td><td>损失(loss)值，如果是True 就会保存</td></tr><tr><td>callbacks</td><td>()</td><td>在训练期间的特定阶段执行的回调序列~gensim.models.callbacks.CallbackAny2Vec</td></tr><tr><td>max_final_vocab</td><td>None</td><td>通过自动选择匹配的min_count将词汇限制为目标词汇大小,如果min_count有参数就用给定的数值</td></tr></tbody></table><h3 id="模型的保存和使用："><a href="#模型的保存和使用：" class="headerlink" title="模型的保存和使用："></a>模型的保存和使用：</h3><p>完成训练后只存储并使用~gensim.models.keyedvectors.KeyedVectors<br>该模型可以通过以下方式存储/加载：<br>~gensim.models.word2vec.Word2Vec.save 保存模型<br>~gensim.models.word2vec.Word2Vec.load 加载模型</p><p>训练过的单词向量也可以从与其兼容的格式存储/加载：<br>gensim.models.keyedvectors.KeyedVectors.save_word2vec_format实现原始 word2vecword2vecword2vec 的保存<br>gensim.models.keyedvectors.KeyedVectors.load_word2vec_format 单词向量的加载</p><h3 id="模型的属性"><a href="#模型的属性" class="headerlink" title="模型的属性"></a>模型的属性</h3><p>wv: 是类 ~gensim.models.keyedvectors.Word2VecKeyedVectors生产的对象，在word2vec是一个属性<br>为了在不同的训练算法（Word2Vec，Fastext，WordRank，VarEmbed）之间共享单词向量查询代码，gensim将单词向量的存储和查询分离为一个单独的类 KeyedVectors<br>        包含单词和对应向量的映射。可以通过它进行词向量的查询</p><pre><code class="python">model_w2v.wv.most_similar(&quot;民生银行&quot;)  # 找最相似的词model_w2v.wv.get_vector(&quot;民生银行&quot;)  # 查看向量model_w2v.wv.syn0  #  model_w2v.wv.vectors 一样都是查看向量model_w2v.wv.vocab  # 查看词和对应向量model_w2v.wv.index2word  # 每个index对应的词</code></pre>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>gensim</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/Python_numpy_seed</title>
    <link href="undefined2020/03/10/0.%E6%A6%82%E5%BF%B5/Python_numpy_seed/"/>
    <url>2020/03/10/0.%E6%A6%82%E5%BF%B5/Python_numpy_seed/</url>
    
    <content type="html"><![CDATA[<h1 id="Numpy"><a href="#Numpy" class="headerlink" title="Numpy"></a>Numpy</h1><h4 id="np-random-seed-函数"><a href="#np-random-seed-函数" class="headerlink" title="np.random.seed()函数"></a>np.random.seed()函数</h4><p>np.random.seed()函数用于生成指定的随机数，seed ()被设定后可以按照顺序产生一组固定的数组，如果使用相同的seed（）值，每次产生的随机数都相同。如果不设置这个值，则每次产生的随机数都</p><h4 id="np-random-seed-函数-1"><a href="#np-random-seed-函数-1" class="headerlink" title="np.random.seed()函数"></a>np.random.seed()函数</h4><p>这个seed（）函数可以说是随机数的编号。比如seed（1） 产生的数组是[1,2,3,4,2,2,1]，那么以后调用seed（1）出来的数组都是[1,2,3,4,2,2,1].</p><p>不使用seed（）就得不到相同的随机数：</p><pre><code class="python">import numpy as npnp.random.seed(1)L1 = np.random.randn(3, 3)L2 = np.random.randn(3, 3)print(L1)print(L2)</code></pre><p>out：</p><pre><code class="python">[[ 1.62434536 -0.61175641 -0.52817175] [-1.07296862  0.86540763 -2.3015387 ] [ 1.74481176 -0.7612069   0.3190391 ]][[-0.24937038  1.46210794 -2.06014071] [-0.3224172  -0.38405435  1.13376944] [-1.09989127 -0.17242821 -0.87785842]]</code></pre><p>使用了seed（）就得到相同的随机数：</p><pre><code class="python">import numpy as npnp.random.seed(1)L1 = np.random.randn(3, 3)np.random.seed(1)L2 = np.random.randn(3, 3)print(L1)print(L2)</code></pre><p>out:</p><pre><code class="python">[[ 1.62434536 -0.61175641 -0.52817175] [-1.07296862  0.86540763 -2.3015387 ] [ 1.74481176 -0.7612069   0.3190391 ]][[ 1.62434536 -0.61175641 -0.52817175] [-1.07296862  0.86540763 -2.3015387 ] [ 1.74481176 -0.7612069   0.3190391 ]]</code></pre>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Numpy</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/Python_Pandas_apply_series</title>
    <link href="undefined2020/03/08/0.%E6%A6%82%E5%BF%B5/Python_Pandas_apply_series/"/>
    <url>2020/03/08/0.%E6%A6%82%E5%BF%B5/Python_Pandas_apply_series/</url>
    
    <content type="html"><![CDATA[<h1 id="Pandas-Series"><a href="#Pandas-Series" class="headerlink" title="Pandas_Series"></a>Pandas_Series</h1><p>Series数据包括：一列数据和一列index组成。和字典非常相似</p><p>创建空的series</p><pre><code class="python">import numpy as npimport pandas as pdS1 = pd.Series()S1    Series([],dtype:float64)</code></pre><p>指定value和index的值</p><pre><code class="python">S2=pd.Series([1,3,5,7,9],index=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;])S2a    1b    3c    5d    7e    9dtype: int64S2.valuesarray([1, 3, 5, 7, 9])S2.indexIndex([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;], dtype=&#39;object&#39;)</code></pre><h2 id="Pandas处理series的方法-map-apply"><a href="#Pandas处理series的方法-map-apply" class="headerlink" title="Pandas处理series的方法:map,apply"></a>Pandas处理series的方法:map,apply</h2><p>数据由如下代码模拟生成</p><pre><code class="python">boolean=[True,False]gender=[&quot;男&quot;,&quot;女&quot;]color=[&quot;white&quot;,&quot;black&quot;,&quot;yellow&quot;]data=pd.DataFrame({    &quot;height&quot;:np.random.randint(150,190,100),    &quot;weight&quot;:np.ra[x] for x in np.random.randint(0,len(color),100) ]})</code></pre><p>数据探索：各行分别代表身高，体重，吸烟与否，性别，年龄和肤色</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200308_1123_56_134.png" alt=""></p><h3 id="1-series-map用法"><a href="#1-series-map用法" class="headerlink" title="1.series.map用法"></a>1.series.map用法</h3><p>如何把数据集中的gender列的男替换为1，女替换为0？可以用for，但是这里用series.map更加容易</p><pre><code class="python"># 使用字典进行映射data[&quot;gender&quot;] = data[&quot;gender&quot;].map({&quot;男&quot;:1,&quot;女&quot;:0})  # 括号里就是字典的键值对# 使用函数进行映射def gender_map(x):    gender = 1 if x == &quot;男&quot; else 0    return gender# 使用函数映射的话映射的就是函数的名称data[&quot;gender&quot;] = data[&quot;gender&quot;].map(gender_map)</code></pre><p>字典映射的原理</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200308_1527_55_450.png" alt=""></p><p>函数映射的原理</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200308_1528_26_477.png" alt=""></p><p>不论是用字典还是函数，map方法都是将对应的数据<strong>逐个当做参数</strong>传入字典或者函数中，得到映射后的值。</p><h3 id="2-Series-apply方法"><a href="#2-Series-apply方法" class="headerlink" title="2.Series.apply方法"></a>2.Series.apply方法</h3><p>apply方法和map方法的作用原理类似，区别是apply方法能传入功能更为复杂的函数。</p><p>比如age列有较大误差，需要对每个数据进行不同的调整，所以每个数据后边加个bias参数。</p><pre><code class="python">def apply_age(x,bias):    return x+bias#以元组的方式传入额外的参数data[&quot;age&quot;] = data[&quot;age&quot;].apply(apply_age,args=(-3,))</code></pre>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pandas</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/技术_Lamada表达式</title>
    <link href="undefined2020/03/07/0.%E6%A6%82%E5%BF%B5/%E6%8A%80%E6%9C%AF_Lamada%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    <url>2020/03/07/0.%E6%A6%82%E5%BF%B5/%E6%8A%80%E6%9C%AF_Lamada%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<h1 id="Python中lamada函数的概念"><a href="#Python中lamada函数的概念" class="headerlink" title="Python中lamada函数的概念"></a>Python中lamada函数的概念</h1><p>lamada函数就是个匿名函数，即定义即用，省去了起名字等等环节，使用方便快捷。</p><p>不用lamada函数的例子</p><pre><code class="python">def f(x):return x**2print f(4)</code></pre><p>使用lamada函数的例子：</p><pre><code class="python">g = lambda x : x**2print g(4)</code></pre><p>lamada用例</p><pre><code class="python"># lamada语句中，冒号前是参数，可以有多个，用逗号隔开。冒号后是返回值，lamada语句构建的其实是一个函数对象&gt;&gt;&gt; foo = [2, 18, 9, 22, 17, 24, 8, 12, 27]&gt;&gt;&gt; print filter(lambda x: x % 3 == 0, foo)[18, 9, 24, 12, 27]&gt;&gt;&gt; print map(lambda x: x * 2 + 10, foo)[14, 46, 28, 54, 44, 58, 26, 34, 64]&gt;&gt;&gt; print reduce(lambda x, y: x + y, foo)139</code></pre>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Lamada</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2.比赛/指导_Google_QA_数据处理部分</title>
    <link href="undefined2020/03/05/2.%E6%AF%94%E8%B5%9B/%E6%8C%87%E5%AF%BC_Google_QA_%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E9%83%A8%E5%88%86/"/>
    <url>2020/03/05/2.%E6%AF%94%E8%B5%9B/%E6%8C%87%E5%AF%BC_Google_QA_%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E9%83%A8%E5%88%86/</url>
    
    <content type="html"><![CDATA[<h1 id="数据处理部分总体代码"><a href="#数据处理部分总体代码" class="headerlink" title="数据处理部分总体代码"></a>数据处理部分总体代码</h1><p>如下，各个程序块在pycharm中分开运行，用#%%分割。</p><p>上来是import部分，然后是数据清洗和构建embedding矩阵等。</p><pre><code class="python">import numpy as np#numpy是主要用于数组计算，线性代数，傅里叶变换等。import pandas as pd#pandas基于numpy，可以处理高纬数据from sklearn.manifold import TSNE# sklearn是机器学习中常用的第三方模块，对常见的机器学习算法进行了封装，包括回归、降维、分类、聚类,sklearn.manifold是流形学习，非# 线性降维的手段。最简单的降维手段是随机投影，但是会导致结构丢失,manifold learning是一种类似主成分分析(PCA)的线性框架，不会错失数据结构中的非线性项  ，TSNE提供了一种画图方式，让高维的数据降低为二维画出来import seaborn as sns# 基于matplotlib的画图工具import glob# glob是查找模块。支持空格 ，问号？，方括号[]这三个通配符。空格代表0个或者多个字符，问号？代表一个字符，[]代表范围，例如0-9.glob主要是两个方法，glob方法获（取全部匹配路径）和iglob方法（逐个获取满足路径）。from urllib.parse import urlparse# url.parse定义了URL的标准接口，实现了对url的解析，合并，编码，解码等，#%matplotlib inline，这一句在pycharm和spyder中可以注释掉，主要是在jupter notebook中用于打印图像的import re# 正则表达式from category_encoders.ordinal import OrdinalEncoder# category_encoders.ordinal可以将非数值型数据比如语言，转换成类别变量例如独热编码，A collection sklearn transformers to encode categorical variables as numericimport spacy# 最快的工业级自然语言处理工具，主要功能是分词，词干化，命名实体识别，名词短语提取等import gc# gc是垃圾回收机制，gc.collect()是核心函数import gensim# genism模块中有word2vec，可以把词汇转换成向量from nltk.tokenize import TweetTokenizer# nltk.tokenize就是把句子令牌化，tweetTokenize就是按照空格进行分词，同时针对推文特性，去除@用户名，保留表情符号import datetime# datetime是python处理时间和日期的标准库from scipy import stats# scipy的stats是统计学的各种分布，stats.chi2是卡方分布，stats.norm是正态分布，还有t和f分布。from scipy.sparse import hstack,csr_matrix# vstack，hstack分别是横向合并和纵向合并，csr_matrix是压缩稀疏行格式的矩阵，sparse matrix (稀疏矩阵) ，sparse就是scipy模块中负责处理稀疏矩阵的模块from sklearn.model_selection import train_test_split,cross_val_score,KFold# sklearn中的model_selection用于交叉验证，三个包分别是数据集分割，交叉验证，K折交叉验证器from wordcloud import WordCloud# 词云from collections import Counter# 计数器from nltk.corpus import stopwords# 停止词from nltk.util import ngrams# ngramfrom sklearn.feature_extraction.text import TfidfVectorizer# sklearn的词频-逆向文件频率from sklearn.preprocessing import StandardScaler# StandardScaler是数据归一化和标准化的类。所谓归一化就是使得数据方差为1，均值为0from sklearn.linear_model import LogisticRegression,LinearRegression,Ridge# 逻辑回归模型from sklearn.svm import LinearSVC# 线性分类支持向量机from sklearn.multiclass import OneVsRestClassifier# 多分类问题的分类器import time# pd.set_option(&#39;max_colwidth&#39;,400)。设置列长度400from scipy.stats import spearmanr# stats是统计函数库，spearmanr是斯皮尔曼等级相关系数from keras.preprocessing.text import Tokenizer# 令牌化from keras.preprocessing.sequence import pad_sequences# keras只能接受相同长度的序列输入，pad_sequences()序列填充让序列长度一致。import scipy as sp# scipy用于数学科学工程领域，可以处插值、积分、优化、图像处理、常微分方程、信号处理等问题。import random# 随机模块from sklearn.preprocessing import OneHotEncoder# 独热码import os# 不同的操作系统目录不同，os模块可以处理此问题import torch# pytorch框架import torch.nn as nn# nn是循环神经网络import torch.optim as optim# 实现各种优化算法的库import torch.nn.functional as F# torch的nn网络里边的函数库，比如距离函数、损失函数、正则化函数等import torch.utils.data# 如上上import，定义了torch的数据格式import tensorflow_hub as hub# tensorflow_hub是tensorflow的hub库，里边有很多训练好的hub模型，module = hub.Module(&lt;&lt;Module URL as string&gt;&gt;, trainable=True)import keras.backend as K# keras可作为tensorflow，CNTK,Theano的应用程序接口。通过keras.json传递数据import sys# sys就是python和解释器打交道的系统。sys.argv是系统带入的参数，sys.version查看版本信息, \n #pip install ..  / / /  &gt; del/null, \n # sys.path.insert(0,/ / /)import transformers   # transformer模型import pickle# 普通数据和python数据之间互相转换，dump是普通换python，load是反过来,pickle是类似于json的一种标准数据格式from torch.utils.data import Dataset,DataLoader# Dataset定义torch的数据格式。loader有如下参数:(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None)from torch.autograd import Variable# Variable是torch的基本构件sb = SnowballStemmer(&quot;english&quot;)# SnowballStemmer另一种词干提取算法from tqdm import tqdm# 进度条from nltk.stem import PorterStemmer# stem是词干提取，PorterStemmer是其中一种提取算法ps = PorterStemmer()from nltk.stem import SnowballStemmerfrom torch.optim import lr_scheduler# lr_scheduler模块提供了根据epoch训练次数来调整学习率的方法from torch.nn.utils.rnn import pack_padded_sequence,pad_packed_sequence# 长度不同的句子们打包成一条线/一条线拆分成几个长度不同的句子#%%pd.set_option(&#39;max_rows&#39;, 500)pd.set_option(&#39;max_columns&#39;, 500)path = &#39;F:/Project/kag1/data&#39;sample_submission = pd.read_csv(f&#39;{path}/sample_submission.csv&#39;)test = pd.read_csv(f&#39;{path}/test.csv&#39;).fillna(&#39; &#39;)train = pd.read_csv(f&#39;{path}/train.csv&#39;).fillna(&#39; &#39;)#pandas#%%def sigmoid(x):    return 1 / (1 + np.exp(-x)) #定义钟形函数#无效字符puncts = [&#39;,&#39;, &#39;.&#39;, &#39;&quot;&#39;, &#39;:&#39;, &#39;)&#39;, &#39;(&#39;, &#39;-&#39;, &#39;!&#39;, &#39;?&#39;, &#39;|&#39;, &#39;;&#39;, &quot;&#39;&quot;, &#39;$&#39;, &#39;&amp;&#39;, &#39;/&#39;, &#39;[&#39;, &#39;]&#39;, &#39;&gt;&#39;, &#39;%&#39;, &#39;=&#39;, &#39;#&#39;, &#39;*&#39;, &#39;+&#39;, &#39;\\&#39;, &#39;•&#39;,  &#39;~&#39;, &#39;@&#39;, &#39;£&#39;, &#39;·&#39;, &#39;_&#39;, &#39;{&#39;, &#39;}&#39;, &#39;©&#39;, &#39;^&#39;, &#39;®&#39;, &#39;`&#39;,  &#39;&lt;&#39;, &#39;→&#39;, &#39;°&#39;, &#39;€&#39;, &#39;™&#39;, &#39;›&#39;,  &#39;♥&#39;, &#39;←&#39;, &#39;×&#39;, &#39;§&#39;, &#39;″&#39;, &#39;′&#39;, &#39;Â&#39;, &#39;█&#39;, &#39;½&#39;, &#39;à&#39;, &#39;…&#39;, &#39;\n&#39;, &#39;\xa0&#39;, &#39;\t&#39;, &#39;“&#39;, &#39;★&#39;, &#39;”&#39;, &#39;–&#39;, &#39;●&#39;, &#39;â&#39;, &#39;►&#39;, &#39;−&#39;, &#39;¢&#39;, &#39;²&#39;, &#39;¬&#39;, &#39;░&#39;, &#39;¶&#39;, &#39;↑&#39;, &#39;±&#39;, &#39;¿&#39;, &#39;▾&#39;, &#39;═&#39;, &#39;¦&#39;, &#39;║&#39;, &#39;―&#39;, &#39;¥&#39;, &#39;▓&#39;, &#39;—&#39;, &#39;‹&#39;, &#39;─&#39;, &#39;\u3000&#39;, &#39;\u202f&#39;, &#39;▒&#39;, &#39;：&#39;, &#39;¼&#39;, &#39;⊕&#39;, &#39;▼&#39;, &#39;▪&#39;, &#39;†&#39;, &#39;■&#39;, &#39;’&#39;, &#39;▀&#39;, &#39;¨&#39;, &#39;▄&#39;, &#39;♫&#39;, &#39;☆&#39;, &#39;é&#39;, &#39;¯&#39;, &#39;♦&#39;, &#39;¤&#39;, &#39;▲&#39;, &#39;è&#39;, &#39;¸&#39;, &#39;¾&#39;, &#39;Ã&#39;, &#39;⋅&#39;, &#39;‘&#39;, &#39;∞&#39;, &#39;«&#39;, &#39;∙&#39;, &#39;）&#39;, &#39;↓&#39;, &#39;、&#39;, &#39;│&#39;, &#39;（&#39;, &#39;»&#39;, &#39;，&#39;, &#39;♪&#39;, &#39;╩&#39;, &#39;╚&#39;, &#39;³&#39;, &#39;・&#39;, &#39;╦&#39;, &#39;╣&#39;, &#39;╔&#39;, &#39;╗&#39;, &#39;▬&#39;, &#39;❤&#39;, &#39;ï&#39;, &#39;Ø&#39;, &#39;¹&#39;, &#39;≤&#39;, &#39;‡&#39;, &#39;√&#39;, ]mispell_dict = {&quot;aren&#39;t&quot; : &quot;are not&quot;,&quot;can&#39;t&quot; : &quot;cannot&quot;,&quot;couldn&#39;t&quot; : &quot;could not&quot;,&quot;couldnt&quot; : &quot;could not&quot;,&quot;didn&#39;t&quot; : &quot;did not&quot;,&quot;doesn&#39;t&quot; : &quot;does not&quot;,&quot;doesnt&quot; : &quot;does not&quot;,&quot;don&#39;t&quot; : &quot;do not&quot;,&quot;hadn&#39;t&quot; : &quot;had not&quot;,&quot;hasn&#39;t&quot; : &quot;has not&quot;,&quot;haven&#39;t&quot; : &quot;have not&quot;,&quot;havent&quot; : &quot;have not&quot;,&quot;he&#39;d&quot; : &quot;he would&quot;,&quot;he&#39;ll&quot; : &quot;he will&quot;,&quot;he&#39;s&quot; : &quot;he is&quot;,&quot;i&#39;d&quot; : &quot;I would&quot;,&quot;i&#39;d&quot; : &quot;I had&quot;,&quot;i&#39;ll&quot; : &quot;I will&quot;,&quot;i&#39;m&quot; : &quot;I am&quot;,&quot;isn&#39;t&quot; : &quot;is not&quot;,&quot;it&#39;s&quot; : &quot;it is&quot;,&quot;it&#39;ll&quot;:&quot;it will&quot;,&quot;i&#39;ve&quot; : &quot;I have&quot;,&quot;let&#39;s&quot; : &quot;let us&quot;,&quot;mightn&#39;t&quot; : &quot;might not&quot;,&quot;mustn&#39;t&quot; : &quot;must not&quot;,&quot;shan&#39;t&quot; : &quot;shall not&quot;,&quot;she&#39;d&quot; : &quot;she would&quot;,&quot;she&#39;ll&quot; : &quot;she will&quot;,&quot;she&#39;s&quot; : &quot;she is&quot;,&quot;shouldn&#39;t&quot; : &quot;should not&quot;,&quot;shouldnt&quot; : &quot;should not&quot;,&quot;that&#39;s&quot; : &quot;that is&quot;,&quot;thats&quot; : &quot;that is&quot;,&quot;there&#39;s&quot; : &quot;there is&quot;,&quot;theres&quot; : &quot;there is&quot;,&quot;they&#39;d&quot; : &quot;they would&quot;,&quot;they&#39;ll&quot; : &quot;they will&quot;,&quot;they&#39;re&quot; : &quot;they are&quot;,&quot;theyre&quot;:  &quot;they are&quot;,&quot;they&#39;ve&quot; : &quot;they have&quot;,&quot;we&#39;d&quot; : &quot;we would&quot;,&quot;we&#39;re&quot; : &quot;we are&quot;,&quot;weren&#39;t&quot; : &quot;were not&quot;,&quot;we&#39;ve&quot; : &quot;we have&quot;,&quot;what&#39;ll&quot; : &quot;what will&quot;,&quot;what&#39;re&quot; : &quot;what are&quot;,&quot;what&#39;s&quot; : &quot;what is&quot;,&quot;what&#39;ve&quot; : &quot;what have&quot;,&quot;where&#39;s&quot; : &quot;where is&quot;,&quot;who&#39;d&quot; : &quot;who would&quot;,&quot;who&#39;ll&quot; : &quot;who will&quot;,&quot;who&#39;re&quot; : &quot;who are&quot;,&quot;who&#39;s&quot; : &quot;who is&quot;,&quot;who&#39;ve&quot; : &quot;who have&quot;,&quot;won&#39;t&quot; : &quot;will not&quot;,&quot;wouldn&#39;t&quot; : &quot;would not&quot;,&quot;you&#39;d&quot; : &quot;you would&quot;,&quot;you&#39;ll&quot; : &quot;you will&quot;,&quot;you&#39;re&quot; : &quot;you are&quot;,&quot;you&#39;ve&quot; : &quot;you have&quot;,&quot;&#39;re&quot;: &quot; are&quot;,&quot;wasn&#39;t&quot;: &quot;was not&quot;,&quot;we&#39;ll&quot;:&quot; will&quot;,&quot;didn&#39;t&quot;: &quot;did not&quot;,&quot;tryin&#39;&quot;:&quot;trying&quot;}def clean_text(x):    x = str(x)    for punct in puncts:        x = x.replace(punct,f&#39;{punct}&#39;)    return x# replace函数就是把第一个old变量换成第二个new变量# f格式化字符串使用举例：# &gt;&gt;&gt; name = &#39;Eric&#39;# &gt;&gt;&gt; f&#39;Hello, my name is {name}&#39;# &#39;Hello, my name is Eric&#39;def clean_numbers(x):    x = re.sub(&#39;[0-9]{5,}&#39;, &#39;#####&#39;, x)# 正则表达式，sub是替换函数，[0-9]是全体数字，{5，}是长度大于等于5的所有字符串# 这一句的意思就是把所有长度大于等于5的所有数字都替换成五个井号，下面的就是替换成1-4的长度的字符    x = re.sub(&#39;[0-9]{4}&#39;, &#39;####&#39;, x)    x = re.sub(&#39;[0-9]{3}&#39;, &#39;###&#39;, x)    x = re.sub(&#39;[0-9]{2}&#39;, &#39;##&#39;, x)    return xdef _get_mispell(mispell_dict):    mispell_re = re.compile(&#39;(%s)&#39; % &#39;|&#39;.join(mispell_dict.keys()))    return mispell_dict, mispell_re# 获取拼写错误的词# re.compile函数：编译一个正则表达式模板，返回一个正则表达式对象，多个正则表达式模板用&#39;|&#39;链接# join函数：把join前的符号依次链接到join后的各个词的间隙中，如下所示：# str = &quot;-&quot;;# seq = (&quot;a&quot;, &quot;b&quot;, &quot;c&quot;); # 字符串序列# print str.join( seq );def replace_typical_misspell(text):    mispellings, mispellings_re = _get_mispell(mispell_dict)    def replace(match):        return mispellings[match.group(0)]    return mispellings_re.sub(replace, text)# 将拼写有误的词替换掉的函数# .group(0) 返回母串中与子串匹配的第一个def clean_data(df,columns:list):    for col in columns:        df[col] = df[col].apply(lambda x : clean_numbers(x))        df[col] = df[col].apply(lambda x : clean_text(x.lower()))        df[col] = df[col].apply(lambda x : replace_typical_misspell(x))    return df# 总的清洗函数：# df是pandas的数据框架，就是excel表。col是数据行。# .apply是对df的操作。lambda x基本泛指所有的数据train = clean_data(train, [&#39;answer&#39;, &#39;question_body&#39;, &#39;question_title&#39;])test = clean_data(test, [&#39;answer&#39;, &#39;question_body&#39;, &#39;question_title&#39;])#%%seed_everything()#%%%%timeembedding_test = get_embedding_features(train, test, [&#39;answer&#39;, &#39;question_body&#39;, &#39;question_title&#39;], only_test=True)embedding_train = {}embedding_train[&#39;answer_embedding&#39;] = np.load(&#39;/kaggle/input/qa-labeling-files-for-inference/embedding_train_answer_embedding.npy&#39;, allow_pickle=True)embedding_train[&#39;question_body_embedding&#39;] = np.load(&#39;/kaggle/input/qa-labeling-files-for-inference/embedding_train_question_body_embedding.npy&#39;, allow_pickle=True)embedding_train[&#39;question_title_embedding&#39;] = np.load(&#39;/kaggle/input/qa-labeling-files-for-inference/embedding_train_question_title_embedding.npy&#39;, allow_pickle=True)#%%%%timedist_features_train, dist_features_test  = get_dist_features(embedding_train, embedding_test)#%%tokenizer = Tokenizer()full_text = list(train[&#39;question_body&#39;]) + \                       list(train[&#39;answer&#39;]) + \                       list(train[&#39;question_title&#39;]) + \                       list(test[&#39;question_body&#39;]) + \                       list(test[&#39;answer&#39;]) + \                       list(test[&#39;question_title&#39;])tokenizer.fit_on_texts(full_text)#%%embed_size=300embedding_path = &quot;/kaggle/input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl&quot;#%%%%timelemma_dict, word_dict = get_word_lemma_dict(full_text)#%%# tk.word_index = {k: v for k, v in tk.word_index.items() if k in word_dict.keys()}# train[&#39;host&#39;] = train[&#39;host&#39;].apply(lambda x: x.split(&#39;.&#39;)[-2])# test[&#39;host&#39;] = test[&#39;host&#39;].apply(lambda x: x.split(&#39;.&#39;)[-2])unique_hosts = list(set(train[&#39;host&#39;].unique().tolist() + test[&#39;host&#39;].unique().tolist()))host_dict = {i + 1: e for i, e in enumerate(unique_hosts)}host_dict_reverse = {v: k for k, v in host_dict.items()}unique_categories = list(set(train[&#39;category&#39;].unique().tolist() + test[&#39;category&#39;].unique().tolist()))category_dict = {i + 1: e for i, e in enumerate(unique_categories)}category_dict_reverse = {v: k for k, v in category_dict.items()}max_len = 500max_len_title = 30train_question_tokenized = pad_sequences(tokenizer.texts_to_sequences(train[&#39;question_body&#39;]), maxlen = max_len)train_answer_tokenized = pad_sequences(tokenizer.texts_to_sequences(train[&#39;answer&#39;]), maxlen = max_len)train_title_tokenized = pad_sequences(tokenizer.texts_to_sequences(train[&#39;question_title&#39;]), maxlen = max_len_title)test_question_tokenized = pad_sequences(tokenizer.texts_to_sequences(test[&#39;question_body&#39;]), maxlen = max_len)test_answer_tokenized = pad_sequences(tokenizer.texts_to_sequences(test[&#39;answer&#39;]), maxlen = max_len)test_title_tokenized = pad_sequences(tokenizer.texts_to_sequences(test[&#39;question_title&#39;]), maxlen = max_len_title)train_host = train[&#39;host&#39;].apply(lambda x: host_dict_reverse[x]).valuestrain_category = train[&#39;category&#39;].apply(lambda x: category_dict_reverse[x]).valuestest_host = test[&#39;host&#39;].apply(lambda x: host_dict_reverse[x]).valuestest_category = test[&#39;category&#39;].apply(lambda x: category_dict_reverse[x]).values#%%y = train[sample_submission.columns[1:]].values#%%num_workers = 0bs = 16n_cat = len(category_dict) + 1cat_emb = min(np.ceil((len(category_dict)) / 2), 50)n_host = len(host_dict)+1host_emb = min(np.ceil((len(host_dict)) / 2), 50)#%%bs_test = 16test_loader = DataLoader(TextDataset(test_question_tokenized, test_answer_tokenized, test_title_tokenized,                                     test_category, test_host, embedding_test, dist_features_test, test.index),                                     batch_size=bs_test, shuffle=False, num_workers=num_workers)#%%def get_coefs(word,*arr):    return word,np.asarray(arr,dtype=&#39;float32&#39;)def load_embeddings(path):    with open(path,&#39;rb&#39;) as f:        emb_arr = pickle.load(f)    return emb_arr# pickle.load(f)是python的标准导入数据的格式def build_matrix_adv(embedding_path : str = &#39;&#39;,                     embedding_path_spellcheck: str = r&#39;f:\embeddings\wiki-news-300d-1M\wiki-news-300d-1M.vec&#39;,                     word_dict : dict None,                     lemma_dict : dict = None,                     max_features : int = 100000,                     embed_size: int = 300, ):    spell_model = gensim.models.KeyedVectors.load_word2vec_format(embedding_path_spellcheck)    # 调用维基字典,字典的用途是训练词向量（本文件目的）    words = spell_model.index2word    # index转换成词    w_rank = {}    for i,word in enumrate(words):        w_rank[word] = i    WORDS = w_rank    def P(word):        &quot;word的概率&quot;        # 使用逆秩作为代理        return - WORDS.get(word,0)    def correction(word):        &quot;最可能的单次拼写的更正：&quot;        return max(candidates(word),key = P)    def candidates(word):        &quot;为word参数 生成可能的拼写更正&quot;        return (known([word])  or known(edits1(word))or [word])    def known(words):        &quot;在字典WORD中出现的words参数的子集&quot;        # 如26行所示，WORD是一个数组，words是训练集训练出来的word2vec矩阵？        # 不可以留疑问，这个文件写完后单步调试进去看看做的是什么事情        return set(w for w in words if w in WORDS)    def edits1(word):        &quot;与word相距离一次编辑的所有编辑&quot;        letters = &#39;abcdefghijklmnopqrstuuvwxyz&#39;        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]        deletes = [L + R[1:] for L, R in splits if R]        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) &gt; 1]        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]        inserts = [L + c + R for L, R in splits for c in letters]        return set(deletes + transposes + replaces + inserts)### L 和 R这里需要单帧步进研究一下    def edits2(word):        &quot;与word相距离两次编辑的所有编辑&quot;        return (e2 for e1 in edits1(word) for e2 in edits1(e1))    def singlify(word):        return &quot;&quot;.join([letter for i,letter in enumerate(word) if i == 0 or letter != word[i - 1]])# embedding_index = dict(get_coefs(*o.strip().split(&quot; &quot;)) for o in open(embedding_path, encoding=&#39;utf-8&#39;))# embedding_index = dict(get_coefs(*o.strip().split(&quot; &quot;)) for o in open(embedding_path, encoding=&#39;utf-8&#39;, errors=&#39;ignore&#39;))    embedding_index = load_embeddings(embedding_path)    nb_words = min(max_features,len(word_dict))    embedding_matrix = np.zeros((nb_words + 1,embed_size))    uknown_words = []    for word, i in word_dict.items():        key = word        embedding_vector = embedding_index.get(word)        if embedding_vector is not None:            embedding_matrix[i] = embedding_vector            continue        embedding_vector = embedding_index.get(word.lower())        if embedding_vector is not None:            embedding_matrix[i] = embedding_vector            continue        embedding_vector = embedding_index.get(word.upper())        if embedding_vector is not None:            embedding_matrix[i] = embedding_vector            continue        embedding_vector = embedding_index.get(word.capitalize())        if embedding_vector is not None:            embedding_matrix[i] = embedding_vector            continue        if embedding_vector is not None:            embedding_matrix[word_dict[key]] = embedding_vector            continue        word = lc.stem(key)        embedding_vector = embedding_index.get(word)        if embedding_vector is not None:            embedding_matrix[word_dict[key]] = embedding_vector            continue        word = sb.stem(key)        embedding_vector = embedding_index.get(word)        if embedding_vector is not None:            embedding_matrix[word_dict[key]] = embedding_vector            continue        word = lemma_dict[key]        embedding_vector = embedding_index.get(word)        if embedding_vector is not None:            embedding_matrix[word_dict[key]] = embedding_vector            continue        if len(key) &gt; 1:            word = correction(key)            embedding_vector = embedding_index.get(word)            if embedding_vector is not None:                embedding_matrix[word_dict[key]] = embedding_vector                continue        unknown_words.append(key)    print(f&#39;{len(unknown_words) * 100 / len(word_dict):.4f}% words are not in embeddings&#39;)    return embedding_matrix, nb_words, unknown_wordsdef get_word_lemma_dict(full_text: list = None, ):    nlp = spacy.load(&#39;en_core_web_lg&#39;, disable=[&#39;parser&#39;,&#39;ner&#39;,&#39;tagger&#39;])    nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)    word_dict = {}    word_index = 1    lemma_dict = {}    docs = nlp.pipe(full_text, n_threads = os.cpu_count())    for doc in docs:        for token in doc:            if (token.text not in word_dict) and (token.pos_ is not &quot;PUNCT&quot;):                word_dict[token.text] = word_index                word_index += 1                lemma_dict[token.text] = token.lemma_    return lemma_dict, word_dictdef build_matrix(embedding_path: str = &#39;&#39;,                 embedding_path_spellcheck: str = r&#39;f:\embeddings\wiki-news-300d-1M\wiki-news-300d-1M.vec&#39;,                 word_dict: dict = None, max_features: int = 100000,                 embed_size: int= 300, ):    # embedding_index = dict(get_coefs(*o.strip().split(&quot; &quot;)) for o in open(embedding_path, encoding=&#39;utf-8&#39;))    embedding_index = load_embeddings(embedding_path)    nb_words = min(max_features, len(word_dict))    embedding_matrix = np.zeros((nb_words + 1, embed_size))    unknown_words = []    for word, i in word_dict.items():        key = word        embedding_vector = embedding_index.get(word)        if embedding_vector is not None:            embedding_matrix[i] = embedding_vector            continue        embedding_vector = embedding_index.get(word.lower())        if embedding_vector is not None:            embedding_matrix[i] = embedding_vector            continue        embedding_vector = embedding_index.get(word.upper())        if embedding_vector is not None:            embedding_matrix[i] = embedding_vector            continue        embedding_vector = embedding_index.get(word.capitalize())        if embedding_vector is not None:            embedding_matrix[i] = embedding_vector            continue        unknown_words.append(key)    print(f&#39;{len(unknown_words) * 100 / len(word_dict):.4f}% words are not in embeddings&#39;)    return embedding_matrix, nb_words, unknown_wordsdef seed_everything(seed=1234):    random.seed(seed)    os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed)    np.random.seed(seed)    torch.manual_seed(seed)    torch.cuda.manual_seed(seed)    torch.backends.cudnn.deterministic = True</code></pre>]]></content>
    
    
    <categories>
      
      <category>2.比赛</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据处理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/批量修改文件后缀名的方式</title>
    <link href="undefined2020/03/04/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E6%89%B9%E9%87%8F%E4%BF%AE%E6%94%B9%E6%96%87%E4%BB%B6%E5%90%8E%E7%BC%80%E5%90%8D%E7%9A%84%E6%96%B9%E5%BC%8F/"/>
    <url>2020/03/04/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E6%89%B9%E9%87%8F%E4%BF%AE%E6%94%B9%E6%96%87%E4%BB%B6%E5%90%8E%E7%BC%80%E5%90%8D%E7%9A%84%E6%96%B9%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<h4 id=""><a href="#" class="headerlink" title=""></a></h4><p>新建一个.txt文档，输入：</p><p>ren *.java *.md</p><p>保存</p><p>将文件后缀名改成.bat,双击运行</p>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/修改pip源</title>
    <link href="undefined2020/03/02/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E4%BF%AE%E6%94%B9pip%E6%BA%90/"/>
    <url>2020/03/02/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E4%BF%AE%E6%94%B9pip%E6%BA%90/</url>
    
    <content type="html"><![CDATA[<h4 id=""><a href="#" class="headerlink" title=""></a></h4><ol><li><h4 id="将pip切换回国内源"><a href="#将pip切换回国内源" class="headerlink" title="将pip切换回国内源"></a>将pip切换回国内源</h4><p><strong>pip国内的一些镜像</strong></p><p>  阿里云 <a href="http://mirrors.aliyun.com/pypi/simple/" target="_blank" rel="noopener">http://mirrors.aliyun.com/pypi/simple/</a><br>  中国科技大学 <a href="https://pypi.mirrors.ustc.edu.cn/simple/" target="_blank" rel="noopener">https://pypi.mirrors.ustc.edu.cn/simple/</a><br>  豆瓣(douban) <a href="http://pypi.douban.com/simple/" target="_blank" rel="noopener">http://pypi.douban.com/simple/</a><br>  清华大学 <a href="https://pypi.tuna.tsinghua.edu.cn/simple/" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple/</a><br>  中国科学技术大学 <a href="http://pypi.mirrors.ustc.edu.cn/simple/" target="_blank" rel="noopener">http://pypi.mirrors.ustc.edu.cn/simple/</a></p><p><strong>修改源方法：</strong></p><p><strong>临时使用：</strong><br>可以在使用pip的时候在后面加上-i参数，指定pip源<br>eg: pip install scrapy -i <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple</a>   –trusted-host  <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">pypi.tuna.tsinghua.edu.cn</a></p><p><strong>永久修改：</strong><br><strong>linux:</strong><br>修改 ~/.pip/pip.conf (没有就创建一个)， 内容如下：</p><pre><code>[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple</code></pre></li></ol><p>   <strong>windows:</strong><br>   直接在user目录中创建一个pip目录，如：C:\Users\xx\pip，新建文件pip.ini，内容如下</p><pre><code>   [global]   index-url = https://pypi.tuna.tsinghua.edu.cn/simple</code></pre><ol start="2"><li><h4 id="-1"><a href="#-1" class="headerlink" title=""></a></h4></li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_Transformer</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_Transformer/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_Transformer/</url>
    
    <content type="html"><![CDATA[<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>总览图：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200507_1042_20_724.png" alt=""></p><p>Transformer由编码器和解码器构成，如下：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_ESVLKlPHhd.png" alt=""></p><h3 id="1-编码器详解"><a href="#1-编码器详解" class="headerlink" title="1.编码器详解"></a>1.<strong>编码器详解</strong></h3><p>编码器有两层（前馈神经网络层，自注意力层）。</p><p>像大部分NLP应用一样，Transformer首先将每个输入单词通过词嵌入算法转换为词向量。每个单词都被嵌入为512维的向量，下图使用方框表示这些向量。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_hkMoMASPhK.png" alt=""></p><p>词嵌入只发生在最下面那个编码器中，然后这个编码器会输出一个向量列表，列表中的每个向量大小都是512维。向量列表的大小是可以调节的超参数–一般被设置为最长句子的长度。每个编码器的输入输出格式都相同。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_WK2a3xSATp.png" alt=""></p><p>开始编码的时候，将列表中的向量传递到自注意力层进行处理，然后传到到前馈神经网络层，然后将输出结果传递到下一层。</p><h3 id="2-自注意力机制详解-其实就是维持了一个关于句子本身的二维数组"><a href="#2-自注意力机制详解-其实就是维持了一个关于句子本身的二维数组" class="headerlink" title="2.自注意力机制详解(其实就是维持了一个关于句子本身的二维数组)"></a>2.<strong>自注意力机制详解</strong>(其实就是维持了一个关于句子本身的二维数组)</h3><p><strong>宏观角度</strong>看，比如翻译一个句子，“The animal didn`t cross the street because it was too tired”，it指代什么？对于人类来说，指代的是animal不是street，很容易但是对于计算机来说这就是一个复杂的问题。</p><p>当模型处理“it”这个单词的时候，自注意力机制会允许it和animal之间发生联系。</p><p>随着模型处理输入序列的每个单词，自注意力机制会关注整个输入序列的所有单词，帮助模型对本单词更好的编码。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_qMZ3PBAKgx.png" alt=""></p><p>如图，RNN循环神经网络中，自注意力机制会关注整个输入序列的所有单词，帮助模型对本单词更好的进行编码。</p><p><strong>微观角度</strong>看，如何使用向量计算自注意力，然后看看如何使用矩阵来实现，计算自注意力的第一步是从每个编码器的输入向量（每个单词的词向量）中生成三个向量。也就是说对于每个单词，我们创造一个查询向量，一个key向量，一个value向量。这三个向量是通过词嵌入与三个权重矩阵相乘创建的。</p><p>这些新向量在维度上比词嵌入向量更低，他们的维度是64，而词嵌入和编码器的输入/输出向量的维度是512。这些分向量比总的词嵌入向量小的原因是基于架构的选择，使得多头注意力的大部分计算保持不变。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_6vXIOeu37l.png" alt=""></p><p><strong>查询向量，key向量，value向量详解</strong></p><p>他们都是有助于计算和理解注意力机制的抽象概念。</p><p>计算自注意力的第二步是计算得分，假设我们这个例子，第一个词是“thinking”，计算自注意力向量，我们需要拿输入句子中的每个单词对“Thinking”<strong>打分</strong>，这些分数决定了在编码单词“Thinking”的过程中有多重视句子的其他部分。</p><p><strong>打分也就是填充自注意力机制的二维数组</strong>，具体是通过每个单词的key向量与“thinking”单词的查询向量的点积来计算的(目前的理解是WE中语义近的向量点积大语义远的点积小)。如果我们是处理位置最靠前的单词的自注意力的话，第一个分数是q1*k1,第二个分数是q1*k2。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_0d3s4CIJ2B.png" alt=""></p><p>接下来对每个打分除以8,8这个数字是默认值，在论文中是维度64的平方根，除以了这个值之后，会让梯度更加稳定。softmax函数是把所有的值都挤压到0-1的区间内，且总值为1。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_pbeEMhw8OP.png" alt=""></p><p>于是完成自注意力的计算，得到的向量可以传给前馈神经网络，实际中，这些计算是通过矩阵完成的，以便算的更快。接下来研究具体如何用矩阵计算。</p><p><strong>用矩阵计算自注意力机制的各个向量</strong></p><p>第一步是计算查询矩阵、key矩阵、value矩阵。我们将输入句子的词嵌入装进矩阵X中，将其乘以我们训练的权重矩阵（WQ,WK,WV）</p><p>如下图，将输入的值装入X矩阵中，乘以我们训练的权重矩阵（WQ,WK,WV）</p><p>X矩阵中的每一行对应于输入句子中的每一个单词，词嵌入向量512维，表示为4个格子，q/k/v向量是64维，表示为3个格子。</p><p>最后，可以用矩阵乘法将上文提到的步骤2到步骤6（查询向量*值向量、除以8、softmax，求和等等）写在一个矩阵公式中：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_Z5aUHOiyMU.png" alt=""></p><p><strong>大战多头怪</strong></p><p>通过增加“多头”注意力，进一步完善了自注意力层，并在两方面提高了注意力层的性能。</p><p>1.拓展了模型专注于不同位置的能力。在上边的例子里，每个编码都在z1中或多或少有些体现，但是他可能被实际的单词本身所支配。翻译开篇的句子“The animal didn`t cross the street because it was too tired”,我们会想知道it指代的是哪个词，这时候模型的“多头”注意机制会起到作用。</p><p>2.他给出了注意力层的多个“表示子空间”，接下来我们会看到，对于“多头”注意力机制，我们有多个查询/key/value的权重矩阵集合（Transformer有八个注意力头，所以每个编码、解码器有八个矩阵集合）。这些集合中的每一个都是随机初始化的。在训练后，每个集合都会被用来将输入词嵌入投影到不同的表示子空间中。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_xJZmBNGucQ.png" alt=""></p><p>在多头注意力机制下，我们为每个头保持独立的查询/KEY/VALUE 的权重矩阵，做和上个session相同的自注意力计算，只需要八次不同的权重运算，我们就会得到八个不同的Z矩阵。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_qSIYj5VkoT.png" alt=""></p><p>可是前馈层只需要一个矩阵，如何把八个矩阵压缩为一个矩阵？可以先将八个矩阵拼接在一起，然后使用附加的权重矩阵WO将他们相乘。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_FABCnIsnJY.png" alt=""></p><p>下图是详细过程</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_e3uK4kPwAH.png" alt=""></p><p>再以上文的句子为例，不同的注意力头注意的是不同的词。在编码“it”的时候，有的头注意“animal”，有的头注意“tired”。从某种意义上来说，模型对“it”一词的表达在某种程度上是“animal”和“tired”的代表。把所有的attention都汇聚在一张图里，如下所示。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_0a2yBSahem.png" alt=""></p><p><strong>使用位置编码表示序列的顺序</strong></p><p>目前为止，我们对模型的描述缺少了一种理解输入单词顺序的方法。为此，Transformer为每个输入的词嵌入添加了一个向量，这些向量遵循学习到的特定模式，这有利于确定每个单词的位置，或者序列中不同单词之间的距离。这个向量就是<strong>位置向量</strong>。将<strong>位置向量</strong>添加到词嵌入中，能使得他们在接下来的运算中更好地表达词与词之间的距离。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_rtIMuWomLy.png" alt=""></p><p>如上图，给词嵌入添加了位置编码向量，使得词嵌入变成了基于时间序列的词嵌入。</p><p>假设词嵌入的维数是4（mini-词嵌入），实际的位置编码如下：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_ogqh4cGQnc.png" alt=""></p><p>下图中，每一行对应一个词向量的位置编码。第一行对应输入序列的第一个词。每行包括512个值。每个值都在-1到1之间。离1越近越黄，离-1越近越蓝。</p><p>下图是一个20个字的位置编码实例，词嵌入的大小是512维，可以看到在中间分成两半，左边一半的值由一个正弦函数生成，右边由余弦函数生成。</p><p>原始论文中描述了位置编码的公式，可以在get_timing_signal_1d()中看到生成位置编码的代码。当然这种方法不是唯一的生成位置编码的方法，然而，他的优点是能够扩展到未知的序列长度。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_eexMR6j1wC.png" alt=""></p><p><strong>残差模块</strong></p><p>编码器架构中有个细节：在编码器中的每个子层（自注意力，前馈）的周围都有一个残差链接，并都跟随一个“层-归一化”步骤。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_7G0MyFlmao.png" alt=""></p><p>编码器和解码器的子层均是这样，一个二层编码-解码结构的Transformer，看起来如下图所示。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_32nAP7wlCy.png" alt=""></p><h3 id="解码组件"><a href="#解码组件" class="headerlink" title="解码组件"></a><strong>解码组件</strong></h3><p>解码器有三层（前馈神经网络，解码-编码注意力层，自注意力层）。</p><p>编码器通过处理输入序列开始工作，顶端编码器的输出会转变为一个包含K向量（Key向量）的V（Value）向量的注意力向量集。这些向量将被每个解码器用于自身的“编码-解码注意力层”，而这些层可以帮助解码器关注输入序列那些位置更加合适。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_5pXw7V2lMP.png" alt=""></p><p>在解码阶段，解码的每个步骤都会输出一个输出序列的元素，本例子是英语翻译的句子。接下来的步骤中重复了这个过程，知道达到一个特殊的终止符号，他表示transformer的解码器已经完成了他的输出。每个步骤的输出在下一个时间步被提供给底端解码器。</p><p>那些在解码器中的自注意力层表现的模式和编码器不同：在解码器中，自注意力层只被允许处理输出序列中更靠前的哪些位置，在softmax步骤前，他会把后面的位置给隐去（设为-inf）</p><p>这个位置-解码注意力层工作方式基本就像多头自注意力层一样，只不过是它通过在它下面的层来创造查询矩阵，并从解码器的输出中取得键/值矩阵。</p><p><strong>最终的线性变换和Softmax层</strong></p><p>上文的解码器组件输出的是一个实数向量。线性变换层把向量解析为单词。之后是softmax层。</p><p>线性变换层是一个简单的全连接神经网络，可以把解码器组件产生的向量投射到一个比他大得多的被称为“对数几率”的向量里。不妨假设我们的模型从训练集中学习了一万个不同的英语单词（输出词表），因此对数几率向量为一万个单元格长度的向量——每个单元格对应一个单词的分数。</p><p>接下来softmax层会把分数变成概率，值在0到1 之间，概率最高的单元格被选中，并且对应的单词作为这个时间步的输出。</p><p>如下图，logit层维持的是每个单词和对应的数字，经过softmax后把大的放大小的缩小。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_bM4NMPLbch.png" alt=""></p><p><strong>训练部分总结</strong></p><p>总结一下transformer的前向传播过程，假设我们的输出词汇仅仅包含六个单词，如下。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_2YT0YCe7uh.png" alt=""></p><p>模型的输出词表在训练前就被设定好了，一但我们定义了我们的输出词表，我们可以使用一个相同宽度的向量来表示我们词汇表中的每一个单词，这也被认为是一个one-hot编码，所以我们用如下的one-hot encoding来表示“am”。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_GqEGmgYoC2.png" alt=""></p><p>*<em>损失函数 *</em></p><p>比如我们在训练模型，现在是第一步，把“merci”翻译成“thanks”，想要的输出是thanks这里的独热码是1，其他都是0，这个向量表示了这个单词，但是因为这个模型还没有被训练好，所以不太可能现在就出现这个结果。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_ggC9OV3tPL.png" alt=""></p><p>模型的参数是被随机生成的，然后采用反向传播算法来调整所有模型的权重，生成更接近结果的输出。</p><p>更现实的情况是处理一个句子。例如，输入“je suis étudiant”并期望输出是“i am a student”。那我们就希望我们的模型能够成功地在这些情况下输出概率分布：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_Pf7i3u2vEV.png" alt=""></p><p>当然这只是理想情况，真实的模型输出的概率分布更可能是这样</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_tf9h4bL4qK.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_BERT</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_BERT/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_BERT/</url>
    
    <content type="html"><![CDATA[<h1 id="Bert-概念"><a href="#Bert-概念" class="headerlink" title="Bert 概念"></a>Bert 概念</h1><h4 id="1-全景介绍"><a href="#1-全景介绍" class="headerlink" title="1.全景介绍"></a>1.全景介绍</h4><p>BERT是一种深度学习模型，他是Transformer的双向编码器表示，在维基百科和Books Corpus上预训练过，运用于特定任务时只需要微调即可。</p><p>他的效果很好，在很多NLP任务中都有最近进展，包括问答系统（Squad）和自然语言推理（MNLI）任务。</p><p>BERT改变了NLP的格局。跑一个在大量未标记数据集上训练的模型，在11个单独的NLP任务中仅仅通过不同的微调，就可以分别得到11个最新的结果，这种表现只有BERT可以做到。</p><p>BERT还启发了TransformerXL,GPT-2,XLNet,ERNIE2.0,RoBERTa等等。</p><h4 id="2-什么是BERT"><a href="#2-什么是BERT" class="headerlink" title="2.什么是BERT?"></a>2.什么是BERT?</h4><p>1.基本上是堆叠在一起的一堆Transformer encoders，注意仅仅是Transformer encoders不是整个Transformer架构。<strong>双向性</strong>的概念十分重要，是BERT和其前身OpenAL GPT的关键区别，BERT是<strong>双向的</strong>因为他在<strong>自我注意层</strong>的两个方向上都执行自我注意。</p><p>2.要注意的是BERT在维基百科（25亿词）和Books Corpus（8亿词）上的预训练非常重要，因为模型在一个大的文本语料库中训练一个模型时，我们的模型开始对语言的工作方式有了更深入的了解。这种了解是瑞士军刀，对各种任务都是有用的。</p><p>3.BERT是深度双向模型，双向的意思就是，每个token在训练阶段，都从上下文的左边和右边学习信息。</p><p>4.BERT通过添加几个额外的输出层来微调它，从而为各种NLP任务创建最先进的模型。</p><h4 id="3-BERT如何工作？"><a href="#3-BERT如何工作？" class="headerlink" title="3.BERT如何工作？"></a>3.BERT如何工作？</h4><p><strong>BERT ARCH</strong></p><p>多层双向Transformer结构：</p><ul><li>BERT base： 12层（transformer blocks），12个attention heads，110 million 参数</li><li>BERT Large：24层，16个attention heads，340 million参数</li></ul><p><strong>text的预处理</strong> </p><p>首先，BERT的输入可以是一个句子，也可以是一对句子（一问一答）。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_Y5XO2ivPic.png" alt=""></p><p>如图，[CLS]符号作用于整个句子序列表示的聚合，在非分类任务中这个符号被忽略。对于单文本任务，[CLS]标记后面跟的是WordPiece标记和[SEP]分隔符标记。</p><p>对于句子对任务，两个句子的词条标记由一个[SEP]标记开始，另一个[SEP]标记结束。</p><p>BERT的开发人员对在输入模型之前已经设置了一组特定的规则来表示语言：</p><ul><li>位置嵌入Position Embedding：学习并使用位置嵌入来表示单词在句子中的位置，这样做是为了克服Transformer的限制，他不像RNN一样可以捕获序列或者顺序的信息。</li><li>段嵌入Segment Embedding：BERT可将句子作为对任务（问答）的输入，这就是为什么他学习第一句和第二句的独特嵌入，以帮助模型区分他们，所有标记为EA的标记，都属于句子A。</li><li>令牌嵌入Toekn Embedding：这些是WordPiece令牌词汇表中为特定的令牌学习的嵌入，对于给定的令牌，其输入表示是通过对相应的令牌，段和位置嵌入求和来构造的。</li></ul><p><strong>令牌化Tokenization</strong>：</p><p>BERT使用词块标记化，用语言中的所有单个字符初始化词汇表，然后迭代添加词汇表中现有单词的最频繁、最可能的组合。</p><h4 id="3-Pre-Training："><a href="#3-Pre-Training：" class="headerlink" title="3.Pre Training："></a>3.Pre Training：</h4><p>该模型同时接受了“蒙面语言学模型”和“下一句猜测”两个任务的训练。</p><h4 id="4-BERT的特定微调技术"><a href="#4-BERT的特定微调技术" class="headerlink" title="4.BERT的特定微调技术"></a>4.BERT的特定微调技术</h4><p>将BERT用于特定任务相对简单，BERT可以应用于各种各样的语言任务，同时只向核心模型添加一个小层。</p><ul><li><p>序列分类任务</p><p>[CLS]标记的最终隐状态被视为输入序列的固定维池表示，这被输入到分类层，分类层是唯一添加的新参数，其维数是K x H,其中K是分类器标签的数目，H是隐藏状态的大小，标签概率用标准的softmax计算。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_YYl72NFjdd.png" alt=""></p></li><li><p>句子对分类任务</p><p>与上边的序列分类任务完全类似，唯一的不同就是输入的表示是两个句子连在一起的。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_wfpkyM0vZ4.png" alt=""></p></li><li><p>QA任务</p><p>QA估计回答任务是一个预测任务，给定一个问题一个一段文章，该模型从最有可能回答该问题的段落预测一个开始标记和一个结束标记。</p><p>和句子对任务一样，QA任务中的问题也会成为输入序列的第一个和第二个句子。在微调开始和结束向量（大小与隐状态相同）的过程中，只有两个参数被学习到。Token i作为应答开始的文本标签的概率，被计算为softmax（S,K）,S是开始向量，K是令牌的最终Transformer输出。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_NPyWFd2L5e.png" alt=""></p></li><li><p>单个句子tagging任务</p><p>在诸如命名实体识别的单句标记任务中，必须为输入的每个单词预测一个标记。最终隐状态（transformer输出）被feed给分类层，已获得对每个token的预测。由于WordPiece标记器将一些单词分解成子单词，因此只考虑单词的第一个标记的预测。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_sUOeH4lp7v.png" alt=""></p></li><li><p>超参数调整</p><p>每个任务都有最适合自己的超参数，如下的超参数在各个任务中普遍表现不差。</p><p>dropout – 0.1<br>Batch Size – 16, 32<br>Learning Rate (Adam) – 5e-5, 3e-5, 2e-5<br>Number of epochs – 3, 4 (yeah you read it right)</p><p>同时，大数据集（标记样本  &gt;  100k）对超参数选择的敏感性低于小数据集。</p></li></ul><h4 id="5-关于QA任务的BERT-Benchmarks"><a href="#5-关于QA任务的BERT-Benchmarks" class="headerlink" title="5.关于QA任务的BERT Benchmarks"></a>5.关于QA任务的BERT Benchmarks</h4><p>斯坦福问答数据集（SQuAD）是10万个众包问答对的集合，给定一个问题和维基百科里包含答案的段落,QA问题的任务就是找到问答段落里的答案文本跨度。</p><p>在SQuAD中，得分最高的模型就是BERT Large。</p><h4 id="6-其他关键点"><a href="#6-其他关键点" class="headerlink" title="6.其他关键点"></a>6.其他关键点</h4><ul><li>模型大小很重要，BERT_LARGE有3.45亿个参数，是同类模型中最大的，它在小规模任务中的优势也要明显高于BERT_BASE，后者使用同样的架构，参数只有1.1亿个。</li><li>在有了足够的训练数据的情况下，更多的训练步骤=更高的准确性，例如，在MNLI（Multi-Genre Natural Language Inference）多题材自然语言推理任务中，128000字的批大小上训练时，训练100万步的精度要比训练50万步的精度高1%。</li><li>BERT的双向方法收敛速度要慢于从左向右的方法，因为每批中只有15%的单词被预测。但是经过少量的预训练步骤后，双向训练仍然优于从左向右的训练。</li></ul>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Bert</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/网络_1-概述</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BD%91%E7%BB%9C_1-%E6%A6%82%E8%BF%B0/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BD%91%E7%BB%9C_1-%E6%A6%82%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="1-计算机网络概述"><a href="#1-计算机网络概述" class="headerlink" title="1.计算机网络概述"></a>1.计算机网络概述</h1><h4 id="1-1局域网"><a href="#1-1局域网" class="headerlink" title="1.1局域网"></a>1.1局域网</h4><p>​        覆盖范围小（１００ｍ）,自己花钱买设备,贷款固定</p><p>​        接入层的交换机们星形连接到汇聚层交换机,这样分散IO</p><p>​        接入层交换机们不该串联,不然最后一台交换机那里IO压力太大</p><h4 id="1-2Internet和广域网"><a href="#1-2Internet和广域网" class="headerlink" title="1.2Internet和广域网"></a>1.2Internet和广域网</h4><p>​    Internet ISP :有自己的机房,对网民提供internet访问</p><p>　广域网：距离较远，花钱租带宽，</p><h4 id="1-3-数据包和数据帧"><a href="#1-3-数据包和数据帧" class="headerlink" title="1.3 数据包和数据帧"></a>1.3 数据包和数据帧</h4><p>在一个计算机想把数据发给另一个计算机时,需要在发送信息里携带ip地址和mac地址,其中ip地址是最终目地,mac地址是下一跳的目的地,ip地址不变,mac地址每次经过路由器都刷新.下一跳的目的地由迪杰斯特拉或其他寻路算法确定.</p><h4 id="1-4-OSI参考模型"><a href="#1-4-OSI参考模型" class="headerlink" title="1.4 OSI参考模型"></a>1.4 OSI参考模型</h4><p> 应用层:所有能产生网络流量的程序</p><p>表示层:在传输之前是否进行加密或压缩处理</p><p>会话层:查木马,netstat -n</p><p>传输层:可靠传输,流量控制,不可靠传输</p><p>网络层:负责选择最佳路径,规划ip地址</p><p>数据链路层:帧的开始和结束,透明传输,差错校验</p><p>物理层:接口标准,电器标准,如何在物理链路层上传输的更快</p><ul><li><p>osi参考模型对网络排错指导</p><ul><li>物理层故障：查看链接状态，发送和接收数据包</li><li>数据链路层故障：MAC冲突，ADSL欠费</li><li>网络层故障：IP地址，子网掩码，网关配置错误，路由器没有配置达到目标网络的路由</li><li>应用层故障：应用程序配置错误</li></ul></li><li><p>OSI参考模型和网络安全</p><ul><li>物理层安全</li><li>数据链路层安全：ADSL账号密码，vlan，交换机端口绑定mac地址</li><li>网络层安全：路由器上设置访问控制列表（ACL），设置某些计算机不能访问Internet，</li><li>应用层安全：开发的程序没有漏洞</li></ul></li><li><p>TCP/IP协议和OSI参考模型</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1241_10_572.png" alt=""></p></li></ul>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机网络</tag>
      
      <tag>OSI参考模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/网络_2-物理层</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BD%91%E7%BB%9C_2-%E7%89%A9%E7%90%86%E5%B1%82/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BD%91%E7%BB%9C_2-%E7%89%A9%E7%90%86%E5%B1%82/</url>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h1 id="2-物理层"><a href="#2-物理层" class="headerlink" title="2.物理层"></a>2.物理层</h1><h3 id="2-1物理层的基本概念"><a href="#2-1物理层的基本概念" class="headerlink" title="2.1物理层的基本概念"></a>2.1物理层的基本概念</h3><p>物理层解决如何在链接各种计算机的传输媒体上传输数据比特流。</p><p>物理层的主要任务描述：确定传输媒体的接口的一些特性。例如：</p><ul><li><p>机械特性：接口形状，大小，引线数目</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1241_35_978.png" alt=""></p></li><li><p>电器特性：电压范围</p></li><li><p>功能特性，过程特性等</p></li></ul><h3 id="2-2数据通信的基本模型"><a href="#2-2数据通信的基本模型" class="headerlink" title="2.2数据通信的基本模型"></a>2.2数据通信的基本模型</h3><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1241_43_905.png" alt=""></p><p>相关术语：</p><ul><li>数据：运送消息的实体</li><li>信号：数据的电器或电磁表现<ul><li>模拟信号：消息的参数取值是连续的</li><li>数字信号：消息的参数取值是离散的</li></ul></li></ul><h4 id="信道"><a href="#信道" class="headerlink" title="信道"></a>信道</h4><p>信道一般表示向一个方向传送信息的媒体，我们常说的信道往往包含了一条发送信息的信道和一条接受信息的信道</p><p>单向信道：信息只能单向流动</p><p>双向交替信道：信息可双向流动，但不能同时流动</p><p>双向同时信道：信息科双向，同时流动</p><h4 id="基带信号和带通信号"><a href="#基带信号和带通信号" class="headerlink" title="基带信号和带通信号"></a>基带信号和带通信号</h4><ul><li><p>基带信号：来自信源的信号，像计算机输出的各种代表文字和图像的信号都属于基带信号，我们说话的声波也是基带信号</p><ul><li><p>基带数字信号的几种调制方法</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1242_08_655.png" alt=""></p></li></ul></li><li><p>带通信号：把基带信号进行载波调制后，把信号的频率范围搬到较高的频段以便在信道中传输</p><ul><li>在传输范围较大的时候，计算机网络必须通过带通信号传输</li></ul></li></ul><p>曼彻斯特编码：由低到高是0，高到低是1</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1242_37_192.png" alt=""></p><p>差分曼彻斯特编码</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1242_47_991.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1242_56_962.png" alt=""></p><h3 id="2-3奈氏准则"><a href="#2-3奈氏准则" class="headerlink" title="2.3奈氏准则"></a>2.3奈氏准则</h3><p>给出了在理想条件下，为了避免码间串扰，码元的传输速率的上限值</p><p>在任何信道中，码元的传输速率需有限制，否则会出现码间串扰的问题，使接收端对码元的判决（识别）成为不可能</p><p>如果信道的带宽很宽，也就能通过的信号的高频分量越多，那就可以用更高的速率传送码元不出现串扰</p><h4 id="信噪比"><a href="#信噪比" class="headerlink" title="信噪比"></a>信噪比</h4><p>香农的信息论推导了带宽受限，有高斯白噪声干扰的信道的极限，无差错的信息传输速率</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1243_14_228.png" alt=""></p><p>如果频带带宽已经确定了的信道，如果信噪比和带宽都不能提高了，还有一种突破方式就是用编码的方式让一个码元携带更多比特量的信息</p><h4 id="2-4物理层的导向传输媒体"><a href="#2-4物理层的导向传输媒体" class="headerlink" title="2.4物理层的导向传输媒体"></a>2.4物理层的导向传输媒体</h4><ul><li><p>双绞线</p><ul><li>屏蔽双绞线</li><li>无屏蔽双绞线</li></ul></li><li><p>同轴电缆</p></li><li><p>光缆</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1243_29_412.png" alt=""></p></li></ul><h4 id="2-5物理层的非导向传输媒体"><a href="#2-5物理层的非导向传输媒体" class="headerlink" title="2.5物理层的非导向传输媒体"></a>2.5物理层的非导向传输媒体</h4><p>无线短波：靠电离层的反射，通信质量较差</p><p>微波：在空间中主要是直线传播（地面微波接力通信，卫星通信）</p><h4 id="2-6信道复用技术"><a href="#2-6信道复用技术" class="headerlink" title="2.6信道复用技术"></a>2.6信道复用技术</h4><p>复用是通信技术的基本概念</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1243_41_204.png" alt=""></p><ul><li>频分复用</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1243_57_759.png" alt=""></p><ul><li><p>时分复用</p><p>将时间划分为一段段等长的时分复用帧（TDM帧），每一个时分复用的用户在每一个TDM帧里占用固定序号的时隙</p><p>每一个用户占用的时隙是周期性出现的，周期就是TDM帧的长度</p><p>TDM信号也称为等时信号</p><p>时分复用的所有用户在不同的时间占用同样的带宽频率</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1244_30_908.png" alt=""></p></li></ul><p>时分复用可能造成资源浪费，优化方案是采用统计时分复用</p><ul><li>波分复用</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1245_07_550.png" alt=""></p><ul><li>码分复用</li></ul><p>各用户采取特殊挑选的不同码型，因此彼此不会造成干扰，这种系统发出的信号有很强的抗干扰能力</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1245_36_782.png" alt=""></p><h3 id="2-8数字传输系统"><a href="#2-8数字传输系统" class="headerlink" title="2.8数字传输系统"></a>2.8数字传输系统</h3><ul><li><p>脉码调制PCM体制</p><ul><li><p>脉码调制PCM体制最初是为了在电话局之间的中继线上传送多路的电话</p></li><li><p>由于历史原因PCM有两个不兼容的国际标准，北美的24路PCM和欧洲的30路PCM，我国采用欧洲的30路PCM</p></li><li><p>同步光纤网SONET和同步数字系列SDH</p><ul><li>旧的数字传输系统存在许多缺点，其中主要的是以下两个方面：<ul><li>速率标准不统一，如果速率无法统一，国际范围的高速数据传输就很难实现</li></ul></li><li>不是同步传输<ul><li>过去相当长时间内，为了节约经费，各国的数字网主要是采用准同步方式</li></ul></li></ul></li><li><p>同步光纤网详细介绍</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1245_47_582.png" alt=""></p><p>SONET标准定义了四个光接口层</p></li></ul></li><li><p>光子层</p><ul><li>处理跨越光缆的比特传送</li></ul></li><li><p>段层</p><ul><li>在光缆上传送STS-N帧</li></ul></li><li><p>线路层</p><ul><li>负责路径层的同步和复用</li></ul></li><li><p>路径层</p><ul><li>处理路径端接设备PTE（path terminating element）之间的业务传输</li></ul></li></ul><h4 id="2-9宽带介入技术"><a href="#2-9宽带介入技术" class="headerlink" title="2.9宽带介入技术"></a>2.9宽带介入技术</h4><ul><li>xDSL技术就是用数字技术对现有的模拟电话用户线进行改造，使他能够承载宽带业务</li><li>虽然标准模拟电话的频带被限制在300-3400khz的范围内，但用户线本身实际可通过的信号频率仍然超过1mhz</li><li>xDSL技术就是把0-4khz低端频谱留给传统电话业务，把原来设备中没有利用的高端频谱留给用户上网使用</li><li>DSL就是数字用户线的缩写，DSL的前缀x则表示在数字用户线上实现的不同带宽方案</li></ul><ul><li><p>ADSL的极限传输距离</p><ul><li>ADSL的极限传输距离与数据率以及用户线的路径有很大关系，用户线越细，信号传输时候的衰减就越大，而所能得到的最高数据传输速录与实际的用户线上的信噪比密切相关</li><li>例如0.5毫米的用户线，在网速为2.0Mb/s的时候可以传输5.5公里，传输速率提高到6.1Mb/s时，传输距离缩短为3.7公里</li><li>如果把用户线直径减少到0.4毫米，传输距离就缩小为2.7公里</li></ul></li><li><p>ADSL的特点</p><ul><li>上行和下行带宽做成不对称的</li><li>上行是用户到网络服务商，下行是反过来</li><li>ADSL在用户线（铜线）两端各安装一个ADSL调制解调器，目前我国采用的多为DMT调制技术</li></ul></li><li><p>ADSL的数据率</p><ul><li><p>当ADSL启动时，用户线的两端ADSL调制解调器就测试可用的频率，各子信道受到的干扰情况，以及在每一个频率上测试信号的传输质量</p></li><li><p>ADSL不能保证固定的数据率，对于质量比较差的用户甚至无法开通ADSL</p></li><li><p>通常下行速率在32kb到6.4mb之间，而上行速度一般在32kb到640kb之间</p></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机网络</tag>
      
      <tag>物理层</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/网络_3-数据链路层</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BD%91%E7%BB%9C_3-%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BD%91%E7%BB%9C_3-%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/</url>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h1 id="3-数据链路层"><a href="#3-数据链路层" class="headerlink" title="3.数据链路层"></a>3.数据链路层</h1><h4 id="3-0概述"><a href="#3-0概述" class="headerlink" title="3.0概述"></a>3.0概述</h4><p>数据链路层使用的信道主要有以下两类</p><ul><li>点对点信道<ul><li>这种信道使用一对一的点对点通信方式</li></ul></li><li>广播信道<ul><li>这种信道使用一对多的广播通信方式，因此过程较为复杂，广播信道上的链接主机很多，因此必须使用专门的共享信道协议来协调这些数据的转发</li></ul></li></ul><h4 id="3-1-数据链路和帧"><a href="#3-1-数据链路和帧" class="headerlink" title="3.1 数据链路和帧"></a>3.1 数据链路和帧</h4><p>链路是一条无源的点到点的物理线路，中间没有任何的其他交换节点。数据链路是除了物理线路之外，还得有通信协议来控制这些数据的传输，若把实现了这些协议的软硬件加到链路上，就构成了数据链路。</p><p>现在最常用的办法是使用适配器（网卡）来实现这些协议的硬件和软件，一般的适配器都包括了数据链路层和物理层这两层的功能。</p><p>数据链路层传输的是<strong>帧</strong>。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1248_20_314.png" alt=""></p><p>三个基本问题</p><ul><li>封装成帧</li><li>透明传输</li><li>差错控制</li></ul><ul><li>封装成帧：</li></ul><p>在一段数据的前后分别打上首部和尾部，首部尾部可以确定帧的界限。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1248_33_763.png" alt=""></p><ul><li>透明传输问题：</li></ul><p>类似于我想在markdown文件中打出[]()字符缺被误以为是超连接一样，需要在[]()字符之前加上转义字符\。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1249_02_535.png" alt=""></p><ul><li><p>CRC差错检测</p><p>传输过程中可能出现比特差错，1变成0或者0变成1。一段时间内传输错误的比特与总比特比值称谓误码率。为保证数据传输的可靠性，必须采用各种差错检测机制。</p></li></ul><p>  CRC差错检验算法就是让一串字符串加上几位0，然后将其除以某个固定的字符串，最后得出余数。然后根据二进制的数学特征，将余数取代之前几个0的位置，然后将这个字符串+余数的串发送出去。接收端收到后将这个大串除以某个固定的字符串，最后要是得到的余数为0就说明传输正确，如果不得0就错误要丢弃。<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1249_31_389.png" alt=""></p><h4 id="3-2点对点协议PPP"><a href="#3-2点对点协议PPP" class="headerlink" title="3.2点对点协议PPP"></a>3.2点对点协议PPP</h4><p>全世界用的最多的数据链路层协议是PPP（Point to Point）</p><p>PPP协议满足的要求：简单，封装成帧，透明性，多种网络层协议，差错检测，网络地址协商等等</p><p>PPP协议不需要的功能：纠错，流量控制，序号，多点线路，半双工或单工链路</p><p>PPP协议的三个组成部分</p><ul><li>一个将IP数据报封装到串行链路的方法</li><li>链路控制协议LCP</li><li>网络控制协议NCP</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1249_51_947.png" alt=""></p><p>​    </p><p>PPP协议的工作状态</p><p>当用户对ISP拨号上网时，路由器的调制解调器对拨号进行确认，并建立一条物理连接。</p><p>PC机向路由器发送一系列LCP分组（封装成多个PPP帧），这些分组和响应选择一些PPP参数，和进行网络层配置，NCP给新接入的的PC机分配一个临时的IP地址，通信完毕后，NCP收回临时IP，释放网络层连接，LCP释放数据链路层和物理层的连接。</p><h4 id="3-3局域网的数据链路层"><a href="#3-3局域网的数据链路层" class="headerlink" title="3.3局域网的数据链路层"></a>3.3局域网的数据链路层</h4><p>局域网最主要特点是，网络为一个单位所有，且地理范围和站点数目有限。</p><p>局域网的优点：</p><p>广播，一个站点可以方便地访问全网，局域网的主机可以共享连接在局域网上的各种硬件和资源。</p><p>可扩展性良好。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1250_02_184.png" alt=""></p><h4 id="3-4适配器的作用"><a href="#3-4适配器的作用" class="headerlink" title="3.4适配器的作用"></a>3.4适配器的作用</h4><p>网络接口板又称为通信适配器，或网络接口卡（网卡），网卡的重要功能是</p><ul><li><p>进行串行/并行转换</p></li><li><p>对数据进行存储</p></li><li><p>在计算机的操作系统安装设备驱动程序</p></li><li><p>实现以太网协议</p></li></ul><p>计算机通过适配器(网卡与局域网通信)</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1250_11_401.png" alt=""></p><h4 id="3-5数据链路层的mac子层"><a href="#3-5数据链路层的mac子层" class="headerlink" title="3.5数据链路层的mac子层"></a>3.5数据链路层的mac子层</h4><p>mac层就是媒体接入控制层（medium access control）。所有与传输媒体有关的内容都放在mac层。</p><p>局域网中，硬件地址又称为物理地址，或者mac地址。适配器从网络接收到mac帧后就首先使用硬件检查mac帧中的mac地址。</p><p>如果是发往本站的帧就收下，否则丢弃。</p><p>mac帧格式</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1251_26_001.png" alt=""></p><h4 id="3-5-1数据链路层的llc子层"><a href="#3-5-1数据链路层的llc子层" class="headerlink" title="3.5.1数据链路层的llc子层"></a>3.5.1数据链路层的llc子层</h4><p>llc层与传输媒体无关，不管采用何种协议的局域网对llc层来说都是透明的。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1252_33_876.png" alt=""></p><h4 id="3-6扩展局域网"><a href="#3-6扩展局域网" class="headerlink" title="3.6扩展局域网"></a>3.6扩展局域网</h4><ul><li>物理层扩展</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1252_43_362.png" alt=""></p><p>使用集线器扩展局域网的优劣势：</p><p>优势：扩大了通信范围</p><p>劣势：碰撞域变大，总吞吐量未变。</p><ul><li>数据链路层扩展</li></ul><p>数据链路层扩展局域网的思路是使用网桥。</p><p>网桥工作在数据链路层，它根据mac帧的目的地址对收到的帧进行转发。</p><p>网桥能过滤帧，当收到帧后，并不向所有的接口转发此帧，而是先检查此帧的目的mac地址，然后确定将帧转发到哪个接口。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1253_13_860.png" alt=""></p><p>使用网桥的好处：</p><p>过滤了通信量，扩大了物理范围，提高了可靠性，可互联不同物理层，不同mac子层和不同速率的局域网</p><p>网桥的劣势：</p><p>存储转发增加了时延，在mac子层并没有流量控制功能。网桥只适用于用户数量不太多的局域网，否则会因为过多的广播造成网络拥塞。</p><h4 id="3-7网桥"><a href="#3-7网桥" class="headerlink" title="3.7网桥"></a>3.7网桥</h4><p>网桥按照如下步骤处理收到的帧和建立转发表</p><ul><li>若从A发出的帧从接口X进入某网桥，那么这个接口出发反向一定可以把一个帧传回A</li><li>网桥每收到一个帧，就记录下其源地址和进入网桥的接口，作为转发表的一个项目</li><li>在建立转发表时把帧首部中的源地址写在地址这一栏的下方</li><li>转发帧时吗，根据帧的首部中的目的地地址来转发。</li></ul><p>网桥转发表的建立图示</p><p><img src="http://q0u9fsub0.bkt.clouddn.com///20191119/2EsnQOcgjqNl.png?imageslim" alt="mark">)<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1253_25_956.png" alt=""></p><p>透明网桥使用了 生成树算法</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1253_42_576.png" alt=""></p><p>互联在一起的网桥在彼此通信时吗，能够找出原来网络拓扑的一个子集，在这个子集里，整个网络中不存在回路，在任何两个网桥之间只有一条路径。</p><p>这样避免了某个转发的帧不停在回路中兜圈子。</p><h4 id="3-8交换机"><a href="#3-8交换机" class="headerlink" title="3.8交换机"></a>3.8交换机</h4><p>多接口网桥-交换机，以太交换机通常有十几个接口，以太网交换机其实就是一个多接口的网桥。交换机工作在链路层。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1255_19_111.png" alt=""></p><p>小结：</p><p>数据链路层的三大问题是解决将信息压缩为帧，透明传输（用转移符防止提前截断），误差控制（CRC检测算法）。</p><p>注意帧的格式分报头报尾和内容。</p><p>通过数据链路层扩展局域网的思路是在网与网之间增加网桥，网桥可以根据帧里的地址对各个主机进行转发操作。每个网桥都要维持一个自己的转发表，里边写的是目的地主机的地址和端口号。使用网桥时要注意maintain图的最小生成树，避免转发帧的时候掉进途中的回路不停兜圈子。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>计算机网络</tag>
      
      <tag>数据链路层</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/统计学习方法_2_感知机</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95_2_%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95_2_%E6%84%9F%E7%9F%A5%E6%9C%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="2-感知机"><a href="#2-感知机" class="headerlink" title="2.感知机"></a>2.感知机</h1><p>感知机是二分类的线性分类模型，输入是实例的特征向量，输出是分类结果（-1,1）。</p><p>感知机对应输入空间（特征空间）中将实例划为正副两类的分离超平面。</p><p>工作原理是导入基于误分类的损失函数，利用梯度下降法岁损失函数极小化。</p><p>感知机1957年提出，是神经网络和SVM的基础。</p><p>本章先介绍感知机的模型，然后叙述学习策略，然后是算法（原始和对偶形式）。</p><h4 id="2-1感知机模型"><a href="#2-1感知机模型" class="headerlink" title="2.1感知机模型"></a>2.1感知机模型</h4><p>感知机模型是f(x) = sign(w.x + b).</p><p>其中sign是如下的函数</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/wps_Y0h9RgRT0Z.png" alt=""></p><p>也就是一个一元一次方程（直线）作为超平面的分类函数。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/wps_nfw9GjBBjJ.png" alt=""></p><h4 id="2-2感知机学习策略"><a href="#2-2感知机学习策略" class="headerlink" title="2.2感知机学习策略"></a>2.2感知机学习策略</h4><p>假设训练集是可分的，感知机的学习目的就是确定一个超平面把训练集分开。</p><p>为了找出这个超平面，就要确定感知机模型的参数w和b。</p><p>损失函数怎么定？定为错分类点到超平面的总距离w。（如上图）</p><p>某个点到超平面的距离怎么求？如下图，||w||是w的范数。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/wps_NC1ume3B2c.png" alt=""></p><p>损失函数的正式定义；</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/wps_xU9URkynlO.png" alt=""></p><h4 id="2-3感知机的学习算法"><a href="#2-3感知机的学习算法" class="headerlink" title="2.3感知机的学习算法"></a>2.3感知机的学习算法</h4><p>随机梯度下降法</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>统计学习方法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/统计学习方法_1_概论</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95_1_%E6%A6%82%E8%AE%BA/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95_1_%E6%A6%82%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="1-概论"><a href="#1-概论" class="headerlink" title="1.概论"></a>1.概论</h1><p>介绍基本概念，是对全书的概括。</p><p>首先叙述统计学习的定义、研究对象和方法，然后叙述监督学习。</p><p>然后提出统计学习方法的三要素：模型，策略和算法。</p><p>介绍模型选择，包括正则化、交叉验证和学习的泛化能力。</p><p>介绍生成模型和判别模型</p><p>介绍监督学习方法的应用： 分类问题，标注问题，回归问题。</p><h4 id="1-1统计学习"><a href="#1-1统计学习" class="headerlink" title="1.1统计学习"></a>1.1统计学习</h4><p><strong>定义</strong></p><p>统计学习是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测和分析的学科。</p><p>统计学习是概率论，统计学，信息论，计算理论，最优化理论，计算机科学等领域的交叉学科。在发展中逐步有了自己的理论体系和方法论。</p><p><strong>目的</strong></p><p>统计学习用于对未知数据预测和分析。</p><p>对数据的预测可以使计算机更加智能化/计算机的性能提升。</p><p>对数据的分析可以使人们获取新的知识。</p><p><strong>方法</strong></p><p>统计学习的方法是基于数据构建统计模型进而对数据进行预测和分析，统计学习由监督学习，半监督学习，无监督学习和强化学习等组成。</p><p>本书主要讨论的是监督学习，这种情况统计学习的方法可以概括如下：</p><p>从给定的有限的训练数据集出发（training set）,假设数据是独立同分布产生的；</p><p>假设要学习的模型属于某个函数的集合，称为假设空间。</p><p>把模型应用于某个评价标准，从假设空间中选取一个最优的模型，使得它对已知训练数据和未知测试数据(test set)在给定的评价准则下有最优的预测。</p><p>最优模型的选取由算法实现（调参？）。</p><p>统计学习的三要素：模型，策略，方法。(model,strategy,algorithm)</p><p>实现统计学习方法的步骤如下：</p><ol><li><p>得到有限的训练数据集合</p></li><li><p>确定包含所有可能的模型的假设空间，即学习模型的集合</p></li><li><p>模型选择的准则，即学习的策略</p></li><li><p>实现求解最优模型的算法，即学习的算法</p></li><li><p>通过学习方法选择最优模型</p></li><li><p>利用最优模型最新数据进行预测和分析</p></li></ol><p> 本书介绍统计学习方法为主，特别是监督学习方法，主要包括用于分类、标记、回归问题的方法。这些方法在自然语言处理、信息检索、文本数据挖掘等领域有着极为广泛的应用。</p><h4 id="1-3统计学习三要素"><a href="#1-3统计学习三要素" class="headerlink" title="1.3统计学习三要素"></a>1.3统计学习三要素</h4><p>方法 = 模型 + 策略 + 算法</p><p><strong>模型</strong></p><p>统计学习的首要问题是学习什么样的模型。</p><p>模型的假设空间包含了所有可能的模型，例如模型是线性函数，假设空间就是包含这些线性函数的函数族。</p><p><strong>策略</strong></p><p>有了模型的假设空间，统计学习接下来要考虑的是按什么样的准则学习最优模型。即明确评价标准。</p><p>引入损失函数和风险函数的概念。<strong>损失函数</strong>度量模型一次预测的好坏，<strong>风险函数</strong>度量平均意义下模型预测的好坏。</p><p>常用的损失函数比如0-1<strong>损失函数</strong>：预测对了得0分，错了得1分。得分越小说明模型越准。对数损失、平方损失同理。</p><p>学习的目标就是<strong>选择期望风险最小的模型</strong>。一方面根据期望风险最小学习模型要用到联合分布，另一方面联合分布式不可知的，所以监督学习是一个病态问题。</p><p>期望风险是模型关于联合分布的期望损失，经验风险是模型关于训练样本集的平均损失。根据大数定理，样本容量趋于N时，经验风险逼近期望风险。所以会试图使用经验风险预估期望风险。</p><p>但是效果通常不理想因为样本数不够。</p><p>所以会对经验风险进行校正。这就关系到统计学习的基本策略：<strong>经验风险最小化</strong>和<strong>结构风险最小化</strong>。</p><ul><li><p>在假设空间、损失函数、训练数据确定的情况下，<strong>经验风险</strong>函数式可以确定。可认为经验风险最小的模型就是最优模型。</p><p>按照这个思路，按经验风险最小化求解最优模型就是个最优化问题。</p><p>样本容量足够大的时候经验风险最小化有不错的效果，例如极大似然估计就是经验风险最小化的一个例子。</p><p>但是数据量小的时候，经验风险最小化就有过拟合的问题。</p></li><li><p>为了防止过拟合，有了<strong>结构风险</strong>最小化的提法。</p><p>就是在经验模型上加上了模型复杂度的罚项（正则化项），模型越复杂惩罚度越高。</p></li></ul><p><strong>算法</strong></p><p>算法是学习模型的具体计算方法，统计学习基于训练数据集，根据学习策略，从假设空间中按照算法求解最优模型。</p><p>这时统计学习问题归结为最优化问题，统计学习的算法成为求解最优化问题的算法。如果最优化问题存在显式的解析解，这个最优化问题就比较简单。</p><p>但通常解析解不存在，这需要用数值计算的方法求解。统计学习可以利用已有的最优化算法，有时也需要开发独立的最优化算法。</p><p>统计学习方法间的不同，主要来自于<strong>模型、策略、算法</strong>的不同。这就是统计学习的三要素。</p><h4 id="1-4模型评估与模型选择"><a href="#1-4模型评估与模型选择" class="headerlink" title="1.4模型评估与模型选择"></a>1.4模型评估与模型选择</h4><p><strong>训练误差和测试误差</strong></p><p>即是training error和test error。训练误差小说说明问题容易被学习，测试误差小说明模型的泛化能力做的不错。</p><p><strong>过拟合问题</strong></p><p>在预测空间里，可以存在1次到多次的模型选择。一次模型欠拟合，9次模型过于复杂过拟合。如何尽量逼近“真模型”？可使用正则化和交叉验证的方法。</p><p><strong>正则化和交叉验证</strong></p><ul><li><p>正则化是结构风险最小化的实施，在经验风险上加上一个正则化项。正则化项一般是关于模型复杂度的单调增函数。模型越复杂正则化项越大。</p><p>正则化项可以采取不同的形式，回归问题中，损失函数是平方损失，正则化项可以使参数向量的二范数/一范数。</p><p>正则化项的目的是选择经验风险和模型复杂度同时较小的模型。</p><p>正则化项符合奥卡姆剃刀原理：在所有可选择的模型中，能很好解释已知数据且简单的模型才是好模型。</p><p>从贝叶斯估计的角度讲，正则化项对应模型的先验概率。复杂的模型有较大的先验概率，简单的模型有较小的先验概率。</p></li><li><p>交叉验证</p><p>比如将训练集73分成，然后循环验证。</p></li></ul><h4 id="1-6泛化能力"><a href="#1-6泛化能力" class="headerlink" title="1.6泛化能力"></a>1.6泛化能力</h4><p>一般通过测试误差来评价学习的泛化能力，但是这种评价是依赖于数据集的，数据集有限，评价可能不准。</p><p>统计学习试图从理论上对泛化能力进行分析。</p><p>什么是泛化误差？如果学习到的模型是f，模型对未知数据的误差即为泛化误差。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/wps_ueli4LT9Sd.png" alt=""></p><h4 id="1-7生成模型和判别模型"><a href="#1-7生成模型和判别模型" class="headerlink" title="1.7生成模型和判别模型"></a>1.7生成模型和判别模型</h4><ul><li>生成模型由数据学习联合概率分布，然后求出条件概率分布作为预测模型。</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/wps_8hp1GOvI07.png" alt=""></p><p>​    典型的生成模型是隐形马尔科夫和朴素贝叶斯。</p><ul><li><p>判别模型直接学习决策函数f(x)，或者条件概率分布P(x|y)。这就是判别模型。</p><p>典型的判别式模型有 k临近，感知机，决策树，逻辑斯谛克模型，最大熵，SVM，提升方法，CRF等。</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>统计学习方法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/线性代数_正交矩阵</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0_%E6%AD%A3%E4%BA%A4%E7%9F%A9%E9%98%B5/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0_%E6%AD%A3%E4%BA%A4%E7%9F%A9%E9%98%B5/</url>
    
    <content type="html"><![CDATA[<h1 id="正交矩阵"><a href="#正交矩阵" class="headerlink" title="正交矩阵"></a>正交矩阵</h1><p>如果一个矩阵乘以它自己的转置矩阵等于E（单位阵），则称为正交矩阵。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200517_1505_26_233.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>线代</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/线性代数_特征值&amp;特征向量计算</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0_%E7%89%B9%E5%BE%81%E5%80%BC&amp;%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F%E8%AE%A1%E7%AE%97/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0_%E7%89%B9%E5%BE%81%E5%80%BC&amp;%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F%E8%AE%A1%E7%AE%97/</url>
    
    <content type="html"><![CDATA[<h3 id="特征值特-amp-特征向量"><a href="#特征值特-amp-特征向量" class="headerlink" title="特征值特 &amp; 特征向量"></a>特征值特 &amp; 特征向量</h3><p>省略理论部分，一个例子讲清：<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1504_19_270.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1504_36_652.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>线代</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/线性代数_奇异值分解</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0_%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0_%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h1 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h1><h4 id="1-应用：影像压缩"><a href="#1-应用：影像压缩" class="headerlink" title="1.应用：影像压缩"></a>1.应用：影像压缩<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1501_07_483.png" alt=""></h4><h4 id="2-应用：过滤噪声"><a href="#2-应用：过滤噪声" class="headerlink" title="2.应用：过滤噪声"></a>2.应用：过滤噪声</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1501_44_400.png" alt=""></p><p>左图是原始图片。右图是经过SVD分解处理的图片，丢失了一些精确度，但是呈现效果更好，很可能丢失的精确度恰好是噪声。</p><p>具体做法是我们对15*25的矩阵做奇异值分解（SVD），然后得到如下15个特征值，其中前三个特征是比较重要，为了压缩数据和过滤噪声，我们只取前三个特征值，后边的全部丢弃。<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1501_57_277.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1502_14_915.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1502_27_306.png" alt=""></p><h4 id="5-计算例子"><a href="#5-计算例子" class="headerlink" title="5.计算例子"></a>5.计算例子<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1502_41_523.png" alt=""></h4><p>​    </p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1502_56_775.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1503_09_630.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1503_18_511.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>线代</tag>
      
      <tag>奇异值分解</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>5.杂谈/王威廉对博士生的要求</title>
    <link href="undefined2020/02/24/5.%E6%9D%82%E8%B0%88/%E7%8E%8B%E5%A8%81%E5%BB%89%E5%AF%B9%E5%8D%9A%E5%A3%AB%E7%94%9F%E7%9A%84%E8%A6%81%E6%B1%82/"/>
    <url>2020/02/24/5.%E6%9D%82%E8%B0%88/%E7%8E%8B%E5%A8%81%E5%BB%89%E5%AF%B9%E5%8D%9A%E5%A3%AB%E7%94%9F%E7%9A%84%E8%A6%81%E6%B1%82/</url>
    
    <content type="html"><![CDATA[<h1 id="Hiring-PhD-students"><a href="#Hiring-PhD-students" class="headerlink" title="Hiring PhD students"></a>Hiring PhD students</h1><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1314_54_995.png" alt=""></p><p>Q: What’s your research interest? </p><p>A: My research interests are in the broad areas of Artificial Intelligence. I am particular interested in Machine Learning and Natural Language Processing. More specifically, I am passionate about statistical relational learning, learning to reason, information extraction, multimodality, social media, and spoken language processing. Currently, I’m interested in deep learning methods for natural language processing, multimodal computing, and knowledge graph reasoning. You may want to read my research statement to get a better idea (though it might be slightly dated). </p><p>Q: What are the basic requirements for working with you? A: I have three basic requirements for my PhD students. First of all, students need to be </p><ul><li><p>Mathematically mature</p><p> (i.e., proficient and comfortable with probability theory, statistics, linear algebra, optimization, and calculus). One way to test yourself is to carefully go through some of the best ML papers published at recent ICML/NIPS/AAAI/IJCAI conferences, and see if you can fully understand the detailed Mathematical presentations from the main equations in those papers. </p></li><li><p>Secondly, I require my students to be proficient in programming</p><p>they should at least know one or more serious programming languages for relatively large-scale software engineering purposes (e.g., C, C++, Python, Java etc), and meanwhile, they have to master one of the command-line scripting languages such as Perl, AWK, Bash, Sed etc. The ability of prototyping ideas quickly in Matlab, Octave, R is often a big plus. Third, and probably most important: </p></li><li><p>hard-working</p><p>No matter how smart you are and what fields you are in, if you don’t spend enough time on your work, there is no way you can be an expert. An observation from the most successful PhD students is that they all work extremely hard. So, if you are a hard-working person who is passionate about advancing AI, and you are solid in Math and programming, please consider applying to UCSB. </p></li></ul><p>Q: What makes me stand out from the application pool? </p><p>A: To make you stand out, prior research experience is often a big plus: if you have prior publications at reputable locations, we know that you are on the right track. If you have published papers at leading conferences or journal, feel free to leave me a note and send me a copy. Alternatively, if you are a system builder / hacker, showing your contributions to major ML/NLP/AI projects on Github is also encouraged. Additionally, if you have very strong support letters from researchers that I know, it helps me to evaluate your potential. For undergraduate applicants without research experience, GPA and grades from standardized tests are often important.  </p><p>Q: How long does it take to do a PhD? </p><p>A: It depends. I’ve seen some very productive PhD students finishing in only 4 years. So the assumption is, if you are productive and you have created enough impacts, it is definitely possible to finish a PhD in a relatively short amount of time. </p><h4 id="总结（数学，编程，文章）"><a href="#总结（数学，编程，文章）" class="headerlink" title="总结（数学，编程，文章）"></a>总结（数学，编程，文章）</h4><p>三个要求和三个加分项：</p><p>要求一，数学上的成熟，精通概率论，线代，微积分，优化，统计。测试你自己的一种方法是仔细阅读最近在ICML/NIPS/AAAI/IJCAI会议上发表的一些最好的ML论文，看看你是否能从这些论文中的主要方程完全理解详细的数学演示。</p><p>要求二，精通编程，掌握如下语言至少一种：C、C++、Python、Java等，同时，必须掌握命令行脚本语言中的一个，如Perl、AWK、BASH、SED等。</p><p>要求三，勤奋，不管你多聪明，如果花在工作中的时间不够，那肯定成不了专家。</p><p>加分项一，著名刊物/会议发文</p><p>加分项二，在github上展示你对主要的ML/NLP/AL项目的贡献</p><p>加分项三，老师认识的研究人员的有力的推荐信。</p><p>tips，对于没有研究经验的本科申请者来说，GPA和标准化考试成绩非常重要。</p>]]></content>
    
    
    <categories>
      
      <category>5.杂谈</category>
      
    </categories>
    
    
    <tags>
      
      <tag>phd申请</tag>
      
      <tag>UCSB</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>5.杂谈/王阿-申请phd思路</title>
    <link href="undefined2020/02/24/5.%E6%9D%82%E8%B0%88/%E7%8E%8B%E9%98%BF-%E7%94%B3%E8%AF%B7phd%E6%80%9D%E8%B7%AF/"/>
    <url>2020/02/24/5.%E6%9D%82%E8%B0%88/%E7%8E%8B%E9%98%BF-%E7%94%B3%E8%AF%B7phd%E6%80%9D%E8%B7%AF/</url>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="我是如何申请到phd的"><a href="#我是如何申请到phd的" class="headerlink" title="我是如何申请到phd的"></a>我是如何申请到phd的</h2><p>转载自知乎–王阿的阿</p><p><strong>谢谢你们。</strong></p><p>在这篇经验贴里，我会告诉你我当时申请时面临的处境、我的决策过程，以及申请当中各个环节最精华的经验和技巧，我都毫无保留地贡献在这里了。</p><p>读完这篇文章，除了我介绍的各种申请技巧以外，我希望能让你认识到申请过程中决策和心态的重要意义：</p><ul><li>想成功申请美国的PhD，除了自己软硬件要出色，还有很重要的一点是对自己的定位，包括认识到自己的目标、兴趣以及自己的长处和缺陷。<strong>不论你是申请哪个学科的PhD，请重点关注我做决策时的思考内容和思考方向，这也许会对你有所启发。</strong></li><li>心理韧性、抗打击能力也是申请成功的一个重要因素。如果你知道GradCafe这个北美grad school申请论坛，你会发现上面有很多人诉说自己第二次、第三次才申请成功的经历。我就是其中一员。</li></ul><p>我的故事是这样婶儿的……</p><p>2017一整年是我这辈子最难熬的一年，这一年我第一次（也只申请了这一次）申请美国的临床心理学PhD（clinical psychology），申了13所学校。2017年的头四个月陆续收到13封拒信，无一录取，甚至连一个面试都没有。当时的我：</p><ul><li>已经在纽约市生活了三年；</li><li>拿到了美国的心理咨询硕士学位；</li><li>从大三下学期开始参与心理学实验室科研算起，满打满算我有了三年科研经验（我在国内读的本科，心理学专业在全国名列前茅）；</li><li>虽然水但也在纽约州和American Psychological Association大会上以第一作者身份做过poster presentation；</li><li>在美国有过一年半的与专业相关的实习经验；</li><li>一位认识八年关系很好的本科中国教授（同时是心理与行为研究院院长）和两位认识两年以上的美国教授为我写推荐信，其中一位教过我课，另一位在她的实验室里做过科研；</li><li>硕士GPA 3.8/4.0；</li><li>GRE verbal 156，quant 164，writing 3.5</li><li>Psych GRE (GRE subject test）740 （88th percentile）</li><li>因为有了美国硕士学位我所申请的这些学校没有要求我再考TOEFL</li></ul><p><img src="https://pic2.zhimg.com/50/v2-8d623aae677c9d6e5d2d773d29f6715c_hd.jpg" alt="img"></p><p>当我看到GradCafe上很多人报offer或是interview invite时我的心情是这样的</p><p>那一年申请，我彻底颗粒无收，即便是我认为的“保底”学校也没有录取我。</p><p>以下是我第一次申请临床心理学时申请的学校，你们可以看出来，我申请的这些学校没有几所是大众认知的“名校”：</p><p>Fairleigh Dickinson University</p><p>Sam Houston State University</p><p>University of Hawaii</p><p>University of Wisconsin-Madison</p><p>University of North Texas</p><p>CUNY Graduate Center</p><p>San Diego State University</p><p>Simon Fraser University</p><p>Louisiana State University</p><p>University of Missouri</p><p>University of Maryland, College Park</p><p>University of Nebraska Lincoln</p><p>University of Texas, El Paso（这个不是clinical psych，是legal psychology）</p><p>我无法判断我是在哪一轮被刷下来的，我自己猜测有以下几点原因：</p><ul><li>当时的推荐信不是很强（在下面我会讲什么是强推）</li><li>没有跟未来的导师（以下称之为professor of interest，或POI）直接相关的研究经验和研究成果</li><li>不会套磁也没有认真套磁（根本原因还是第二点，没有相关的研究经验和成果）</li></ul><p>我差点被击垮。当时大学挚友准备国内考研的面试，我每天早上帮他准备英语口语。每天能跟他说一个来小时话是帮我挺过那段艰难时期的良药。诺大的纽约，我周围却没有可以谈心的朋友，所以每次跟他微信语音都会把我们之间的对话录音，心情低落的时候就听一听。在这期间，我考虑过回国找工作，所以我跟几位国内的朋友聊过回国的前景。朋友们能说什么呢？无非也就是“回国也挺好”罢了。五月份之前，我也在美国投简历，希望一边工作一边再准备申请。投出的简历没有下文，我只好在为我写推荐信的教授的实验室继续做volunteer research assistant来维持OPT学生身份。五月底，过了我28岁生日后，我搬离Manhattan来到Queens，在这里我终于不用再住用屏风围起来的客厅，还有了自己的卫生间和浴室。当你住了三年多客厅后你不知道有自己的房间有多爽（fun fact：在Manhattan住客厅也要大概在$600-$900之间，如果你住的是$1500-$4000的apartment）。</p><p>我们停一下，存个盘。如果换作是你：</p><ul><li>在你读了13封拒信后；</li><li>身边没有父母或知心朋友倾诉；</li><li>找不到工作没有收入；</li><li>不可能知道第二次申请是否能够成功</li></ul><p>你还会再申请第二次吗？我没法替你回答这个问题。你需要全盘考虑自己的人生目标、生活状态、家庭经济收入以及其他因素再决定接下来的第二年要不要再申请一次。但有一点我希望你知道，如果你真的热爱做研究，那就不要害怕再经历一遍申请，不管你选择什么时候再次申请。</p><p>有了第一次失败的经验后，我重新思考了自己的定位：</p><ul><li>我第一次申请的clinical psychology，是心理学博士申请里最难申请的心理学分支（没有之一）。根据APA出版的Graduate Study in Psychology 2016年的申请数据显示，录取率在2%-8%之间。另外从我认识的教授那里得知，有些学校录取后还不保证有funding（你只能自费或自己想办法找funding）。所以，我第二次申请时决定不再申请这个方向了，我没有必要死磕这个方向，因为实在太冒险。</li><li>我意识到，<strong>我无法在不到半年的准备时间里（6月开始到12月1日申请截止日期）实现质的突破</strong>（例如发表一篇高质量的论文）。这种情况下再申请基本是又要浪费一年时间、精力和金钱。</li><li>根据我父母的财力状况和我自己的身心状态，如果我第二次申请再不成，我基本就没有机会再申请第三次了。</li></ul><p>所以为了最大化我的申请成功可能性，我要选择一个相对更容易被录取的专业方向。现在想来，这是我当时做的最恰当的一个关键决定。</p><p>在这里我说一下博士选校的问题，因为我发现有些人博士选校竟然是顺着所谓的专排一口气申请20个学校。博士选校最关键的几点建议：</p><ul><li>请主要按研究兴趣选校。你的研究兴趣应该跟你POI的研究兴趣一致，在一个小领域内。比方说，你想研究抑郁症里的一个topic，POI也研究抑郁症，但POI主要研究的topic不是你想研究的topic，这就没有问题，但如果POI主要是研究精神分裂症的，那你们的研究兴趣就不一致，POI就会觉得你跟他/她不match；实在不确定你们是否match，直接给POI发邮件问！表害羞！</li><li>不要把自己的研究兴趣限制得太小，否则你的选择余地会很小。你可以有一个主要的研究兴趣（如抑郁症），在这个领域里你有比较丰富的研究经验，然后为了扩大选择POI的范围，你可以有一些其他的研究兴趣（比如精神分裂症），这样你就可以选择这个program里其他的POI成为你的potential advisor，具体参考我下面提供的我的PS里如何表述的。扩大自己的研究兴趣是申请成功的一个很重要的一点，因为如果你只选择一个program里的一个POI，如果他录取了其他申请者，你就直接出局了，但如果你选择了两个到三个（不要再多了），那你被录取的可能性就大多了；</li><li>你当然可以参考专排，但如果专排靠前的某个学校里的教授没有你感兴趣的研究领域，你干嘛申请这个学校呢？你没法跟这样的教授套磁，没法把你的PS写得让他们感觉你们之间的fit，所以不要申请这样的学校；</li><li>衡量一个program适不适合你，除了看你的研究兴趣，还要衡量一下自己够不够格申请地到这个学校。硬件方面，你应该至少达到admitted students的GPA、GRE的中位数，软件方面，你需要直接跟current students联系，浏览他们的CV，看他们做了什么，咨询他们如何脱颖而出的。你也可以直接问POI（如果你没考虑好谁是你的POI，那就随便问一个），他们录取学生的标准，他们看重什么特质、经验和技能，然后朝这个方向努力。</li></ul><p>再插一句。Clinical psychology的很多program要求考GRE sub（psych GRE），或者你本科是其他专业的想在读博士转到clinical psychology，这种情况下你也可能被学校要求考sub。在这里我分享一下我当时自己整理出来的考点，可以到这里下载：</p><p><a href="https://drive.google.com/drive/folders/1nyCu4prLMCDU4PdPNfYARLL3rb75MJmx?usp=sharingdrive.google.com" target="_blank" rel="noopener">https://drive.google.com/drive/folders/1nyCu4prLMCDU4PdPNfYARLL3rb75MJmx?usp=sharingdrive.google.com</a></p><p>我用思维导图的方式覆盖了需要你掌握的知识点，你需要配合ETS官方推出的考试大纲、一两本英文心理学入门教材（Psychology and Life之类的）以及一本模拟考题集来准备。我当时还看了这个网站上的视频（不是ETS官方出品，而且是收费的）：</p><p><a href="https://study.com/academy/course/gre-psychology-study-guide-test-prep.htmlstudy.com" target="_blank" rel="noopener">https://study.com/academy/course/gre-psychology-study-guide-test-prep.htmlstudy.com</a></p><p>Sub全是选择题，当时考完感觉非常懵逼，以为要再考一遍了，但最后分数挺好（建议你考到85%甚至90%以上，尤其针对转专业的申请者），究其原因是其中很多题目你不能靠死记硬背来拿分，需要你真正理解这些心理学概念，所以很多题目拿不准。</p><p>这里有一位被哥大clinical psych PhD录取的中国学生在寄托论坛上发的经验帖，大家感受一下从国内被美国常春藤顶尖名校PhD program录取的学生的水平：</p><p>Clinical/Other Psych+Neuroscience PhD - 美国留学 寄托家园留学论坛bbs.gter.net</p><p>如果有其他跟clinical psychology申请相关的问题，也许你可以在下面这个帖子里找到。这个帖子里是一位clinical psych的教授回答申请者的各种问题，很有参考价值：</p><p><a href="https://forums.studentdoctor.net/threads/advice-from-prof-for-applicants-to-phd-programs.489204/forums.studentdoctor.net" target="_blank" rel="noopener">https://forums.studentdoctor.net/threads/advice-from-prof-for-applicants-to-phd-programs.489204/forums.studentdoctor.net</a></p><p>回到正题。第二次申请，我决定申请quantitative psychology和educational measurement and statistics（计量心理学和教育测量和统计）。在看GradCafe上以下几个讨论话题之前我从来没听说过这个分支，但当我读完这些讨论内容后，我脑子里已经做出申请这个方向的决定了（如果你感兴趣，请重点看Spunky的回答，他是UBC的quant psych的博士）：</p><p><a href="https://forum.thegradcafe.com/topic/61618-quantitative-psychology-phd/forum.thegradcafe.com" target="_blank" rel="noopener">https://forum.thegradcafe.com/topic/61618-quantitative-psychology-phd/forum.thegradcafe.com</a></p><p><a href="https://forum.thegradcafe.com/topic/46739-quantitative-psychology-educational-measurement-fall-2014-sop-help/?tab=comments#comment-1058018960forum.thegradcafe.com" target="_blank" rel="noopener">https://forum.thegradcafe.com/topic/46739-quantitative-psychology-educational-measurement-fall-2014-sop-help/?tab=comments#comment-1058018960forum.thegradcafe.com</a></p><p>DIfference in Quantitative Psychology Programsforum.thegradcafe.com</p><p><a href="https://forum.thegradcafe.com/topic/56006-which-field-is-right-for-me/?tab=comments#comment-1058152406forum.thegradcafe.com" target="_blank" rel="noopener">https://forum.thegradcafe.com/topic/56006-which-field-is-right-for-me/?tab=comments#comment-1058152406forum.thegradcafe.com</a></p><p>这里有一个比较完整的学校名单（不过这个单子里落了University of Texas, Austin和我现在所在的学校）：</p><p>Programs in Quantitative Psychology and Quantitative Methodssmep.org</p><p>我做这出这个改变申请方向的决定有以下几个原因：</p><ul><li>申请clinical psychology的难度真的很高。如果你去各个学校的网站上看faculty和current student的介绍，你很少会发现中国人的名字。每个program的申请人数都太多了，我极少见到有低于100个申请者的program：先说几个名校来举例，Graduate Study in Psychology (2016）的数据显示，Berkeley的申请人数达到了397个，仅录取8个（录取率2%）；Yale的申请人数为264，录取5个（录取率2%）；再说几个知名度低的学校：The Catholic University of America的申请人数是222，录了11个（录取率5%）；Georgia State University的申请者达到436人，录了19个（录取率4%）。你可以看出来，竞争激烈程度只能用rediculously competitive来形容。</li><li>我一直很喜欢学心理统计学，同时我这门课的成绩很好，我也非常喜欢用R来做数据分析，这是最根本的原因。我的理念一贯是兴趣第一，如果我对统计不感兴趣，我不会考虑申请这个方向的。我也有认知心理学的研究经验，但我没有申请那个方向，就是因为我对认知心理学不感兴趣；</li><li>所有社会科学分支都需要数据分析，因而计量心理学专业的博士生非常抢手，我可以去很多实验室做助研，跟不同专业的学者合作，可以学到很多不同领域的专业知识；</li><li>我有计量心理学方面的研究经验：我参与过几个meta-analysis project，如果你往下读就会发现，我这方面的经验是我被录取的最关键因素之一；</li><li>知道这个心理学分支的学生很少，所以申请者数量比临床心理学要少的多，竞争压力就小，成功申请的可能性就可能更高一些；</li><li>计量心理学是心理学学科里唯一一个博士毕业数量少于教职需求量的分支，也就是说将来更容易找到教职，如果我打算进入academia的话</li></ul><p>我必须声明的是，我没有任何要劝你们放弃申请临床心理学这个方向的意思。我之所以没有再申请第二次临床心理学的原因已经写在上面了，这是我在当时做出的最符合我实际情况的选择。</p><p>在第二次申请中，我集种精力提高我的软件方面，包括writing sample和statement of purpose，提前告诉我的三位letter writer（推荐人）我要重新申请以及我的新的申请方向是什么并得到了他们的支持。我没有盲目地再去刷GRE成绩。之所以走这条路线，是因为：</p><ul><li>我在GradCafe上阅读了大量经验帖，都是quant psych PhD students分享的很有价值的申请经验—他们强调，申请者应该突出自己对quant方面的热爱和经验，尤其是体现R programming方面的能力；</li><li>APA出版的Graduate Study in Psychology 2016年的申请数据显示，拿到录取的申请人的GRE成绩中位数大概在310-315之间，即便是quant部分只要上160就算是不错的分数了，<strong>并没有国内考试机构鼓吹的要上325分以上，quant要考满分170</strong></li></ul><p>根据我制定的申请路线，在接下来的半年内，我主要做了如下工作：</p><ul><li>为了利用好我做过meta-analysis研究的这一优势，我几乎只申请了以meta-analysis为主要研究兴趣的POI所在的program。这一选校和定位策略后来证明非常有效，在所有我申请的学校里，没有录取我的POI都不是研究meta-analysis的，也就是说录取我的POI都是研究meta-analysis的。所以PhD的申请一定要记住：自己的研究兴趣和POI的研究兴趣要匹配（但不用完全重合，只要在一个研究领域即可）。对于申请PhD，请不要像申硕士那样，顺着排名申前10或前20的。</li><li>花了三个月时间打磨出一篇达到接近发表水平的writing sample—还记得我之前提到过的我参与的meta-analysis项目吗？我在那个项目中自学了如何用R进行单组率的meta-analysis（一种小众的统计方法）。我在这篇writing sample中系统地总结了我自学的内容，写成了一篇60多页的tutorial，既包括理论部分也包括用R编程的部分。头两个多月全都花在coding上了，这个过程颇为艰辛，在coding过程中我遇到过数不清的问题。我在ResearchGate、CrossValidated、Stack Overflow上问问题，或者直接发邮件问一些专家，一个问题一个问题解决的。在我之前还没有哪个学者系统写过这样一篇tutorial，所以这是我的亮点，能把我跟其他申请人区分出来。写成之后，为保证没有任何语法错误，我找了美国的proofreading机构帮我修改润色语法。如果你感兴趣，可以在我的ResearchGate上下载阅读，当前阅读量已经超过2000人，并且得到不少好评：</li></ul><p>(PDF) How to Conduct a Meta-Analysis of Proportions in R: A Comprehensive Tutorial”<a href="https://pic2.zhimg.com/v2-f13a77e5df055b6d49f36a6559fb946d&quot;" target="_blank" rel="noopener">https://pic2.zhimg.com/v2-f13a77e5df055b6d49f36a6559fb946d&quot;</a></p><ul><li>我很清楚我申请学校的POI不可能有耐心和时间读完这篇超长的writing sample。所以我又花了两周录制了一段三十分钟的R programming教学视频并上传到YouTube来演示我的writing sample到底写了些什么。我的视频也得到不少好评（33个赞，其中一个是我自己点的，0差评）。感兴趣的可以在这里观看：</li></ul><p><a href="https://www.youtube.com/watch?v=2wbXTFvaRnM&amp;list=PLmYuhFRfrz_5n_9iZ_kwZcK65IUjDXgqR&amp;index=2&amp;t=1129swww.youtube.com" target="_blank" rel="noopener">https://www.youtube.com/watch?v=2wbXTFvaRnM&amp;list=PLmYuhFRfrz_5n_9iZ_kwZcK65IUjDXgqR&amp;index=2&amp;t=1129swww.youtube.com</a></p><ul><li>到这里还没有结束—我不想发两个链接给POI，所以为了展示自己的特色，我又制作了自己的学术网站，把简历、writing sample以及视频上传到网站上。这样我在发给POI的套磁email里只需要附上网址就可以很优雅地让POI了解我了。我当时是用WordPress做的网站，现在已经失效了，但我又用GitBook做了一个新的，感兴趣的话可以在这里浏览（网站修葺中）。</li></ul><p>以上这三部分绝对是我application package里面最有亮点的部分。接下来我们谈谈如何向POI介绍并展示自己（套磁）。以下是我的套磁信：</p><p>11/7/2017 11:56（我套磁很晚了，但不要紧，只要你真有料，deadline一周前套磁都可以）。补充一点：一般是提交申请前套瓷，这样你可以知道你的POI今年收不收学生，是否鼓励你继续申请。</p><p>Subject: Inquiry from a prospective graduate student for Fall 2018</p><p>Dear Dr. LastName,</p><p>I am interested in doing a PhD in the area of meta-analysis. I would like to inquire whether you are currently accepting graduate students.</p><p>I have research experience in meta-analysis. I hope that you can take a quick look at my paper on how to do a meta-analysis of proportions using R on my personal academic site (website address).</p><p>I also have a few specific questions about your program that I’d like to ask you. I know you are very busy, so I appreciate any time you can give me.</p><p>Regards,</p><p>MyFirstandLastName</p><p>M.A. in Forensic Mental Health Counseling</p><p>John Jay College of Criminal Justice, the City University of New York</p><p>这封套磁信有以下几个特点和功能：</p><ul><li>简洁，切忌长篇大论。教授每天会收到几十封邮件，你要保证他/她能在一分钟内读完你的邮件，记住你并想继续了解你；</li><li>核心功能：我跟你很搭（因为我有你研究领域的经验和你需要的研究技术，通过writing sample或publication来体现）；</li><li>展示自己的强项，impress your POIs so they would want to advocate for you；</li><li>邮件内容包括：</li></ul><ol><li>我想在你（POI）的研究领域读博士；</li><li>请问你今年是否招生（非常重要！）；</li><li>我有相关的研究经验，请见我的CV、论文或网站（最好有自己的网站）；</li><li>我有几个关于这个program的问题想咨询一下你；</li><li>感谢你阅读这封邮件</li></ol><p>之所以要询问POI几个问题，是为了：</p><ul><li>加深POI对你的印象</li><li>展示你真的对该program非常感兴趣</li><li>还有一个很实用的作用，就是POI的回答可以用在你的PS或SOP中</li></ul><p>你可以自行发挥，想出五个问题，但我建议你一定要问这个问题：</p><p>As far as you are concerned, what are the strengths of your program, in terms of faculty, course offerings, job prospects, etc.?</p><p>看到了吧，你可以把POI的回答内容放进你的PS里，向admission committee解释你为什么要申请该项目（因为POI回答中提到的这些优势），这表明你是经过认真调查后才申请该项目的，而不是随意申请的。</p><p>我跟每一个申请学校的POI都发邮件介绍了自己，让他们看到我的网站（我也给每个program里的一到两个老生发了邮件，询问他们program的优缺点，得到很多有用的信息）。我的套磁信的效果让我自己都感到吃惊，举几个例子：</p><p>Response from POI A:</p><p>11/7/2017 12:02</p><p>Yes I am taking students. Frankly your tutorial page is better than my web site… I am tempted to accept you just based on that!</p><p>Actually I can’t do that even if I wanted to but I am happy to talk with you or write back and forth to answer any questions. Also, I am cc’ing one of my new doc students, J.M., who may also share some insights into our program.</p><p>If you want to talk, today I am working at home and am quite open. Or tomorrow morning would also work. Suggest a time and I can see if it works for me, or just reply to my email with your questions.</p><p>Response from POI B:</p><p>11/9/2017</p><p>Thank you for the email and for your interest in our program. Really nice to learn a little bit about your work and interest from your introduction and your website (and good job in maintaining your online presence). Yes, I am currently accepting doctoral students and we definitely need more methodologists working on effect size statistics and meta-analysis. Therefore, I would strongly encourage you to apply for our program. Feel free to ask questions about our program and I look forward to talking to you.</p><p>P.S. Your HTML and YouTube tutorial are really neat. Great job!</p><p>Reponse from POI C:</p><p>11/10/2017</p><p>Thank you for reaching out to me concerning applying for the PhD program in Educational Psychology and Methodology. I am in particular interested in your potential application as your research interest is in line with my research expertise. I have a lot of projects to collaborate on in the area of multilevel meta-analysis of SCEDs (and applying Bayesian estimation). Enclosed you can find my CV.</p><p>Yes, currently we have full assistantships available (for four years) and I encourage you to apply given your research interest.</p><p>I am very much looking forward learning more about you and your background. Of course I am also happy to answer any questions you have concerning the program and such. If you’d like, we can arrange a Skype conversation for the upcoming week. Or you can always send me your questions per email and I will answer them this weekend. Please feel free to send me your application package (if you have that ready) so I can already have a look at that.</p><p>如果你的套磁信能达到这样的效果，那录取就有戏了（我不想说稳了，我比较保守）。</p><p>接下来我讲一下另一个重头戏：personal statement/statement of purpose（以下统称PS）。在这里我推荐各位读两篇同一个人写的PS。作者是Berkeley数学系的博士Piotr Achinger，他第一次申请被拒，第二次申请成功了。</p><ul><li>这是被拒那次写的PS：</li></ul><p><a href="https://drive.google.com/file/d/1f2skv0yq5Xo9RQr29m89dWiUTVycrwgG/view?usp=sharingdrive.google.com" target="_blank" rel="noopener">https://drive.google.com/file/d/1f2skv0yq5Xo9RQr29m89dWiUTVycrwgG/view?usp=sharingdrive.google.com</a></p><ul><li>这是拿到录取的那次写的PS：</li></ul><p><a href="https://drive.google.com/file/d/16QnxOBaZ9jfEKNR4yIF3dfPraXIPr476/view?usp=sharingdrive.google.com" target="_blank" rel="noopener">https://drive.google.com/file/d/16QnxOBaZ9jfEKNR4yIF3dfPraXIPr476/view?usp=sharingdrive.google.com</a></p><p>我来帮你总结一下弱PS和强PS到底有什么差别。</p><p>强PS：</p><ul><li>能让读者知道你做过什么并体会到你的<strong>批判性思维</strong>（PhD的核心能力）；</li><li>是既有广度又有深度的立体结构（体现你的专业性）；</li><li>读起来带劲，能让读者随你一起思考</li></ul><p>弱PS：</p><ul><li>只能让读者知道你做过什么；</li><li>只有广度而缺乏深度的平面结构；</li><li>读起来无聊，因为没人关心陌生人的生平事迹</li></ul><p>那如何体现你的思考能力呢？</p><ul><li>举2-3个科研经历；</li><li>描述你研究的问题，可以是前人没有解决的问题以及/或你发现前人研究中的缺陷：</li></ul><p>- 前人没有解决的问题：你用什么方法解决的这个问题以及解决方法的效果</p><p>- 前人研究中的缺陷：用推理证明为什么是缺陷</p><p>- 提出新问题（optional: 提出可能的大体解决方法）</p><ul><li>不要写成下面这样，这样写仅仅是扩充了你的CV：</li></ul><p>- 我参与了一个什么研究，我负责干什么，我克服了什么困难，我们的成果是什么</p><p>我推荐的写作结构：</p><ul><li>简略：读博原因—因为我对最近进行的一项科研项目很感兴趣，想继续钻研下去（而非童年往事，不要灵光乍现突然顿悟了，也无需参透人生使命是什么）；</li><li>详细：体现思考能力的科研经历并展示你具备这个专业方向所需要的科研技术，例如编程；</li><li>简略：读博期间的研究方向；</li><li>简略：毕业后的目标—academia or industry?</li><li>详细：为什么想申请贵校博士项目（夸赞该项目的强项，你跟POI套磁过程中得到的信息在这里排上用场了）以及想跟哪几位POI读博士（实际上这一段也可以放在PS开头）；</li><li>optional—没有什么意义总结性结尾：我相信自己是跟贵项目匹配的一名出色的申请者</li></ul><p>附上我的一篇PS：</p><p>After conducting a series of meta-analyses <strong>(这是POI的研究兴趣，开头就点出来，吸引POI的眼球)</strong> at the Sex Offender Research Lab led by Dr. E. J. at John Jay College of Criminal Justice where I pursued my master’s degree, I discovered I was passionate about learning to improve existing quantitative approaches and wanted to try to develop new ones to advance the field of psychology. Thus, my first academic objective is to pursue a doctorate in quantitative methods in order to effectively pursue my research interests and attain my professional goals <strong>(因为最近做的一系列研究我对这个研究领域感兴趣啦，我还想继续在这个领域干，所以我要申请读博士)</strong>.</p><p>To better prepare for rigorous doctoral study, I accumulated skills and knowledge that will aid my doctoral studies and research over the course of my involvement in various research projects <strong>(下面我告诉你们我最得意的研究项目，我改进了前人的研究，而且从这个项目里我获取的经验和技能是你们非常value的)</strong>. For example, one of the meta-analytic studies I was involved in was a meta-analysis of proportions, which allowed me to explore my keen interests in meta-analytic methods and gain coding experience. When I first started working on this project, few studies were dedicated to illustrating how a meta-analysis of proportions should be performed in R. To fill this gap, I created comprehensive written and video tutorials showing how to conduct such an analysis in R. My written tutorial consists of a critical review of the meta-analytic approach to synthesizing single-arm studies with proportional outcomes and a step-by-step guide to conducting the analysis in R. The R code can yield exactly the same results as those delivered by Comprehensive Meta-Analysis, software developed by meta-analysis experts. One of the prominent features of the R code is that it incorporated three transformation options for users to choose from to convert proportional data to improve its statistical properties, whereas other statistical software can only perform one.</p><p>While writing the review part of the tutorial, I investigated over 30 published meta-analyses of proportions and compared their results with those calculated by my R code. My research unearthed two problematic practices that plagued these studies <strong>(我开始展示自己很厉害的critical thinking abilities啦，因为我发现了前人研究中的很严重的问题)</strong>. The first issue I identified was related to effect size calculation in the presence of subgroups. Specifically, these studies failed to accurately estimate mean proportions because they did not take study size and within-group estimates of between-study variance (τ^2) into consideration. My R code fixed this problem by enabling users to pool the within-group estimates of τ^2 and apply this common estimate to the included studies. I also highlighted the questionable practice of improperly employing publication bias tests developed for meta-analyses of randomized controlled trials in meta-analyses of singlearm studies. In these faulty cases, test utility was limited due to the violation of underlying assumptions. I believe this practice could result in the failure to identify potential publication bias in observational meta-analyses.</p><p>My experience with this project contributed greatly to my ability to carry out independent research and resulted in a deep interest in meta-analysis and its related research areas. I enjoyed the challenging process of unearthing and evaluating methodological and statistical issues in existing studies <strong>(我对这些研究富有热情，很有动力)</strong>. Specifically, my interest was piqued regarding the impact of violating the normality assumption on meta-analytic results because I found that even after data transformation, very often the distribution of proportional data remained quite skewed. I plan to improve the estimation of effect size for meta-analysis of proportions in such situations by developing a more flexible and accurate alternative to the transformation-based approach based on the betabinomial model in a Bayesian framework <strong>(我未来的研究方向和可能的解决方法是什么)</strong>. Additionally, after reading Meta-analysis: A Structural Equation Modeling Approach by Mike Cheung, I was fascinated by the idea of integrating the two powerful statistical techniques into a unified framework to allow the synthesis of research findings for testing hypothesized models. This is an area that I am looking forward to specializing in <strong>(我的研究兴趣不止一个，所以我对你们program里的好几个POI都感兴趣，保证我有更多的选择余地)</strong>. Furthermore, I also developed an interest in transforming statistical procedures into computer code when I developed the R code for the tutorial. I am eager to learn more sophisticated software that enables latent variable modeling and Monte Carlo simulations, such as Mplus, Python, and SAS.</p><p>My research interests in quantitative methods evolved further during my internship at two criminal courts where I observed mentally ill defendants in forensic evaluations. My clinical observations challenged my deeply ingrained assumptions that mental disorders were single, cohesive disease constructs. Instead, there is considerable heterogeneity in severe mental illnesses, not only in their clinical presentations, but also in etiology, growth trajectory, and laboratory task performance measures. With further study, I was excited to learn that a promising way to account for the high degree of heterogeneity in mental disorders lies in quantitative methods, such as employing latent growth curve modeling and latent growth mixture modeling in the context of SEM. I hope to develop new SEM approaches to longitudinal data analysis and improve existing ones so that researchers can tackle such problems in mental health studies and other realms of psychological research <strong>(同样，这一段也是为了说明我的研究兴趣不局限在一个POI身上，如果这个POI收了另一个申请者，我可以选择其他POI来增加我被录取的机会)</strong>.</p><p>Upon completing the Ph.D. program, I intend to obtain a position in academia to focus on studying and teaching quantitative methods. One of my long-term goals is to lead a team to develop data-analysis software that allow psychologists without a comprehensive statistical background to conduct meta-analyses and all kinds of latent variable modeling. In addition, despite quantitative methods’ undeniable value and promising career opportunities, there is little public information about this area. Therefore, I am also dedicated to promoting its wider use in research and as an area of study among students <strong>(拿到学位后我的未来计划和目标)</strong>.</p><p>The PhD in Measurement and Statistics at Florida State University (FSU) is the ideal program to prepare me with the knowledge and skills needed to accomplish my academic and professional goals. What sets this program apart from many others is the breadth and depth of training that allows students to pursue their research interests and career goals. The program offers a large selection of statistics and methodology courses, which not only cover the standard quantitative methods subjects, but also include many less common ones, such as Meta-analysis, Causal Modeling, and Bayesian Data Analysis, subjects I am passionate about. I was also excited to learn that students in this program can also take classes in the Department of Statisticsto further enhance their understanding of statistical methodologies. In addition, after communicating over email with Dr. B. B. and Dr. Q. Z. and their PhD students C and D<strong>（只有你联系过program里的教授和学生，才能写出其他申请者从program官网得不到的信息，才能把你跟其他申请者区分开，加深committee对你的印象和好感）</strong>, I learned that the mentoring model employed in the program creates an environment where professors not only care about their students, but also encourage early participation in research and collaboration with multiple professors in the program and scholars from across the university. These factors ultimately benefit students in the program by providing the opportunity to gain different perspectives and valuable research experience. I believe I can truly make a difference in the advancement of the quantitative field with the training I will receive at FSU. Furthermore, I am attracted to FSU because it has a large, tight-knit community of graduate students who are able to harmoniously collaborate with one another and the faculty. It appears that everyone gets along and is willing to help each other like a family. I really feel I can thrive in such an environment. Lastly, the program is very successful at finding placements for its students. I believe that when I graduate I can leave with a firm understanding of what it takes to obtain jobs in either academia or industry <strong>(这个夸赞这个program各种强项的段落会让POI感觉我是tailor过我的PS的，而不是写的非常generic，体现出我满满的诚意)</strong>.</p><p>Although my current research experience in quantitative methods is mainly in meta-analysis, I am also eager to tackle problems in other areas of this discipline. I am looking forward to having the opportunity to work with Dr. B, Dr. Z, and Dr. Y. Their expertise in the areas of meta-analysis, SEM, Bayesian statistics, and longitudinal data analysis will be crucial in providing my future research with the appropriate theoretical and practical guidance.</p><p>With my determination and tenacity, along with my academic qualifications, I am confident that l will be able to excel in the doctoral program at FSU. In the future, my international background may also allow me to foster research collaborations between the U.S. and China.</p><p>请注意，没有一个POI会要求你完全按照PS里写的内容让你百分之百地去履行。所以为什么要有PS呢？我认为有以下几点原因：</p><ul><li>像我上面说的，体现你的批判性思维，这是核心能力，可以通过PS看出来；</li><li>你对自己将来的研究和工作生涯是有规划的；</li><li>你不是随便选的学校，你是认真考虑后才申请这个学校的；</li><li>能一定程度上反映你的逻辑和写作能力。</li></ul><p><strong>Bear with me, guys. We’ve still got two important topics to cover: letters of recommendation and how to ace your interview. So, 醒醒！</strong></p><p>关于推荐信，如果你申请心理学的PhD，请务必拿到至少一封美国教授的推荐信（可能申请其他人文社科类的PhD也是如此）。美国的录取委员会不傻，他们知道中国申请者的推荐信是怎么编造出来的，只有美国教授的推荐信才有可信度（除非你的推荐人跟你的POI私交甚好）。想拿到一封强推荐信，说到底是跟你的推荐人培养感情，所以我建议大家：</p><ul><li>尽早参与科研，在不同lab体验科研的过程，这样你才能挑选出你喜欢的科研方向，在这个过程中你同时跟不同教授混熟了；</li><li>上课时积极参与讨论，或者下课后常问教授问题；</li><li>用发邮件、office hour这样的方式多跟教授联络，你们可以讨论问题，不问问题你也可以谈谈最近的学习、科研感悟。不要临近申请的时候才想起教授来，没人喜欢这样的学生。</li><li>如果教授对你有不错的印象的话，你完全可以问教授有没有认识的其他临床心理学的教授在招博士生，这种通过熟人关系推荐来的学生显然比从一个不认识的教授那里推荐来的学生要更可信。但话又说回来，只有你有很强的读博动力、科研经验和水平并且会做人跟教授相处愉快，教授才会真心推荐给自己的同事或朋友。我想强调的是，一定要张口问教授可不可以把自己推荐给其他教授。<strong>如果能拿到一封熟人推荐信，对你非常有利。</strong></li><li>积极争取去美国交流的机会，国内的学生至少去修一个学期的课；</li><li>有条件的学生读美本或美国的master’s</li></ul><p>为什么我非常强调要有在美国读书的经历？这样能证明：</p><ul><li>你有在英语环境中学习并取得高GPA的学术能力；</li><li>如果你能取得高GPA说明你的英语能力得到了实战的证明，而非仅仅是一个苍白的托福分数（托福考得高但写PS满篇语法错误的学生还少吗？）</li></ul><p>In my case，当我的教授Betsy在第二次给我写推荐信后告诉我，她为我写了一封“强得不能再强”的推荐信。之所以我能得到她的支持，是因为我让她看到了我的努力、坚持和科研能力。我从没读过她为我写的推荐信，所以也没法展示在这里。但我可以从她给我回的一封邮件里推断出她是怎样为我写推荐信的（我给她发邮件说我被什么学校录取了并给她寄了一张感谢卡）：</p><p><img src="https://pic3.zhimg.com/50/v2-6e56a0270d79bbb264a591cefb6e2309_hd.jpg" alt="img">![img]</p><p>我的教授能这样称赞我是我荣幸，只有做到这样才能拿到一封名副其实的强推荐信。</p><p>最后我们讲讲PhD interview。</p><p>如果你进入面试环节，那恭喜你！我恭喜你的原因是因为你为自己赢得了展示自己的最佳机会。我想强调的是，面试不是考试，如果你把面试当作回答专业问题的考试，那就大错特错了。</p><p>对于校方，interview有以下几个功能：</p><ul><li>更加深入地了解你；</li><li>让教授和老生感受你是否跟他们的program匹配，观察你是不是真的对他们感兴趣，你有没有red flag</li></ul><p>对于你，interview有以下几个功能：</p><ul><li>直观地感受校园和校外的环境；</li><li>program里的老生是否真的喜欢他们学校；</li><li>更加深入了解这个program的机会：多问教授和学生问题！</li><li>最重要的是，展示你的强项的机会</li></ul><p>下面是我当时准备的几个最关键的面试问题和回答（有些问题真的问到了所以我会用过去式写我当时如何回答的）。这些回答是为了更好地展示我自己而准备的，请不要把它们当作标准答案，因为我再说一遍：面试不是考试：</p><ul><li>Your educational background;</li><li>Your best quality, character, personal trait—this should be something that can facilitate your future research career, such as perseverance;</li><li>Why do you want to do a PhD—I love doing research; I love teaching and tutoring students; being a professor is a rewarding career;</li><li>Why do you want to do a PhD in this area (my area is in quant psych)—I am more interested in methodologies and statistics than substantive topics; this is an area full of opportunities for growth; I love coding; great job prospects;</li><li>What are your research interests—I gave a very general answer (in my area, POIs don’t expect you to have a very clear idea of what topics you want to do in the future. You can just tell them you are interested in such and such areas, such as meta-analysis, SEM, longituidinal data-analysis). A more detailed answer wouldn’t hurt so long as your interests match your POIs’;</li><li>What are your career goals—when it comes to goals, I think you should be specific, like I want to get 5 first author publications by the time I graduate (you can even name a few prestigious journals in which you want to publish); I want to graduate within 5 years; I want to become an assistant professor in 6 years; I want to create very popular statistics packages. I would suggest that you choose some challenging but realistic goals;</li><li>What is your greatest strength and weakness (yes, they asked me what my biggest weakness was)- I said that my greatest strength would be my ability to solve problems in very challenging situations and I gave them an example; as far as my weakness, I said that I don’t have a heavy background in math/stats but I’m planning on taking a few refresher online courses like calculus and that I will take as many stats courses as possible during the progarm (the truth is that most incoming PhD students don’t have a strong background in math/stats in our field, so this is not a fatal weakness and it is something I can improve upon);</li><li>Which research project are you most proud of—they are giving you an opportunity to impress them, so be prepared with a great answer; remember to find out what skills are valued in your program and highlight them in your answer;</li><li>Why do you want to join their program—I said that my research interests match yours very well and I like the courses your program offers.</li></ul><p>If you have these questions prepared, you will talk confidently in the interview.</p><p>面试最后教授们肯定会问你你有没有问题问他们。以下是我准备的一些问题：</p><p>For POIs</p><ul><li>Could you tell me about your current projects? What are you working on? Where do you see your work going in the next few years?</li><li>Could you tell me about your advising style?</li><li>How many students do you advise now?</li><li>How often do you meet with your students?</li><li>What kind of research would I be working on if I were to go to your program?</li><li>Is there a group meeting every week? What do you all talk about?</li><li>What are you looking for in an incoming student? What are your expectations of students as they progress through the program?</li><li>On average, how many publications do students leave this program with?</li><li>What is the academic community like at this university? Is there much collaboration between the faculty members?</li><li>What type of internships do students usually get? Do they have research opportunities during summer or winter?</li><li>How long do most people take to finish the program? What are some common things that prevent people from finishing in this time frame?</li><li>What are your graduated students doing now? What type of placements do students usually get? Did they go into academia, or are they pursuing other careers?</li><li>What kind of financial support/funding opportunities are there for a Ph.D. student? How are students funded? Do you offer tuition waiver and stipend?</li><li>What is the funding package like for admitted students?</li></ul><p>For current students</p><ul><li>What are your favorite and least favorite things about this program?</li><li>What are your favorite and least favorite things about working with your adviser？</li><li>Why did you choose to come here?</li><li>What do you do for fun? How much social time do you spend with other students in the program?</li><li>How supportive is your adviser of you publishing?</li><li>How often do you meet with your adviser? When you want to talk with him/her, how do you go about scheduling a meeting?</li><li>Have you ever had a research idea outside of your adviser’s immediate research agenda? Did you pursue the idea and if so, how?</li><li>Do you need a car to get around?</li><li>What happens in this program to students who make a mistake (give the wrong answer in class, forget to randomize participants in one of their studies, etc.)?</li><li>How common/easy is it for graduate students to collaborate with someone who isn’t their primary adviser?</li><li>What is the biggest challenge you have faced as a graduate student?</li><li>Given my stipend, will I be able to live by myself?</li><li>What do you wish you would have known or understood better before coming here?</li></ul><p>你肯定问不全，所以挑最关键的以及你最感兴趣的问吧。</p><p>几个小tips：</p><ul><li>Keep your answers succinct, easy to understand, and to the point—they don’t expect you to give very detailed and long answers within 30 minutes (that’s the normal length of an interview). A very complicated answer will make interviewers confused because 1) some of them don’t know your background very well; 2) when you are being interviewed, your speech may become unorganized under pressure；</li><li>Show your eagerness to learn and do research! They prefer to hire an indivisual who is driven to do a PhD；</li><li>把你准备要问POI和录取委员会的问题打印出来并让他们看到！这样他们会直观地感受到你是很认真地来面试的，肯定会增加对你的好感！我当时拿出我的问题时，教授们都笑了；</li><li>千万不要尝试去背诵你的回答！可以写逐字稿（script），但不要一字不差地去背诵script，而是要记住要点和那些你临场想不出来的表达。如果你反复修改你的逐字稿，最后定稿以后你基本都会记住你要说什么了，但此时你记住的内容并不是你死记硬背记住的，而是顺着一条逻辑线记下来的。接下来，在琐碎的时间（起床前、睡觉前、坐车时、散步、锻炼、做饭的时候）不停地问自己这些面试问题，按你写好的内容现场组织语言，并反复重复。</li><li>dress code: at least business casual (tie is optional).</li></ul><p>2018年这一次，你能想到的申请的各个方面我都做到了极致。这回我申请了11所学校，拿到了9所录取（拒掉8位POI的感觉其实没有想象中那么简单。我非常感激他们选择我，给我机会去他们学校读博士，所以今年感恩节我给他们每个人发了邮件再次表达感谢）。</p><p>我现在在Texas读博士。</p><p>我觉得我的经历是可以给我孩子讲的一个不错的故事，并且告诉他我总结出的一个大道理：</p><p>At the end of the day, you don’t get what you want, you get what you <strong>deserve</strong>.</p><p><img src="https://pic3.zhimg.com/50/v2-3d4d4a9abd1d47a400422143a47e4663_hd.jpg" alt="img"></p><p>最后我来讲讲我都被哪些学校录取了，以及收offer期间遇到的有（qi）趣（pa）的事情。我选的这些学校一方面有研究meta-analysis的POI，另一方面从整体看faculty研究兴趣比较广，这样即便我将来不想研究meta-analysis了，也可以轻松地换方向（有些program的faculty只有两个教授，我就不会选那样的program）。</p><p>直接给我录取的学校有：</p><p>University of Cincinnati（<a href="https://link.zhihu.com/?target=https%3A//cech.uc.edu/education/programs/educational-studies/phd-in-edst/concentrations/qmrm.html">Quantitative and Mixed Methods Research Methodologies</a>）：这是最早给我录取的学校（1/29）。跟POI用Skype面试得不错，也答应给funding。最有意思的是POI两个月后告诉我他跳槽去USC（University of South California）了，所以今年不能收学生了。当时让我焦虑了几天，害怕其他老师也突然告诉我他/她要跳槽。这位POI就是我现在读博士这个学校毕业的。</p><p>Florida State University（<a href="https://link.zhihu.com/?target=https%3A//education.fsu.edu/degrees-and-programs/measurement-and-statistics">Measurement and Statistics</a>）：1月26号给我的录取。这其实是我最想去的学校，因为POI是meta-analysis这个领域里的“Queen”，最厉害的一位学者之一，也是我现在的导师的导师。当我得知我被录取的时候，我在Houston刚刚结束另一个学校的面试。当时实在是太开心了，所以我去酒店一层一间昏暗但别致的酒吧自己庆祝了一翻：配着一个汉堡，第一次喝了一点威士忌（极少喝酒，平时最多喝一点Mojito），彻底放松了下来。当时感觉一切付出都是值得的。FSU没有面试（估计是因为系里没钱没法给参加面试的申请人报销机票），但POI邀请我去她家住一晚第二天带我去逛逛校园了解一下他们的program。POI跟她老公住在一座森林里的一座别墅里，POI一个学生夜里载着我到的她的住处，如果你们看过《人体蜈蚣》第一部，往森林深处开的感觉就跟电影里那样。家里有两条大狗狗，墙上挂着各种从世界各地带回来的纪念品（她老公是人类学家，以前满世界到处跑）。她老公跟我讲，选PhD的项目要看导师而不是这个学校怎么样，以前他一开始在哥大读博士，后来他从哥大转到City College是因为他认为哥大那个项目里的导师学术上“太保守”，他不喜欢。第二天我逛了一下FSU的校园，认识了一下他们的学生，一切都很满意……直到我们谈钱（划掉），谈funding。我发现这个program里的学生基本要么是贷款读博，要么是家里给钱读博，asisstantship只给10个小时（一般RA和TA都要求每周工作20小时）。我问POI为什么会这样，她跟我讲，系里希望给每个学生都有补助，弊端就是每个人都分的少。POI帮我申请了fellowship，希望我去他们的program，但最终因为funding的问题我没有去。我是在4月16日收到邮件告知我拿到了fellowship，但当时我早就接了现在这个学校的offer了。</p><p>UC Merced（<a href="https://link.zhihu.com/?target=http%3A//psychology.ucmerced.edu/graduate-program/quantitative">Quantitative Psychology</a>）：1月29日给我的录取。UC系统里最新的一个学校，program也比较新，由Dr. William Shadish带领创办的program（可惜他去世了），faculty里有好几个我都很喜欢的，有一个POI是FSU的POI的“academic brother”（FSU的POI用的这个词），另一位POI只有30多岁就成了Associate Professor，是一个rising star。我通过Skype跟POI们interview，还跟他们的学生interview（不是正式的，只是通过学生了解这个program）。面试后一个小时就拿到录取，直接给了我五年的funding。实际上我很喜欢这个项目，除了faculty之外，这个项目另一个大卖点就是campus附近就是Yosemite（优山美地）。最后我没去，实话实说，假设我毕业后回国的话，我还是希望有人听说过我读博士的学校的名字的（我的解释比较婉转，你们懂）。</p><p>Texas A&amp;M University（<a href="https://link.zhihu.com/?target=https%3A//epsy.tamu.edu/academics/research-measurements-and-statistics-doctoral/">Research, Measurement, and Statistics</a>）：2月6日给的我录取。导师是FSU的POI的弟子。导师、funding、校园文化以及各个方面我都满意，最后来这里了。</p><p>University of Iowa（<a href="https://link.zhihu.com/?target=https%3A//education.uiowa.edu/academic-programs/educational-measurement-and-statistics">Educational Measurement and Statistics</a>）：2月7日给的录取。POI也是FSU的POI的弟子。这位POI来自南美某国家，口音非常重，乃至于跟他Skype面试时我只听懂了他问的问题，当他自己介绍自己时，我只能点头微笑随声附和，说一说that’s interesting。最后因为没有funding没去。实际上，我有个阴谋论的观点：因为FSU、TAMU和Iowa三个学校的POI平时联系非常紧密，经常邮件来往，所以他们非常清楚我的情况，最后让我选择TAMU会不会是他们三人商量好的结果呢？这仅仅是阴谋论，我的gut feeling，完全没有根据。</p><p>University of Houston（<a href="https://link.zhihu.com/?target=http%3A//www.uh.edu/education/degree-programs/mqm-ls-phd/">Measurement, Quantitative Methods, and Learning Sciences</a>）：2月9日给的录取，也有funding。没去是因为我更喜欢FSU和TAMU。最有意思的是，我去这个学校面试时，另外来面试的两个女生，一个是我济南老乡（她最后选择了这所学校），我们回国后还在济南见了一面（不要多想，人家都结婚了）；另一个最后选择来TAMU了（人家也结婚了），上学期我们还一起上了一门课；还有一个隐藏的我没见到的电话面试者，他最后也来TAMU了，现在跟我一间办公室。</p><p>SUNY Albany（<a href="https://link.zhihu.com/?target=https%3A//www.albany.edu/graduate/ed-psychology-phd-degree.php">Educational Psychology and Methodology</a>）：2月15日给的我录取（我记得收到录取信时我在去New Port的地铁上）。Skype面试很顺利，很喜欢这个来自荷兰毕业于荷兰最古老大学的POI。Funding也很不错。最后没去一是我更喜欢FSU和TAMU，二是这个program虽然POI是研究meta-analysis的，但整个项目更倾向于Educational Psychology，最后一个原因是我想离开纽约州去其他州生活生活（所以我来到跟New York对角线的Texas）。</p><p>Univerisity of Texas, Austin（<a href="https://link.zhihu.com/?target=https%3A//education.utexas.edu/departments/educational-psychology/graduate-programs/quantitative-methods">Quantitative Methods</a>）：没记住几号给的录取。没去这个学校最主要的原因是这个program要求必须有他们学校的Quant Psych的M.S.（其他学校的也可以），才能申请他们的博士。他们审核我的材料后认为我的心理咨询的硕士不符合他们的要求，最终把我录进他们的M.S.，没有funding，所以没去。</p><p>好吧，我来讲讲最最奇葩的一个学校了：University of Notre Dame<strong>（</strong><a href="https://link.zhihu.com/?target=https%3A//psychology.nd.edu/graduate-programs/areas-of-study/quantitative/">Quantitative Psychology</a><strong>）。</strong></p><p>直接被拒后来又问想不想来我们program啊这种事情是真能发生的。</p><p>我在二月份某天直接从Notre Dame的申请系统里收到他们的拒信，后来四月十几号POI直接发短信询问我还有兴趣来我们这儿吗，当时距离我接TAMU的offer都过去俩月了。虽然我本来也很想去这所学校，但不能反悔已经答应的offer（不然我在学术圈里名声就完蛋了，还记得TAMU的POI跟FSU和Iowa都是什么关系吧？），所以拒掉了。后来我在GradCafe上得知，Notre Dame那一年prematurely拒掉了很多优秀的申请者（圣母大学，waitlist了解一下呀？），我还看到比我还强很多的申请者在GradCafe上面留言说他跟我有同样的遭遇（人家最后去了UCLA，活该啊Notre Dame）。</p><p>最后，两所直接拒我（但最后没有改主意再录我）的学校：</p><p>University of Maryland, College Park（<a href="https://link.zhihu.com/?target=https%3A//education.umd.edu/measurement-statistics-evaluation-program%23faculty">Measurement, Statistics, and Evaluation</a>）</p><p>University of Minnesoda, Twin Cities（<a href="https://link.zhihu.com/?target=http%3A//www.cehd.umn.edu/edpsych/programs/qme/">Quantitative Methods in Education</a>）</p><p><a href="https://www.zhihu.com/question/31712813/answer/176492357" target="_blank" rel="noopener">https://www.zhihu.com/question/31712813/answer/176492357</a></p><h4 id="感悟"><a href="#感悟" class="headerlink" title="感悟"></a>感悟</h4><p>多看看大神们是如何做事的，和作者比起来我的努力程度让自己感到羞愧。作者的座右铭非常应景：you dont get what you want,you get want you derserve，好好努力吧，科研这种事怎么能投机。</p><p>顺着作者的思路，先问自己两个问题：</p><p>老师凭什么推断我可做科研?——–有科研产出:paper或writing sample</p><p>老师凭什么推断我能博士毕业?—–思维能力强,专业基础扎实,心态健康,沟通能力强,语言能力强—-&gt;</p><p>​                            ——–发表优质论文,能证明思维能力和专业基础以及语言能力</p><p>​                             ——-在合适时间点（2020.1-3月）与老师套词，在套词的过程中呈现自己的paper或                              writing sample进展，在getting better的过程中证明自己的心态和沟通技巧</p><p>规律作息,戒咖啡,回避不必要的社交</p><p>在2019年结束前写出像样的writing sample是主要任务，投递论文的技术细节请教小鸿后另开一文。</p><p>​                                    </p><h4 id="新加坡国立调研"><a href="#新加坡国立调研" class="headerlink" title="新加坡国立调研"></a>新加坡国立调研</h4><ul><li>全球排名20</li><li>最低要求<ul><li>研究型    <ul><li>硕士<ul><li>GRE成绩</li><li>通常要求本科相关专业(就只有这个是本科相关专业就好)</li><li>如果有相关工作/科研经历可做加分项</li></ul></li><li>博士<ul><li>相关领域硕士学历</li></ul></li></ul></li><li>授课型<ul><li>硕士<ul><li>本科相关专业</li></ul></li></ul></li></ul></li><li>费用:<ul><li>担任助教的研究型计算机硕士每年20350新元</li><li>不担任助教的研究性计算机硕士每年38500新元</li></ul></li></ul><p>结论:</p><ul><li>硕士没有奖学金/很少</li><li>博士要求必须是本专业的硕士</li><li>硕士对本科专业要求不是特别严</li><li>硕士完了跨校读博基本就得7年</li><li>研究性硕士可以在第一年就申请转博士</li><li>研究性硕士一般要给博士让路,博士没满才招研究性硕士<ul><li>【cs专业就没有研究性硕士这一栏,不存在博士没招满】</li></ul></li><li>新加坡硕士也是要考gre的</li></ul><p>总结:直接读博要求是本专业硕士,cs无硕博连读，pass.</p><p>先把文章投出去与老师套词,然后考语言,这样有希望申请上授课型硕士</p><p>硕士期间花费预估在70w，招行+新加坡本地贷款+助教项目等，问题不大，需注意读完硕还要读博，并不能工作还贷款，所以贷款年限需拉长。</p><p>这条路时间成本巨大，预估得7年。</p><p>科研能建构内心秩序，是一种lifestyle。</p>]]></content>
    
    
    <categories>
      
      <category>5.杂谈</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文写法</tag>
      
      <tag>思考</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>5.杂谈/996 - 超时工作的异化问题</title>
    <link href="undefined2020/02/24/5.%E6%9D%82%E8%B0%88/996%20-%20%E8%B6%85%E6%97%B6%E5%B7%A5%E4%BD%9C%E7%9A%84%E5%BC%82%E5%8C%96%E9%97%AE%E9%A2%98/"/>
    <url>2020/02/24/5.%E6%9D%82%E8%B0%88/996%20-%20%E8%B6%85%E6%97%B6%E5%B7%A5%E4%BD%9C%E7%9A%84%E5%BC%82%E5%8C%96%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h1 id="996-超时工作与异化问题"><a href="#996-超时工作与异化问题" class="headerlink" title="996 - 超时工作与异化问题"></a>996 - 超时工作与异化问题</h1><h5 id="1-感性认识"><a href="#1-感性认识" class="headerlink" title="1.感性认识"></a>1.感性认识</h5><p>我国现阶段各行业超时工作现象十分普遍，有的互联网公司晚上十一二点整个大楼还灯火通明，有的公司虽然晚上按时下班，但回家后仍然要求有工作输出，很多岗位还要求7*24小时on call。</p><p>我前些年所在的公司就有制度要求员工每月加班22天以上才算考核达标，而我经常需要加班27天才能完成工作，每晚九点离开公司后经一小时通勤才能到家，洗洗漱漱刚好十一点上床，十二点入睡，第二天早上6点50分起床后经过一小时通勤抵达公司，在公司门口的快餐店里缩着身子与其他同事挤在餐桌上潦草进食后便赶往工位，开始一天的埋头苦干。算上通勤时间，我的工作强度是7107。高强度劳动和了无希望所积累的疲劳深入骨髓，在此期间我感到生命力与灵魂的飞速流逝。</p><p>深夜的华为大楼</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1305_42_222.png" alt=""></p><p>早上五点半排队等公交上班的年轻人</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1306_00_748.png" alt=""></p><h5 id="2-八小时工作制的来由"><a href="#2-八小时工作制的来由" class="headerlink" title="2.八小时工作制的来由"></a>2.八小时工作制的来由</h5><p>八小时工作制国家法律规定的工作日长度为8小时的工作制度。目前世界各国普遍实行八小时工作制。正常一天工作时间为早上九点至下午五点为8小时。</p><p>理论起源</p><p>八小时工作制最早由社会主义者<a href="https://baike.baidu.com/item/%E7%BD%97%E4%BC%AF%E7%89%B9%C2%B7%E6%AC%A7%E6%96%87" target="_blank" rel="noopener">罗伯特·欧文</a>于1817年8月提出。他还发明了一个口号， “8小时劳动， 8小时休闲， 8个小时休息”1833年，在<a href="https://baike.baidu.com/item/%E6%AC%A7%E6%96%87" target="_blank" rel="noopener">欧文</a>的支持下，具有同情心的工厂主约翰·多赫尔蒂等人发动了一场争取八小时工作制的运动。1866年，第一国际<a href="https://baike.baidu.com/item/%E6%97%A5%E5%86%85%E7%93%A6" target="_blank" rel="noopener">日内瓦</a>代表大会提出了“8小时工作，8小时自己支配，8 小时休息”的口号，要求各国制定法律给予确认。</p><p>历史延革</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1313_15_391.png" alt=""></p><p>19世纪80年代的美国工人运动。当时，美国和欧洲的许多国家，逐步由<a href="https://baike.baidu.com/item/%E8%B5%84%E6%9C%AC%E4%B8%BB%E4%B9%89" target="_blank" rel="noopener">资本主义</a>发展到帝国主义阶段，为了刺激经济的高速发展，榨取更多的剩余价值，以维护这个高速运转的资本主义机器，资本家不断采取增加劳动时间和劳动强度的办法来残酷地剥削工人。</p><p>在美国，工人们每天要劳动14至16个小时，有的甚至长达18个小时，但工资却很低。马萨诸塞州一个鞋厂的监工说：“让一个身强力壮体格健全的18岁小伙子，在这里的任何一架机器旁边工作，我能够使他在22岁时头发变成灰白。”沉重的阶级压迫激起了无产者巨大的愤怒。他们知道，要争取生存的条件，就只有团结起来，通过罢工运动与资本家作斗争。工人们提出的罢工口号，就是要求实行八小时工作制。</p><p>1877年，美国历史上第一次全国罢工开始了。工人阶级走向街头游行示威，向政府提出改善劳动与生活条件，要求缩短工时，实行八小时工作制。罢工不久，队伍日渐扩大，工会会员人数激增，各地工人也纷纷参加罢工运动。</p><p>在工人运动的强大压力下，美国国会被迫制定了八小时工作制的法律。但是，狠毒的资本家根本不予理睬，这项法律只不过是一纸空文，工人们仍然是生活在水深火热之中，倍受资本家的折磨。忍无可忍的工人们决定将这场争取生存权利的斗争，推向一个新的高潮，准备举行更大规模的罢工运动。</p><p>1884年10月，美国和加拿大的八个国际性和全国性工人团体，在<a href="https://baike.baidu.com/item/%E7%BE%8E%E5%9B%BD%E8%8A%9D%E5%8A%A0%E5%93%A5" target="_blank" rel="noopener">美国芝加哥</a>举行一个集会，决定于1886年5月1日举行总罢工，迫使资本家实施八小时工作制。这一天终于来到了。5月1日，美国2万多个企业的35万工人停工上街，举行了声势浩大的示威游行，各种肤色，各个工种的工人一齐进行总罢工。仅<a href="https://baike.baidu.com/item/%E8%8A%9D%E5%8A%A0%E5%93%A5" target="_blank" rel="noopener">芝加哥</a>一个城市，就有4．5万名工人涌上街头。这下，美国的主要工业部门处于瘫痪状态，火车变成了僵蛇，商店更是鸦雀无声，所有的仓库也都关门并贴上封条。</p><p>1889年7月14日，各国马克思主义者召集的社会主义者代表大会在法国巴黎隆重开幕。这次大会上，法国代表拉文提议：把1886年5月1日美国工人争取八小时工作制的斗争日，定为国际无产阶级的共同节日。与会代表一致同意，通过了这项具有历史意义的决议。</p><h5 id="3-996的起因"><a href="#3-996的起因" class="headerlink" title="3.996的起因"></a>3.996的起因</h5><p>行业特征与恶性竞争</p><p>互联网本身时效性极强，如果其他所有公司都能很快推出自己的产品，唯独自己的公司不行，那就会很快被市场淘汰。所以各公司都会延长员工的工作时间，以实现产品产出的最大化。当然，在这种严酷的加班制度下，资方并没有好心地将超时工作的时长以假期或补贴的形式返还给员工，也没有扩招人手来降低人均工作强度。资方常常采取的策略是拼命地使用员工，当员工不再年富力强或疾病缠身时就果断抛弃，反正每年都有大把的应届毕业生兴高采烈地加入被剥削大军。</p><h5 id="4-996本质是剥削"><a href="#4-996本质是剥削" class="headerlink" title="4.996本质是剥削"></a>4.996本质是剥削</h5><p>强迫人超时劳动本质是剥削，剥削的坏处是让一个完整的人变得不完整，这个长期超时工作的人会工具化和异化。</p><p>上班的人在出卖自己的劳动力换得薪水，劳动力是要再生产的。再生产的方式是吃东西，充足的睡眠，维持与家人的健康关系，发展自己的业务爱好，让自己保持身心愉悦，这样才能源源不断产生充沛的劳动力。</p><p>本身人工作8小时后剩下的16个小时是用于恢复劳动力的，但是老板拿掉了剩下恢复生产力的时间，平常8-11，周末只放一天假也用来补平常缺的觉了。长期这样以来人的正常劳动力再生产过程就会被破坏掉，这样人的劳动表现就不会好，跟不上高压的工作节奏，这样以来人干个五年十年身体与灵魂都会垮掉。这个人垮掉之后当然会被炒掉，资方只用再换一个年轻力壮的就OK了。</p><h5 id="5-深刻认识剥削"><a href="#5-深刻认识剥削" class="headerlink" title="5.深刻认识剥削"></a>5.深刻认识剥削</h5><p>作者：李劼</p><p>链接：<a href="https://www.zhihu.com/question/320073567/answer/777226626" target="_blank" rel="noopener">https://www.zhihu.com/question/320073567/answer/777226626</a></p><p>来源：知乎</p><p>如果大家认为“剥削”就是自己挣10块钱，资本家抽走7块，只给留3块。那这个话题就没有讨论的必要了。</p><p>因为这掩盖了剥削的残酷本质。</p><p>资本主义的核心并不在“私有制”（其实在原始社会末期就已经有私有财产了，封建时代也是，但都称不上资本主义）。而在于“资本做主才是第一要义”，简称“资本主义”。</p><p>由此导致的局面就是，资本凌驾于人类之上，人类为资本服务。而并非人类拥有资本，资本听命于人。虽然资本主义环境下有财产私有制度，但这极具迷惑性，容易让人误以为资本家是资本的主人（财产所有人）。但实际上，资本家只是傀儡，人形外壳，传声筒，使魔，皮囊，英灵（随便你们叫什么吧），资本本身才是真正的“主人”和“灵魂”。</p><p>到了这一步，资本即将展现它真正的杀伤力――异化。</p><p>上面那一大坨文字看不懂没关系，下面讲点简单的，举个例子。</p><p>比如你是一个老板，搞到（甭管是捡的研发的还是买的）一把超级牛逼的镐头，能让矿工挖矿的产量提高到原先的10倍。假如你面前有两个人，一个是超级矿工，即便不用这把超级镐头，他也能挖10个单位的矿（用镐头将会是100）。另一个是包身工，他只能挖1个单位的矿（用镐头就会变为10）。</p><p>现在，包身工在你面前跪下来了，他儿子得了白血病，他恳求你把超级镐头分配给他，他不介意被剥削，他愿意挖出来的矿你得7，他得3（换钱给儿子治病），实在是不行，你8他2也行。</p><p>超级矿工的辞职报告也递上来了，他的兴趣是做菜（虽然他的天赋是挖矿），他想改行当厨师。</p><p>你把镐头分配给谁？（无视人情，也无视另一人的天赋）</p><p>别想多了，犹豫必将败北。作为称职的资本家，你给超级矿工跪下了。</p><p>你求他拿上超级镐头去挖矿，你许诺跟他五五开分成，如有必要，你还可以只拿100个单位中的40，他拿60，折成股份也行。你甚至允许他每天只挖半天矿，另外半天可以兼职做厨师。至于那个可怜的包身工，谁叫他不努力呢，是吧。又或者，等超级矿工挣到钱，你的企业也成功上市，你再用财税渠道和福利政策“转移支付”给包身工，那样，你，包身工（和他倒霉儿子），超级矿工，就“共赢”了，是吧。</p><p>怎么样，如此温情脉脉，优胜劣汰，按劳分配，是不是很市场经济，很效率优先，很兼顾公平？是不是跟政治（或政治经济学）教科书里讲的剥削不一样？</p><p>不一样就对了，教科书太仁慈，不敢告诉大家剥削的残酷本质。</p><p>好，再回到前面说的“以资本为主”，“异化”。</p><p>上文就是“以资本为主”，“异化”的例子。</p><p>――“以资本为主”，所以，资本不是分配给最需要它的人，而是分配给最能实现他价值的人。</p><p>――“异化”，所以“劳动”（其实喜欢做菜）被扭曲为“就业”（挖矿最出成果）。</p><p>而就在这“以资本为主”和“异化”的过程中，“剥削”粉墨登场。</p><p>再次强调一遍，剥削并不（或不仅）表现为对成果分配的掠夺（如各种三七开，二八开，“剩余价值”），它真正残忍的面貌在于对时间与自由的剥夺。</p><p>本来，自由与时间对每一个人都是（或应该是）平等的，谁都是一天24小时（就算马云资产是我的一亿倍，他的一天也变不成24亿小时）。</p><p>然而，一旦进入“以资本为主”的“异化”过程，你的自由将不再是你的自由，你的时间将不再是你的时间。</p><p>你不能自由的，随心所欲的去“劳动”，你只能有目标有任务的去“就业”，并为此消耗时间。</p><p>但，第一，时间是不能重叠的（虽然资本允许重复投资）。</p><p>你花时间学习使用超级镐头，就不能同时陪伴白血病儿子或做菜，正所谓“放下砖头没钱养你，抱起砖头没时间陪你”。</p><p>第二，时间是不会倒流的（虽然投资可以撤回）。</p><p>当某一天，你花光了挖矿换来的钱，儿子依然死在了病榻上，又或者经济危机，你就是花时间学了怎么使用超级镐头但依然找不到工作的时候，你为此耗费的时间照样流逝，永不回头。</p><p>这才是剥削的真谛――你必须为了实现资本的价值（而非直接满足自身需要）来消耗时间（然后再诉诸于市场交换），而且，不论你的消耗能否换来自身需要的满足和自身价值的实现，时间都被实打实的消耗了，不可重叠，不可倒流，不可撤回，不可再生，不可增殖。</p><p>现在，我们可以回答问题了。</p><p>为什么资本家明明拥有大量资产，却还要剥削无产阶级？</p><p>因为还需要时间啊！</p><p>资本要实现自身价值（增殖），就必须通过运作（生产，经营，市场交换）的过程才能实现，而维持这一运作过程，需要包括资本家在内的全人类全身心投入，而“私有”却偏偏能保证大家死心塌地去投入。因为“私有”能划分出“所拥有的”和“所需要的”。要满足“所需要的”，就必须提供“所拥有的”来交换。</p><p>作为资本代言人的资本家，他们打开了名为“市场”的炉盖。</p><p>无产阶级，他们往熔炉里投入了名为“时间”（也可写作“花时间做事”）的燃料。</p><p>从此，他们的时间不为自己而生，而为资本支配。</p><p>资本家？就一烧锅炉的。</p><p>这就是为什么马克思主义者的终极追求是“解放全人类”，而非“打倒资本家”。</p><h5 id="6-探究异化"><a href="#6-探究异化" class="headerlink" title="6.探究异化"></a>6.探究异化</h5><p>作者：雾雨之灵</p><p>链接：<a href="https://www.zhihu.com/question/297726264/answer/608965993" target="_blank" rel="noopener">https://www.zhihu.com/question/297726264/answer/608965993</a></p><p>来源：知乎</p><p><img src="!%5B%5D(http://bqlab-pic.test.upcdn.net/pic/20191207_1313_37_944.png)" alt="mark"></p><p>这个问题，马克思早就预言过了，在我们中学的课本也提过了无数次。这就是资本对劳动的异化！同时也是对人的异化！</p><p>劳动，本来是人的本质属性。何为本质属性？就是这个属性不像吃喝拉撒睡，饱暖思淫欲去啪啪啪一样，这个属性别的动物都没有，唯独人有。人是无法不劳动的，让一个人休息一星期、一个月可能会很开心，但让他一直闲着，他就总要找一点事情去实现自己的价值，创造价值就是劳动，价值就是凝结于商品中的人类劳动。</p><p>所以劳动本来应该是快乐的，因为劳动创造价值嘛，劳动是自己本质属性的表达嘛。如果想找这样快乐的劳动者聊聊，就来北京打一辆出租车。北京开出租的本地人好多都不差这几个钱，为啥还要开出租？有一次我出差，出租车司机师傅特别贫，一直和我说“现在好多人都看重钱，我才不在乎，我一分存款都没有，我儿子我也不打算给他一分钱…”我一听就赶紧打住，师傅您先说说自己有几套房吧。师傅说也不多，就两套，不过有一套快拆了，拆了我能再买八套。我边听边带上耳机说，师傅我有些晕车，不说话了啊我眯一会儿……</p><p>同样，马云说他不喜欢钱，我也觉得他是真心的。他犯得着撒谎吗？</p><p>但现在呢？很显然没有年轻人觉得劳动是快乐的，除非你有出租车师傅那样的资本。现在年轻人劳动仅仅是为了活下去，这就是异化。强调一点，异化不是指一个东西失去了他的本义，异化是指，本来是客体的存在却反客为主，进而控制了主体。本来劳动是人实现价值的手段，劳动是客体，现在却成了人不得不劳动，劳动成了主体。本来金钱是为了方便人的生活，现在却是人的生活为了获取金钱。本来是人在玩手机，现在却是手机在玩人…这些才是异化。</p><p>过去的国企工人，虽然钱不多，但是单位有食堂，有房子，国家管了教育、养老和医疗，自己挣多少花多少，可不就“咱们工人有力量”了？现在的年轻人，哪怕在互联网这种高薪行业，工资扣个知道自己一时半会用不上的五险一金，再交一交房租水电，真的也不剩几个子儿了。可不就自嘲是社畜、肥宅？这不是开玩笑，调查一下当年的工人幸福指数绝对爆现在年轻人几条街不止。</p><p>这还没有完，因为马克思说资本是有自己的意志的，它的意志就是实现自己的最大化。所以劳动和价值开始完全脱钩，10年前有个房子，现在抵得上别人一辈子的劳动，另一方面，是分工越来越细，人越来越原子化。我那些进互联网的同学，一个个都是名校研究生毕业，可他们觉得自己的工作随便一个本科生来学段时间就把自己取代了。</p><p>再者，就是加班时间越来越长，八小时工作制和双休日就是个笑话。什么叫“对剩余价值的剥削”？就是劳动者一天劳动所得，仅仅够他恢复劳动力进行第二天的再次劳动。这下好了，广大年轻人累死累活，忙到晚上连性生活都不想有，结果却发现没有多少时间是属于自己的。本来以为自己赚钱就可以想买什么买什么，本来以为工作没有人管，周末时间可以自由支配，但是现实却是下了班累得只想躺在床上，点个外卖刷刷手机。</p><p>另外，资本可以自己进行增殖，即马太效应，强者愈强，弱者愈弱。有了京户，就有了学区房，然后又有了区重点和市重点…一个阶层强者为了巩固和展现自己的优势自然会集中资源再打造一个阶层，于是阶层层层化分。据我了解，去年北京海淀区某区重点高中——按理说也是北京二流学校——的理科状元的分数只能去哈工大。更何况还有多少当年从高考独木桥冲过来的精英，他们的孩子连区重点都没得上？</p><p>这就是所谓的阶级天花板。随着楼上的人越来越多，楼上的人不但拼命踹想要往上爬的人，还要在楼上再起一层高楼，楼下的空间自然越来越小，天花板越来越低。之前当一名工人也挺滋润，现在当个小领导都焦头烂额。之前的孩子放了学扔了书包就可以去玩，现在的孩子放了学就要去补习班。所以现在人们觉得努力没有希望并不是没有道理，而是源自于我们对周围这个世界长时间的认识而得出的结论。</p><p>最后，资本还要做一件事，就是登堂入室，为自己合理化，占领舆论的高地。为这样的不平等冠以各种堂而皇之的理由。宣传各种奋斗实现财务自由、阶级跃升的例子，让年轻人普遍焦虑，觉得自己活成这个样子完全是因为自己不努力；另一方面，资本又是需要市场的，为了扩大市场资本再次鼓吹消费主义，如网红餐厅之类，并且给年轻的脑力劳动者一个华丽丽的名词——白领，中产阶层。让年轻人误以为自己相比父辈们，相比工人农民地位有了本质的提高，而为了配得上这个地位，必须有相应的消费。这就是小资产阶级，这就是小布尔乔亚。</p><p>什么是小资产阶级？明明没有掌握任何生产资料，却把自己当成了资产阶级，明明还是个劳动人民，却总觉得自己比劳动人民要优越。</p><p>最后的最后，马克思的追求是什么？就是人的解放，解放生产力，发展生产力。一个疲于奔命的人是不会有劳动的动力的，你找不到一个奴隶会兴高采烈地为奴隶主干活，就算你举出有的奴隶通过自身的奋斗而当上了宰相，也改变不了这个事实。当然，个人的奋斗还是很重要的，虽然个人的奋斗常常敌不过历史的进程，但是个人的奋斗总会或多或少决定着自己的命运。所以，但行好事，莫问前程!</p><p>It’s not your fault!</p><p>小结：</p><p>资本本身是主，资本家是管家，工人是slave，资本的意志是让自己增殖，增殖需要人类大量投入时间。时间不可再生，我的时间提供给资本增殖，就必定不能提供给自己的天赋，不能提供给心之所向。我应该把时间投入到自己真正感兴趣的地方，而且是straight forward去做。 </p><p>劳动本身该是快乐的，因为创造了价值，也符合人的天性，但是我们的劳动常常被迫超时超量奉献给资本增殖，同时忽略天赋与inner voice，则变得痛苦不堪。上学同理。保持内心的愉悦与平和，如果不能保持，反思问题，付出代价，解决问题。</p><p>不要透支自己的劳动力。如果干的事情符合天赋与内心认知并且干的快乐，工作时间长一点无所谓。如果干的很痛苦，干的过程中需要付出意志力硬抗，则一定严控工作时长，工作之余好好休息放松，必须认清劳动力不是无限供应的资源这一事实。考研废掉/做事无恒心其实就是没有设计好劳动力再造的过程,我不懒而是勤奋地过了头透支了劳动力,以后要要注意劳逸结合。</p><p>警惕消费主义和小资的陷阱，不要买太贵的东西，不要高消费。高消费不能实现阶级跃迁，明明是个在格子间办公鸽子笼睡觉的被剥削的劳动人民，却觉得自己比劳动人民优越，这是精心设计的认知错位陷阱，注意堤防。</p><p>找准研究方向，既要符合自己的天赋，也要考虑历史的进程！</p>]]></content>
    
    
    <categories>
      
      <category>5.杂谈</category>
      
    </categories>
    
    
    <tags>
      
      <tag>思考</tag>
      
      <tag>996</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>5.杂谈/博士这五年-李沐</title>
    <link href="undefined2020/02/24/5.%E6%9D%82%E8%B0%88/%E5%8D%9A%E5%A3%AB%E8%BF%99%E4%BA%94%E5%B9%B4-%E6%9D%8E%E6%B2%90/"/>
    <url>2020/02/24/5.%E6%9D%82%E8%B0%88/%E5%8D%9A%E5%A3%AB%E8%BF%99%E4%BA%94%E5%B9%B4-%E6%9D%8E%E6%B2%90/</url>
    
    <content type="html"><![CDATA[<p>转自知乎-李沐</p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>12年8月提着一个行李箱降落在匹兹堡机场。没找住的地方，也不知道CMU应该怎么去。对未来一片迷茫，但充满乐观。 现在，刚完成了博士期间最后的一场报告，在同样的机场，不过是在等待离开的航班。</p><p>回想过去的五年，是折腾的五年，也是自我感悟和提升的五年。这里我尝试记录这五年主要做过的事情和其中的感想，希望对大家有所启发。</p><h2 id="第0年：3-11-8-12"><a href="#第0年：3-11-8-12" class="headerlink" title="第0年：3/11-8/12"></a>第0年：3/11-8/12</h2><p>我第一次申请美国的博士是在11年，但拿到的offer并没有特别合适的导师，于是就北上投奔文渊去了。 我当时在百度商务搜索部门做广告的点击预估。具体是使用机器学习来预测一个广告是不是会被用户点击。 这时候离“大数据”这个词流行还有两年，但百度那时候的数据即使现在来看仍然是大的。我的任务是如何高效的利用数百台机器快速的在数十T的数据上训练出模型。</p><p>当时产品用的算法基于LBFGS，我于是想是不是可以换个收敛更快的算法。没几天就找到个不错 。但实现上发现了各种问题，包括性能，收敛，和稳定性。而且那时有的就是一个裸的Linux和很老版本的GCC，什么都是需要从头开始写。花了大量时间做系统优化，算法改动，和线上实验，最后一年后在整个广告流量上上了线。</p><p>现在再回顾会觉得整个一年时间都在<strong>打磨</strong>各种细节上，有时候为了5%的性能提升花上上千行代码。这些都导致算法过于复杂，有过度设计之嫌。<strong>但深入各个细节对个人能力提升很大</strong>，而且很多遇到的问题成为了之后研究方向的来源。一些算法上的思考曾写在<a href="https://link.zhihu.com/?target=http%3A//mli.github.io/2013/03/24/the-end-of-feature-engineering-and-linear-model/">这里</a>，当时候深度学习刚刚出来，冥冥中觉得这个应该是大规模机器学习的未来，不过真正开始跟进是好几年以后了。</p><p>11年12月中的时候突然心血来潮随手把材料重新寄了一遍，就选了CMU和MIT，结果意外收到了CMU的offer。有天在百度食堂同凯哥（余凯）和潼哥（张潼）吃饭，我说收了CMU offer，在纠结去不去。他们立马说去跟Alex Smola啊，他要要加入CMU了，我们给你引荐下。</p><p>记得是离开的前一天才开始打包行李，早上去公司开完会，中午离职，跟小伙伴打招呼说出个国，然后就奔机场了。那天北京天气特别好，完全不记得前一天雾霾刚爆了表。</p><h2 id="第一年：9-12-8-13"><a href="#第一年：9-12-8-13" class="headerlink" title="第一年：9/12-8/13"></a>第一年：9/12-8/13</h2><p>第一年的主要事情是熟悉环境和上课。CMU课程比较重，博士需要学8门课，每门课工作量巨大。而且要求做两门课助教，做助教比上课更累。</p><p>这一年上的课中对我最有用的是“高级分布式系统”。之前在上交ACM班的时候已经学过很多质量都还不错课，纯知识性的课程一般对我帮助不大。但这门课主要是读论文，然后大家讨论。不仅仅是关于知识，很多是对设计理念的领悟。大家知道对于系统而言，设计是一门艺术而不是科学，这是设计者审美和哲学理念的体现。同时系统界历史也是由一波又一波的潮流组成，了解历史的发展以及其中不断重复的规律非常有意义。</p><p>那年这门课上课老师是Hui Zhang（神人之一，20多岁就在CMU任教了，学生包括了Ion Stoica，他是Spark作者Matei的导师），他有非常好的大局观，对于“Why”这个问题阐述非常到位。我是通过这门课才对分布式系统有了比较清晰的认识。两年之后我偶然发现我的一篇论文也在这门课的阅读列表里了，算是小成就达成 。</p><p>除了上课，更重要是做研究。我去CMU的时候Alex那时还在Google，而且没经费，所以把我丢给了 Dave Andersen。于是我有了两个导师，一个做机器学习，一个做分布式系统。</p><p>前面半年都是在相互熟悉的过程。我们每周会一起聊一个小时。前半年因为Alex不在，所以我们只能视频。Alex那边信号经常不好，而且他有德国和澳大利亚口音，外加思维跳跃，经常我听不懂他说啥只能卖萌傻笑。还是靠着Dave不断的打字告诉我Alex说了什么才度过了前几次的会。</p><p>两个导师风格迥异。Alex是属于反应特别快，通常你说一点，他已经想好了接下来十点，要跟上他节奏很难。一般抛出问题的时候他就想好了好几个解决方法。这时候要证明自己的想法比他的更好不容易，需要大量的沟通和实验数据支撑。我想我大概是花了两年证明了在某些方向上我的方案一般更好，所以这时候他就不那么hands-on了。</p><p>Dave不会给很多想法，但会帮助把一个东西理解透，然后讲得很清楚。因为我研究方向主要是机器学习上，基本上前两年基本都是我在教Dave什么叫机器学习，而且是尽量不用公式那种教法。</p><p>我的第一个研究工作是关于如果划分数据和计算使得减少机器学习求解中的网络通讯量。Alex体现了他的强项，几分钟就把问题归纳成了一个优化问题，然后我们三各自提出一个解法。我做了做实验发现Dave的算法更好。接下来两个月把算法做了很多优化，然后又做了点理论分析就把论文写了。</p><p>可惜这个想法似乎有点超前，虽然我们一遍又一遍的改进写作，但投了好几个会审稿人就是不理解，或者觉得这个问题不重要。那个时候学术界已经开始吹嘘“大数据”，但我觉得其实大部分人是不懂的，或者他们的“大数据”仍然是几个GB的规模，烤U盘需要十来分钟的那种。</p><p>这是我在CMU的一个工作，我觉得挺有用，但却是唯一没能发表的。</p><p>当时跟我坐同一个办公室的是Richard Peng，他做的是理论研究。我经常跟他讨论问题，然后有了些想法合作了一个工作。大体思想是把图压缩的快速算法做到矩阵的低秩近似上。这个工作写了三十页公式但没有任何实验，我主要当做写代码间隙的悠闲娱乐，不过运气很好的中了FOCS。</p><p>坦白说我不是特别喜欢纯理论这种，例如在bound的证明中很多大量的项直接丢掉了，导致我觉得bound特别的近似。对于做系统的人来说，最后拼的是常数。这个工作中这种大开大合的做法我觉得很不踏实。所以我觉得以后还是应该做更实在点的东西。</p><p>在CMU回到了去百度前的一周七天工作无休的节奏。每周至少80个小时花在学校。如果累了就去健身房，我一般晚上12点去。不仅是我一个人，大家都很努力，例如凌晨的健身房，早3点的办公室，四处都可以见到中国或者印度学生。我那时候的室友田渊栋花在学校的时候比我多很多。</p><p>那一阵子有读了很多关于优化的文章。其中对我启发最大的是Bertsekas写于80年代末的那本关于分布式计算的书。此书可以认为是MIT控制领域黄金一代研究成果总结，换到现在仍然不过时。</p><p>受启发我转去研究异步算法，就是分布式下不保证数据的及时性来提升系统性能。我基于在百度期间做的算法，做了一些改进和理论分析，然后投了NIPS。</p><p>投完NIPS就动身去了Google Research实习。那时候Google Brain成立不久，在“宇宙的答案”42楼，包括Jeff Dean，Geoffrey Hinton，Prabhakar Raghavan好些大牛挤在一起，加起来论文引用率能超80万。</p><p>Alex跟我说，你去读读Jure Leskovec的文章，学学人家怎么讲故事。我在Google也尝试用了些用户GPS数据来对用户行为建模。可是写文章的时候怎么也写不出Jure的那种故事感，发现自己不是那块料。这篇文章因为用了用户数据，恰逢Snowden让大家意识到隐私的重要性，历经艰辛删了一半结果Google才允许发出来。有些累觉不爱。</p><p>不过在Google期间我主要时间花在研究内部代码和文档上。Google的基础架构很好，文档也很健全。虽然没有直接学到了什么，但至少是开了眼界。</p><h2 id="第二年：9-13-8-14"><a href="#第二年：9-13-8-14" class="headerlink" title="第二年：9/13-8/14"></a>第二年：9/13-8/14</h2><p>这学期上了Tuomas Sandholm的机制设计，此乃另一大神，例如最近德州扑克赢了专业选手，之前开公司也卖了上亿。不过这门课我是完完全全没学懂，连承诺的课程大作业都没怎么做出来。之后的两年里我一遇到Tuomas他都会问下有什么进展没。我只能远远看见他就绕开。</p><p>NIPS被拒了，发现审稿人不懂线程和进程的区别，有点沮丧。隔壁实验室一篇想法类似但简单很多的论文倒是中了oral，所以那阵子压力很大。Alex安慰说这种事情常有发生，看淡点，然后举了很多自己的例子。</p><p>之后想了想，一篇好文章自然需要有足够多的“干货”，或者说信息量， 但一篇能被接受的文章需要满足下面这个公式：</p><blockquote><p>文章的信息量 / 文章的易读性 &lt; 审稿人水平 * 审稿人花的时间</p></blockquote><p>对于机器学习会议，因为投稿量大，所以审稿人很多自然平均水平就会下降。而且很多审稿人就花半个小时到一个小时来读文章，所以公式右边数值通常是很小，而且不是我们能控制。</p><p>如果文章的信息量不大，例如是改进前面工作或者一些简单的新想法，那么公式成立的概率很大。而对于信息量大的文章，就需要努力提升易读性，包括清晰的问题设定，足够的上下文解释等等。而前面投的那篇NIPS，以及更早的那个被拒工作，就是因为我们假设了审稿人有足够多的相关专业知识，而我们塞进了太多干货使得大家都读糊涂了。</p><p>即使对于已经发表的文章，上面那个公式同样可以用来衡量一篇论文的引用率。例如经常见到干货很多的文章没有什么人引用，而同时期的某些工作就是考虑了其中简单特殊情况结果被大引特引。</p><p>接下来的半年我主要在做一个通用的分布式机器学习框架，是想以后做实验方便些。名字就叫parameter server，沿用了Alex 10年论文提出的名字。花了很多时间在接口设计上，做了好几个版本实现，也跑了些工业界级别的大规模的实验。</p><p>不过真正花了我大量时间的是在写论文上。目标是把这个工作投到OSDI上，OSDI是系统界两大会之一。我们预计审稿人跟Dave两年前状态差不多，不会有太多机器学习和数学背景，所以需要尽量的少用公式。整整一个月就花在写论文上，14页的<a href="http://www.cs.cmu.edu/~muli/file/parameter_server_osdi14.pdf" target="_blank" rel="noopener">文章</a>满满都是文字和示意图。不过努力没有白费，最终论文被接受了。随后又花了好几周准备大会报告上。相对于平时花一周写论文，两三天准备报告，这次在写作和报告水平上有了很大的提升。没有放进去的公式和定理投了接下来的<a href="<http://www.cs.cmu.edu/~muli/file/parameter_server_nips14.pdf">NIPS</a>，这次运气很好的中了。</p><p>有了文章后稍微心安了点可以更自由的做些事情。</p><p>寒假回了趟国，跑去百度找了凯哥和潼哥。潼哥说他最近有个想法，于是快糙猛的把实验做了然后写了篇论文投了KDD。同时期Alex一个学生也把他一个一直想让我做但我觉得这个小trick不值得我花时间的想法投了KDD，结果中了<a href="http://www.cs.cmu.edu/~muli/file/minibatch_sgd.pdf" target="_blank" rel="noopener">最佳论文</a>作报告那天我在的会场稀稀疏疏几个人，他们隔壁会场人山人海。这个使得好长一段时间我都在琢磨是不是还是要跟着导师走比较好。</p><p>那时凯哥在百度搞少帅计划，觉得蛮合适就加入了。这时凯哥正带着一大帮兄弟轰轰烈烈的搞深度学习，我自然也是跳坑了。试过好几个想法后，我觉得做做分布式的深度学习框架比较对胃口。我挑了CXXNet作为起点，主要是因为跟天奇比较熟。同时也慢慢上手跑一些Alexnet之类的实验。</p><p>我是因为少帅计划才开始开始做深度学习相关项目，凯哥也很支持我做开源开发回馈社会而不是只做公司内部的产品。但在少帅期间并没有做出什么对公司有帮助的事，很是惭愧。</p><h2 id="第三年：9-14-8-15"><a href="#第三年：9-14-8-15" class="headerlink" title="第三年：9/14-8/15"></a>第三年：9/14-8/15</h2><p>回CMU后Alex看见深度学习这么火，说我们也去买点GPU玩玩。但我们比较穷，只能去newegg上掏点便宜货。这个开启了轰轰烈烈的机器折腾之旅。整个一年我觉得我都在买买买装装装上。最终我们可能就花了小几万刀攒出了一个有80块GPU的集群。现在想想时间上花费不值得，而且为了图便宜买了各种型号的硬件导致维护成本高。但当时候乐在其中。具体细节可以看这篇<a href="https://link.zhihu.com/?target=http%3A//mli.github.io/gpu/2016/01/17/build-gpu-clusters/">blog</a></p><p>这一年写了很多parameter server代码，同时花了很时间帮助用户使用这些代码。很难说做得很成功，现在想想有几个原因。写代码时我会优先考虑性能和支持最多的机器学习算法。但正如前面的错误，忽略了代码的易读性，从而导致只有少部分人能理解代码从而做一些开发。例如我尝试让Alex组的学生来使用这些代码，但其中的各种异步和callback让他们觉得很是难懂。其次是没有人能一起审核代码接口，导致这些接口有浓浓的个人味道，很难做到对所有人都简单明了。</p><p>不过幸运的是找到一帮志同道合的小伙伴。最早是我发现天奇在写xgboost的分布式启动脚本，我看了看发现挺好用，就跟他聊了聊。聊下的发现有很多基础部件例如启动脚本，文件读取应该是可以多个项目共同使用，而不是每个项目都造一个轮子。于是跟天奇在Github上创建了一个叫DMLC的组织，用来加强合作和沟通。第一个项目是dmlc-core，放置了启动和数据读取代码。</p><p>DMLC的第二个新项目叫wormhole。想法是提供一系列分布式机器学习算法，他们使用差不多相同的配置参数来统一用户体验。我把parameter server里面的机器学习相关算法移植了过来，天奇移植了xgboost。Parameter server原有的系统代码简化到了ps-lite。</p><p>中途我听百度同学说factorization machine（FM）在广告数据上效果不错，所以在wormhole上实现了下。针对分布式做了一些优化，然后投了WSDM。前后没有花到一个月，但神奇的竟然拿了最佳论文提名。</p><p>在wormhole的开发中发现一个问题，就是各个算法还是挺不一样，他们可以共用一些代码，但又有各自的特点，需要特别的优化来保证性能。这样导致维护有些困难，例如对共用代码的改动导致所有项目都要检查下。总结下来觉得一个项目最好只做一件事情。所以天奇把xgboost代码放回原来项目，我也把FM独立出来一个项目叫difacto。</p><p>通过一系列的项目，我学到的一点是，以目前的水平和人力，做一个通用而且高效的分布式机器学习框架是很难的一件事情。比较可行的是针对一类相似的机器学习算法做针对性的项目。这个项目的接口必须是符合这类算法结构，所以做算法开发的同学也能容易理解，而不是过多暴露底层系统细节。</p><p>真正的让DMLC社区壮大的项目是第三个，叫做MXNet。当时的背景是CXXNet达到了一定的成熟度，但它的灵活性有局限性。用户只能通过一个配置项来定义模型，而不是交互式的编程。另外一个项目是zz和敏捷他们做的Minerva，是一个类似numpy的交互式编程接口，但这个灵活的接口对稳定性和性能优化带来很多挑战。我当时候同时给两个项目做分布式的扩展，所有都有一定的了解。然后一个自然的想法是，把两个项目合并起来取长补短岂不是很好。</p><p>召集了两个项目的开发人员讨论了几次，有了大致的眉目。新项目取名MXNet，可以叫做mixed-net，是前面两个名字（Minerva和CXXNet）的组合。放弃开发了几年的项目不是容易的决定，但幸运的是小伙伴都愿意最求更好，所以 MXNet进展挺顺利。很快就有了可以跑的第一个版本。</p><h2 id="第四年：9-15-8-16"><a href="#第四年：9-15-8-16" class="headerlink" title="第四年：9/15-8/16"></a>第四年：9/15-8/16</h2><p>前半年为difacto和MXNet写了很多代码。其实一开始的时候我觉得difacto更重要些，毕竟它对于线性算法的提升非常显著而且额外的计算开销并不大，这对广告预估之类的应用会有非常大的提升。但有次遇到Andrew Ng，我跟他说我同时在做这两个项目，他立即告诉我我应该全部精力放在MXNet上，这个的未来空间会大很多。我一直很佩服Andrew的眼光，所以听了他的建议。</p><p>11月的时候MXNet就有了很高的完成度。写了个小论文投去了NIPS的workshop也算是歇了口气。但随后就听到了TensorFlow（TF）开源的消息。由 Jeff Dean领导大量全职工程师开发，Google庞大的宣传机器支持，不出意料迅速成为最流行的深度学习平台。TF对我们压力还是蛮大，我们有核心开发者转去用了TF。不过TF的存在让我领悟到一点，与其过分关心和担忧对手，不如把精力集中在把自己的做得更好。</p><p>NIPS的时候MXNet的小伙伴聚了一次，有好几个我其实是第一次见面。随后Nvidia的GTC邀请我们去做报告。在这两次之间大家爆发了一把，做了很多地方的改进。同时用户也在稳步增长。我们一直觉得MXNet是小开发团队所以做新东西快这是一个优势，但随着用户增加，收到抱怨说开发太快导致很多模块兼容性有问题。有段时间也在反思要在新技术开发速度和稳定性之间做一些权衡。</p><p>这时一夜之间大数据不再流行，大家都在谈深度学习了。</p><p>我也花了很多力气在宣传MXNet和争取开发者上。包括微博知乎上吼一吼，四处给报告。在大量的点赞声中有些陶醉，但很多中肯的批评也让我意识到重要的一点，就是应该真诚的分享而不是简单的吹嘘。</p><p>因为大量的媒体介入，整个深度学习有娱乐化的趋势。娱乐化的报道很多都只是一些简单信息，（有偏见）的观点，而没有太多干货。不仅对别人没营养，对自己来说也就是满足虚荣心。与其写这些简单的水文，不如静下心做一些有深度的分享，包括技术细节，设计思路，和其中的体会。</p><p>此类分享一个容易陷入的误区是只关注自己做了什么，结果多么好。这些确实能证明个人能力，对于想重复这个工作的人来说会有很大帮助。但更多的人更关心的是适用范围在哪里，就是什么情况下效果会减弱；为什么结果会那么好；insight是什么。这个需要更多深入的理解和思考，而不是简单的展示结果。</p><p>这个对写论文也是如此。只说自己的结果比基线好多少只能说明这是不错的工作，但结果再好并不能意味这个工作有深度。</p><p>深度学习的火热导致了各种巨资收购初创司不断。Alex也有点按耐不住， 结果是他，Dave，Ash（曾经是YahooCTO）和我合伙弄了一家公司，拿了几十万的天使投资就开工了。Alex写爬虫，Dave写框架，我跑模型，风风火火干了好一阵子。可惜中途Dave跑路去跟Jeff做TF了。后来这个公司卖给了一个小上市公司。再后来我们觉得这个公司不靠谱也就没考虑跟他们干了。</p><p>第一次创业不能说很成功，从中学到几点：一是跟教授开公司一定要注意有太多想法但没死死的掐住一个做，二是找一堆兼职的博士生来干活不是特别靠谱，尤其是产品不明确的时候，三是即使要卖公司也一定要做一个产品出来。我们卖的时候给很多人的感觉是团队人太强但产品太弱，所以他们只想要人而已。四是试图想要通过技术去改变一个非技术公司是很难的事情，尤其是过于新的技术。</p><p>然后我们就奔去折腾下一个公司。Ash早财务自由所以想做一个大的想法，但这时Alex刚在湾区买了个房，有还贷压力，他选择去了Amazon。于是算是胎死腹中。</p><p>随后收到Jeff的邮件说有没有兴趣加入Google，自然这是一个很诱人的机会。同时我觉得小的创业技术性强的公司是不错的选择。但从MXNet的发展上来书，去Amazon是最好选择之一。自己挖的坑，总是要自己填的。所以我以兼职的身份去了Amazon，领着一帮小弟做些MXNet开发和AWS上深度学习的应用。</p><h2 id="第五年：9-16-2-17"><a href="#第五年：9-16-2-17" class="headerlink" title="第五年：9/16-2/17"></a>第五年：9/16-2/17</h2><p>早在15年初Alex就表示我可以毕业了，但作为拖延晚期患者，迟迟没开始准备。这时候感觉不能再拖了，于是窝在湾区写毕业论文。Alex觉得毕业论文应该好好写，但我对把前面都做完的东西再捣鼓写写实在是没兴趣，尤其是加州太阳那么好，大部分时间我都是躺在后院晒太阳。此时B站已经完全被小学生占领，这边买书也不方便，无聊之余刷了很多起点。然后还写了篇<a href="https://zhuanlan.zhihu.com/p/23781756" target="_blank" rel="noopener">炼丹文</a>。</p><p>CMU要求答辩委员会需要有三个CMU老师和一个学校外的。除了两个导师外，我找了Jeff Dean和刚加入CMU的Ruslan Salakhutdinov. 结果Russ随后就加入了Apple，整个委员会的人都在湾区了。Jeff开玩笑说可以来Google答辩。可惜跟CMU争吵了好多次，还是不允许在校外答辩，而且必须要三个人委员会成员在场。这些限制导致答辩一拖再拖，而且临时加了Barnabas Poczos来凑人数。最后是Jeff的助理快刀斩乱麻的协调好了时间把所有东西定好了。没有她估计我还可以拖几个月。</p><p>答辩的时候是一个比较奇异的状态，委员会里有Google, Amazon, Apple的AI负责人，剩下两个和我又分别在这三家公司兼职。这个反应了当下AI领域学术界纷纷跑去工业界的趋势。</p><p>不过答辩这个事情倒是挺简单，跟平常做个报告没什么太多区别。一片祥和，即使Russ问了MXNet和TensorFlow哪家强这个问题也没有打起来。</p><p>答辩后我问委员会说，我在考虑找个学术界的工作，有什么建议没。大家介绍了一大堆经验，不过大家都强调的一个重点是：学术界好忙好忙，而且好穷好穷，工业界的薪水（就差指自己脸了）分分钟秒掉CMU校长。你要好好想。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>答辩前一天的晚上，我想了两个问题，一个是“博士收获最大的是什么”，另一个是“如果可以重来会怎么办”。对于第一个问题，这五年时间自然学到了很多东西，例如系统的学习了分布式系统，紧跟了机器学习这五年的发展，写文章做幻灯片做报告水平有提升，代码能力也加强了些。自信上有所提高，觉得既可以做一流的研究，也可以写跟大团队PK的代码。只要努力，对手没什么可怕的。</p><p>但更重要的是博士的五年的时间可以专注的把一些事情从技术上做到最好，做出新的突破，这个氛围没有其他地方能给予。</p><p>第二个问题的一个选项是当年留在国内会怎么样？ 当年百度的伙伴们多数现在都做得很好，都在引领这一波AI的潮流，甚至有好几个创造了上亿价值的公司。所以从金钱或者影响力角度来看，一直在工业界也不差，说不定现在已经是土豪了。</p><p>不过我觉得还是会选择读博。赚钱以后还有大把时间可以，但是能花几年时间在某个领域从入门到精通甚至到推动这个领域发展的机会就一次。站在这个领域的高点会发现世界虽然很大，但其实其他领域也使用差不多的技术，有着同样的发展规律。博士期间领悟到的学习的方法可以在各个方向上都会大有作为。</p><p>更重要的是理想和情怀。人一生要工作五十年，为什么不花五年来追求下理想和情怀呢？</p><h4 id="我的感悟："><a href="#我的感悟：" class="headerlink" title="我的感悟："></a>我的感悟：</h4><p>–做的东西一定要反复打磨，每个细节都做好了再呈现给这个世界。李沐和小熊都讲深入细节（必须为提升5%性能写千行代码）会明显提升个人能力，细致钻研才能遇到有深度的问题，这些问题可能是以后做项目或者发文章的思路来源。</p><h5 id="“11年12月中的时候突然心血来潮随手把材料重新寄了一遍，就选了CMU和MIT，结果意外收到了CMU的offer。有天在百度食堂同凯哥（余凯）和潼哥（张潼）吃饭，我说收了CMU-offer，在纠结去不去。他们立马说去跟Alex-Smola啊，他要要加入CMU了，我们给你引荐下”"><a href="#“11年12月中的时候突然心血来潮随手把材料重新寄了一遍，就选了CMU和MIT，结果意外收到了CMU的offer。有天在百度食堂同凯哥（余凯）和潼哥（张潼）吃饭，我说收了CMU-offer，在纠结去不去。他们立马说去跟Alex-Smola啊，他要要加入CMU了，我们给你引荐下”" class="headerlink" title="“11年12月中的时候突然心血来潮随手把材料重新寄了一遍，就选了CMU和MIT，结果意外收到了CMU的offer。有天在百度食堂同凯哥（余凯）和潼哥（张潼）吃饭，我说收了CMU offer，在纠结去不去。他们立马说去跟Alex Smola啊，他要要加入CMU了，我们给你引荐下”"></a>“11年12月中的时候突然心血来潮随手把材料重新寄了一遍，就选了CMU和MIT，结果意外收到了CMU的offer。有天在百度食堂同凯哥（余凯）和潼哥（张潼）吃饭，我说收了CMU offer，在纠结去不去。他们立马说去跟Alex Smola啊，他要要加入CMU了，我们给你引荐下”</h5><p>–圈子和良好人际关系的重要性。</p><p>–李一周工作八十个小时，累了就深夜去健身房健身，CMU深夜随处可见中国和印度留学生。自己目前的努力程度还未达到需要拼天赋的时候。</p><h5 id="接下来的半年我主要在做一个通用的分布式机器学习框架，是想以后做实验方便些。名字就叫parameter-server，沿用了Alex-10年论文提出的名字。花了很多时间在接口设计上，做了好几个版本实现，也跑了些工业界级别的大规模的实验。"><a href="#接下来的半年我主要在做一个通用的分布式机器学习框架，是想以后做实验方便些。名字就叫parameter-server，沿用了Alex-10年论文提出的名字。花了很多时间在接口设计上，做了好几个版本实现，也跑了些工业界级别的大规模的实验。" class="headerlink" title="接下来的半年我主要在做一个通用的分布式机器学习框架，是想以后做实验方便些。名字就叫parameter server，沿用了Alex 10年论文提出的名字。花了很多时间在接口设计上，做了好几个版本实现，也跑了些工业界级别的大规模的实验。"></a><strong>接下来的半年我主要在做一个通用的分布式机器学习框架，是想以后做实验方便些。名字就叫parameter server，沿用了Alex 10年论文提出的名字。花了很多时间在接口设计上，做了好几个版本实现，也跑了些工业界级别的大规模的实验。</strong></h5><h5 id="不过真正花了我大量时间的是在写论文上。目标是把这个工作投到OSDI上，OSDI是系统界两大会之一。我们预计审稿人跟Dave两年前状态差不多，不会有太多机器学习和数学背景，所以需要尽量的少用公式。整整一个月就花在写论文上，14页的文章满满都是文字和示意图。不过努力没有白费，最终论文被接受了。随后又花了好几周准备大会报告上。相对于平时花一周写论文，两三天准备报告，这次在写作和报告水平上有了很大的提升。没有放进去的公式和定理投了接下来的NIPS，这次运气很好的中了"><a href="#不过真正花了我大量时间的是在写论文上。目标是把这个工作投到OSDI上，OSDI是系统界两大会之一。我们预计审稿人跟Dave两年前状态差不多，不会有太多机器学习和数学背景，所以需要尽量的少用公式。整整一个月就花在写论文上，14页的文章满满都是文字和示意图。不过努力没有白费，最终论文被接受了。随后又花了好几周准备大会报告上。相对于平时花一周写论文，两三天准备报告，这次在写作和报告水平上有了很大的提升。没有放进去的公式和定理投了接下来的NIPS，这次运气很好的中了" class="headerlink" title="不过真正花了我大量时间的是在写论文上。目标是把这个工作投到OSDI上，OSDI是系统界两大会之一。我们预计审稿人跟Dave两年前状态差不多，不会有太多机器学习和数学背景，所以需要尽量的少用公式。整整一个月就花在写论文上，14页的文章满满都是文字和示意图。不过努力没有白费，最终论文被接受了。随后又花了好几周准备大会报告上。相对于平时花一周写论文，两三天准备报告，这次在写作和报告水平上有了很大的提升。没有放进去的公式和定理投了接下来的NIPS，这次运气很好的中了."></a><strong>不过真正花了我大量时间的是在写论文上。目标是把这个工作投到OSDI上，OSDI是系统界两大会之一。我们预计审稿人跟Dave两年前状态差不多，不会有太多机器学习和数学背景，所以需要尽量的少用公式。整整一个月就花在写论文上，14页的文章满满都是文字和示意图。不过努力没有白费，最终论文被接受了。随后又花了好几周准备大会报告上。相对于平时花一周写论文，两三天准备报告，这次在写作和报告水平上有了很大的提升。没有放进去的公式和定理投了接下来的NIPS，这次运气很好的中了.</strong></h5><p>–要有工程能力，做出框架，设计接口，更新版本，跑了工业界级别的大规模实验。然后再用这个框架的内容去发论文。这样的论文工作扎实，比堆砌算法的好很多。小熊给的nlp论文列表里的文章都是带源代码的，要好好研究一下好的论文是怎么做的。李沐提到的百度速发文章和两篇parameter server的文章都打上了超链接，好好读一读。</p><p>–申请博士是要与全球各位优秀的科研工作者竞争的，当别人有完美的本科GPA和一堆科研经历甚至还有多篇发出去的论文，我投不出去的没有工作量和工程量的文章完全没有竞争力。</p><p>–parameter server项目李沐自己也说做的不够成熟，但是已经够发文章了。后来他与陈天奇成立的DMLC组织又做了很多项目。even大牛也都有一个进步的过程，关键是勇敢去做，以及在做的过程中多总结和思考。</p><p>–李沐、陈天奇的github主页，工作量叹为观止。<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1314_05_679.png" alt=""></p><p>–MXnet遭遇TensorFlow竞争，他们面对竞争的做法是做好自己的工作，而非担忧对手有多强大。</p><p>–当然，时至今日tensorflow面对pytorch的竞争时也显得力不从心了，谁知道呢。</p><h5 id="通过一系列的项目，我学到的一点是，以目前的水平和人力，做一个通用而且高效的分布式机器学习框架是很难的一件事情。比较可行的是针对一类相似的机器学习算法做针对性的项目。这个项目的接口必须是符合这类算法结构，所以做算法开发的同学也能容易理解，而不是过多暴露底层系统细节"><a href="#通过一系列的项目，我学到的一点是，以目前的水平和人力，做一个通用而且高效的分布式机器学习框架是很难的一件事情。比较可行的是针对一类相似的机器学习算法做针对性的项目。这个项目的接口必须是符合这类算法结构，所以做算法开发的同学也能容易理解，而不是过多暴露底层系统细节" class="headerlink" title="通过一系列的项目，我学到的一点是，以目前的水平和人力，做一个通用而且高效的分布式机器学习框架是很难的一件事情。比较可行的是针对一类相似的机器学习算法做针对性的项目。这个项目的接口必须是符合这类算法结构，所以做算法开发的同学也能容易理解，而不是过多暴露底层系统细节"></a><strong>通过一系列的项目，我学到的一点是，以目前的水平和人力，做一个通用而且高效的分布式机器学习框架是很难的一件事情。比较可行的是针对一类相似的机器学习算法做针对性的项目。这个项目的接口必须是符合这类算法结构，所以做算法开发的同学也能容易理解，而不是过多暴露底层系统细节</strong></h5><p>–做项目/发文章要精在某个点上比较靠谱。</p><h5 id="我也花了很多力气在宣传MXNet和争取开发者上。包括微博知乎上吼一吼，四处给报告。在大量的点赞声中有些陶醉，但很多中肯的批评也让我意识到重要的一点，就是应该真诚的分享而不是简单的吹嘘。"><a href="#我也花了很多力气在宣传MXNet和争取开发者上。包括微博知乎上吼一吼，四处给报告。在大量的点赞声中有些陶醉，但很多中肯的批评也让我意识到重要的一点，就是应该真诚的分享而不是简单的吹嘘。" class="headerlink" title="我也花了很多力气在宣传MXNet和争取开发者上。包括微博知乎上吼一吼，四处给报告。在大量的点赞声中有些陶醉，但很多中肯的批评也让我意识到重要的一点，就是应该真诚的分享而不是简单的吹嘘。"></a><strong>我也花了很多力气在宣传MXNet和争取开发者上。包括微博知乎上吼一吼，四处给报告。在大量的点赞声中有些陶醉，但很多中肯的批评也让我意识到重要的一点，就是应该真诚的分享而不是简单的吹嘘。</strong></h5><h5 id="赚钱以后还有大把时间可以，但是能花几年时间在某个领域从入门到精通甚至到推动这个领域发展的机会就一次。"><a href="#赚钱以后还有大把时间可以，但是能花几年时间在某个领域从入门到精通甚至到推动这个领域发展的机会就一次。" class="headerlink" title="赚钱以后还有大把时间可以，但是能花几年时间在某个领域从入门到精通甚至到推动这个领域发展的机会就一次。"></a><strong>赚钱以后还有大把时间可以，但是能花几年时间在某个领域从入门到精通甚至到推动这个领域发展的机会就一次。</strong></h5><p>–想方设法读博，在选择方向时注意跟紧时代的发展。</p>]]></content>
    
    
    <categories>
      
      <category>5.杂谈</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文写法</tag>
      
      <tag>思考</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_搜索_A[星号]寻路算法</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E6%90%9C%E7%B4%A2_A%5B%E6%98%9F%E5%8F%B7%5D%E5%AF%BB%E8%B7%AF%E7%AE%97%E6%B3%95/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E6%90%9C%E7%B4%A2_A%5B%E6%98%9F%E5%8F%B7%5D%E5%AF%BB%E8%B7%AF%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="A-寻路算法"><a href="#A-寻路算法" class="headerlink" title="A*寻路算法"></a>A*寻路算法</h1><p>算法要解决的问题：</p><p>在有障碍的格子地图上已知终点和起点，横走代价小于斜走，求由始到终代价最小的路径。</p><p>F = G + H </p><p>总代价  = 到起点的格子数 + 到终点的曼哈顿距离</p><p>算法描述：</p><ol><li><p>初始节点即为当前节点，放入open列表</p></li><li><p>初始节点从open列表移除，放入close列表</p></li></ol><ol start="3"><li><p>当前格子周围八个格子用如下三选一方式处理:</p><ol><li>若在close列表中，忽略</li><li>若不在open/close列表中，加入open列表，并将当前格子设为它的父节点</li><li>若在open列表中，计算这个格子经过当前格子的F值，如果F值更小，更新之，并将当前格子设为它的新父节点。</li></ol></li><li><p>把当前格子从open列表拿入close列表；</p><p>设置刚才加入open列表中F值最小的格子为当前格子。</p></li></ol><ol start="5"><li>重复3.4.两步，知道找到终点，随后按照父节点顺序回溯，找到最优路径</li></ol><p>不需要更新F值的图示</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1226_56_264.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1227_17_162.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1227_37_630.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1227_53_344.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1228_18_128.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1228_32_410.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1228_44_755.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1228_58_530.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1229_12_312.png" alt=""></p><p>需要更新F值的图示</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1229_30_596.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1229_40_541.png" alt=""></p><p>小结：</p><p>A*寻路算法需要对每个格子维持三个状态值：总代价，到起点的距离，到终点的距离。下一步如何选择由总代价值指导。通常比DFS和BFS更高效。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>A*寻路算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_搜索_回溯算法</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E6%90%9C%E7%B4%A2_%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E6%90%9C%E7%B4%A2_%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="8-查找"><a href="#8-查找" class="headerlink" title="8.查找"></a>8.查找</h1><h4 id="8-1回溯"><a href="#8-1回溯" class="headerlink" title="8.1回溯"></a>8.1回溯</h4><p>回溯算法也叫试探法，是一种系统地解决搜索问题的解的方法</p><p>用回溯算法解决问题的一般步骤：</p><p>1.针对问题，定义解空间，使得能用回溯法方便地搜索整个解空间</p><p>2.确定易于搜索的解空间结构，使得能用回溯法方便地搜索整个空间结构</p><p>3.以深度优先的方式搜索解空间，并且在搜索过程中使用剪枝函数避免无效搜索</p><p>回溯法在解空间树里对结果进行深度优先遍历，如果发现本节点不属于解的范围，则退出，剪枝，然后递归地对子节点搜索</p><p>经典问题是八皇后问题，见代码</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>查找</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_搜索_贪心算法与最小生成树</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E6%90%9C%E7%B4%A2_%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E4%B8%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E6%90%9C%E7%B4%A2_%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E4%B8%8E%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91/</url>
    
    <content type="html"><![CDATA[<h1 id="贪心算法"><a href="#贪心算法" class="headerlink" title="贪心算法"></a>贪心算法</h1><p>贪心算法是指，在对问题求解时，总是做出当前来看最好的选择，比较短视，不从总体上考虑最优解，每一步都在向局部最优解迈进（类似于梯度下降算法）</p><p>贪心算法并不能总是得到最优解，关键是贪心策略的选择，选择贪心策略必须具备无后效性，即某个状态以前的过程只与当前状态有关，不会影响到以后的状态。</p><h5 id="引入找零问题"><a href="#引入找零问题" class="headerlink" title="引入找零问题"></a>引入找零问题</h5><p>尽可能用少的硬币和纸币加出一个指定金额总数。思路是从尽可能大的面额的货币开始处理，附上代码</p><pre><code class="python">denom = [10000, 5000, 2000, 1000, 500, 200, 100, 50, 25, 10, 5, 1]owed = 9876payed = []for d in denom:    while owed &gt;= d:        owed -= d        payed.append(d)print(sum(payed))print(payed)</code></pre><p>输出如下结果</p><pre><code class="python">9876[5000, 2000, 2000, 500, 200, 100, 50, 25, 1]</code></pre><h5 id="引入背包问题"><a href="#引入背包问题" class="headerlink" title="引入背包问题"></a>引入背包问题</h5><p>背包问题可视为纸币找零问题的泛化版，背包问题是组合优化的NP完全问题，描述是给定一组物品，每种物品都有自己的重量和价格，在限定的总重量内，我们如何选择才能使得物品的总价值最高。</p><ul><li><p>分数背包问题</p><p>比如去野餐，往背包里边放什么？可以放金沙，威士忌，和水。</p><p>我们先放价值最高的金沙，然后放价值稍次的威士忌，最后放水。背包问题的核心是找到权重比，然后按照权重比排序，从高到低顺序一个个装包就好了。</p></li></ul><h5 id="引入哈夫曼算法"><a href="#引入哈夫曼算法" class="headerlink" title="引入哈夫曼算法"></a>引入哈夫曼算法</h5><pre><code class="python"># 哈弗曼算法from heapq import heapify, heappush, heappopfrom itertools import countdef huffman(seq, frq):    num = count()    trees = list(zip(frenquence, num, seq))    heapify(trees)    # 数组按照频率组建成小顶堆，每次pop都pop出值最小的    while len(trees) &gt; 1:        fa, _, a = heappop(trees)        fb, _, b = heappop(trees)        n = next(num)        heappush(trees, (fa+fb, n, [a, b]))    # 将刚才弹出的两个频率相加，然后合并回堆的右下角，堆自然会按照小顶堆的构建规则做调整。    # 频率值在小顶堆的秩序里做操作，跟他一起绑定的哈夫曼编码值也在两两合并中通过多层数组构建为哈夫曼树    return trees[0][-1]seq = &quot;abcdefghi&quot;frq = [4, 5, 6, 9, 11, 12, 15, 16, 20]print(huffman(seq, frenquence))</code></pre><p>哈夫曼算法在构建哈夫曼树的过程中就运用了贪心算法，每次都先解决看起来最好的选择（不管全局规划，先将最小的两个频率值相加合并）。</p><h5 id="引入Kruskal算法和Prim算法"><a href="#引入Kruskal算法和Prim算法" class="headerlink" title="引入Kruskal算法和Prim算法"></a>引入Kruskal算法和Prim算法</h5><p>最小生成树定义</p><ul><li><p>定义</p><ul><li>一个有向带权图,需要删除一些边,使这个图变成权值最小的树</li></ul></li><li><p>实际应用</p><ul><li>几个城市之间如何修高速公路/通信网络实现几个城市互联最有效率?(点组网)</li><li>皇帝如何保持通知体系的运转,又能清理大臣之间跨部门的互相勾结(原图删除边成为最小生成树)</li></ul></li><li><p>构建最小生成树的两种算法</p><ul><li><p>Kruskal算法        —        常用于稀疏图(先取出边再判断节点,稠密的话很慢)</p><ul><li><p>贪心算法,将图的每个边按照权重排序,每次从边集中取出权重最小且两个顶点都不在同一个集合的边加入生成树中,反复执行,直到所有节点都链接成功</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1230_10_308.png" alt=""></p></li><li><p>上图节点的颜色代表他们是否属于同一个集合,寻找到本步最小边后需判断两点是否在同一集合?</p><ul><li>具体判断步骤为:并查集<ul><li>对所有节点遍历建立并查集,按照边的权重建立最小堆</li><li>取出最小堆堆顶数据,并判断两端节点是否在同一集合</li><li>如果不在,将两个节点添加到同一集合,接着将边加入生成边,如果在,则不进行操作.</li><li>重复上述步骤,直到所有的边都检查完</li></ul></li></ul></li></ul></li></ul></li></ul><ul><li><p>Prim算法   —   常用于稠密图(只是对节点遍历)</p><ul><li><p>Kruskral算法主要对边操作</p></li><li><p>Prim算法主要对节点操作</p><ul><li><p>Prim算法没有判断两个点是否在同一集合的步骤,所以不使用并查集</p></li><li><p>具体步骤如下</p><ul><li><p>建立基本容器</p><ul><li>建立已完事儿集合存放已经完事儿节点</li><li>建立未完事儿集合存放没有被处理节点</li><li>建立完事儿节点相连的边的最小堆</li></ul></li><li><p>遍历所有节点,若没有被访问,则添加进节点set,然后将其相连的边加入最小堆</p></li><li><p>从堆中取最小的边,然后判断to节点是否被访问过,若没有,将此边加入生成树,并标记此节点被访问过</p></li><li><p>然后将to节点所连的边加入最小堆中,不然这个网络就不会扩展了</p></li><li><p>循环,知道所有节点遍历完</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1230_39_087.png" alt=""></p></li></ul></li></ul></li></ul></li></ul><p>小结：</p><p>贪心算法是在一步步构建出属于自己的解决方案时，慢慢逼近最优解，在到达终点时，就变成最优解了。</p><p>贪心算法的优缺点：</p><p>优点：简单，高效，省去了为了找最优解可能需要穷举操作，通常作为其它算法的辅助算法来使用</p><p>缺点：不从总体上考虑其它可能情况，每次选取局部最优解，不再进行回溯处理，所以很少情况下得到最优解。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>Kruskal</tag>
      
      <tag>Prim</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_网络流，最短路径，字典树</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E7%BD%91%E7%BB%9C%E6%B5%81%EF%BC%8C%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%EF%BC%8C%E5%AD%97%E5%85%B8%E6%A0%91/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E7%BD%91%E7%BB%9C%E6%B5%81%EF%BC%8C%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%EF%BC%8C%E5%AD%97%E5%85%B8%E6%A0%91/</url>
    
    <content type="html"><![CDATA[<h1 id="7-最短路径-迪杰斯特拉算法"><a href="#7-最短路径-迪杰斯特拉算法" class="headerlink" title="7.最短路径:迪杰斯特拉算法"></a>7.最短路径:迪杰斯特拉算法</h1><h3 id="7-1释义"><a href="#7-1释义" class="headerlink" title="7.1释义"></a>7.1释义</h3><p>迪杰斯特拉算法就是求一个顶点到其他所有顶点的最短路径</p><p>需要维持两个数据集,一个最开始只有顶点,另一个最开始有除了顶点之外的所有点</p><p>然后每次都把距离顶点最近的点放入前一个数据集,然后遍历一次顶点到和前一个数据集接触的点的最短路径,有更近的就更新一次</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1238_23_066.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1238_45_004.png" alt=""></p><h1 id="8-网络流建模"><a href="#8-网络流建模" class="headerlink" title="8.网络流建模"></a>8.网络流建模</h1><h3 id="8-1几个概念"><a href="#8-1几个概念" class="headerlink" title="8.1几个概念"></a>8.1几个概念</h3><ul><li><p>原点:起点</p></li><li><p>汇点:目标点</p></li><li><p>流:从原点到汇点的一条路径</p></li><li><p>流量:通过一条边的水的体积</p></li><li><p>容量:每条管道允许通过的最大流量</p></li><li><p>()实际流量:取决于流上最小的容量,最小流量是流的短板</p></li><li><p>最小割:如下图,切断哪些管道后,源头的水就不能流向汇点了?</p><ul><li><p>最小割=最大流</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1239_14_419.png" alt=""></p></li></ul></li></ul><h1 id="9-哈希表"><a href="#9-哈希表" class="headerlink" title="9.哈希表"></a>9.哈希表</h1><h4 id="9-1什么是哈希表"><a href="#9-1什么是哈希表" class="headerlink" title="9.1什么是哈希表"></a>9.1什么是哈希表</h4><p>举例说明</p><p>去商场停车,有三种策略</p><ul><li>随机停,然后找车的时候从头到尾顺序搜索,时间复杂度是O(N)</li><li>所有车辆按照牌照顺序停,然后每次二分查找找车,时间复杂度是log(n)</li><li>每个车都有一个与牌照对应的停车位,专车专位,时间复杂度是O(1)<ul><li>这种策略时间复杂度确实低,但是空间开销过大,每个车牌照都需要分配一个存储空间</li></ul></li></ul><p>所以在时间复杂度和空间复杂度之间做个取舍，降低专车专位的空间开销，同时尽量保留时间开销的优势</p><p>哈希表：比如车牌照是０　－－　９８５７３６２８之间</p><p>取末尾四位作为下标依据，塞入散列表</p><ul><li>碰撞问题<ul><li>如果两个车的尾号都是3628怎么办?<ul><li>开链表法:如果两个车都是3628,就在原来的3628后边跟一个链表,把后一个3628接在后边</li><li>再散列法:如果3628碰撞,第二个3628就放在3629好了</li></ul></li></ul></li><li>哈希表的评估标准:<ul><li>优秀的哈希表一定是通过巧妙的散列函数,尽量避免碰撞问题的</li></ul></li></ul><h1 id="10-1后缀树"><a href="#10-1后缀树" class="headerlink" title="10.1后缀树"></a>10.1后缀树</h1><p>字典树和压缩字典树    </p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1239_59_443.png" alt=""></p><ul><li>把不会引起歧义的节点串压缩为一个节点</li></ul><p>后缀树就是把一颗以所有的后缀为关键字建立的压缩字典树<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1240_13_018.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>字典树</tag>
      
      <tag>Kruskal</tag>
      
      <tag>Prim</tag>
      
      <tag>最小生成树</tag>
      
      <tag>迪杰斯特拉算法</tag>
      
      <tag>哈希表</tag>
      
      <tag>后缀树</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_排序_7.2-归并排序</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E6%8E%92%E5%BA%8F_7.2-%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E6%8E%92%E5%BA%8F_7.2-%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/</url>
    
    <content type="html"><![CDATA[<h1 id="7-2归并排序"><a href="#7-2归并排序" class="headerlink" title="7.2归并排序"></a>7.2归并排序</h1><p>归并排序思路：</p><p>1.将序列中待排序数字分为若干组，每个数字分为一组</p><p>2.将若干组两两合并，保证合并后的组是有序的</p><p>3.重复第二步，知道所有组都被处理完成</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1224_39_967.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1225_05_846.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>排序</tag>
      
      <tag>归并排序</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_排序_7.3-计数排序</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E6%8E%92%E5%BA%8F_7.3-%E8%AE%A1%E6%95%B0%E6%8E%92%E5%BA%8F/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E6%8E%92%E5%BA%8F_7.3-%E8%AE%A1%E6%95%B0%E6%8E%92%E5%BA%8F/</url>
    
    <content type="html"><![CDATA[<h1 id="7-3计数排序"><a href="#7-3计数排序" class="headerlink" title="7.3计数排序"></a>7.3计数排序</h1><p>适用范围：</p><p>数据量很大但是取值范围很小</p><p>适用案例：</p><p>对某企业三万员工的年龄排序，快速取得高考成绩等</p><p>桶排序的思想：</p><p>以年龄排序为例子：</p><p>假设年龄以0-60为区间</p><p>创建一个新的数组，长度为61，遍历待排序数组，将出现的数字频率记录在以本数字为下标的计数数组中，</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1225_35_726.png" alt=""></p><p>然后按照计数数组记录的频率值输出数字就OK了</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>排序</tag>
      
      <tag>计数排序</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_7.字典树Trie</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_7.%E5%AD%97%E5%85%B8%E6%A0%91Trie/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_7.%E5%AD%97%E5%85%B8%E6%A0%91Trie/</url>
    
    <content type="html"><![CDATA[<h1 id="7-字典树"><a href="#7-字典树" class="headerlink" title="7.字典树"></a>7.字典树</h1><p>在实际的搜索引擎中，当我们在数据库中搜索一个关键字的时候，如何快速准确的进行定位是一个关键的问题，在面临大规模数据的时候，使用暴力的手段往往会造成检索和查找性能的低下，因此我们需要更加高效的数据结构。</p><p>这时候我们引入一种新的数据结构：Trie树（字典树）。</p><p>又称单词查找树，<a href="https://baike.baidu.com/item/Trie%E6%A0%91" target="_blank" rel="noopener">Trie树</a>，是一种<a href="https://baike.baidu.com/item/%E6%A0%91%E5%BD%A2%E7%BB%93%E6%9E%84/9663807" target="_blank" rel="noopener">树形结构</a>，是一种哈希树的变种。典型应用是用于统计，排序和保存大量的<a href="https://baike.baidu.com/item/%E5%AD%97%E7%AC%A6" target="_blank" rel="noopener">字符</a>串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。它的优点是：利用字符串的公共前缀来减少查询时间，最大限度地减少无谓的字符串比较，查询效率比哈希树高。</p><p>字典树每一个节点代表一个字符,有相同前缀的树就有相同的根节点,每个节点结束的时候用一个特殊的标记来表示(-1)</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1239_46_215.png" alt=""></p><p>从图中可以看出：</p><p>1.每一个节点代表一个字符</p><p>2.有相同前缀的单词在树中就有公共的前缀节点，由于一共有26个小写英文字母（在这篇文章中，我们主要讨论小写的英文字母查询），因此每个节点最多有26个子节点。</p><p>3.整棵树的根节点是空的（这里我们设置根节点为root=0），这便于查找和插入，可以通过根节点快速的进入树结构，稍后就会明白。</p><p>4.每个节点结束的时候用一个特殊的标记来表示，这里我们用-1来表示结束，从根节点到-1所经过的所有的节点对应一个英文单词。</p><pre><code class="java">packagecom.suning.search.test.tree.trie;public class Trie{    private int SIZE=26;    private TrieNode root;//字典树的根    Trie() //初始化字典树    {        root=new TrieNode();    }    private class TrieNode //字典树节点    {        private int num;//有多少单词通过这个节点,即由根至该节点组成的字符串模式出现的次数        private TrieNode[]  son;//所有的儿子节点        private boolean isEnd;//是不是最后一个节点        private char val;//节点的值p       private boolean haveSon;        TrieNode()        {            num=1;            son=new TrieNode[SIZE];            isEnd=false;h           haveSons=false;        }    }//建立字典树    public void insert(String str) //在字典树中插入一个单词    {        if(str==null||str.length()==0)        {            return;        }        TrieNode node=root;        char[]letters=str.toCharArray();        for(int i=0,len=str.length(); i&lt;len; i++)        {            int pos=letters[i]-&#39;a&#39;;            if(node.son[pos]==null)            {                node.haveSon = true;                node.son[pos]=newTrieNode();                node.son[pos].val=letters[i];            }            else            {                node.son[pos].num++;            }            node=node.son[pos];        }        node.isEnd=true;    }//计算单词前缀的数量    public int countPrefix(Stringprefix)    {        if(prefix==null||prefix.length()==0)        {            return-1;        }        TrieNode node=root;        char[]letters=prefix.toCharArray();        for(inti=0,len=prefix.length(); i&lt;len; i++)        {            int pos=letters[i]-&#39;a&#39;;            if(node.son[pos]==null)            {                return 0;            }            else            {                node=node.son[pos];            }        }        return node.num;    }//打印指定前缀的单词    public String hasPrefix(String prefix)    {        if (prefix == null || prefix.length() == 0)        {            return null;        }        TrieNode node = root;        char[] letters = prefix.toCharArray();        for (int i = 0, len = prefix.length(); i &lt; len; i++)        {            int pos = letters[i] - &#39;a&#39;;            if (node.son[pos] == null)            {                return null;            }            else            {                node = node.son[pos];            }        }        preTraverse(node, prefix);        return null;    }// 遍历经过此节点的单词.    public void preTraverse(TrieNode node, String prefix)    {        if (node.haveSon)        {for (TrieNode child : node.son)            {                if (child!=null)                {                    preTraverse(child, prefix+child.val);                }            }            return;        }        System.out.println(prefix);    }//在字典树中查找一个完全匹配的单词.    public boolean has(Stringstr)    {        if(str==null||str.length()==0)        {            return false;        }        TrieNode node=root;        char[]letters=str.toCharArray();        for(inti=0,len=str.length(); i&lt;len; i++)        {            intpos=letters[i]-&#39;a&#39;;            if(node.son[pos]!=null)            {                node=node.son[pos];            }            else            {                return false;            }        }        return node.isEnd;    }//前序遍历字典树.    public void preTraverse(TrieNodenode)    {        if(node!=null)        {            System.out.print(node.val+&quot;-&quot;);for(TrieNodechild:node.son)            {                preTraverse(child);            }        }    }    public TrieNode getRoot()    {        return this.root;    }    public static void main(String[]args)    {        Trietree=newTrie();        String[]strs= {&quot;banana&quot;,&quot;band&quot;,&quot;bee&quot;,&quot;absolute&quot;,&quot;acm&quot;,};        String[]prefix= {&quot;ba&quot;,&quot;b&quot;,&quot;band&quot;,&quot;abc&quot;,};for(Stringstr:strs)        {            tree.insert(str);        }        System.out.println(tree.has(&quot;abc&quot;));        tree.preTraverse(tree.getRoot());        System.out.println();//tree.printAllWords();for(Stringpre:prefix)        {            int num=tree.countPrefix(pre);            System.out.println(pre+&quot;&quot;+num);        }    }}</code></pre>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>字典树</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_排序_7.1-快排</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E6%8E%92%E5%BA%8F_7.1-%E5%BF%AB%E6%8E%92/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_%E6%8E%92%E5%BA%8F_7.1-%E5%BF%AB%E6%8E%92/</url>
    
    <content type="html"><![CDATA[<h1 id="7-排序"><a href="#7-排序" class="headerlink" title="7.排序"></a>7.排序</h1><h3 id="7-1快速排序"><a href="#7-1快速排序" class="headerlink" title="7.1快速排序"></a>7.1快速排序</h3><p>快排是对冒泡排序的改进.</p><p>基本思想:</p><p>通过一趟排序将数据分割成独立的两部分,其中一部分的数据要小于另一部分</p><p>然后用同样方法递归地对这两部分数据做快速排序,直到整个数据都变成有序序列.</p><p>如下图所示</p><h4 id="7-1-左指针放最左-右指针放最右-Basic指针随机放"><a href="#7-1-左指针放最左-右指针放最右-Basic指针随机放" class="headerlink" title="7.1.左指针放最左,右指针放最右,Basic指针随机放"></a>7.1.左指针放最左,右指针放最右,Basic指针随机放</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1222_45_969.png" alt=""></p><h4 id="7-2-左指针向右移动-遇见比basic大的停下来"><a href="#7-2-左指针向右移动-遇见比basic大的停下来" class="headerlink" title="7.2.左指针向右移动,遇见比basic大的停下来"></a>7.2.左指针向右移动,遇见比basic大的停下来</h4><h4 id="右指针向左移动-遇见比basic小的停下来"><a href="#右指针向左移动-遇见比basic小的停下来" class="headerlink" title="右指针向左移动,遇见比basic小的停下来"></a>右指针向左移动,遇见比basic小的停下来</h4><h4 id="左右指针指的对象交换位置"><a href="#左右指针指的对象交换位置" class="headerlink" title="左右指针指的对象交换位置"></a>左右指针指的对象交换位置</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1222_54_965.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1223_04_091.png" alt=""></p><h4 id="7-3-交换位置后重复刚才步骤-直到左右指针相碰"><a href="#7-3-交换位置后重复刚才步骤-直到左右指针相碰" class="headerlink" title="7.3.交换位置后重复刚才步骤,直到左右指针相碰"></a>7.3.交换位置后重复刚才步骤,直到左右指针相碰</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1223_12_618.png" alt=""></p><h4 id="7-4-相碰后交换本对象和basic所指对象的位置"><a href="#7-4-相碰后交换本对象和basic所指对象的位置" class="headerlink" title="7.4.相碰后交换本对象和basic所指对象的位置"></a>7.4.相碰后交换本对象和basic所指对象的位置</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1223_20_577.png" alt=""></p><h4 id="7-5-于是将数据一分为二-可递归地操作剩下数据"><a href="#7-5-于是将数据一分为二-可递归地操作剩下数据" class="headerlink" title="7.5.于是将数据一分为二,可递归地操作剩下数据"></a>7.5.于是将数据一分为二,可递归地操作剩下数据</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1223_43_080.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>排序</tag>
      
      <tag>快速排序</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_3.树</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_3.%E6%A0%91/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_3.%E6%A0%91/</url>
    
    <content type="html"><![CDATA[<h1 id="树"><a href="#树" class="headerlink" title="树"></a>树</h1><ul><li>树的特征<ul><li>树是非线性结构</li><li>树是由一个集合以及在集合上定义的一种关系构成的<ul><li>集合的元素称树的节点</li><li>定义的关系成为父子关系</li><li>父子关系在树的节点之间建立了一个层次关系</li></ul></li></ul></li></ul><ul><li><p>二叉树的定义</p><ul><li>递归定义:二叉树或者是一个空树,或者是由一个根节点和两颗互不相交分别为根的左子树和右子树钩构成的</li></ul></li><li><p>遍历</p><ul><li>先序</li><li>中序</li><li>后序</li></ul></li><li><ul><li><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1232_34_492.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1232_46_883.png" alt=""></p></li></ul></li></ul><ul><li><p>哈弗曼树</p><ul><li><p>哈弗曼编码</p><ul><li><p>在一个字符集的编码过程中,为了避免出现前缀歧义的情况,通常把待编码的项都安排在树的叶子节点上,因为没有任何一个叶子节点与其他叶子节点共享到根节点的路径,遂可保证无歧义性</p></li><li><p>把待编码的项目根据使用频率可赋值一个”权”,权大的使用频率高,放在更靠近根节点的位置,这样它的编码就较短,可拉低整体编码的长度.</p></li><li><p>哈弗曼树的构建过程如下图所示,代码在study项目中</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1233_11_078.png" alt=""></p></li></ul></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>树</tag>
      
      <tag>二叉树</tag>
      
      <tag>哈夫曼树</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_4.图</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_4.%E5%9B%BE/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_4.%E5%9B%BE/</url>
    
    <content type="html"><![CDATA[<h1 id="4-图"><a href="#4-图" class="headerlink" title="4.图"></a>4.图</h1><p>图是一种网状数据结构,由顶点和边构成,Graph = (V,E)</p><h3 id="4-1子图"><a href="#4-1子图" class="headerlink" title="4.1子图"></a>4.1子图</h3><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1233_28_077.png" alt=""></p><h3 id="4-2强连通分量"><a href="#4-2强连通分量" class="headerlink" title="4.2强连通分量:"></a>4.2强连通分量:</h3><ul><li><p>构造镜像图G’</p><ul><li>对G 和 G’中s的可达分量求并集既可得强连通分量</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1233_51_550.png" alt=""></p></li></ul><h3 id="4-3图的存储方式"><a href="#4-3图的存储方式" class="headerlink" title="4.3图的存储方式"></a>4.3图的存储方式</h3><p>首先,从图的逻辑结构定义来看,无法将图的顶点排列成唯一的线性序列,在图中任意顶点都可以是图的第一个顶点.</p><p>对任意顶点来说他的邻接点也不存在顺序关系</p><p>所以顶点在图中的位置就是指该顶点在图中[人为]确定的序列的位置</p><p>由于图的结构比较复杂,任意两个顶点之间都可能存在联系,所以无法将数据元素存储区的位置来表示元素之间的关系,即图没有顺序映像的存储结构,但可借助数组来表示数据元素之间的关系</p><h3 id="4-3-1邻接矩阵"><a href="#4-3-1邻接矩阵" class="headerlink" title="4.3.1邻接矩阵"></a>4.3.1邻接矩阵</h3><p>图的邻接矩阵表示法就是用数组来存放图的结构,也成为[数组表示法],采用两个数组来表示图</p><ul><li><p>存储所有顶点信息的一维数组</p></li><li><p>存储图中顶点之间关系的二维数组,这个关联数组也叫邻接矩阵</p><p>邻接矩阵的示意图(∞代表没有相连)</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1234_24_383.png" alt=""></p></li></ul><h3 id="4-3-2邻接表"><a href="#4-3-2邻接表" class="headerlink" title="4.3.2邻接表"></a>4.3.2邻接表</h3><p>临接矩阵的问题:空间使用效率低,因为大量的单元所对应的边可能没有在图中出现</p><p>按照改进空间使用效率的思路想,可以将静态数组的存储结构改成链式存储结构<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1235_28_643.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>图</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_5.堆</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_5.%E5%A0%86/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_5.%E5%A0%86/</url>
    
    <content type="html"><![CDATA[<h1 id="5-堆"><a href="#5-堆" class="headerlink" title="5.堆"></a>5.堆</h1><ul><li><p>堆中存储的值是偏序</p></li><li><p>堆属于完全二叉树</p></li><li><p>大顶堆：父节点的值小于或等于子节点的值</p></li><li><p>小顶堆：父节点的值大于或等于子节点的值</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1236_05_814.png" alt=""></p></li></ul><h4 id="1-堆的存储"><a href="#1-堆的存储" class="headerlink" title="1.堆的存储"></a>1.堆的存储</h4><p>一般都用数组来表示堆，i节点的父节点下标就是（i - 1）/ 2,它的左右节点的下标分别是2 * i + 1和2 * i + 2。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1236_15_981.png" alt=""></p><h4 id="2-堆的操作：insert"><a href="#2-堆的操作：insert" class="headerlink" title="2.堆的操作：insert"></a>2.堆的操作：insert</h4><p>插入元素时，新元素被加到heap的末尾，然后更新树以恢复堆的次序。从新加元素到根节点之间必为一个有序数列，现在的任务是将这个新数据插入到原来的有序数列中，使数列仍保持有序。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1236_56_211.png" alt=""></p><h4 id="3-堆的操作：删除"><a href="#3-堆的操作：删除" class="headerlink" title="3.堆的操作：删除"></a>3.堆的操作：删除</h4><p>以大顶堆为例，删除元素时，每次都删除堆的第0个元素，为了便于重建堆，实际的操作是将堆的最后一个数据的值赋给根节点，然后从根节点开始依次从上往下调整。调整时先在左右儿子中找到最小的，若父节点比小子节点还还小就不交换，完成。如果不是就和大子节点交换位置，一直到交换不了为止。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1237_11_033.png" alt=""></p><h4 id="4-堆的操作：堆化数组"><a href="#4-堆的操作：堆化数组" class="headerlink" title="4.堆的操作：堆化数组"></a>4.堆的操作：堆化数组</h4><p>堆化数组就是把数组从头到尾过一遍，把不满足大/小顶堆要求的数据依次交换。</p><p>需注意叶子节点一律不用处理，应该从非叶子节点的右下角开始处理，处理时将本节点与下方节点比较，不符合要求的直接交换。本次交换后下沉的节点要再次与下方节点比较并交换，但本次交换后上浮的节点不需要与上方节点比较。（确定比较的方向是向下）</p><p>如下图例子，从2号节点开始依次处理到0号节点，2号1号节点均与下方节点交换，下沉节点无比较，上浮节点不比较。0号节点发生交换，上浮节点不比较，下沉节点要与下方节点比较并交换。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1237_40_622.png" alt=""></p><h4 id="小结："><a href="#小结：" class="headerlink" title="小结："></a>小结：</h4><p>堆是完全二叉树，存储在数组中，以大顶堆为例，数据不是完全有序，但是每一条从叶子到根节点的数列都是严格降序的。</p><p>pop操作就是把最大值（root）删除，也就是把之前的root换成右下角最小的元素，然后依次从上往下调整。push操作就是把新加入值放在右下角，然后依次进行交换操作。<br>注意pop和push操作都仅限于在某个叶子节点到root节点这一条通路上做交换排序，其他通路不受影响。</p><p>数组初始化为堆的时候注意交换操作从非叶子结点的右下角开始。每个节点都与下方节点做比较并交换，交换完毕后两个被交换节点仍可能与分别的上下节点存在不符合大顶堆条件的情况，这时进一步的交换和比较仅限于下方节点与更下方节点，上方节点先不管，因为自然会轮到他。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>堆</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_0.常见算法总览</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_0.%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95%E6%80%BB%E8%A7%88/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_0.%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95%E6%80%BB%E8%A7%88/</url>
    
    <content type="html"><![CDATA[<p><strong>十大排序算法</strong></p><ul><li>简单排序：插入排序、选择排序、冒泡排序（必学）</li><li>分治排序：快速排序、归并排序（必学，快速排序还要关注中轴的选取方式）</li><li>分配排序：桶排序、基数排序</li><li>树状排序：堆排序（必学）</li><li>其他：计数排序（必学）、希尔排序</li></ul><p>对于十大算法的学习，假如你不大懂的话，那么我还是挺推荐你去看书的，因为看了书，你可能不仅仅知道这个算法怎么写，还能知道他是怎么来的。推荐书籍是《算法第四版》，这本书讲的很详细，而且配了很多图演示，还是挺好懂的。</p><p>推荐文章：</p><p><a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/IAZnN00i65Ad3BicZy5kzQ">必学十大经典排序算法，看这篇就够了(附完整代码/动图/优质文章)(修订版)</a></p><p><strong>2、图论算法</strong></p><ul><li>图的表示：邻接矩阵和邻接表</li><li>遍历算法：深度搜索和广度搜索(必学)</li><li>最短路径算法：Floyd，Dijkstra（必学）</li><li>最小生成树算法：Prim，Kruskal（必学）</li><li>实际常用算法：关键路径、拓扑排序（原理与应用）</li><li>二分图匹配：配对、匈牙利算法（原理与应用）</li><li>拓展：中心性算法、社区发现算法（原理与应用）</li></ul><p>图还是比较难的，不过我觉得图涉及到的挺多算法都是挺实用的，例如最短路径的计算等，图相关的，我这里还是建议看书的，可以看《算法第四版》。</p><p><a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/4JEHZWanGtsQHYrZ0MDq7Q">漫画：什么是 “图”？（修订版）</a></p><p><a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/WA5hQXkcACIarcdVnRnuiw">漫画：深度优先遍历 和 广度优先遍历</a></p><p><a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/gjjrsj95X4w7QdWBlAKnaA">漫画：图的 “最短路径” 问题</a></p><p><a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/ALQntqQJkdWf4RbPaGOOhg">漫画：Dijkstra 算法的优化</a></p><p><a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/qnPSzv_xWSZN0VpdUgwvMg">漫画：图的 “多源” 最短路径</a></p><p><strong>3、搜索与回溯算法</strong></p><ul><li>贪心算法（必学）</li><li>启发式搜索算法：A*寻路算法（了解）</li><li>地图着色算法、N 皇后问题、最优加工顺序</li><li>旅行商问题</li></ul><p>这方便的只是都是一些算法相关的，我觉得如果可以，都学一下。像贪心算法的思想，就必须学的了。建议通过刷题来学习，leetcode 直接专题刷。</p><p><strong>4、动态规划</strong></p><ul><li>树形DP：01背包问题</li><li>线性DP：最长公共子序列、最长公共子串</li><li>区间DP：矩阵最大值（和以及积）</li><li>数位DP：数字游戏</li><li>状态压缩DP：旅行商</li></ul><p>我觉得动态规划是最难的一个算法思想了，记得当初第一次接触动态规划的时候，是看01背包问题的，看了好久都不大懂，懵懵懂懂，后面懂了基本思想，可是做题下不了手，但是看的懂答案。一气之下，再leetcdoe专题连续刷了几十道，才掌握了动态规划的<strong>套路</strong>，也有了自己的一套模板。不过说实话，动态规划，是考的真他妈多，学习算法、刷题，一定要掌握。这里建议先了解动态规划是什么，之后 leetcode 专题刷，反正就一般上面这几种题型。后面有时间，我也写一下我学到的<strong>套路</strong>，有点类似于我之前写的递归那样，算是一种经验。也就是我做题时的模板，不过感觉得写七八个小时，，，，，有时间就写。之前写的递归文章：<a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/mJ_jZZoak7uhItNgnfmZvQ">为什么你学不会递归？告别递归，谈谈我的一些经验</a></p><p><strong>5、字符匹配算法</strong></p><ul><li>正则表达式</li><li>模式匹配：KMP、Boyer-Moore</li></ul><p>我写过两篇字符串匹配的文章，感觉还不错，看了这两篇文章，我觉得你就差不多懂 kmp 和 Boyer-Moore 了。</p><p><a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/7IZTuLrPSuxvFRqsv5PiXQ">字符串匹配Boyer-Moore算法：文本编辑器中的查找功能是如何实现的？</a></p><p><strong>6、流相关算法</strong></p><ul><li>最大流：最短增广路、Dinic 算法</li><li>最大流最小割：最大收益问题、方格取数问题</li><li>最小费用最大流：最小费用路、消遣</li></ul><p>这方面的一些算法，我也只了解过一些，感兴趣的可以学习下。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_2.栈与队列-</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_2.%E6%A0%88%E4%B8%8E%E9%98%9F%E5%88%97-/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_2.%E6%A0%88%E4%B8%8E%E9%98%9F%E5%88%97-/</url>
    
    <content type="html"><![CDATA[<h1 id="4-1栈"><a href="#4-1栈" class="headerlink" title="4.1栈"></a>4.1栈</h1><ul><li><p>栈的定义</p><ul><li><p>栈又称堆栈,是运算首先的线性表,仅允许从表的一端进行插入和删除.</p></li><li><p>插入,删除的一端称为栈顶(top),另外一端称为栈底(bottom)</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1231_10_277.png" alt=""></p></li><li><p>栈的顺序存储和链式存储和线性表一模一样,只是在栈顶加上了增删元素的限制.</p></li><li><p>有后进先出的特征</p></li></ul></li></ul><h1 id="4-2队列"><a href="#4-2队列" class="headerlink" title="4.2队列"></a>4.2队列</h1><ul><li>队列的定义<ul><li>队列和栈一样,是运算受限制的线性表,仅允许从一头插入元素(rear),从另一头读取(head)</li><li>有先进先出的特征</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>栈</tag>
      
      <tag>队列</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据结构_1.数组,链表</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_1.%E6%95%B0%E7%BB%84,%E9%93%BE%E8%A1%A8/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_1.%E6%95%B0%E7%BB%84,%E9%93%BE%E8%A1%A8/</url>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h1 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h1><ul><li>数组是用来存放一组有相同数据类型的数据结构,通过整形下标来访问数组中的值</li><li>如果越过数据的下标访问数据,会返回ArrayIndexOutOfBoundException</li><li>Java中数组是一个类:所以两个数组变量可以指向同一个数组</li></ul><h1 id="线性表"><a href="#线性表" class="headerlink" title="线性表"></a>线性表</h1><ul><li>顺序存储结构<ul><li>用数组实现</li><li>插入元素的平均时间复杂度是O(n)</li><li>查找的平均时间复杂度是O(1)</li><li>适合多查找的场景</li></ul></li><li>链式存储结构<ul><li>典型的node节点由[data]和[next]两个域组成</li><li>插入元素的平均时间复杂度是O(1)</li><li>查找的平均时间复杂度是O(n)</li><li>适合多增删的场景</li></ul></li></ul><h1 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h1><p>迭代器是程序设计模式中的行为模式,功能是提供一种方法顺序访问一个聚集对象中各个元素,又不暴露该对象的内部表示.</p><p>简单来说迭代器就是对遍历操作的抽象.</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据结构</tag>
      
      <tag>数组</tag>
      
      <tag>链表</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/数据库_概论</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E5%BA%93_%E6%A6%82%E8%AE%BA/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E6%95%B0%E6%8D%AE%E5%BA%93_%E6%A6%82%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h2 id="1-数据库的四个基本概念"><a href="#1-数据库的四个基本概念" class="headerlink" title="1.数据库的四个基本概念"></a>1.数据库的四个基本概念</h2><ul><li><p>数据语义</p><ul><li>“93”的语义<ul><li>可以是年龄</li><li>也可以是班级人数</li></ul></li></ul></li></ul><ul><li><p>数据和自然语言</p><ul><li><p>自然语言可以通过上下文完整描述事物</p></li><li><p>数据是计算机的记录:</p><ul><li><p>(李明,男,199005,江苏南京,计算机,自行车协会)</p><p>记录不能完整表达事物内容,因缺少语义(列信息)</p></li></ul></li></ul></li></ul><ul><li>什么是数据库<ul><li>长期储存在计算机内,有组织的,可共享的大量数据集合</li></ul></li><li>为什么要建立数据库<ul><li>可收集所需的大量数据,然后进一步加工处理,转换为有用的知识</li></ul></li><li>数据库的基本特征<ul><li>数据按照一定的数据模型(结构)存储,组织,描述</li><li>冗余度小,可扩展</li><li>数据独立性高</li></ul></li><li>什么是数据库管理系统DBMS<ul><li>位于用户和操作系统之间的一层数据管理软件</li><li>基础软件,也是一层服务</li></ul></li><li>DBMS的用途<ul><li>科学的组织和存储数据,高效的获取和维护数据 </li></ul></li><li>DBMS的主要功能<ul><li>数据定义功能<ul><li>提供数据定义语言DDL</li><li>定于数据库中的数据对象</li></ul></li><li>数据组织/存储/管理<ul><li>管理数据</li><li>确定数据在存储级别上的结构和存取方式</li><li>实现数据之间的联系</li><li>提供多种存储方法提高存储效率</li></ul></li><li>数据操纵功能<ul><li>增删改查</li><li>提供数据操纵语言(DML)<ul><li>关系代数</li><li>关系演算</li><li>SQL</li></ul></li></ul></li><li>数据库的事务管理和运行管理<ul><li>数据的安全,高并发管理</li><li>故障恢复</li></ul></li></ul></li></ul><h2 id="2-数据模型"><a href="#2-数据模型" class="headerlink" title="2.数据模型"></a>2.数据模型</h2><ul><li>数据模型应满足三个特点<ul><li>能真实模拟现实世界</li><li>容易理解</li><li>易在计算机实现</li></ul></li><li>数据模型是数据库系统的核心和基础</li></ul><p>两类数据模型</p><ul><li>概念模型<ul><li>用户视角,用于数据库设计</li></ul></li><li>逻辑模型和物理模型<ul><li>逻辑模型<ul><li>计算机视角,用于DBMS的实现</li></ul></li><li>物理模型<ul><li>描述数据在系统内的表示方法和存取方法,是对数据底层的抽象</li></ul></li></ul></li></ul><p>三层建模</p><ul><li>现实世界–&gt;概念模型<ul><li>人类的认识抽象,概念模型只是形成概念,计算机不懂</li><li>由数据库设计人员完成</li></ul></li><li>概念模型–&gt;逻辑模型<ul><li>把概念转换为计算机可读的逻辑模型</li><li>由数据库设计人员完成</li></ul></li><li>逻辑模型–&gt;物理模型<ul><li>把逻辑转换为物理执行方案</li><li>由DBMS完成</li></ul></li></ul><h2 id="3-常用的数据模型"><a href="#3-常用的数据模型" class="headerlink" title="3.常用的数据模型"></a>3.常用的数据模型</h2><ul><li>层次模型</li><li>网状模型</li><li>关系模型(现在最常用的模型)</li><li>面向对象数据模型</li><li>对象关系数据模型</li><li>半结构化数据模型</li><li>非结构化数据模型,图模型</li></ul><p>关系模型</p><ul><li>在用户观点下,关系模型中数据的逻辑结构是一张二维表</li><li>概念<ul><li>关系(Relation)   —-一个关系对应一张表</li><li>元组(Tuple)        —-表中的一行就是一个元组</li><li>属性(Attribute)  —-表中的一列为一个属性</li><li>主键(Key])           —-表中的属性组,可唯一确定一个元组</li><li>域(Domain)        —-一组具有相同数据结构类型的值的集合,属性的取值范围来自某个域<ul><li>​    域的举例:<ul><li>一个属性的全集,例如学校的系名的域就是全部系</li></ul></li></ul></li><li>关系模型—对关系的描述<ul><li>学生(学号,姓名,年龄,性别,系名,年级等)</li></ul></li></ul></li></ul><h2 id="4-数据库系统的结构"><a href="#4-数据库系统的结构" class="headerlink" title="4.数据库系统的结构"></a>4.数据库系统的结构</h2><ul><li><p>数据库开发人员角度:</p><ul><li><p>模式Schema的概念</p><ul><li><p>是对数据库逻辑结构和特征的描述</p></li><li><p>是类的描述,不涉及具体值</p><ul><li>模式是稳定的</li></ul></li><li><p>模式举例:学生选课数据库</p></li><li><p>模式类比：数据库是仓库，模式是一个个房间，表格是房间里的床。</p><p>用户的权限的对房间分配，不对床分配</p><ul><li>学生,课程,学生选的课三个关系模式(三张表)<ul><li>学生表Student(Sno,Sname,Ssex,Sage,Sdept)</li><li>课程表Coursera(Cno,Cname,Cpno,Ccredit)</li><li>学生选课表SC(SNo,Cno,Grade)</li></ul></li></ul></li></ul></li><li><p>实例(instance)</p><ul><li>数据库某时刻的状态,模式的一个具体值</li><li>同一个模式有很多实例</li><li>实例举例:学生选课数据库实例<ul><li>2014年的学生数据库实例<ul><li>2014年学校所有学生的记录</li></ul></li><li>2013年的学生数据库实例<ul><li>2013年学校所有开设的所有课程的记录</li></ul></li></ul></li></ul></li></ul></li><li><p>从数据库用户角度看:</p><ul><li>单用户结构</li><li>分布式结构</li><li>客户-服务器结构</li><li>浏览器-数据服务器结构</li></ul></li></ul><ul><li>数据库三级模式(schema)结构,是数据库系统内部系统结构<ul><li>(中)模式<ul><li>数据库中全体数据的逻辑结构和特征的描述</li><li>所有用户的公共数据视图</li><li>定义模式<ul><li>DDL定义数据的逻辑结构,以某种数据模型为基础</li><li>定义数据之间的联系</li><li>定义数据的完整性/安全性</li></ul></li></ul></li><li>外模式<ul><li>又称子模式/用户模式<ul><li>外模式是模式的子集,通常是与某一应用有关的数据的逻辑表示</li><li>通常可以有多个外模式</li></ul></li></ul></li><li>内模式<ul><li>是数据物理结构和存储方式的描述</li><li>是数据在数据库内部的表示方式<ul><li>记录的存储方式    <ul><li>例如顺序存储,堆存储,hash存储等</li></ul></li><li>索引的组织方式<ul><li>B+树,Bitmap,Hash等</li></ul></li><li>数据是否被压缩</li><li>数据是否被加密</li><li>数据存储记录的规定–如定长还是变长</li></ul></li><li>一个数据库只有一个内模式</li></ul></li></ul></li></ul><h2 id="5-关系代数"><a href="#5-关系代数" class="headerlink" title="5.关系代数"></a>5.关系代数</h2><p>1.选择select—&gt;取出满足条件的信息</p><p>2.投影—-&gt;列变换</p><p>3.链接—&gt;  R(A@B)S  链接运算从R和S的广义笛卡尔积R*S中选取R关系在A属性组上的值与S关系在B属性组上的值满足比较关系@的元组</p><p>​                我的理解:R和S拼成一行,选取满足条件A@B的行</p><p>​                当@等于=时,即为等值链接,这种链接最常用</p><p>​                R(A=B)S :从关系R和S的广义笛卡尔积中选取A/B属性值相等的那些元组</p><p>​    悬浮元组:R和S元组在链接时有些公共属性上值不等的元组,导致这些元组在连接时被舍弃了,这些舍弃的元祖被成         为悬浮元组</p><p>外链接:如果把悬浮元组也保留在结果关系中,而在其他属性上填空值,就叫做外连接</p><p>左外连接:只保留左边关系R中的悬浮元祖</p><p>右外连接:只保留右边关系S中的悬浮元祖</p><p>​    </p><p>4.除运算</p><p>​    </p><h2 id="6-关系演算"><a href="#6-关系演算" class="headerlink" title="6.关系演算"></a>6.关系演算</h2><ul><li>关系演算以数理逻辑中的谓词演算为基础<ul><li>按谓词变元不同可以分为<ul><li>元祖关系演算<ul><li>以元祖变量作为谓词变量的 基本对象</li><li>元祖关系演算语言ALPHA</li></ul></li><li>域关系演算<ul><li>以与变量作为谓词变量的基本对象</li><li>域关系演算语言GBE</li></ul></li></ul></li></ul></li></ul><h2 id="7-sql语言概述"><a href="#7-sql语言概述" class="headerlink" title="7.sql语言概述"></a>7.sql语言概述</h2><p>关系型数据库标准语言SQL</p><ul><li><p>关系代数太数学了,性能差接受度差,于是IBM开展了system R的研制工作</p><ul><li>system R 以关系模型为基础,但是摒弃了数学语言,以自然语言为方向,诞生了结构化英语查询语言<ul><li>Structed English Query  Language–&gt;SEQUEL</li><li>后来改名为SQL,发音不变,为seeQu</li></ul></li><li>目前为止,sql应用最为广泛,但是没有一个数据库能支持sql所有概念和特性</li></ul></li><li><p>Sql例子</p><ul><li><p>SELECT Sno,Grade</p><p>FROM SC</p><p>WHERE Cno = ‘3’</p><p>Order by Grade</p></li><li><p>在SC表中选取Cno=3的行,抽取除Sno和Grade成表,按照Grade排序</p></li></ul></li><li><p>SQL特点</p><ul><li>综合统一,集成了数据定义语言/数据操纵语言/数据控制语言为一体</li><li>可以独立完成数据库生命周期中的全部活动<ul><li>增删改查</li><li>维护</li><li>安全性完整性控制</li><li>嵌入式sql和动态sql定义</li></ul></li><li>高度非过程化:<ul><li>这对于面向过程的语言来说,不必指定路径,不必指定细节</li><li>它只要提出做什么,无需提出做的详细过程</li><li>例如做两个表的链接,不必说哪个表做外循环,哪个表做内循环,只用join就可以了</li></ul></li><li>面向集合的操作方式<ul><li>操作的对象,返回的对象都是元组/元组的集合</li><li>同一种语法结构提供多种使用方式<ul><li>独立性,可以独立地用于联机交互</li><li>嵌入性,又能够嵌入到高级语言C,C++,JAVA等中设计使用</li></ul></li></ul></li><li>功能实现只用了9个动词<ul><li>数据查询:select</li><li>数据定义:create,drop,alter</li><li>数据操纵:insert,update,delete</li><li>数据控制:grant,revoke</li></ul></li><li>sql支持数据库的三级模式结构<ul><li>视图                                外模式<ul><li>从一个或者几个基本表导出的表</li><li>数据库中只存放视图的定义不存放视图对应的数据</li><li>视图是一个虚表</li><li>用户可以在视图在再定义视图</li></ul></li><li>基本表                           模式<ul><li>本身独立存在的表</li><li>sql一个关系对应一个基本表</li><li>一个基本表对应一个存储文件</li><li>一个表可以带若干索引</li></ul></li><li>存储文件                       内模式<ul><li>物理结构对用户是隐藏的</li></ul></li></ul></li></ul></li></ul><h2 id="8-sql数据定义"><a href="#8-sql数据定义" class="headerlink" title="8.sql数据定义"></a>8.sql数据定义</h2><ul><li><p>数据定义是对数据库中的各种对象进行创建的过程</p></li><li><p>数据库中有各种对象:</p><ul><li><p>模式</p></li><li><p>表</p></li><li><p>视图</p></li><li><p>索引等</p></li><li><p>sql的数据定义语句</p><table><thead><tr><th></th><th>创建</th><th>删除</th><th>修改</th></tr></thead><tbody><tr><td>模式(schema)</td><td>create schema</td><td>drop schema</td><td></td></tr><tr><td>表(table)</td><td>create table</td><td>drop table</td><td>alter table</td></tr><tr><td>视图(view)</td><td>create view</td><td>drop view</td><td></td></tr><tr><td>索引(index)</td><td>create index</td><td>drop index</td><td>alter index</td></tr></tbody></table></li></ul></li></ul><ul><li><p>数据定义详解</p><ul><li><p>模式定义</p><ul><li>模式就是一个目录(命名空间),在此模式中包含了数据库对象,例如基本表,视图,索引等<ul><li>创建:create schema “name”</li><li>删除:drpo schema <ul><li>drop schema “name”&lt;cascade&gt;级联删除<ul><li>删除schema时同时删除其中所有数据库对象</li></ul></li><li>drop schema “name”&lt;restrict&gt;限制删除<ul><li>如果该schema中有定义的对象(表,视图,索引等),则拒绝该语句的删除</li><li>仅当该模式中没有任何下属对象的时候才能执行</li></ul></li></ul></li></ul></li></ul></li><li><p>表定义</p><ul><li>表是关系数据库的核心<ul><li>create table&lt;表名&gt;</li></ul></li></ul></li><li><p>索引定义</p><ul><li><p>索引的目的:加快查询速度</p></li><li><p>关系数据库管理系统常见索引:</p><ul><li>顺序文件上的索引</li><li>B+树索引<ul><li>特点时可以动态平衡</li></ul></li><li>散列hash索引<ul><li>特点是速度快</li></ul></li><li>位图索引</li></ul></li><li><p>创建索引</p><ul><li>create &lt;unique&gt;&lt;cluster&gt;index&lt;索引名&gt;<ul><li>unique索引是否唯一</li><li>cluster是否是聚簇索引</li></ul></li><li>举例<ul><li>create unique index Stusno On Student(sno)</li></ul></li></ul></li></ul></li></ul></li></ul><pre><code>  ## 9.数据查询  * SELECT             指定要显示的属性列  * from                   制定查询对象  * where                制定查询条件  * group by           对查询结果按指定列的值分组  * order by            对查询结果按指定列值的升序或者降序排序  * 别名    * 使用列的别名改变查询结果中的列标题    * SELECT Sname NAME,Year of birth BIRTH,    ​                NAME,BIRTH就是别名  ​                              * DISTINCT关键词取消重复的行    * SELECT DISTINCT sno From SC      * 执行结果        * sno        * 201215706        * 201215707      * 没有DISTINCT关键词的执行结果        * sno        * 201215706        * 201215706        * 201215706        * 201215707        * 201215707  * 聚集函数    * 统计个数count(*)    * 计算一列值的总和sun    * 计算一列值的平均值avg    * 求一列值中的最大值和最小值      * max      * min  ## 10.链接查询  * 等值与非等值链接查询    * 链接条件运算符是=    * 例子      * SELECT Student.\*,SC.\*        From student,sc        where student.sno=sc.sno      * 链接操作的执行过程        - 嵌套循环法          - for表一{for{表二}}        - 排序合并法sort-merge          - 常用于等值链接          - 按链接属性对表一表二进行排序          - 设置指针,指向表一和表二的第一个元祖,如果这两个元祖满足链接条件就拼接,然后            i++,j++            否则            i++,j不动或者            j++,i不动  * 自身链接    * 一个表与自己链接    * 由于所有属性都是同名属性,必须使用别名前缀      * 例如        * SELECT FIRST.Cname,SECOND.Cname          FROM Coursera FIRST,Courera SECOND           取别名          WHERE FIRST.Cpno = SECOND.Cno  * 外链接    * 普通链接只输出满足链接条件的元组    * 外链接以指定表为链接主体,将主表中不满足链接条件的元组一起输出    * 左链接      * 列出左边关系中的所有元组    * 右链接      * 列出右边关系中的所有元组    * 外链接举例      * SELECT student.sno,Sname,Ssex,Sage,Sdept,Cno,Grade        From Student LEFT OUT JOIN SC ON        (Student.sno = SC.Sno)      * 左链接以左表为主,左边满足匹配条件的就链接了,不满足匹配条件的也把左边的信息列出来,右边本该链接的地方空着      * 右链接与左链接完全对称  * 两个以上的表的链接    * 举例      * SELECT student.sno,Sname,Cname,Grade        From Student,SC,Coursera     //有多个from就是链接        WHERE Student.sno =SC.Sno        And SC.Cno = Coursera.Cno;  ## 10.嵌套查询  嵌套查询概览  * 一个 SELECT-FROM-WHERE语句称谓一个查询模块  * 将一个查询模块嵌套在另一个查询模块where或者having中的查询称谓嵌套查询  * 举例    * SELECT Sname       from Student      where sno in      ​                    (select sno       ​                     from sc      ​                     where Cno = &#39;2&#39; )  * 上层的查询称为外查询,下层的查询称谓内查询  * sql允许多层嵌套  * 子查询不能使用order by  * 不相关子查询    * 子查询的查询条件不依赖于父查询    * 由里向外逐层查询  * 相关子查询    * 相关子查询的查询条件依赖于父查询      * 首先取外层查询中表的第一个元祖,根据它与内层查询相关的属性值处理内层查询      * 若where子句返回值为真,则此元组放入结果表      * 然后再取外层表的下一个元组  * 带有in谓词的子查询    * 举例    * SELECT sno,sname,sdept      from student       where sdept IN      ​                            (select sdept      ​                            from student       ​                             where sname = &#39;刘成&#39;)      此例为不相关子查询  * 带有比较运算的子查询  * 带有any(some)或者all的子查寻    * 例子:查询非计算机系中比计算机系任意一个学生年龄小的学生姓名和年龄      SELECT Sname,Sage      From Student      where Sage &lt; ANY(SELECT Sage      ​                                        FROM Student       ​                                        where Sdept  = &#39;CS&#39;)      处理过程:      首先计算子查询,得到一个结果集合(19,20)      然后求出Sage小于任何(19,20)也就是19的所有集合  * 带有exist的子查询    * 是个存在量词    * 带有exist的查询不返回任何数据只返回true或者false</code></pre><h2 id="11-集合查询"><a href="#11-集合查询" class="headerlink" title="11.集合查询"></a>11.集合查询</h2><ul><li>集合操作的种类<ul><li>并操作union</li><li>交操作intersect</li><li>差操作except</li></ul></li><li>参加集合操作的各查询结果的列数必须相同,对应的数据类型也必须相同</li></ul><ul><li><p>union举例</p><ul><li><p>查询计算机科学系的学生,以及年龄不大于19岁的学生</p></li><li><p>SELECT * </p><p>FROM Student</p><p>WHERE Sdept  = ‘CS’</p><p>UNION </p><p>Select *</p><p>FROM     Student </p><p>WHERE Sage &lt;= 19;</p><p>两个查询用union合并</p><ul><li>union可以自动合并重复的元素</li><li>union all会保留重复的元素</li></ul></li></ul></li><li><p>intersect交集举例</p><ul><li><p>查询计算机的学生和年龄不大于19岁的学生的交集</p></li><li><p>SELECT *</p><p>from Student</p><p>where Sdept  = ‘CS’</p><p>INTERSECT </p><p>SELECT *</p><p>FROM Student </p><p>WHERE Sage &lt;= 19</p></li></ul></li><li><p>except差集举例</p><ul><li><p>查询计算机的学生和年龄不大于19岁的学生的差集</p><p>SELECT * </p><p>From Student </p><p>where Sdept = ‘CS’</p><p>EXCEPT </p><p>SELECT *</p><p>FROM Student </p><p>WHERE Sage &lt;= 19;</p></li></ul></li></ul><p>聚集函数的一般模式</p><ul><li>Count</li><li>sum</li><li>avg</li><li>max</li><li>min</li></ul><h2 id="12-数据更新"><a href="#12-数据更新" class="headerlink" title="12.数据更新"></a>12.数据更新</h2><ul><li>增</li><li>删</li><li>改</li></ul><ul><li><p>增</p></li><li><p>两种插入数据方式</p><ul><li><p>插入元组 </p><ul><li><p>插入数据举例</p><ul><li><p>将一个新生元组学号:201215128,姓名:陈冬,性别:男,所在系:is,年龄:18—-&gt;插入到Student表中</p></li><li><p>insert</p><p>INTO student (Sno,Sname,Ssex,Sdept,Sage)</p><p>VALUES(‘201215128,’陈冬’,’男’,18</p></li><li><p>关键字into就是插入的元素种类</p><p>关键字VALUES就是插入的具体内容</p></li></ul></li></ul><ul><li><p>插入子查询结果</p><ul><li><p>可以一次插入多个元组</p></li><li><p>插入子查询结果举例</p><ul><li><p>建表</p><ul><li><p>CREATE TABLE Dept_age</p><p>(Sdept    CHAR(15)</p><p>Avg_age SMALLINT);</p></li></ul></li><li><p>插入数据</p><ul><li><p>INSERT INTO Dept_age(Sdept,Avg_age)     //插入以下查询结果</p><p>select Sdept,AVG(Sage)</p><p>FROM Student </p><p>Group by Sdept</p></li></ul></li></ul></li></ul></li></ul></li></ul></li><li><p>改</p></li><li><p>update关键字</p></li><li><p>举例</p><ul><li><p>将学生201215121的年龄改成22岁 </p></li><li><p>update Student</p><p>set Sage= ‘22’</p><p>where SNO = ‘201215121’</p></li></ul></li></ul><ul><li><p>删除</p></li><li><p>关键字delete</p><ul><li><p>delete from</p><p>where &lt;条件&gt;</p></li></ul></li><li><p>举例</p><ul><li><p>删除学号为201215121的学生记录</p><p>delete FROM stduent</p><p>where Sno = ‘201215121’</p></li></ul></li></ul><h2 id="13-视图"><a href="#13-视图" class="headerlink" title="13.视图"></a>13.视图</h2><ul><li><p>视图的特点</p><ul><li><p>视图是一种虚拟的表</p></li><li><p>形式上和表一致</p></li><li><p>不是用create table来一个字段一个字段定义的</p></li><li><p>使用sql的SELECT 来定义的</p></li><li><p>所以视图是一种虚表,普通表是实表</p></li><li><p>视图由一个或者几个普通表构建的</p><ul><li>视图中只存放视图的定义,不存放视图的数据</li></ul></li><li><p>视图举例</p><ul><li><p>建立信息系的学生的视图</p><ul><li><p>Creat VIEW IS_STUDENT</p><p>AS </p><p>SELECT Sno,Sname,Sage</p><p>FROM STUDENT</p><p>WHERE Sdept = ‘IS’</p><p>WITH CHECK OPTION;</p></li></ul></li><li><p>建立信息系选修了1号课程的学生的视图(包括学号,姓名,成绩)</p></li><li><p>CREATE VIEW IS_S1(Sno,Sname,Grade)</p><p>AS</p><p>SELECT  Student.sno,Sname,Grade</p><p>FROM  Student,SC</p><p>WHERE Sdept = ‘IS’  And </p><p>​                Student.Sno=SC.Sno AND</p><p>​                SC.Cno = ‘1’</p></li></ul></li></ul></li><li><p>视图消解法</p><ul><li><p>进行有效性检查</p></li><li><p>转换成等价的对基本表的查询</p><ul><li><p>转换举例</p><ul><li><p>SELECT Sno,Sage</p><p>From IS_Student</p><p>WHERE SAGE  &lt; 20;</p></li><li><p>转换为</p><p>SELECT Sno,Sage</p><p>FROM STUDENT </p><p>WHERE Sdept = ‘IS’  AND Sage &lt;  20;</p></li></ul></li></ul></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据库</tag>
      
      <tag>sql</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/网站部署记录</title>
    <link href="undefined2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E7%BD%91%E7%AB%99%E9%83%A8%E7%BD%B2%E8%AE%B0%E5%BD%95/"/>
    <url>2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E7%BD%91%E7%AB%99%E9%83%A8%E7%BD%B2%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<h4 id="1-获取root权限"><a href="#1-获取root权限" class="headerlink" title="1.获取root权限"></a>1.获取root权限</h4><p>-sudo passwd</p><p>-su root</p><h4 id="2-云主机安装git"><a href="#2-云主机安装git" class="headerlink" title="2.云主机安装git"></a>2.云主机安装git</h4><p>-yum update</p><p>-yum install git</p><p>-mkdir App</p><p>-cd App</p><h4 id="3-云主机与github建立sshkey连接"><a href="#3-云主机与github建立sshkey连接" class="headerlink" title="3.云主机与github建立sshkey连接"></a>3.云主机与github建立sshkey连接</h4><p>返回：Permissiondenied (publickey)，fatal:Could not read from remote repository.</p><p>cd ~/.ssh ls  //查看是否有ssh key</p><p>ssh-keygen -t rsa -C “<a href="mailto:youremail@example.com">youremail@example.com</a>“    //一路回车</p><p>ssh -v <a href="mailto:git@github.com">git@github.com</a> </p><p>ssh-agent -s</p><p>ssh-add ~/.ssh/ github new SSH KEY</p><p>ssh -T <a href="mailto:git@github.com">git@github.com</a></p><p>若出错：“Could not open a connection to your authentication agent”</p><p>eval `ssh-agent -s``</p><p>ssh-add ~/.ssh/id_rsa，</p><p>github new SSH KEY</p><h3 id="4-克隆项目，安装maven"><a href="#4-克隆项目，安装maven" class="headerlink" title="4.克隆项目，安装maven"></a>4.克隆项目，安装maven</h3><pre><code>git clone https://git@github.com:BqLion/SpringBoot_HelloWorld.gitls //显示名称cd SpringBoot_HelloWorldyum install mavenmvn -vmvn complie package</code></pre><h4 id="5-编译，运行"><a href="#5-编译，运行" class="headerlink" title="5.编译，运行"></a>5.编译，运行</h4><p>more src/main/resources/application.properties</p><p>cp src/main/resources/application.properties src/main/resources/application-production.properties</p><p>vim src/main/resources/application.properties</p><p>vim src/main/resources/application-production.properties</p><p>【修改配置文件为服务端配置】</p><p>【编译】mvn package</p><p>报错后改用：mvn clean compile package</p><p>报错后改用：mvn compile package</p><p>查看端口号netstat -a -n</p><p>【运行】</p><p>本次启动：</p><p>java -jar -Dspring.profiles.active=production target/springboothellloworld-0.0.1-SNAPSHOT.jar</p><p>直接启动：</p><p>java -jar springboothellloworld-0.0.1-SNAPSHOT.jar</p><hr><h4 id="6-服务器连接mysql数据库"><a href="#6-服务器连接mysql数据库" class="headerlink" title="6.服务器连接mysql数据库"></a>6.服务器连接mysql数据库</h4><p>1.谷歌云sql，用Cloud Shell远端操作本机</p><p>2.mysql -u root -p </p><p>​    报错：Access denied</p><p>3.尝试mysql -h 127.0.0.1 -P 3306 -u root -p</p><p>​    报错：RROR 2002 (HY000): Can’t connect to local MySQL server through socket ‘/var/run/mysqld/mysqld.sock’</p><p>4.apt-get update</p><p>5.sudo apt-get install mysql-server</p><p>​    如果不能sudo：  sudo passwd root重置密码</p><p>6.RROR 2002没有解决</p><pre><code class="sql">sudo /etc/init.d/mysqld sto</code></pre><pre><code class="sql">sudo mysqld_safe --skip-grant-tables &amp;</code></pre><pre><code class="sql">mysql -uroot</code></pre><pre><code class="sql">use mysql;</code></pre><pre><code class="sql">update user set password=PASSWORD(&quot;math1122&quot;) where User=&#39;root&#39;;</code></pre><p>flush privileges;</p><pre><code class="sql">quit</code></pre><pre><code class="sql">sudo /etc/init.d/mysql stop...sudo /etc/init.d/mysql start</code></pre><p>7.解决</p><h4 id="7-检查端口号是否打开："><a href="#7-检查端口号是否打开：" class="headerlink" title="7.检查端口号是否打开："></a>7.检查端口号是否打开：</h4><p>1.控制面板-程序-打开或关闭windows功能 -勾选telent</p><p>2.cmd - telent 192.168.1.1 3306</p><h4 id="8-远程连接sql："><a href="#8-远程连接sql：" class="headerlink" title="8.远程连接sql："></a>8.远程连接sql：</h4><p>首先要在谷歌控制台SQL-用户-创建用户账号</p><p>创建一个不限制ip的账号和密码，之后才能连接</p><h4 id="9-项目在服务端持久运行"><a href="#9-项目在服务端持久运行" class="headerlink" title="9.项目在服务端持久运行"></a>9.项目在服务端持久运行</h4><p>nohup java -jar springboothellloworld-0.0.1-SNAPSHOT.jar&amp; &gt;/dev/null 2&gt;&amp;1 &amp;</p><p>exit</p><p>PID:2385</p><p><code>ps</code> <code>-p 2385 -o etime</code>查看运行时间</p><p>​    </p><p>chmod +x springboothellloworld-0.0.1-SNAPSHOT.jar</p><p>nohup java -jar -Dspring.profiles.active=production springboothellloworld-0.0.1-SNAPSHOT.jar &gt; /dev/null 2&gt; /dev/null &amp;</p><h3 id="10-数据库连接不上："><a href="#10-数据库连接不上：" class="headerlink" title="10.数据库连接不上："></a>10.数据库连接不上：</h3><ol><li>进入数据库的shell<ol><li>grant all privileges on <em>.</em> to ‘root’@’%’ identified by ‘password’ with grant option;</li></ol></li><li>flush privileges;</li><li>关了VPN!!!!!</li></ol>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>java</tag>
      
      <tag>博客</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/重装Git和博客</title>
    <link href="undefined2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E9%87%8D%E8%A3%85Git%E5%92%8C%E5%8D%9A%E5%AE%A2/"/>
    <url>2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E9%87%8D%E8%A3%85Git%E5%92%8C%E5%8D%9A%E5%AE%A2/</url>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h1 id="重装Git和博客："><a href="#重装Git和博客：" class="headerlink" title="重装Git和博客："></a>重装Git和博客：</h1><h4 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h4><p>下载安装git.exe，官网速度慢就从腾讯下载</p><p>git bash</p><ol><li><p>git config –global user.name “yubin（自定义用户名）”</p></li><li><p>git config –global user.email “<a href="mailto:488459564@qq.com">488459564@qq.com</a>（自定义邮箱）”</p></li><li><p>ssh-keygen -t rsa -C “你的邮箱”　</p><p>三个回车</p></li></ol><p>在C:\users\bqlion\.ssh找到公钥，复制，粘贴到github里的new sshkey中</p><p>git clone <a href="mailto:git@github.com">git@github.com</a>::BqLion/_post.git</p><p>推送时若报转义问题：warning: LF will be replaced by CRLF in 字典树.md.<br>The file will have its original line endings in your working directory</p><p>输入git config –global core.autocrlf false解决</p><h4 id="hexo"><a href="#hexo" class="headerlink" title="hexo"></a>hexo</h4><p>下载安装node：<a href="https://nodejs.org/en/download/" target="_blank" rel="noopener">https://nodejs.org/en/download/</a></p><p>检查是否安好：node -v，npm -v</p><p>在Project目录：</p><p>git bash here</p><p>npm install -g hexo-cli</p><p>hexo init bqlion.github.io</p><p>cd bqlion.github.io</p><p>hexo new test</p><p>hexo g</p><p>hexo s</p><p>localhost:4000查看是否拉起</p><p>替换根目录下的_config.yml文件</p><p>将fliud压缩包解压到theme文件夹中，重命名为fliud</p><p>将老文件夹中 - 根目录 - source的CNAME复制到对应位置</p><p>将老文件夹中 - 根目录 - source的_data文件夹复制到对应位置</p><p>npm install hexo-deployer-git –save</p><p>hexo clean</p><p>hexo g</p><p>hexo d</p><p>在弹出的session中输入github账号密码</p><p>成功重新拉起网站</p>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/博客建站记录</title>
    <link href="undefined2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E5%8D%9A%E5%AE%A2%E5%BB%BA%E7%AB%99%E8%AE%B0%E5%BD%95/"/>
    <url>2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E5%8D%9A%E5%AE%A2%E5%BB%BA%E7%AB%99%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<h1 id="windows环境下使用hexo搭建博客记录"><a href="#windows环境下使用hexo搭建博客记录" class="headerlink" title="windows环境下使用hexo搭建博客记录"></a>windows环境下使用hexo搭建博客记录</h1><h3 id="搭建步骤"><a href="#搭建步骤" class="headerlink" title="搭建步骤"></a>搭建步骤</h3><ul><li><p>购买域名</p></li><li><p>github创建个人仓库</p></li><li><p>安装node.js</p></li><li><p>安装hexo</p></li><li><p>推送网站</p></li><li><p>域名解析</p></li><li><p>更换主题</p></li><li><p>更改配置</p></li><li><p>发布文章</p></li><li><p>使用图床</p></li></ul><h4 id="1-域名购买"><a href="#1-域名购买" class="headerlink" title="1.域名购买"></a>1.域名购买</h4><p>godaddy.com: 域名很全但价格较高，首年年费均价￥60左右,往后每年续费￥120左右。</p><p>阿里云：价格适中但个性化后缀较少,cc后缀域名无法备案</p><p>namesilo.com: 域名较全,价格最低,bqlab.cc十年￥280,最终在此购买。 </p><h4 id="2-github创建仓库"><a href="#2-github创建仓库" class="headerlink" title="2.github创建仓库"></a>2.github创建仓库</h4><p>仓库名命名为bqlion.github.io，亲测改成其他域名网站不能正常访问，</p><h4 id="3-安装node-js"><a href="#3-安装node-js" class="headerlink" title="3.安装node.js"></a>3.安装node.js</h4><p>下载地址：<a href="https://nodejs.org/en/download/" target="_blank" rel="noopener">https://nodejs.org/en/download/</a></p><p>安装完成后检测是否成功：node -v</p><p>检测npm是否一起安装成功: npm - v</p><h4 id="4-安装hexo"><a href="#4-安装hexo" class="headerlink" title="4.安装hexo"></a>4.安装hexo</h4><p>创建名为bqlion.github.io的文件夹，在这里打开cmd命令行</p><p>输入</p><pre><code>npm install -g hexo-cli</code></pre><p>初始化博客</p><pre><code>hexo bqlion.github.io</code></pre><p>检查网站雏形</p><pre><code>cd bqlion.github.iohexo new test_my_sitehexo ghexo s</code></pre><p>打开localhost:4000 检查网站是否已经拉起</p><p>常用hexo命令</p><pre><code>npm install hexo -g #安装Hexonpm update hexo -g #升级 hexo init #初始化博客命令简写hexo n  &quot;我的博客&quot; == hexo new  &quot;我的博客&quot;   #新建文章hexo g == hexo generate   #生成hexo s == hexo server   #启动服务预览hexo d == hexo deploy   #部署hexo server     #Hexo会监视文件变动并自动更新，无须重启服务器hexo server -s    #静态模式hexo server -p 5000     #更改端口hexo server -i 192.168.1.1     #自定义 IPhexo clean     #清除缓存，若是网页正常情况下可以忽略这条命令</code></pre><h4 id="5-推送网站"><a href="#5-推送网站" class="headerlink" title="5.推送网站"></a>5.推送网站</h4><p>根目录下的_config.yml是站点配置文件，themes文件夹里的_config.yml文件是主题配置文件</p><p>将hexo与github关联起来：</p><p>打开_config.yml，修改最后几行为：</p><pre><code>deploy: type: gitrepo: https://github.com/BqLion/bqlion.github.io.gitbranch: master</code></pre><p>其实就是给 hexo d这个命令做配置，让hexo知道要把blog部署在哪个位置</p><p>安装git部署插件</p><pre><code>npm install hexo-deployer-git --save</code></pre><p>分别输入三条命令（清理，生成，部署）</p><pre><code>hexo clean hexo g hexo d</code></pre><p>到这里博客就上线了</p><h4 id="6-域名解析"><a href="#6-域名解析" class="headerlink" title="6.域名解析"></a>6.域名解析</h4><p>阿里云在添加两条bqlab.cc中添加两条记录</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1217_10_817.png" alt=""></p><p>第一条记录值是bqlion.github.io，第二条记录值是bqlion.github.io的ip</p><p>ip地址获取方法是打开cmd</p><pre><code>ping bqlion.github.io</code></pre><p>点开github之前创建的仓库，点settings，设置custom domain,输入bqlab.cc</p><p>在blog根目录/source文件夹里创建记事本，内容就是自己的网址</p><pre><code>bqlab.cc</code></pre><p>退出保存为CNAME,注意保存为所有文件而不是txt文件</p><p>然后输入</p><pre><code class="bash">hexo cleanhexo ghexo d</code></pre><p>即可看到博客</p><h4 id="7-更换主题"><a href="#7-更换主题" class="headerlink" title="7.更换主题"></a>7.更换主题</h4><p>我使用的是fliud主题，参照<a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="noopener">https://github.com/fluid-dev/hexo-theme-fluid</a>设置</p><p>_config.yml修改红框主题为fliud</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1217_43_784.png" alt=""></p><p>下载相应版本<a href="https://github.com/fluid-dev/hexo-theme-fluid/releases" target="_blank" rel="noopener">https://github.com/fluid-dev/hexo-theme-fluid/releases</a></p><p>解压到themes目录下，重命名为fluid</p><ul><li>配置</li></ul><p>按如下代码块修改hexo根目录的_config.yml</p><pre><code># 设置语言，需要对应下面目录内的文件名，可以自定义文件内容# https://github.com/fluid-dev/hexo-theme-fluid/tree/master/languageslanguage: zh-CN# 关闭默认的代码高亮highlight:  enable: false</code></pre><ul><li>启用主题</li></ul><p>修改根目录的_config.yml</p><pre><code># Extensionstheme: fluid</code></pre><ul><li></li></ul><h4 id="8-配置主题（需特别注意yml的缩进问题）"><a href="#8-配置主题（需特别注意yml的缩进问题）" class="headerlink" title="8.配置主题（需特别注意yml的缩进问题）"></a>8.配置主题（需特别注意yml的缩进问题）</h4><p>参考文件<a href="https://fluid-dev.github.io/hexo-fluid-docs/guide" target="_blank" rel="noopener">https://fluid-dev.github.io/hexo-fluid-docs/guide</a></p><p>覆盖配置</p><ol><li><p>根目录的source目录，创建_data文件夹和_post文件夹，</p></li><li><p>在 <code>_data</code> 目录下创建 <code>fluid_config.yml</code> 文件，将 <code>/theme/fluid/_config.yml</code> 中被修改的配置或者全部配置，复制到 <code>fluid_config.yml</code> 中；</p></li><li><p>以后配置都在 <code>fluid_config.yml</code> 中修改，配置会在 <code>hexo g</code> 时自动覆盖。</p></li><li><p>所有资源静态文件的 Url 可以通过 <code>_static_prefix</code> 自定义配置，同样也支持覆盖配置，写入 <code>_data/fluid_static_prefix.yml</code> 即可。</p></li><li><p>其他情况，建议更新前备份 <code>/theme/fluid/_config.yml</code>，以免覆盖自定义的配置项。</p></li></ol><p>config.yml</p><ul><li>site</li></ul><pre><code>title: 刘秉乾的技术博客 :)subtitle: &quot;Life Oriented Programming...&quot;description: &#39;desc&#39;keywords:author: LiuBingqianlanguage: zh-CNtimezone: Asia/Shanghai</code></pre><p>subtitle:首页打印信息</p><p>language 和 timezone为必填项</p><ul><li>url</li></ul><pre><code>url: bqlab.ccroot: /permalink: :year/:month/:day/:title/permalink_defaults:pretty_urls:  trailing_index: true # Set to false to remove trailing index.html from permalinks</code></pre><p>url 必填</p><ul><li>extension/deployment</li></ul><pre><code># Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: fluid# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy:  type: git  repo: https://github.com/BqLion/bqlion.github.io.git  branck: master</code></pre><p>之前的步骤中已经修改</p><p>fliud_config.yml</p><p>修改头图</p><pre><code>favicon: /img/myicon.png # 网站标签页的 iconapple_touch_icon: /img/myicon.png # 用于苹果设备的 icon</code></pre><p>修改导航栏</p><pre><code>navbar:  blog_title: LiuBingqian`s Blog # 导航栏左侧的标题，为空则按 hexo config.title 显示  menu:  # 可自行增减，若想减去某个入口，可以将值留空    &quot;主页&quot;: /    &quot;目录&quot;: /categories/    &quot;归档&quot;: /archives/    &quot;标签&quot;: /tags/    &quot;关于&quot;: /about/</code></pre><p>修改各页图片和高度</p><pre><code>banner_img: https://static.zkqiang.cn/images/20190728023253.jpg-slim # 首页 Banner 头图，以下相同banner_img_height: 100  # 头图高度，屏幕百分比，available: 0 - 100</code></pre><p>修改about页信息</p><pre><code>#---------------------------# 关于页# About Page#---------------------------about:  # 以下仅为页面顶部的基本信息，更多内容请在 ./pages/about.md 中编辑，支持 markdown 和 HTML  md_path: ./pages/about.md  # 关于页文档的相对路径，可以按相对文档设置主题之外的路径，从而避免更新冲突  banner_img: https://static.zkqiang.cn/images/20190728021541.jpg-slim  banner_img_height: 90  # available: 0 - 100  avatar: /img/slava.png # 头像  name: 刘秉乾  introduce: 程序猿 / 健身爱好者 / 搬砖狂魔 # 支持 HTML  icons: # 更多图标可从 https://fontawesome.com/icons 查找，并以 &quot;图标名: url&quot; 的格式添加在下方    &quot;fab fa-github&quot;: https://github.com/BqLion    #&quot;fab fa-twitter&quot;:    #&quot;fab fa-weixin&quot;: # 微信图标，这个是特殊图标，点击不会跳转而是悬浮二维码，所以链接需要对应二维码图片地址</code></pre><p>参考博客：</p><p><a href="https://zkqiang.cn/" target="_blank" rel="noopener">https://zkqiang.cn/</a></p><p><a href="https://0x2e.github.io/" target="_blank" rel="noopener">https://0x2e.github.io/</a></p><h4 id="9-发布文章"><a href="#9-发布文章" class="headerlink" title="9.发布文章"></a>9.发布文章</h4><p>可直接把.md文件拖进_post目录，注意在每个.md文件头部插入yml块，输入如下内容配置好标签和分类</p><pre><code>categories:- 算法- CS229tags:- 机器学习- 梯度下降</code></pre><h4 id="10-使用图床"><a href="#10-使用图床" class="headerlink" title="10.使用图床"></a>10.使用图床</h4><ol><li>注册七牛云，实名认证，新建存储空间</li><li>使用MPic图床神器，配置AK SK后即可使用，直接把图片拖拽上去即可在剪切板生成md插入图片链接</li><li>使用snipaste截屏工具，比windows自带工具好用</li></ol><h4 id="11-常用命令"><a href="#11-常用命令" class="headerlink" title="11.常用命令"></a>11.常用命令</h4><p>hexo clean</p><p>hexo g</p><p>hexo d</p><p>hexo new &lt;title&gt;</p>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>博客</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/更新图床+截屏工作流</title>
    <link href="undefined2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E6%9B%B4%E6%96%B0%E5%9B%BE%E5%BA%8A+%E6%88%AA%E5%B1%8F%E5%B7%A5%E4%BD%9C%E6%B5%81/"/>
    <url>2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E6%9B%B4%E6%96%B0%E5%9B%BE%E5%BA%8A+%E6%88%AA%E5%B1%8F%E5%B7%A5%E4%BD%9C%E6%B5%81/</url>
    
    <content type="html"><![CDATA[<p>1.七牛云的试用域名只能用一个月，过期后要用自己已备案的域名。不管是否有备案域名所有已上传图片都要改链接，坑。</p><p>2.改用sm.ms图床，username:bqlion,email:<a href="mailto:398703917@qq.com">398703917@qq.com</a>,password:8</p><p>3.sm.ms限流，改用又拍云。</p><p>4.又拍云的shareX配置信息如下，密码用又拍云操作员控制台生成。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1220_19_603.png" alt=""></p><p>4.shareX简直神器。可以取代Snipaste+mpic。</p><p>5.ShareX更改快捷键：</p><ul><li>捕捉矩形区域为F1</li><li>设置“上传至”为FTP，</li><li>动作设置-高级设置 - after upload - chipboardContentFormat  - 修改为![]($result)</li></ul>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图床</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/win安装linux子系统</title>
    <link href="undefined2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/win%E5%AE%89%E8%A3%85linux%E5%AD%90%E7%B3%BB%E7%BB%9F/"/>
    <url>2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/win%E5%AE%89%E8%A3%85linux%E5%AD%90%E7%B3%BB%E7%BB%9F/</url>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h3 id="1-安装"><a href="#1-安装" class="headerlink" title="1.安装"></a>1.安装</h3><p>​    win10设置-针对开发者人员-勾选开发人员模式</p><p>​    控制面板-程序-程序和功能-启动或关闭windows功能-勾选适用于linux的windows子系统</p><p>​    windows appstore下载ubuntu</p><h3 id="2-优化"><a href="#2-优化" class="headerlink" title="2.优化"></a>2.优化</h3><p>​    备份源路径文件sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak</p><p>​    编辑源路径文件sudo vim  /etc/apt/sources.list</p><p>​    切换镜像源    vim删除当前文件所有内容：ggdG</p><pre><code class="cpp">deb http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiverse</code></pre><pre><code>deb https://mirrors.ustc.edu.cn/ubuntu/ xenial main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ xenial-updates main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ xenial-backports main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ xenial-security main restricted universe multiverse</code></pre><p>​    sudo apt-get update</p><h3 id="3-图形界面"><a href="#3-图形界面" class="headerlink" title="3.图形界面"></a>3.图形界面</h3><p>​    windows10下载xming</p><pre><code>sudo apt-get install x11-apps </code></pre><p>​    运行程序的图形界面版：加DISPLAY=:0前缀，例如</p><p>​    DISPLAY=:0 geany</p><h2 id="4-cmder"><a href="#4-cmder" class="headerlink" title="4.cmder"></a>4.cmder</h2><p>设置可随时bash：settings-startup-command line -输入bash -cur_console:p</p><p>配置环境变量：控制面板-系统和安全-系统-高级系统设置-高级-环境变量-Path编辑</p><p>-新建D:\programs\cmder</p><p>cmd设置：管理员权限打开cmd-Cmder.exe /REGISTER ALL</p><p>5.安装yum后卡死或者报错</p><p>error: db5 error(-30973) from db_create: BDB0087 DB_RUNRECOVERY: Fatal error, run database recovery<br>error: rpmdb: BDB0060 PANIC: fatal region error detected; run recovery</p><p>原因：rpm数据库被损坏了</p><p>解决：<em>rpm –rebuilddb</em> </p>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/ShadowSocks在AWS搭梯子</title>
    <link href="undefined2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/ShadowSocks%E5%9C%A8AWS%E6%90%AD%E6%A2%AF%E5%AD%90/"/>
    <url>2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/ShadowSocks%E5%9C%A8AWS%E6%90%AD%E6%A2%AF%E5%AD%90/</url>
    
    <content type="html"><![CDATA[<h1 id="ShadowSocks在AWS搭梯子"><a href="#ShadowSocks在AWS搭梯子" class="headerlink" title="ShadowSocks在AWS搭梯子"></a>ShadowSocks在AWS搭梯子</h1><p>注册AWS账号，启动新的实例。</p><p>配置安全组，给实例增加端口，点击添加规则（类型：自定义TCP规则，协议：默认TCP，端口：8989，来源：任何位置）</p><p>下载密钥对</p><p>下载putty，把密钥放在putty根目录里</p><p>打开puttygen，选择Type of key to generate为RSA</p><p>点击puttygen里边的load，文件类型改成所有文件，打开putty根目录里的密钥</p><p>点save private key,弹出是否不要密码地保存私钥点”是”</p><p>打开putty，Connection页面，seconds between keepalives填写10。</p><p>打开putty，第一页HostName填写user_name<code>@</code>public_dns_name，也就是</p><pre><code>ec2-user@ec2-18-191-169-15.us-east-2.compute.amazonaws.com</code></pre><p>putty - connection - ssh - auth - browser,选中刚才生成的pkk文件，点击open，即可连上实例。</p><p>sudo yum install python-pip</p><p>sudo pip install shadowsocks</p><p>sudo ssserver -p 8989 -k password -m aes-256-cfb -d start</p><p>至此服务启动（关闭服务:输入ps - A查看进程号3960，输入sudo kill -s 9 3960关闭）</p><p>netstat -tln检验8989端口是否启动</p><p>下载shadowsocks客户端，填写ip，密码和端口号，加密方式和上文相同，aes-256-cfb 。最后右键app图标，选择启动系统代理。</p>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SSR搭梯子</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/Docker在AWS搭梯子</title>
    <link href="undefined2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/Docker%E5%9C%A8AWS%E6%90%AD%E6%A2%AF%E5%AD%90/"/>
    <url>2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/Docker%E5%9C%A8AWS%E6%90%AD%E6%A2%AF%E5%AD%90/</url>
    
    <content type="html"><![CDATA[<h1 id="使用Docker在AWS上搭梯子"><a href="#使用Docker在AWS上搭梯子" class="headerlink" title="使用Docker在AWS上搭梯子"></a>使用Docker在AWS上搭梯子</h1><h4 id="1-在aws上安装docker"><a href="#1-在aws上安装docker" class="headerlink" title="1.在aws上安装docker"></a>1.在aws上安装docker</h4><p>sudo apt-get update</p>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>test</title>
    <link href="undefined2020/02/24/test/"/>
    <url>2020/02/24/test/</url>
    
    <content type="html"><![CDATA[<p><img src="https://pic1.zhimg.com/v2-5ad8ddc664115133c73a40cb0c9662b0_r.jpg" alt="preview"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>0.文章/SpringBoot_论坛项目知识点</title>
    <link href="undefined2020/02/24/0.%E6%96%87%E7%AB%A0/SpringBoot_%E8%AE%BA%E5%9D%9B%E9%A1%B9%E7%9B%AE%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
    <url>2020/02/24/0.%E6%96%87%E7%AB%A0/SpringBoot_%E8%AE%BA%E5%9D%9B%E9%A1%B9%E7%9B%AE%E7%9F%A5%E8%AF%86%E7%82%B9/</url>
    
    <content type="html"><![CDATA[<h1 id="SpringBootHelloWorld"><a href="#SpringBootHelloWorld" class="headerlink" title="SpringBootHelloWorld"></a>SpringBootHelloWorld</h1><h4 id="Project-Struct"><a href="#Project-Struct" class="headerlink" title="Project Struct"></a>Project Struct</h4><p>1.controller–控制页面组织数据</p><p>2.model–按照数据库的数据结构设置的对象</p><p>3.mapper–mybatis控制model对数据库的crud</p><p>4.service：组装数据库中不同的表格（mapper）</p><p>5.dto：类和类之间传输数据</p><p>4.html – 控制前端页面的装修</p><p>5.migrate–flyway控制数据库的版本</p><h4 id="mybatis"><a href="#mybatis" class="headerlink" title="mybatis"></a>mybatis</h4><p>持久层框架，java操作数据库的工具</p><p>通常在mapper文件夹中使用</p><pre><code class="java">@Mapperpublic interface UserMapper {    @Insert(&quot;insert into user(name,account_id,token,gmt_create,gmt_modified) values(#{name},#{accountId},#{token},#{gmtCreate},#{gmtModified})&quot;)    void insert(User user);@Select(&quot;select * from user  where token = #{token}&quot;)              //mybatis语法：在user表格里寻找token = 形参token的数据User findByToken(@Param(&quot;token&quot;)String token);//#{形参类} 就会把形参类放进括号中，如果不是类是参数，需加@param}</code></pre><h4 id="flyway"><a href="#flyway" class="headerlink" title="flyway"></a>flyway</h4><ul><li><p>管理数据库版本工具</p></li><li><p>数据库更改需在resource-db-migration文件夹下创建形如V1_Create.sql文件</p><pre><code class="sql">create table user(    id int auto_increment primary key not null,    account_id varchar(100),    name varchar(50),    token varchar(36),    gmt_create bigint,    gmt_modified bigint);</code></pre><p>每次改变数据库结构都要输入：</p><p>mvn flyway:migrate    </p><p>mvn -Dmybatis.generator.overwrite=true mybatis-generator:generate</p></li></ul><h4 id="lombok"><a href="#lombok" class="headerlink" title="lombok"></a>lombok</h4><p>@Data 可以使数据类直接获取getter setter toString方法</p><h4 id="数据model起名bug"><a href="#数据model起名bug" class="headerlink" title="数据model起名bug"></a>数据model起名bug</h4><p>model层，dto层，service层。数据起名不要带_</p><p>avatar_url总是get不到，改为avatarUrl就好了</p><p>数据层–html–【数据库imgrate】三个都要改，数据库imgrate容易忘记，切记</p><h4 id="token"><a href="#token" class="headerlink" title="token"></a>token</h4><p>edf3ae7d2aa5264735d41ec0ca8e27e3c03b7940</p><pre><code>https://avatars3.githubusercontent.com/u/16172531?v=4</code></pre><h4 id="热部署"><a href="#热部署" class="headerlink" title="热部署"></a>热部署</h4><p>不想每次修改代码都要重启服务</p><p>遂使用</p><p>1.</p><pre><code>&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;    &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt;</code></pre><p>2.setting-compiler-勾选Rebuild module on dependency change</p><p>3.SHIFT + CTRL + ? –register – 勾选compiler.automake.allow.when.app.running</p><p>遂运行时也自动编译。改动代码后刷新页面就会显示新特种，无需重启服务。</p><p>4.如想让浏览器自动刷新页面，安装livereload</p><p><a href="http://livereload.com/extensions/" target="_blank" rel="noopener">http://livereload.com/extensions/</a></p><h4 id="前端工具栏复用"><a href="#前端工具栏复用" class="headerlink" title="前端工具栏复用"></a>前端工具栏复用</h4><p>网站上端的工具栏每个子页面都需要。为提高复用率，可把工具栏封装为一个html文件，在其他页面中直接调用之。</p><p>例子：</p><p>1.新建navigation.html文件，把需复用的脚本写在&amp;copy中</p><pre><code class="html">&lt;!DOCTYPE html&gt;&lt;html xmlns:th=&quot;http://www.thymeleaf.org&quot;&gt;  &lt;body&gt;    &lt;div th:fragment=&quot;copy&quot;&gt;      &amp;copy; 2011 The Good Thymes Virtual Grocery    &lt;/div&gt;  &lt;/body&gt;&lt;/html&gt;</code></pre><p>2.在其他调用的地方insert：</p><pre><code class="html">&lt;div th:insert=&quot;~{navigation :: nav}&quot;&gt;&lt;/div&gt;</code></pre><p>3.单引号转义：  &amp; apos;</p><p>   双引号转义： &amp; quot;</p><h4 id="sql语句查询"><a href="#sql语句查询" class="headerlink" title="sql语句查询"></a>sql语句查询</h4><p>新写mapper或mybatis时，可在菜鸟教程上寻找sql语句写法</p><h4 id="MyBatis-Generator"><a href="#MyBatis-Generator" class="headerlink" title="MyBatis Generator"></a>MyBatis Generator</h4><p>以前数据库结构变了总要重新写mapper文件的spl语句</p><p>用了MBG后就直接配置generatorConfig文件即可</p><p>根据数据库结构生成</p><p>usermapper.xml配置文件 + Mapper.userMapper + Model.User / Model.UserExample</p><p>对数据库CRUD操作例如 find [user] by [token]:</p><p>UserExample userExample = new UserExample;//创建example对象，userMapper只认example对象</p><p>userExample.createCriteria().andTokenEqualsTo(token);</p><p>//example后边拼接的criteria可大于小于等于某元素</p><p>List<User>users = userMapper.selectByExample(userExample);//mapper返回的是符合条件的一个列表</p><p>如果是findById:</p><p>可直接调用userMapper；不需mapper.selectByExample(example.createCriteria(andIDEqualsTp(id));</p><p>新增表单：</p><pre><code>    &lt;table tableName=&quot;question&quot; domainObjectName=&quot;Question&quot;&gt;&lt;/table&gt;&lt;/context&gt;</code></pre><p>​    命令行：mvn -Dmybatis.generator.overwrite=true mybatis-generator:generate</p><h1 id="LibingTeam知识点"><a href="#LibingTeam知识点" class="headerlink" title="LibingTeam知识点"></a>LibingTeam知识点</h1><h3 id="1-Redis"><a href="#1-Redis" class="headerlink" title="1.Redis"></a>1.Redis</h3><p>windows下配置网址</p><p>&lt;<a href="https://www.cnblogs.com/chuankang/p/10308771.html&gt;" target="_blank" rel="noopener">https://www.cnblogs.com/chuankang/p/10308771.html&gt;</a></p><p>服务启动方式：</p><p>cd D:/redis</p><p>启动服务：redis-server.exe  –service-start –service-name redisserver1</p><p>客户端   ：redis-cli -p 6379</p><p>【</p><p>​    停止服务：redis-server.exe  –service-stop –service-name redisserver1</p><p>​    卸载服务：redis-server.exe  –service-uninstall–service-name redisserver1</p><p>​    安装服务：redis-server.exe –service-install redis.windows.conf –service-name redisserver1 –loglevel     verbose</p><p>】</p><h3 id="2-MVC的职责划分原则"><a href="#2-MVC的职责划分原则" class="headerlink" title="2.MVC的职责划分原则"></a>2.MVC的职责划分原则</h3><h4 id="2-1什么是MVC"><a href="#2-1什么是MVC" class="headerlink" title="2.1什么是MVC"></a>2.1什么是MVC</h4><p>模型-视图-控制器一种设计框架（设计模式），MVC的目标是将业务逻辑从用户界面的考虑中分离</p><ul><li>View - 包含用户界面元素，文本表单等（前端）–  PHP等实现</li><li>Model - 代表数据和业务逻辑（数据库）–  实体bean实现</li><li>Controller - 管理模型和视图之间的通信（连接者）–  一个servlet，用Struts或者MyBatis实现</li></ul><ul><li><p>View的职责</p><p>view负责显示，一切与展示界面无关的东西，都不该出现在view里面，view中不该出现负责的判断语句或运算过程</p><p>对于PHP中的web应用来说，HTML应该是view中的主要内容</p><p>View应该从不调用Model的方法，“老死不相往来”。</p><p>View只从model中读数据，但不改写model</p><p>查询数据库的内容应该放在controller里边，查到的数据以变量的形式传给view</p><p>view要用的数据就是一个变量</p></li><li><p>Model的职责</p><p>model用于保存和输出信息</p><p>数据，行为，方法是model的主要内容</p><p>model是逻辑最复杂的地方，因为应用的业务逻辑也要在这里展示</p><p>Model处理业务方面的逻辑，Controller只是简单的协调model和view之间的关系</p><p>与业务有关的，就应该放在model里边</p><p>数据校验，public常量和变量，可能被复用的属性和方法，都应该放在model层，一次定义到处使用</p><p>model不应该访问request，session和其他环境数据，这些应该由controller注入</p></li><li><p>【service层】</p><p>可以在model和controller之间加一个service层，专门放业务代码，条理更清晰</p></li><li><p>Controller的职责</p><p>响应用户请求，决定使用什么视图，准备用什么数据来显示</p><p>对于request的访问代码，比如$_GET    $_POST等都应该放在controller里边</p><p>controller应该仅限获取用户请求数据，不该对数据有任何操作或者预处理，这应该放在model里面</p></li></ul><h4 id="3-Swagger2"><a href="#3-Swagger2" class="headerlink" title="3.Swagger2"></a>3.Swagger2</h4><p>注解 @ApiOperation：</p><pre><code class="java">@ApiOperation(    value = &quot;获取主研方向简要列表&quot;,    notes = &quot;获取主研方向简要列表&quot;)//在http://localhost:8080/swagger-ui.html#/前后端交互平台上显示为一项</code></pre><h4 id="4-RestController"><a href="#4-RestController" class="headerlink" title="4.@RestController"></a>4.@RestController</h4><p>Spring4之后加入的注解，原来在@Controller中返回json需要@ResponseBody来配合，如果直接用@RestController替代@Controller就不需要再配置@ResponseBody，默认返回json格式。</p><h4 id="5-RequestMapping"><a href="#5-RequestMapping" class="headerlink" title="5.@RequestMapping"></a>5.@RequestMapping</h4><p>@RequestMapping注解用来把web请求映射到相应的处理函数。</p><h4 id="6-Permission"><a href="#6-Permission" class="headerlink" title="6.@Permission"></a>6.@Permission</h4><p>@Permission：<br>    作用于类上<br>    @Permission(“A”)<br>    这个Controller对应的Permission是A</p><h4 id="7-Transactional"><a href="#7-Transactional" class="headerlink" title="7.@Transactional"></a>7.@Transactional</h4><p>@transactional：事务</p><p>rollbackFor = Exception.class：事务回滚</p>]]></content>
    
    
    <categories>
      
      <category>文章</category>
      
    </categories>
    
    
    <tags>
      
      <tag>java</tag>
      
      <tag>springboot</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/技术_RestfulApi编程</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/%E6%8A%80%E6%9C%AF_RestfulApi%E7%BC%96%E7%A8%8B/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/%E6%8A%80%E6%9C%AF_RestfulApi%E7%BC%96%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="1-相关概念"><a href="#1-相关概念" class="headerlink" title="1.相关概念"></a>1.相关概念</h1><p>RESTful API:可重新表达的状态迁移的API</p><p>在这种编程风格里，所有东西都是资源，每个资源带有唯一全局定位符（URL），对URL的操作（POST,DELETE,PUT,GET）实现对资源的增删改查。所有操作都是无状态的，不要使用session。</p><h4 id="1-1-RESTful-API-URI示例"><a href="#1-1-RESTful-API-URI示例" class="headerlink" title="1.1 RESTful API URI示例"></a>1.1 RESTful API URI示例</h4><ul><li><p>URL: <a href="http://somehost/tvseries" target="_blank" rel="noopener">http://somehost/tvseries</a></p></li><li><p>GET /tvseries 获取电视剧列表</p></li><li><p>POST /tvseries 创建一个新电视剧</p></li><li><p>GET /tvseries/101 创建编号为101的电视剧信息</p></li><li><p>PUT /tvseries/101 修改编号为101的电视剧信息</p></li><li><p>DELETE /tvseries/101 删除编号为101的电视剧</p></li><li><p>GET /tvseries/101/characters 获取编号为101的电视剧的人物列表</p></li></ul><ul><li><p>POST /authorization 登录</p></li><li><p>DELETE /authorization退出</p></li></ul><h4 id="1-2-Spring-Boot简介"><a href="#1-2-Spring-Boot简介" class="headerlink" title="1.2 Spring Boot简介"></a>1.2 Spring Boot简介</h4><ul><li>大量使用注解，自动配置，无需配置xml</li><li>自带嵌入式web服务器</li><li>带系统监控功能，便于开发</li></ul><h4 id="1-3-Maven简介"><a href="#1-3-Maven简介" class="headerlink" title="1.3 Maven简介"></a>1.3 Maven简介</h4><ul><li><p>maven是一个项目管理工具，可管理项目的依赖，生成项目的文档，编译，打包等功能* </p></li><li><p>maven项目结构：</p></li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1205_22_822.png" alt=""></p><ul><li><p>main目录和test目录的区别：test目录下的文件仅在跑测试用例时使用，打包时不放入package</p></li><li><p>maven常见命令</p><pre><code>mvn test                     编译并运行测试用例mvn spring-boot:run             运行springboot项目mvn package                     打包项目mvn clean                     清除target目录，可以与其他命令一起用，例如mvn clean package</code></pre></li></ul><h4 id="1-4-RESTful-API-Hello-World"><a href="#1-4-RESTful-API-Hello-World" class="headerlink" title="1.4 RESTful API Hello World"></a>1.4 RESTful API Hello World</h4><pre><code class="java">package cc.bqlab.demo;/* * * Created by BqLion on 2019/11/13 */import java.util.HashMap;import java.util.Map;@RestController@RequestMapping(&quot;/tvseries&quot;)             //指定本类响应/tvseries页面的request请求public class TVSeriesController {    @GetMapping                        //指定本函数若收到get请求则返回result对象    public Map&lt;String,Object&gt; sayHello(){        Map&lt;String,Object&gt;result = new HashMap&lt;&gt;();        result.put(&quot;message&quot;,&quot;hello,world.&quot;);        return result;    }}</code></pre><h1 id="2-RestController详解"><a href="#2-RestController详解" class="headerlink" title="2.RestController详解"></a>2.RestController详解</h1><h4 id="2-1日志，热部署，API测试工具"><a href="#2-1日志，热部署，API测试工具" class="headerlink" title="2.1日志，热部署，API测试工具"></a>2.1日志，热部署，API测试工具</h4><ul><li><p>热部署:</p><p>在pom中添加如下依赖，即可实现热部署 </p><pre><code class="java"> &lt;dependency&gt;       &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;        &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;        &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt;</code></pre></li><li><p>日志</p><ul><li>日志是springboot运行时记录的各种信息，日志分为如下几个级别：</li></ul><p>​        TRACE &lt; DEBUG &lt; INFO &lt; WARN &lt; ERROR &lt; FATAL</p><ul><li><p>日志对象 - commons-logging or SLF4j</p><pre><code class="java">private static final Log log = LogFactory.getLog(Xxxx.class);private static final Logger logger = LogFactory.getLog(Xxxx.class);</code></pre></li><li><p>application.yml配置文件</p><pre><code class="yaml">logging:  file: target/app.log  level:    ROOT: WARN    cn.devmgr: TRACE    # WARN和TRACE代表本级或以上的日志被记录，输出在target/app.log文件中</code></pre></li></ul></li><li><p>测试</p><p>postman</p></li></ul><h4 id="2-2-GetMappping-PathVariable-PostMapping注解释义"><a href="#2-2-GetMappping-PathVariable-PostMapping注解释义" class="headerlink" title="2.2@GetMappping/@PathVariable/@PostMapping注解释义"></a>2.2@GetMappping/@PathVariable/@PostMapping注解释义</h4><p>如下文代码所示，</p><p>@GetMapping注解下边的函数是对前端get请求的答复</p><p>@PathVariable注解表示此参数由前端传来</p><p>@PostMapping注解表示此函数是对前端传来的post请求的答复</p><p>@RequestBody注解表示将requestbody里边的数据转化为被注解的对象的类型</p><pre><code class="java">        @RestController        @GetMapping(&quot;/{id}&quot;)                public tvSeriesDto getOne(@PathVariable int id){                if(id ==1){                        return createBreakingBad();                }                else if (id ==  2)                        return createWestWorld();                else                        throw new ResourceNotFoundException();        }</code></pre><pre><code class="java">        @PostMapping        public tvSeriesDto insertOne(@RequestBody tvSeriesDto tvseriesdto){                tvseriesdto.setId(99999);                return tvseriesdto;        }</code></pre><h1 id="3-在Springboot项目中使用Mybatis"><a href="#3-在Springboot项目中使用Mybatis" class="headerlink" title="3. 在Springboot项目中使用Mybatis"></a>3. 在Springboot项目中使用Mybatis</h1><h4 id="3-1-程序的层次结构"><a href="#3-1-程序的层次结构" class="headerlink" title="3.1 程序的层次结构"></a>3.1 程序的层次结构</h4><p>如下几个注解功能类似，都是定义组件，都可被@Autowired装载。但他们是对程序的不同层提供注解。</p><pre><code class="java">@Controller        @RestController       //控制层@Service                              //业务逻辑层@Repository                              //持久层@Component                              //all</code></pre><p>程序层次结构如下图所示</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1205_46_359.png" alt=""></p><h4 id="3-2-层与层之间的数据传输"><a href="#3-2-层与层之间的数据传输" class="headerlink" title="3.2 层与层之间的数据传输"></a>3.2 层与层之间的数据传输</h4><p>controller层与业务逻辑层之间的数据传输有如下两种方式，一种是逐个传参数，一种是传整个类，显然第二种方式可读性更好</p><pre><code class="java">StudentService.addStudent(int id,String name,int age,Date birthday)StudentService.addStudent(Student student)</code></pre><p>层之间的传输对象有如下几种</p><pre><code class="java">PO = persistant object : 持久化对象，一般用在数据访问层，用于把数据保存到数据库中DTO = data transfer object : 数据传输对象，程序如果与外界通信最外层一般是DTOPOJO = plain ordinary java object : 单纯的交换对象//以上均是POJO，是简单的java对象DO = domain objectBO = business objectDAO = data access object : 数据访问对象，用来处理和数据库发生交互（增删改查）//以上比较复杂，包含处理业务逻辑的方法和处理数据增删改查的方法</code></pre><p>还有java bean对象，有如下三个特点</p><pre><code>有一个public的无参构造方法属性private，且通过get，set，is等方法访问可序列化，实现了Serializable接口</code></pre><p>POJO VS JavaBean</p><ul><li><p>POJO更简单，严格遵守简单对象的概念，而javabean往往封装一些业务逻辑</p></li><li><p>POJO通常用于数据的临时传递，只作为载体，不具备处理业务逻辑的能力</p></li><li><p>虽然javabean获取数据的方式与POJO一样，但在javabean当中还能有其他的获取方法</p></li></ul><p>  <img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1206_12_797.png" alt=""></p><p>​    <img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1206_37_352.png" alt=""></p><p>层与层间对象总览图</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1206_58_450.png" alt=""></p><p>注意：</p><p>JSON 和 DTO之间的转换 ： Springboot自动娃后才能</p><p>DTO 和 PO之间的转换 ： 由commons - beanutils或者dozer工具完成</p><p>层之间传递信息的简化方案：</p><ul><li>POJO从控制层到数据访问层 – POJO穿透</li><li>用JavaBean代替 POJO</li><li>POJO的get/set写起来麻烦，用public的field代替</li></ul><h1 id="4-SpringBoot单元测试"><a href="#4-SpringBoot单元测试" class="headerlink" title="4.SpringBoot单元测试"></a>4.SpringBoot单元测试</h1><h4 id="4-1-Junit断言"><a href="#4-1-Junit断言" class="headerlink" title="4.1 Junit断言"></a>4.1 Junit断言</h4><p>​    Junit断言的对象Assert就是测试时经常用的对象，它有如下各种功能：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1207_16_924.png" alt=""></p><h4 id="4-2-桩模块"><a href="#4-2-桩模块" class="headerlink" title="4.2 桩模块"></a>4.2 桩模块</h4><ul><li><p>驱动模块</p><p>调用被测模块的代码称为驱动模块</p></li><li><p>被测模块</p><p>被测试的模块</p></li><li><p>桩模块</p><p>有时被测模块的输入数据需要被加工，然后再传给下层模块。装载到被测模块上，用于处理输入数据的模块就是桩模块</p><p>桩模块的使用场景：</p><ul><li>替代尚未开发完成的子模块</li><li>替代对环境依赖较大的子模块，如数据访问层（依赖数据库里的表和各种配置等）</li></ul></li></ul><p>  桩模块应用场景如下图所示，被测模块是一个计算当月应发工资的函数，逻辑是当月实际工作天数 * 日薪 加扣税，但扣税函数还没有写好，于是用桩模块代替扣税函数顶上，粗暴地以统一百分之50扣税。然后就可以用Assert把断言写出来。</p><p>  <img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1208_07_461.png" alt=""></p><p>  如此使用桩模块会使代码不优雅，正常开发会使用mockito框架，会在下面章节讲解</p><p>  TDD（test driven dev）测试驱动开发</p><p>  先写测试用例，后写实现代码，重构先有代码时这种思路特别好用</p><h4 id="4-3用mockito的桩模块测试业务逻辑层"><a href="#4-3用mockito的桩模块测试业务逻辑层" class="headerlink" title="4.3用mockito的桩模块测试业务逻辑层"></a>4.3用mockito的桩模块测试业务逻辑层</h4><pre><code class="java">  package com.bqlion.springboothelloworld;  import com.bqlion.springboothelloworld.dto.tvSeriesDto;  import org.junit.Assert;  import org.junit.Test;  import org.junit.runner.RunWith;  import org.mockito.Mockito;  import org.springframework.beans.factory.annotation.Autowired;  import org.springframework.boot.test.context.SpringBootTest;  import org.springframework.boot.test.mock.mockito.MockBean;  import org.springframework.test.context.junit4.SpringRunner;  import java.util.ArrayList;  /* *   * Created by BqLion on 2019/11/15   */  @RunWith(SpringRunner.class)  @SpringBootTest  public class TVSeriesServiceTest {      /**       * 实现一个单元测试用的桩模块       * @MockBean 可以给当前的spring context装载一个假的bean替代原有同名bean       * mock了dao的所有bean之后，数据访问层就被接管了，从而实现了测试不受具体数据库内容的影响       */      @MockBean TvSeriesDao tvSeriesDao;      @Autowired      TVSeriesService tvSeriesService;      @Test      public void testGetAllWithoutMockit(){          //本方法不用桩模块测试，所以要依赖数据库里的具体内容          //把mock出来的dao模块作为桩模块使用，可避免此情况，如下个函数          List&lt;tvSeries&gt;list = tvSeriesService.getAlltvSeries();          Assert.assertTrue(list.size() &gt;= 0);      }      @Test      public void testGetAll(){      //设置一个tvSerise list          List&lt;tvSeries&gt;list = new ArrayList&lt;&gt;();          tvSeries ts = new tvSeries();          ts.setName(&quot;POI&quot;);          list.add(ts);          //下面的语句是告诉mock出来的tvSeriesDao当执行getAll方法时，返回上面创建的list          Mockito.when(tvSeriesDao.getAll()).thenReturn(list);          List&lt;tvSeries&gt;result = tvSeriesService.getAllTvSeries();          Assert.assertTrue(result.size() == list.size());          Assert.assertTrue(“POI”.equals(result.get(0).getName()));      }  }</code></pre><h1 id="5-其他"><a href="#5-其他" class="headerlink" title="5 其他"></a>5 其他</h1><h4 id="5-1-缓存"><a href="#5-1-缓存" class="headerlink" title="5.1 缓存"></a>5.1 缓存</h4><p>  在程序中，会遇到一些耗时较长结果又不容易改变的计算结果，这时可以把此计算结果放在全局变量里，下次计算可直接调用此全局变量。</p><p>  实现缓存的两种办法：</p><p>  利用程序里的变量：简单，但集群环境中多个实例无法同步</p><p>  缓存服务器       ： Memcached，Redis</p><p>​    Spring中通过注解使用缓存</p><p>​    @EnableCaching 启用缓存注解</p><p>​    @Cacheable</p><p>​    @CacheEvict</p><p>​    @CachePut</p><p>​    @CacheConfig</p><p>​    </p><p>​    @Cache注解生效的条件是必须在注解@Service、@Componet下的类的public方法中，在此类被                @Autowired装载到其他类中被外部调用时，内部调用不生效。下文是本注解实例</p><pre><code class="java">@Cacheable(cacheNames = &quot;user&quot;,key = &quot;#id&quot;)public User getUserById(int id){}//本例子意为可以先在缓存中寻找键值对为user和id的值，如果缓存中无则用下边函数计算，计算后再存储到缓存中@CachePut(cacheNames = &quot;user&quot;,key = &quot;#u.id&quot;)public User updateUser(User u){}//每次都执行方法，每次执行完更新缓存@CacheEvict(cacheNames = &quot;user&quot;)public void deleteUserById(int id){}//删除缓存</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li><p>提高了对前端和后端的认识，以前是大后端，网页的渲染之类的程序也跑在服务器上，渲染好了再把结果传给浏览器，这样做必然提高服务器和网络传输的压力，而前后端分离就是把展示页面的、不需要在数据库查询的东西全丢给浏览器，让浏览器自己去渲染布局，实在需要对数据增删改查了再通过浏览器上的按钮/输入栏将信息转为JSON格式传到后端的来，后端再根据传进来的信息做具体处理。这样的架构能充分利用浏览器端的电脑性能，减少网络压力，明显提高速度。</p></li><li><p>libingtem我的任务是查询单条研究方向、查询全部研究方向、动态设置导航栏、管理端管理研究方向。对应的思路分别是</p><pre><code>        @RestController        @GetMapping(&quot;direciton/{id}&quot;)                public DirectionDto getOne(@PathVariable int id){                DirectionDTO result = DirectionService.selectRelated(id);                return result;}</code></pre><pre><code>        @RestController        @GetMapping(&quot;direciton&quot;)                public DirectionDto getAll(){                List&lt;DirectionDTO&gt;result = DirectionService.selectAll;                return result;}</code></pre><pre><code>    @RestController    @GetMapping(&quot;direciton/navbar&quot;)            public DirectionDto settingNavbar(){            List&lt;DirectionDTO&gt;navBar = DirectionService.selectAll;            return result;}//在后端看来前端部分管理端和展示端，只要能传进来http请求就给处理。//怎么区分老师/学生/root权限？应该会在http请求中携带token/    @RestController    @GetMapping(&quot;direciton&quot;)            public DirectionDto set(@PathVariable int id){}            public DirectionDto add(){}            public DirectionDto delete(){}            //并没有查询估计是因为研究方向太少，一两页就可以搞定}</code></pre></li></ol><p>   ​    </p><p>​    </p>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>java</tag>
      
      <tag>RESTful API</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.文章/Flink_比赛思路</title>
    <link href="undefined2020/02/24/0.%E6%96%87%E7%AB%A0/Flink_%E6%AF%94%E8%B5%9B%E6%80%9D%E8%B7%AF/"/>
    <url>2020/02/24/0.%E6%96%87%E7%AB%A0/Flink_%E6%AF%94%E8%B5%9B%E6%80%9D%E8%B7%AF/</url>
    
    <content type="html"><![CDATA[<h1 id="flink思路"><a href="#flink思路" class="headerlink" title="flink思路:"></a>flink思路:</h1><ul><li><p>tpcds</p></li><li><p>sql</p><ul><li><p>parser and validator   –&gt;把sql构建为operator expression tree</p></li><li><p>adapter                                    –&gt;包含不同数据源</p><ul><li>model                              –&gt;数据源物理属性</li><li>schema                            –&gt;在model中找到的关于数据格式和布局的定义</li><li>schema factory              –&gt;从model中获得源数据信息,构建schema</li></ul></li><li><p>优化器optimizer                  —-&gt;优化器的组件是可扩展的(关系算子,规则,cost模型,数据)</p><ul><li>planner rule                  —&gt;to transform operator expression tree </li><li>metadata providers</li><li>planner engines                <ul><li>cost-based planner engines      -&gt;guide by cost model</li><li>exhaustive planner                       -&gt;遍历所有rule,适合小数据处理</li><li></li></ul></li></ul><p>debug到了collectTableSink.scala,把全套流程都调通后研究sql转换为物理执行计划的过程,然后寻找优化的点</p></li></ul></li></ul><h4 id="flink的sql解析流程："><a href="#flink的sql解析流程：" class="headerlink" title="flink的sql解析流程："></a>flink的sql解析流程：</h4><ul><li><p>SQL–&gt;[Calcite]–&gt;[Planner]–&gt;StreamGraph–&gt;JobGRaph–&gt;ExecutionGraph–&gt;物理执行计划    </p><ul><li><p>[Calcite]:          </p><ul><li><p>Parser–&gt;SQLNode tree–&gt;Validator–&gt;Operation dag–&gt;</p><p>QueryOperation Convertor–&gt;RelNode dag–&gt;Optimizer根据[calcite rules]和[定制rules]优化</p></li></ul></li><li><p>[Planner]:</p><ul><li><p>BatchPhysical Rel dag–&gt;BatchExecNode dag–&gt;transformation dag</p><p>转换过程是调用相应的translatetoPlan方法转换和利用CodeGen元编程成flink各种算子</p></li></ul></li></ul></li></ul><h4 id="逐步debug深入源代码过程"><a href="#逐步debug深入源代码过程" class="headerlink" title="逐步debug深入源代码过程"></a>逐步debug深入源代码过程</h4><p>执行中:–&gt;下次调复杂的sqlQuery,这次下边好多对象都是null</p><ul><li><p>benchmark:</p><p>—&gt;Table table = tEnv.sqlQuery(sqlQuery);</p></li><li><p>TableEnviromentImpl.class:</p><p>—&gt;operation = this.planner.parse(query)</p></li><li><p>PlannerBase.parse:</p></li><li><p>case query if query.getKind.belongsTo(SqlKind.QUERY) =&gt;  List(SqlToOperationConverter.convert(planner, query))</p><ul><li><p>sqlToOperationConverter.java</p><ul><li>sqlSelect.class</li></ul></li><li><p>sqlToOperationConverter.java.convertSqlQuery:</p><ul><li>return toQueryOperation(flinkPlanner, node)</li></ul></li><li><p>flinkPlannerImpl.scala:</p><ul><li>def rel</li></ul></li><li><p>sqlToRelConverter.class(属于calcite)</p><ul><li><p>public RelRoot convertQuery</p></li><li><p>public RelRoot convertQuery(SqlNode query, boolean needsValidation, boolean top)</p></li></ul></li></ul></li><li><p>PlannerQueryOperation.java</p><ul><li><p>public PlannerQueryOperation(RelNode calciteTree) </p><pre><code>Calcite tree = logicalSort#2  RelDataType rowType = calciteTree.getRowType()</code></pre><p>​    AbstractRelNode.class</p><p>​            public final RelDataType getRowType() {</p><p>​            </p></li></ul></li></ul><ul><li>operation<ul><li>calciteTree<ul><li>collation</li><li>fieldExps</li><li>offset</li><li>fetch</li><li>input</li><li>desc =”LogicalSort#2”</li><li>rowType</li><li>digest</li><li>cluster</li><li>id</li><li>traitSet</li></ul></li><li>tableSchema <ul><li>fieldNames</li><li>fieldDataTypes</li><li>fieldNameToIndex</li></ul></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>文章</category>
      
    </categories>
    
    
    <tags>
      
      <tag>flink</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.文章/Flink_调参记录</title>
    <link href="undefined2020/02/24/0.%E6%96%87%E7%AB%A0/Flink_%E8%B0%83%E5%8F%82%E8%AE%B0%E5%BD%95/"/>
    <url>2020/02/24/0.%E6%96%87%E7%AB%A0/Flink_%E8%B0%83%E5%8F%82%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<p>jm内存 + tm内存 = 26分别是[2,24]  [4,22],[3+25]要挂</p><p>每个tm的slot :1 – 8 ,[1,8,4,2]    </p><p>并发度: 1-9,[3,7,1,5] </p><p>打包命令:</p><p>cp ~/.m2/repository/org/apache/parquet/parquet-column/1.8.2/parquet-column-1.8.2.jar ./lib/<br>cp ~/.m2/repository/org/apache/parquet/parquet-hadoop/1.8.2/parquet-hadoop-1.8.2.jar ./lib/<br>cp ~/.m2/repository/org/apache/parquet/parquet-common/1.8.2/parquet-common-1.8.2.jar ./lib/<br>cp ~/.m2/repository/org/apache/parquet/parquet-encoding/1.8.2/parquet-encoding-1.8.2.jar ./lib/<br>cp ~/.m2/repository/org/apache/parquet/parquet-format/2.3.1/parquet-format-2.3.1.jar ./lib/<br>cp ~/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar ./lib/<br>cp ~/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar ./lib/<br>cp ~/.m2/repository/org/apache/flink/flink-shaded-hadoop-2-uber/2.4.1-7.0/flink-shaded-hadoop-2-uber-2.4.1-7.0.jar ./lib/  </p><p>tar –exclude=flink-1.9-tpcds-master/opt/ -zcvf flink-1.9-tpcds-master.tar.gz flink-1.9-tpcds-master/ </p><p>我需要测试出28G内存是不是 slot数 * 并发数 * tm内存</p><p>批处理,数据落盘则无需太高的并发度,因为实时性弱.</p><p>可提高slot数量来构建足够复杂的作业图</p><p>jobmanager.heap.size: 1G<br>taskmanager.heap.size: 1G<br>taskmanager.numberOfTaskSlots: 1<br>parallelism.default: 1</p><p>score:4889000.0000</p><p>jobmanager.heap.size: 3G<br>taskmanager.heap.size: 18G<br>taskmanager.numberOfTaskSlots: 1<br>parallelism.default: 8<br>score:5452000.0000</p><p>jobmanager.heap.size: 2G<br>taskmanager.heap.size: 24G<br>taskmanager.numberOfTaskSlots: 1<br>parallelism.default: 3<br>结果:5280000.0000</p><p>jobmanager.heap.size: 1G<br>taskmanager.heap.size: 1G<br>taskmanager.numberOfTaskSlots: 3<br>parallelism.default: 1<br>结果:fall</p><p>jobmanager.heap.size: 2G<br>taskmanager.heap.size: 24G<br>taskmanager.numberOfTaskSlots: 1<br>parallelism.default: 1<br>结果:5255000.0000</p><p>jobmanager.heap.size: 2G<br>taskmanager.heap.size: 4G<br>taskmanager.numberOfTaskSlots: 1<br>parallelism.default: 1<br>结果:5010000.0000</p><p>jobmanager.heap.size: 2G<br>taskmanager.heap.size: 4G<br>taskmanager.numberOfTaskSlots: 1<br>parallelism.default: 5<br>结果:4938000.0000</p><p>jobmanager.heap.size: 2G<br>taskmanager.heap.size: 4G<br>taskmanager.numberOfTaskSlots: 1<br>parallelism.default: 4<br>结果:4850000.0000</p><p>jobmanager.heap.size: 2G<br>taskmanager.heap.size: 4G<br>taskmanager.numberOfTaskSlots: 1<br>parallelism.default: 3<br>结果:4845000.0000</p><p>jobmanager.heap.size: 2G<br>taskmanager.heap.size: 4G<br>taskmanager.numberOfTaskSlots: 1<br>parallelism.default: 2<br>结果:4909000.0000</p><p>jobmanager.heap.size: 2G<br>taskmanager.heap.size: 4G<br>taskmanager.numberOfTaskSlots: 1<br>parallelism.default: 1<br>结果:5098000.0000</p>]]></content>
    
    
    <categories>
      
      <category>文章</category>
      
    </categories>
    
    
    <tags>
      
      <tag>flink</tag>
      
      <tag>调参</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/SpringBoot_Debug记录</title>
    <link href="undefined2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/SpringBoot_Debug%E8%AE%B0%E5%BD%95/"/>
    <url>2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/SpringBoot_Debug%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<h2 id="0-问题解决记录"><a href="#0-问题解决记录" class="headerlink" title="0.问题解决记录"></a>0.问题解决记录</h2><h4 id="0-1链接不上下文远程数据库"><a href="#0-1链接不上下文远程数据库" class="headerlink" title="0.1链接不上下文远程数据库"></a>0.1链接不上下文远程数据库</h4><p>–建立本地数据库，修改了application-dev.yml,</p><pre><code class="java">  datasource:#    url: jdbc:mysql://127.0.0.1:3306/libingteam?useUnicode=true&amp;characterEncoding=utf8&amp;serverTimezone=UTC##    url: jdbc:log4jdbc:mysql://120.78.150.152:3306/checky?useUnicode=true&amp;characterEncoding=utf8&amp;serverTimezone=UTC#    username: root#    password: 123456</code></pre><h4 id="0-2链接本地数据库之后时区报错"><a href="#0-2链接本地数据库之后时区报错" class="headerlink" title="0.2链接本地数据库之后时区报错"></a>0.2链接本地数据库之后时区报错</h4><p>报错信息如下</p><pre><code class="java">The server time zone value &#39;�й���׼ʱ��&#39; is unrecognized or represents more than one time zone</code></pre><p>–参考之前数据库配置，在配置语句最后加上?useUnicode=true&amp;characterEncoding=utf8&amp;serverTimezone=UTC</p><p>拉完代码：</p><p>application.yml</p><p>改成 active：dev</p><pre><code>#开发环境配置spring:  profiles: dev  datasource:    ##############################这两条线之间的内容修改为本机数据库地址######################    url: jdbc:mysql://localhost:3306/libingteam?useUnicode=true&amp;characterEncoding=utf8&amp;serverTimezone=UTC    username: root    password: math1122    ##############################这两条线之间的内容修改为本机数据库地址######################    driver-class-name: com.mysql.cj.jdbc.Driver    #连接池配置    hikari:      max-lifetime: 60000      maximum-pool-size: 20      idle-timeout: 60000      connection-timeout: 60000      validation-timeout: 3000      LoginTimeout: 60000  redis:    host: 127.0.0.1    port: 6379    timeout: 5000    database: 0    password:mybatis:  typeAliasesPackage: com.whu.libingteam.*.entity  mapperLocations: classpath*:mapper/**/*.xmllogging:  level:    com.whu.libingteam.system.dao : debug    com.whu.libingteam.user.dao : debug</code></pre>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Springboot</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.文章/Flink_Calcite</title>
    <link href="undefined2020/02/24/0.%E6%96%87%E7%AB%A0/Flink_Calcite/"/>
    <url>2020/02/24/0.%E6%96%87%E7%AB%A0/Flink_Calcite/</url>
    
    <content type="html"><![CDATA[<h2 id="Apache-Calcite"><a href="#Apache-Calcite" class="headerlink" title="Apache Calcite"></a>Apache Calcite</h2><p>calcite是数据库的解析器+优化器。</p><p>数据库的三大组件：</p><ul><li><p>解析器</p></li><li><p>执行引擎</p></li><li><p>优化器—–&gt;数据库把关系表达式转换为执行计划的组件,决定数据库性能</p><ul><li><p>基于规则的优化器</p><ul><li>根据规则优化关系表达式,产生最终执行计划</li><li>只认规则,对数据不敏感,易局部优全局差</li></ul></li><li><p>基于代价的优化器</p><ul><li><p>根据规则优化关系表达式,产生多个执行计划,计算各计划cost,择优而动</p></li><li><p>灵活智能,多被采用</p></li></ul></li></ul></li></ul><p>Calcite优化器是:</p><ul><li>基于代价的优化器<ul><li>组件有：<ul><li>JDBC Client</li><li>JDBC Server</li><li>SQL Parser and validator</li><li>Operator Expression </li><li>Query Optimizer<ul><li>两个插拔组件<ul><li>Metadata provider</li><li>Pluginable Rules</li></ul></li></ul></li></ul></li></ul></li></ul><ul><li><p>Calcite理解</p><p>是编程框架，因为有可插拔的优化rules，有对接底层数据源的Adaptor模式</p><p>是工具库，因为flink把它嵌入自身的sql处理流程中</p></li></ul><ul><li>优化器Optimizer的三个元件<ul><li>planner rule                    优化expression树的规则</li><li>metadata providers     提供元数据</li><li>planner engines                <ul><li>cost-based planner engines     尽量找到最低cost</li><li>exhaustive planner                      遍历所有rules,直到plan不变</li></ul></li></ul></li></ul><p>Calcite在flink的架构</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1155_04_601.png" alt=""></p><p>Calcite的优化器optimizer中从logical plan到DataSet程序的详细转换</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1155_18_695.png" alt=""></p><p>Sql转换为物理执行方案的过程是：</p><ul><li>Sql Parser:将sql解析成语法树(AST),本例用SqlNode表示AST</li><li>Sql Validator: 结合catalog验证sql语法</li><li>生成logical plan,将sqlnode表示的AST转换成LogicalPlan,用relNode表示</li><li>生成optimized logicalPlan<ul><li>calcite rules优化</li><li>flink定制rules优化</li></ul></li><li>生成Flink物理执行计划:基于flink本身的rules将optimized logicalPlan转换为Flink物理执行计划</li><li>生成FlinkExecutionPlan:调用translateToPlan方法,转换和利用CodeGen元编程成Flink的各种算子</li></ul>]]></content>
    
    
    <categories>
      
      <category>文章</category>
      
    </categories>
    
    
    <tags>
      
      <tag>flink</tag>
      
      <tag>calcite</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.文章/Flink_培训笔记</title>
    <link href="undefined2020/02/24/0.%E6%96%87%E7%AB%A0/Flink_%E5%9F%B9%E8%AE%AD%E7%AC%94%E8%AE%B0/"/>
    <url>2020/02/24/0.%E6%96%87%E7%AB%A0/Flink_%E5%9F%B9%E8%AE%AD%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="1-1为什么要学习Apache-FLink"><a href="#1-1为什么要学习Apache-FLink" class="headerlink" title="1.1为什么要学习Apache FLink"></a>1.1为什么要学习Apache FLink</h2><p>讲师:陈守元 阿里巴巴高级产品专家 </p><ul><li><p>概念</p><ul><li><p>framework框架定义:只写业务逻辑,其他交给框架就可以执行</p></li><li><p>有状态流计算</p><p>当前的计算需根据之前沉淀的数据进行,沉淀的数据就是状态</p><ul><li>例如select操作就是无状态的,进来–计算–输出–下一个</li><li>例如计算一个小时内某商品的点击量就是有状态的计算</li><li>例如机器学习的训练等操作就是有状态的计算</li></ul></li><li><p>四层flink API                (越往下越贴近底层)</p><ul><li>SQL</li><li>Table  Api(dynamic tables)</li><li>DataStream Aip(streams,windows)</li><li>ProcessFunction(event,time,state)</li></ul></li><li><p>flink和storm对比</p><ul><li>fllink把含状态的计算完全抽象在了系统中,storm想要含状态计算需要在引擎外套reddis或Hbase     </li></ul></li><li><p>Batch analytics</p><ul><li>处理批数据时提到类似mapreduce        </li></ul></li><li><p>学习方法</p><ul><li><p>先实践再理论。尝试构建复杂的Flink Application项目</p><p>性学习更适合我。spring，coursera的percolation，还有现在的Flink比赛        </p></li><li><p>横向扩展。构建复杂的flink业务后，可横向学习Strom，Spark，DataFlow等,知识是演化来的，必有前置和铺垫，多横向看看，打开视野。</p></li><li><p>关注Apache Flink和Flink China社区，多交流，多提问，多输出。注册社区账号，你并不孤单。              </p></li></ul></li></ul></li></ul><h2 id="1-2Apache-FLink概念介绍"><a href="#1-2Apache-FLink概念介绍" class="headerlink" title="1.2Apache FLink概念介绍"></a>1.2Apache FLink概念介绍</h2><p>讲师:戴资力 Apache PMC</p><ul><li><p>概念</p><ul><li><p>处理连续数据的最佳解决方案:Stateful Stream Processor</p><ul><li>重点1:能存储和维护大量的状态</li><li>重点2:能依据时间知道是不是该收的数据都收完了才产生结果</li></ul></li><li></li><li><p>有状态流式处理的挑战</p><ul><li>状态容错<ul><li>如何保证精确一次的容错保证?<ul><li>产生快照,若错回退</li><li>如何在分布式场景下给多个拥有本地状态的算子产生全域一致的快照?(且不中断运算)<ul><li>JobManager(source)定时在数据流中掺入checkpoint barrier n</li><li>checkpoint barrier n流经每个operator(算子)时将正在执行的subtask的state写入自身<ul><li>checkpoint barrier们会时时在另一个独立文件存储系统中备份</li><li>各checkpoint barrier理解为并集为满,交集不为空,</li><li>交集存在于一同走过的干道</li></ul></li><li>一旦fail,全局根据存储系统中最近的checkpoint barriers并集恢复各算子状态</li></ul></li></ul></li></ul></li><li>状态维护<ul><li>本地状态后端<ul><li>从本地状态后端注册状态:val state = getRuntimeContext().getState(desc);</li><li>读取注册的状态中的信息:val p = state.value();</li><li>写入注册的状态中的信息:state.update(…);</li></ul></li></ul></li><li>Event-time处理<ul><li>Event-time是真实的产生数据的时间戳</li><li>引擎只有processor-time的概念,数据的传入可能乱序,那么输出的结果可能没有时效性保证</li><li>为了引擎认识真实的数据产生时间的概念,需要引入watermark</li><li>watermark:混在数据中的时间戳,当引擎收到watermark代表他不会预期收到比watermark更早的数据了,可以以此为界处理一批数据<ul><li>预期所有数据都最多延误五分钟到达,那所有数据的watermark都标注延迟五分钟</li><li>在10min时收到了5min的watermark,可开始执行5min前的数据了</li></ul></li></ul></li><li>状态保存与迁移<ul><li>Savepoint<ul><li>理解为手动产生的checkpoint,分布存储的全局快照</li><li>在升级系统等时刻多应用,升级完毕后用event-time来赶上最新的数据</li></ul></li></ul></li></ul></li></ul></li></ul><pre><code>### 1.4DataStream Api编程***讲师:崔星灿,Apache Flink Committer* 关于DataStream的基本概念  * 分布式流处理的基本模型    * 逻辑上:在数据源和数据汇之间,会有若干处理算子进行操作(operator)    * 物理模型:在数据源和数据汇之间有若干并发的物理计算单元      * 基本模型就是一个有向无环图                   * 流处理Api的衍变    * Storm:就是围绕如何构建有向无环图设计的,所以了解storm能更好地了解和优化flink    * storm和flink的对比      * storm--------面向数据本身编程        * TopologyBuilder builder = new TopologyBuilder            //图(拓扑)的构建者        * builder.setSpout    //构建图的点        * builder.setBolt        //构建图的边      * flink--------面向数据流这个集合编程,抽象度有上升        * StreamExecutionEnvironment env = StreamExeciutionEnvironment.getExecution();                    //申请环境        * DataStream\&lt;String&gt;text = env.readTextFile(&quot;input&quot;)        //传入源数据        * DataStream\&lt;Tuple2&lt;String,Integer&gt;&gt; count = text.flatMap(new Tokennizer()).keyBy(0).sum(1);                                                   //切分处理数据        * counts.writerAsText(&quot;output&quot;);                                                    //记录输出    * Storm对DAG图有更好的控制,写出来什么样跑的就是什么,学术界更偏爱    * Flink有更好的抽象和优化,工业界更偏爱   * 在显式地调用execute之前分布式计算引擎一直都在画Dag图,构建并优化后才执行</code></pre><ul><li>DataStream Api的操作概览<ul><li>基本操作<ul><li>基于单条记录:fliter,map  —-输入一条流,出来还是一条流</li><li>基于窗口:window</li><li>合并多条流:union,join,connect</li><li>拆分:split                               —-SplitStream : 把一条流拆分为多条流 </li></ul></li><li>数据路由方法<ul><li>dataStream.global()                    全发往第一个task</li><li>dataStream.broadcast()           广播</li><li>dataStream.forward()                上下游并发度一样时一对一发送</li><li>dataStream..shuffle()                 随机均匀发送</li><li>dataStream.rebalance()             轮流分配</li><li>dataStream.recale()                     本地轮流分配</li><li>dataStream.partitionCustom()自定义</li></ul></li><li>类型系统<ul><li>//flink里的抽象都是强类型,这和它自身的序列化反序列化机制有关</li><li>//引擎知道的类型信息越多,就能对数据进行越充足的优化,序列化反序列化的过程就越快</li><li>DataStream&lt;String&gt;text = env.readTextFile(“input”);             //java用String</li><li>DataStream&lt;Tuple2&lt;String,Integer&gt;&gt;                                           //java用Tuple2<ul><li>基本类型:void,string,data,bigDecimal,bigInterger</li><li>复合类型:Tuple,scala case classes,ROW,POJO</li><li>辅助,集合类型:Option,either,list,map</li><li>其他类型:自定义TypeInformation,Kryo处理,不推荐使用,可能造成迁移上的问题</li></ul></li></ul></li></ul></li></ul><h2 id="1-8TableApi编程"><a href="#1-8TableApi编程" class="headerlink" title="1.8TableApi编程"></a>1.8TableApi编程</h2><p>讲师:程鹤群,ali高工,FLink Contributor</p><ul><li><p>​    相关概念</p><ul><li><p>TableApi特点</p><ul><li>声明式,用户只关心做什么,不关心怎么做</li><li>高性能,支持查询优化——————————————如何查询优化?</li><li>流批同意</li><li>标准统一,易理解</li></ul></li><li><p>Table API和SQL的对比</p><ul><li><p>Table Api :  tab.GroupBY(“word”.select(word,count(1)as count””))</p></li><li><p>SQL            : SELECT word,COUNT(*)as cnt</p><p>​                        FROM MyTable</p><p>​                        GROUP By word</p></li></ul></li></ul></li></ul><h3 id="2-1Flink-RunTime-核心机制编程"><a href="#2-1Flink-RunTime-核心机制编程" class="headerlink" title="2.1Flink RunTime 核心机制编程"></a>2.1Flink RunTime 核心机制编程</h3><hr><p>讲师:高赟,阿里高工</p><ul><li>作业DAG图结构<ul><li>JobGraph<ul><li>客户端提供的DAG图结构</li><li>逻辑结构,不考虑并发</li><li>JobGraph会展开成ExecutionGraph</li></ul></li><li>ExecutionGraph<ul><li>JobMaster实际维护的数据结构,最核心</li><li>维护各个job之间具体的状态</li><li>物理结构,考虑并发</li></ul></li></ul></li><li>Eagar调度<ul><li>所有slot一起起,吃资源延迟低,多用于流作业</li></ul></li><li>LAZY_FROM_SOURCE调度<ul><li>一个进度内需要用的slot一起起,运算完了把数据存好,释放slot,把数据coyp到下一层slot上执行,不吃资源,</li></ul></li><li>优化空间<ul><li>更完善的资源管理:目前所有slot一样大,明示这是不够用的</li><li>更灵活的调度策略:目前只有eagar和lazy_from_source,具体问题应该有更系统化的调度策略</li></ul></li></ul><h2 id="2-6Flink作业执行解析"><a href="#2-6Flink作业执行解析" class="headerlink" title="2.6Flink作业执行解析"></a>2.6Flink作业执行解析</h2><p>讲师:岳猛,网易云研发</p><ul><li>总览<ul><li>如何从program到物理执行计划?</li><li>subtasks如何进行调度?</li></ul></li></ul><ul><li>四个转换流程:<ul><li>第一个Program-&gt;StreamGraph</li><li>第二个:StreamGraph-&gt;JobGraph</li><li>第三个:JobGraph-&gt;ExecutionGraph</li><li>第四个:Execution-&gt;物理执行计划</li></ul></li></ul><p>第一层:StreamGraph</p><ul><li><p>执行流程</p><ul><li>StreamExecutionEnviroment.execute开始执行程序                                                        //构建环境</li><li>将transform添加到StreamExecutionEnvironment的transformations里                //添加变换</li><li>调用StreamGraphGenerator的generateInternal方法,遍历transformations          //遍历变换</li><li>构建StreamNode和StreamEdge                                                                                               //构建边和节点</li></ul></li></ul><ul><li>StreamNode            <ul><li>是描述operator的逻辑节点</li><li>关键成员变量:<ul><li>slotSharingGroup                  //用来调度task</li><li>jobVertexClass                        //具体执行task</li><li>inEdges                                     //入边 </li><li>outEdges                                  //出边</li><li>transformationUID              //UID</li></ul></li></ul></li><li>StreamEdge<ul><li>是描述两个operator逻辑的链接边</li><li>关键变量:<ul><li>sourceVertex</li><li>targetVertex</li></ul></li></ul></li></ul><p>第二层:JobGraph</p><ul><li>从StreamGraph到JobGraph的转换<ul><li>设置调度模式Eagar/lazy_from_source</li><li>遍历StreamGraph,寻求operator的chain<ul><li>能chain在一起的chain在一起,头结点生成JobVertex,其他节点序列化写入StreamConfig</li><li>不能chain在一起的节点单独生成JobVertex</li></ul></li><li>JobEdge链接JobVertex</li></ul></li><li>job.vertex们提交到jobMaster之后,会通过ExecutionGraphBuilder转换为ExecutionGraph</li></ul><p>第三层:ExecutionGraph</p><ul><li>JobVertex进行排序</li><li>ExecutionGraph.attachJobGraph(sortedTopology)构造<ul><li>JobVertex生成ExecutionVertex</li><li>IntermediateDatSet生成IntermediateResult</li><li></li></ul></li><li>ExecutionGraph在deploy的时候会提交到taskManager上去,转化为物理执行图</li></ul><p>Job的调度和执行</p><p>代码经过四层调度怎么放到集群上去的</p><h2 id="2-11flink-state最佳实践"><a href="#2-11flink-state最佳实践" class="headerlink" title="2.11flink state最佳实践"></a>2.11flink state最佳实践</h2><p>讲师,唐云,阿里高工</p><ul><li><p>概念</p><ul><li><p>State Overview</p><ul><li>state就是数据进来又出去,留下来的东西</li></ul></li><li><p>两种state的区别</p><ul><li><p>OperatorState</p><ul><li>没有current概念</li><li>只有堆内存一种实现,只能存储在java heap中</li></ul></li><li><p>KeyedState</p><ul><li><p>总有一个current key对应</p></li><li><p>可以用堆内存和RocksDB两种实现</p></li></ul></li></ul></li></ul></li></ul><h2 id="2-12-SQL深入探究"><a href="#2-12-SQL深入探究" class="headerlink" title="2.12 SQL深入探究"></a>2.12 SQL深入探究</h2><p>讲师,贺小令,阿里技术专家</p><p>sql转化流程:</p><ul><li><p>SQL–&gt;[Calcite]–&gt;[Planner]–&gt;StreamGraph–&gt;JobGRaph–&gt;ExecutionGraph–&gt;物理执行计划    </p><ul><li><p>[Calcite]详解:        </p><ul><li><p>Parser–&gt;SQLNode tree–&gt;Validator–&gt;Operation dag–&gt;</p><p>QueryOperation Convertor–&gt;RelNode dag–&gt;Optimizer</p></li></ul></li><li><p>[Planner]详解:</p><ul><li>BatchPhysical Rel dag–&gt;BatchExecNode dag–&gt;transformation dag</li></ul></li></ul></li></ul><p>​    </p><p>本版本flink性能优化</p><ul><li>分段优化 sub-plan reuse</li><li>100多个优化rule,batch和Stream共享</li><li>更高效的数据结构BinaryRow,对序列化和反序列化有帮助</li><li>mini-batch只对stream支持</li><li>节省shuffle和sort的能力–shuffle和sort占计算的大头</li></ul>]]></content>
    
    
    <categories>
      
      <category>文章</category>
      
    </categories>
    
    
    <tags>
      
      <tag>flink</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/技术_hadoop</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/%E6%8A%80%E6%9C%AF_hadoop/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/%E6%8A%80%E6%9C%AF_hadoop/</url>
    
    <content type="html"><![CDATA[<h1 id="hadoop"><a href="#hadoop" class="headerlink" title="hadoop"></a>hadoop</h1><ul><li>大数据概念<ul><li>无法在一定时间范围内用常规的工具进行捕捉管理和处理数据的集合</li><li>需要用新的处理模式才能有更强的决策力洞察力的海量高增长的多样化的信息资产</li></ul></li><li>mysql:只能储存千万条数据,不能搭建大量的集群.处理海量数据时速度极慢</li></ul><ul><li>MapReduce定义<ul><li>是一个分布式运算程序的编程框架</li><li>是用户基于hadoop的数据分析应用的核心框架</li><li>Mapreduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序,并发运行在一个hadoop集群上</li></ul></li><li>MapReduce优点<ul><li>易于编程<ul><li>简单实现一些接口,就可以完成一个分布式程序,跟写一个串行程序一样</li></ul></li><li>扩展性<ul><li>计算资源得不到满足时,增加机器扩展计算能力</li></ul></li><li>高容错性<ul><li>其中一台机器down了,可把上边的计算任务转移到其他节点上去,不至于导致任务失败</li></ul></li><li>适合PB级数据的离线处理</li></ul></li><li>MapReduce缺点<ul><li>不擅长实时计算<ul><li>不能像mysql一样在毫秒内返回计算结果</li></ul></li><li>不擅长流式计算<ul><li>流计算的输入数据是动态的,MapReduce输入数据是静态的.这由MapReduce自身的设计特点决定</li><li>不擅长做DAG有向图计算                                                                 //谁擅长做有向图计算?flink比赛需要DAG</li><li>多个程序存在依赖关系,后一个程序的输入为前一个程序的输出.在这种情况下每个作业都有大量的磁盘IO性能十分低下</li></ul></li></ul></li></ul><h4 id="编程模型"><a href="#编程模型" class="headerlink" title="编程模型"></a>编程模型</h4><ul><li>水源头<ul><li>datasource,kafka</li></ul></li><li>水龙头<ul><li>spout</li></ul></li><li>转接头<ul><li>bolt,执行相应操作</li></ul></li><li>处理后的水存放池<ul><li>reddis等</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hadoop</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_SLP_6_维特比算法</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_SLP_6_%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_SLP_6_%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="维特比算法"><a href="#维特比算法" class="headerlink" title="维特比算法"></a>维特比算法</h1><p>HMMs的解码算法为Viterbi算法，如图8.5所示。作为动态规划的一个实例，维特比类似于动态规划最小距离算法。维特比算法是一个通用的求序列最短路径的动态规划算法，维特比算法可以将HMM的状态序列作为一个整体来考虑，求给定观测序列O条件下，最可能出现的对应的状态序列 <img src="https://www.zhihu.com/equation?tex=I%5E%2A%3D+%5C%7Bi_1%5E%2A%2Ci_2%5E%2A%2C...i_T%5E%2A%5C%7D" alt="[公式]"> ,即 <img src="https://www.zhihu.com/equation?tex=P%28I%5E%2A%7CO%29" alt="[公式]"> 要最大化，从后向前计算的</p><p><img src="https://pic3.zhimg.com/v2-93cf83bd063bbbf3046023217fefb9e2_b.jpg" alt="img">用于寻找最优标签序列的维特比算法。给定一个观察序列和一个HMM λ= (A;B)，算法通过HMM返回状态路径，该状态路径为观察序列分配最大似然值。</p><p>维特比算法首先建立一个概率矩阵( <img src="https://www.zhihu.com/equation?tex=N%5Ctimes+T" alt="[公式]">) )，每个观察 <img src="https://www.zhihu.com/equation?tex=o_t" alt="[公式]"> 有一列，状态图中的每个状态有一行。因此，在单个组合自动机中，每个列都有一个用于每个状态 <img src="https://www.zhihu.com/equation?tex=q_i" alt="[公式]"> 的单元。图8.6显示了这个格子的直觉，因为Janet will back the bill这句话。</p><p><strong>网格的每个单元格 <img src="https://www.zhihu.com/equation?tex=v_t+%28j%29" alt="[公式]">表示在看到第一个t观察值并且在给定HMM λ的情况下经过最可能的状态序列 <img src="https://www.zhihu.com/equation?tex=q_1%2C...%2Cq_%7Bt-1%7D" alt="[公式]"> 之后HMM处于状态j的概率</strong>。 每个单元 <img src="https://www.zhihu.com/equation?tex=v_t+%28j%29" alt="[公式]"> 的值通过递归地获取可能导致我们到达该单元的最可能路径来计算。 形式上，每个cell表达概率:</p><p><img src="https://pic3.zhimg.com/v2-47d1c03717cde6e1568fc4313187458e_b.png" alt="img"></p><p>我们通过对所有可能的先前状态序列 <img src="https://www.zhihu.com/equation?tex=max+%5Bq_1%2C...%2Cq_%7Bt-1%7D%5D" alt="[公式]"> 取最大值来表示最可能的路径</p><p><img src="https://pic2.zhimg.com/v2-cc5d0c1537c8992f84e0a30af3d66ea1_b.jpg" alt="img"></p><p>“Janet will back the bill”的格子草图，显示每个单词的可能标签（qi），并通过隐藏状态突出显示与正确标签序列对应的路径。 根据B矩阵生成特定单词的概率为零的状态（词性）（例如，确定器DT将被实现为Janet的概率）是灰色的。</p><p>和其他动态规划算法一样，维特比递归地填充每个单元格。假设我们已经计算了t-1时刻处于每一种状态的概率，我们计算维特比概率的方法是对当前单元格的路径进行最可能的扩展。对于t时刻给定的状态 <img src="https://www.zhihu.com/equation?tex=q_j" alt="[公式]"> ，计算值 <img src="https://www.zhihu.com/equation?tex=v_t+%28j%29" alt="[公式]"> 为:</p><p><img src="https://pic3.zhimg.com/v2-7383a81c8b3acd8ec6344d0a061eab1e_b.png" alt="img"></p><p>在公式8.20中，为了扩展前面的路径来计算t时刻的维特比概率，需要乘以三个因子</p><p><img src="https://www.zhihu.com/equation?tex=v_%7Bt-1%7D%28i%29" alt="[公式]"> : 来自前面的时间步长的维特比路径概率</p><p><img src="https://www.zhihu.com/equation?tex=a_%7Bij%7D" alt="[公式]"> : 从前态qi到当前态qj的过渡概率</p><p><img src="https://www.zhihu.com/equation?tex=b_%7Bj%7D%28o_t%29" alt="[公式]"> : 给定当前状态j的观测符号 <img src="https://www.zhihu.com/equation?tex=o_t" alt="[公式]"> 的状态观测似然</p><p><strong>一个例子</strong></p><p>我们标记句子“Janet will back the bill”; 目标是正确的标签系列（另见图8.6）</p><p>(8.21) Janet/<strong>NNP</strong> will/<strong>MD</strong> back/<strong>VB</strong> the/<strong>DT</strong> bill/<strong>NN</strong></p><p>设HMM由图8.7和图8.8中的两个表定义。图8.7列出了隐藏状态(词性标记)之间转换的 <img src="https://www.zhihu.com/equation?tex=a_%7Bij%7D" alt="[公式]"> 概率。图8.8表示 <img src="https://www.zhihu.com/equation?tex=b_i%28o_t%29" alt="[公式]"> 概率，即<strong>给定标签的单词的观察概率</strong>。这张表(略为简化)来自《华尔街日报》语料库中的数据。所以Janet这个词只作为NNP出现，back有4个可能的词性，而the这个词可以作为限定词或NNP出现(在像《在彩虹之上》这样的标题中，所有的词都被标记为NNP)。</p><p>图8.9显示了我们在图8.6中看到的草图的一个比较充实版本，这是用于计算“Janet will back the bill”观察序列的最佳隐藏状态序列的维特比格架。</p><p><img src="https://pic2.zhimg.com/v2-3f613cd58ba424f8d96ba3d8423e0241_b.jpg" alt="img"></p><p><img src="https://pic1.zhimg.com/v2-1ad21a6c5a0a0bcef18140fce55bb150_b.jpg" alt="img"></p><p>有N = 5个状态列。我们从第1列（对于Janet这个词）开始，<strong>将每个单元格中的维特比值设置为p转换概率的乘积</strong>(该状态i的起始概率，我们从图8.7的<s>条目得到)，以及Janet这个词的观察可能性给出了该单元格的标签。列中的大多数单元格都是零，因为Janet这个单词不能是任何标签。读者应该在图8.9中找到它。接下来，will列中的每个单元格都会更新。对于每个cell，我们计算值viterbi [s; t]根据式8.20，取最大值来自前一列的所有路径的扩展，这些路径指向当前单元格。我们已经显示了MD，VB和NN cell的值。每个单元格获得前一列中7个值的最大值，乘以适当的转移概率;在这种情况下，它们中的大部分都是前一列的零。将剩余值乘以相关的观察概率，并取（平凡）最大值。在这种情况下，最终值.0000002772来自前一列的NNP状态。读者应填写图8.9中的其余格子并进行回溯以重建正确的状态序列NNP MD VB DT NN。</p><p><img src="https://pic1.zhimg.com/v2-ed5c4316e625e49399ae855f3743777c_b.jpg" alt="img">维特比算法的各个状态列中的前几项。每个单元格都保留到目前为止最佳路径的概率，以及沿该路径指向前一个单元格的指针。我们只填了第1列和第2列;为了避免混乱，大多数值为0的单元格都是空的。剩下的部分留给读者作为练习。当cell被填充后，从最终状态回溯，我们应该能够重建正确的状态序列NNP MD VB DT</p>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>维特比算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_SLP_6_动态规划</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_SLP_6_%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_SLP_6_%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</url>
    
    <content type="html"><![CDATA[<h1 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h1><h2 id="1-贪心策略的不足"><a href="#1-贪心策略的不足" class="headerlink" title="1. 贪心策略的不足"></a>1. 贪心策略的不足</h2><p>先来看看生活中经常遇到的事吧——假设您是个土豪，身上带了足够的1、5、10、20、50、100元面值的钞票。现在您的目标是凑出某个金额w，<strong>需要用到尽量少的钞票。</strong></p><p>依据生活经验，我们显然可以采取这样的策略：能用100的就尽量用100的，否则尽量用50的……依次类推。在这种策略下，666=6×100+1×50+1×10+1×5+1×1，共使用了10张钞票。</p><p>这种策略称为“<strong>贪心</strong>”：假设我们面对的局面是“需要凑出w”，<strong>贪心策略会尽快让w变得更小</strong>。能让w少100就尽量让它少100，这样我们接下来面对的局面就是凑出w-100。长期的生活经验表明，贪心策略是正确的。</p><p><strong>贪心策略的翻车</strong></p><p>但是，如果我们换一组钞票的面值，贪心策略就也许不成立了。如果一个奇葩国家的钞票面额分别是1、5、11，那么我们在凑出15的时候，贪心策略会出错：<br>15=1×11+4×1    （贪心策略使用了5张钞票）<br>15=3×5               （正确的策略，只用3张钞票）<br>为什么会这样呢？贪心策略错在了哪里？</p><p><strong>鼠目寸光</strong><br>刚刚已经说过，贪心策略的纲领是：“尽量使接下来面对的w更小”。这样，贪心策略在w=15的局面时，会优先使用11来把w降到4；但是在这个问题中，凑出4的代价是很高的，必须使用4×1。如果使用了5，w会降为10，虽然没有4那么小，但是凑出10只需要两张5元。<br>在这里我们发现，贪心是一种<strong>只考虑眼前情况</strong>的策略。</p><p>那么，现在我们怎样才能避免鼠目寸光呢？</p><p>如果直接暴力枚举凑出w的方案，明显复杂度过高。太多种方法可以凑出w了，枚举它们的时间是不可承受的。我们现在来尝试找一下性质。</p><p>重新分析刚刚的例子。w=15时，我们如果取11，接下来就面对w=4的情况；如果取5，则接下来面对w=10的情况。我们发现这些问题都有相同的形式：“给定w，凑出w所用的最少钞票是多少张？”接下来，我们用<strong>f(n)来表示“凑出n所需的最少钞票数量”。</strong></p><p>　　那么，如果我们取了11，最后的代价（用掉的钞票总数）是多少呢？<br>　　明显<img src="https://www.zhihu.com/equation?tex=%5Ctext%7Bcost%7D+%3D+f%284%29+%2B+1+%3D+4+%2B+1+%3D+5" alt="[公式]"> ，它的意义是：利用11来凑出15，付出的代价等于f(4)加上自己这一张钞票。现在我们暂时不管f(4)怎么求出来。<br>　　依次类推，马上可以知道：如果我们用5来凑出15，cost就是<img src="https://www.zhihu.com/equation?tex=f%2810%29+%2B+1+%3D+2+%2B+1+%3D+3" alt="[公式]"> 。</p><p>　　那么，现在w=15的时候，我们该取那种钞票呢？<strong>当然是各种方案中，cost值最低的那一个</strong>！</p><p>　　- 取11：<img src="https://www.zhihu.com/equation?tex=%5Ctext%7Bcost%7D%3Df%284%29%2B1%3D4%2B1%3D5" alt="[公式]"><br>　　- 取5：  <img src="https://www.zhihu.com/equation?tex=%5Ctext%7Bcost%7D%3Df%2810%29%2B1%3D2%2B1%3D3" alt="[公式]"><br>　　- 取1：  <img src="https://www.zhihu.com/equation?tex=%5Ctext%7Bcost%7D%3Df%2814%29%2B1%3D4%2B1%3D5" alt="[公式]"></p><p>　　显而易见，cost值最低的是取5的方案。<strong>我们通过上面三个式子，做出了正确的决策</strong>！</p><p>　　这给了我们一个<strong>至关重要</strong>的启示—— <img src="https://www.zhihu.com/equation?tex=f%28n%29" alt="[公式]"> 只与 <img src="https://www.zhihu.com/equation?tex=f%28n-1%29%2Cf%28n-5%29%2Cf%28n-11%29" alt="[公式]"> 相关；更确切地说：</p><p><img src="https://www.zhihu.com/equation?tex=f%28n%29%3D%5Cmin%5C%7Bf%28n-1%29%2Cf%28n-5%29%2Cf%28n-11%29%5C%7D%2B1" alt="[公式]"></p><p>　　这个式子是非常激动人心的。我们要求出f(n)，只需要求出几个更小的f值；既然如此，我们从小到大把所有的f(i)求出来不就好了？注意一下边界情况即可。代码如下：</p><p><img src="https://pic2.zhimg.com/50/v2-6a5ba74fb90968533ece429ed329c903_hd.jpg" alt="img">)<img src="https://pic2.zhimg.com/80/v2-6a5ba74fb90968533ece429ed329c903_hd.jpg" alt="img"></p><p>　　我们以 <img src="https://www.zhihu.com/equation?tex=O%28n%29" alt="[公式]"> 的复杂度解决了这个问题。现在回过头来，我们看看它的原理：</p><p>　　- <img src="https://www.zhihu.com/equation?tex=f%28n%29" alt="[公式]"> 只与<img src="https://www.zhihu.com/equation?tex=f%28n-1%29%2Cf%28n-5%29%2Cf%28n-11%29" alt="[公式]">的<strong>值</strong>相关。<br>　　-  我们只关心 <img src="https://www.zhihu.com/equation?tex=f%28w%29" alt="[公式]"> 的<strong>值</strong>，不关心是怎么凑出w的。</p><p>　　这两个事实，保证了我们做法的正确性。它比起贪心策略，会分别算出取1、5、11的代价，从而做出一个正确决策，这样就避免掉了“鼠目寸光”！</p><p>　　它与暴力的区别在哪里？暴力枚举包含了“使用的硬币”，然而这属于冗余信息。我们要的是答案，根本不关心这个答案是怎么凑出来的。譬如，要求出f(15)，只需要知道f(14),f(10),f(4)的值。<strong>其他信息并不需要。</strong>我们舍弃了冗余信息。我们只记录了对解决问题有帮助的信息——f(n).</p><p>　　我们能这样干，取决于问题的性质：求出f(n)，只需要知道几个更小的f(c)。<strong>我们将求解f(c)称作求解f(n)的“子问题”。</strong></p><p>　　<strong>这就是DP</strong>（动态规划，dynamic programming）.</p><p>　　<strong>将一个问题拆成几个子问题，分别求解这些子问题，即可推断出大问题的解</strong>。</p><blockquote><p>思考题：请稍微修改代码，输出我们凑出w的<strong>方案</strong>。</p></blockquote><h2 id="2-几个简单的概念"><a href="#2-几个简单的概念" class="headerlink" title="2. 几个简单的概念"></a>2. 几个简单的概念</h2><p>【无后效性】</p><p>　　一旦f(n)确定，“我们如何凑出f(n)”就再也用不着了。</p><p>　　要求出f(15)，只需要知道f(14),f(10),f(4)的值，而f(14),f(10),f(4)是如何算出来的，对之后的问题没有影响。</p><p>　　<strong>“未来与过去无关”，</strong>这就是<strong>无后效性</strong>。</p><p>　　（严格定义：如果给定某一阶段的状态，则在这一阶段以后过程的发展不受这阶段以前各段状态的影响。）</p><p>【最优子结构】</p><p>　　回顾我们对f(n)的定义：我们记“凑出n所需的<strong>最少</strong>钞票数量”为f(n).</p><p>　　f(n)的定义就已经蕴含了“最优”。利用w=14,10,4的<strong>最优</strong>解，我们即可算出w=15的<strong>最优</strong>解。</p><p>　　大问题的<strong>最优解</strong>可以由小问题的<strong>最优解</strong>推出，这个性质叫做“最优子结构性质”。</p><p>　　引入这两个概念之后，我们如何判断一个问题能否使用DP解决呢？</p><p>　　<strong>能将大问题拆成几个小问题，且满足无后效性、最优子结构性质。</strong></p><h2 id="3-DP的典型应用：DAG最短路"><a href="#3-DP的典型应用：DAG最短路" class="headerlink" title="3. DP的典型应用：DAG最短路"></a><strong>3. DP的典型应用：DAG最短路</strong></h2><p>　　问题很简单：给定一个城市的地图，所有的道路都是单行道，而且不会构成环。每条道路都有过路费，问您从S点到T点花费的最少费用。</p><p><img src="https://pic1.zhimg.com/50/v2-38e9a487997d2eea979097fbc9e9e674_hd.jpg" alt="img">)<img src="https://pic1.zhimg.com/80/v2-38e9a487997d2eea979097fbc9e9e674_hd.jpg" alt="img">一张地图。边上的数字表示过路费。</p><p>　　这个问题能用DP解决吗？我们先试着记从S到P的最少费用为f(P).<br>　　想要到T，要么经过C，要么经过D。从而<img src="https://www.zhihu.com/equation?tex=f%28T%29%3D%5Cmin%E2%81%A1%5C%7Bf%28C%29%2B20%2Cf%28D%29%2B10%5C%7D" alt="[公式]">.</p><p>　　好像看起来可以DP。现在我们检验刚刚那两个性质：<br>　　- 无后效性：对于点P，一旦f(P)确定，以后就只关心f(P)的值，不关心怎么去的。<br>　　- 最优子结构：对于P，我们当然只关心到P的最小费用，即f(P)。如果我们从S走到T是 <img src="https://www.zhihu.com/equation?tex=S+%5Cto+P%5Cto+Q%5Cto+T" alt="[公式]"> ，那肯定S走到Q的最优路径是 <img src="https://www.zhihu.com/equation?tex=S%5Cto+P%5Cto+Q" alt="[公式]"> 。对一条最优的路径而言，从S走到<strong>沿途上所有的点（子问题）</strong>的最优路径，都是这条大路的一部分。这个问题的最优子结构性质是显然的。</p><p>　　既然这两个性质都满足，那么本题可以DP。式子明显为：</p><p><img src="https://www.zhihu.com/equation?tex=f%28P%29%3D%5Cmin%E2%81%A1%5C%7Bf%28R%29%2Bw_%7BR%E2%86%92P%7D%5C%7D" alt="[公式]"></p><p>　　其中R为有路通到P的所有的点， <img src="https://www.zhihu.com/equation?tex=w_%7BR%E2%86%92P%7D" alt="[公式]"> 为R到P的过路费。</p><p>　　代码实现也很简单，拓扑排序即可。</p><h2 id="4-对DP原理的一点讨论"><a href="#4-对DP原理的一点讨论" class="headerlink" title="4. 对DP原理的一点讨论"></a>4. 对DP原理的一点讨论</h2><p>【DP的核心思想】</p><p>　　DP为什么会快？<br>　　无论是DP还是暴力，我们的算法都是在<strong>可能解空间</strong>内，寻找<strong>最优解</strong>。</p><p>　　来看钞票问题。暴力做法是枚举所有的可能解，这是最大的可能解空间。<br>　　DP是枚举<strong>有希望成为答案的解</strong>。这个空间比暴力的小得多。</p><p>　<strong>也就是说：DP自带剪枝。</strong></p><p>　　DP舍弃了一大堆不可能成为最优解的答案。譬如：<br>　　15 = 5+5+5 被考虑了。<br>　　15 = 5+5+1+1+1+1+1 从来没有考虑过，因为这不可能成为最优解。</p><p>　　从而我们可以得到DP的核心思想：<strong>尽量缩小可能解空间。</strong></p><p>　　在暴力算法中，可能解空间往往是指数级的大小；如果我们采用DP，那么有可能把解空间的大小降到多项式级。</p><p>　　一般来说，解空间越小，寻找解就越快。这样就完成了优化。</p><p>【DP的操作过程】</p><p>　　一言以蔽之：<strong>大事化小，小事化了。</strong></p><p>　　将一个大问题转化成几个小问题；<br>　　求解小问题；<br>　　推出大问题的解。</p><p>【如何设计DP算法】</p><p>　　下面介绍比较通用的设计DP算法的步骤。</p><p>　　首先，把我们面对的<strong>局面</strong>表示为x。这一步称为<strong>设计状态</strong>。<br>　　对于状态x，记我们要求出的答案(e.g. 最小费用)为f(x).我们的目标是求出f(T).<br><strong>找出f(x)与哪些局面有关（记为p）</strong>，写出一个式子（称为<strong>状态转移方程</strong>），通过f(p)来推出f(x).</p><p>【DP三连】</p><p>　　设计DP算法，往往可以遵循DP三连：</p><p>　　我是谁？  ——设计状态，表示局面<br>　　我从哪里来？<br>　　我要到哪里去？  ——设计转移</p><p>　　设计状态是DP的基础。接下来的设计转移，有两种方式：一种是考虑我从哪里来（本文之前提到的两个例子，都是在考虑“我从哪里来”）；另一种是考虑我到哪里去，这常见于求出f(x)之后，<strong>更新能从x走到的一些解</strong>。这种DP也是不少的，我们以后会遇到。</p><p>　　总而言之，“我从哪里来”和“我要到哪里去”只需要考虑清楚其中一个，就能设计出状态转移方程，从而写代码求解问题。前者又称pull型的转移，后者又称push型的转移。</p><blockquote><p>思考题：如何把钞票问题的代码改写成“我到哪里去”的形式？<br>提示：求出f(x)之后，更新f(x+1),f(x+5),f(x+11).</p></blockquote><h2 id="5-例题：最长上升子序列"><a href="#5-例题：最长上升子序列" class="headerlink" title="5. 例题：最长上升子序列"></a>5. 例题：最长上升子序列</h2><p>　　扯了这么多形而上的内容，还是做一道例题吧。</p><p>　　最长上升子序列（LIS）问题：给定长度为n的序列a，从a中抽取出一个子序列，这个子序列需要单调递增。问最长的上升子序列（LIS）的长度。<br>　　e.g. 1,5,3,4,6,9,7,8的LIS为1,3,4,6,7,8，长度为6。</p><p>　　如何设计状态（我是谁）？</p><p>　　我们记 <img src="https://www.zhihu.com/equation?tex=f%28x%29" alt="[公式]"> 为以 <img src="https://www.zhihu.com/equation?tex=a_x" alt="[公式]"> 结尾的LIS长度，那么答案就是 <img src="https://www.zhihu.com/equation?tex=%5Cmax%5C%7Bf%28x%29%5C%7D" alt="[公式]"> .</p><p>　　状态x从哪里推过来（我从哪里来）？</p><p>　　考虑比x小的每一个p：如果 <img src="https://www.zhihu.com/equation?tex=a_x%3Ea_p" alt="[公式]"> ，那么f(x)可以取f(p)+1.<br>　　解释：我们把 <img src="https://www.zhihu.com/equation?tex=a_x" alt="[公式]"> 接在 <img src="https://www.zhihu.com/equation?tex=a_p" alt="[公式]"> 的后面，肯定能构造一个以 <img src="https://www.zhihu.com/equation?tex=a_x" alt="[公式]"> 结尾的上升子序列，长度比以 <img src="https://www.zhihu.com/equation?tex=a_p" alt="[公式]"> 结尾的LIS大1.那么，我们可以写出状态转移方程了：</p><p><img src="https://www.zhihu.com/equation?tex=f%28x%29%3D%5Cmax_%7Bp%3Cx+%2C+a_p%3Ca_x+%7D%E2%81%A1%5C%7Bf%28p%29%5C%7D%2B1" alt="[公式]"></p><p>　　至此解决问题。两层for循环，复杂度 <img src="https://www.zhihu.com/equation?tex=O%28n%5E2%29" alt="[公式]"> .</p><p><img src="https://pic3.zhimg.com/50/v2-73ea19922aaac11c15dff9146a5c5b41_hd.jpg" alt="img">)<img src="https://pic3.zhimg.com/80/v2-73ea19922aaac11c15dff9146a5c5b41_hd.jpg" alt="img"></p><p>　　从这三个例题中可以看出，DP是一种思想，一种“大事化小，小事化了”的思想。带着这种思想，DP将会成为我们解决问题的利器。</p><p>　　最后，我们一起念一遍DP三连吧——我是谁？我从哪里来？我要到哪里去？</p><h2 id="6-习题"><a href="#6-习题" class="headerlink" title="6. 习题"></a>6. 习题</h2><p>一、请采取一些优化手段，以 <img src="https://www.zhihu.com/equation?tex=O%28n%5Clog+n%29" alt="[公式]"> 的复杂度解决LIS问题。</p><p>提示：可以参考这篇博客 <a href="https://link.zhihu.com/?target=https%3A//pks-loving.blog.luogu.org/junior-dynamic-programming-dong-tai-gui-hua-chu-bu-ge-zhong-zi-xu-lie">Junior Dynamic Programming–动态规划初步·各种子序列问题</a></p><p>二、“按顺序递推”和“记忆化搜索”是实现DP的两种方式。请查阅资料，简单描述“记忆化搜索”是什么。并采用记忆化搜索写出钞票问题的代码，然后完成<a href="https://link.zhihu.com/?target=https%3A//www.luogu.org/problemnew/show/P1541">P1541 乌龟棋 - 洛谷</a> 。</p><p>三、01背包问题是一种常见的DP模型。请完成<a href="https://link.zhihu.com/?target=https%3A//www.luogu.org/problemnew/show/P1048">P1048 采药 - 洛谷</a>。</p>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>一阶逻辑</tag>
      
      <tag>动态规划</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_SLP_6_HMM&amp;HEMM&amp;CRF的比较pt2</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_SLP_6_HMM&amp;HEMM&amp;CRF%E7%9A%84%E6%AF%94%E8%BE%83pt2/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_SLP_6_HMM&amp;HEMM&amp;CRF%E7%9A%84%E6%AF%94%E8%BE%83pt2/</url>
    
    <content type="html"><![CDATA[<h2 id="2-1-1-概览"><a href="#2-1-1-概览" class="headerlink" title="2.1.1 概览"></a><strong>2.1.1 概览</strong></h2><p>在统计概率图（probability graph models）中，参考宗成庆老师的书，是这样的体系结构（个人非常喜欢这种类型的图）：</p><p><img src="https://pic3.zhimg.com/v2-714c1843f78b6aecdb0c57cdd08e1c6a_b.jpg" alt="img"></p><p>在概率图模型中，数据(样本)由公式 <img src="https://www.zhihu.com/equation?tex=G%3D%28V%2CE%29" alt="[公式]"> 建模表示： </p><ul><li><img src="https://www.zhihu.com/equation?tex=V" alt="[公式]"> 表示节点，即随机变量（放在此处的，可以是一个token或者一个label），具体地，用 <img src="https://www.zhihu.com/equation?tex=Y+%3D+%28y_%7B1%7D%2C+%7B%5Ccdots%7D%2C+y_%7Bn%7D+%29+" alt="[公式]"> 为随机变量建模，注意 <img src="https://www.zhihu.com/equation?tex=Y+" alt="[公式]"> 现在是代表了一批随机变量（想象对应一条sequence，包含了很多的token）， <img src="https://www.zhihu.com/equation?tex=+P%28Y%29+" alt="[公式]"> 为这些随机变量的分布；</li><li><img src="https://www.zhihu.com/equation?tex=E" alt="[公式]"> 表示边，即概率依赖关系。具体咋理解，还是要在后面结合HMM或CRF的graph具体解释。</li></ul><h2 id="2-1-2-有向图-vs-无向图"><a href="#2-1-2-有向图-vs-无向图" class="headerlink" title="2.1.2 有向图 vs. 无向图"></a><strong>2.1.2 有向图 vs. 无向图</strong></h2><p>上图可以看到，贝叶斯网络（信念网络）都是有向的，马尔科夫网络无向。所以，贝叶斯网络适合为有单向依赖的数据建模，马尔科夫网络适合实体之间互相依赖的建模。具体地，他们的核心差异表现在如何求 <img src="https://www.zhihu.com/equation?tex=P%3D%28Y%29" alt="[公式]"> ，即怎么表示 <img src="https://www.zhihu.com/equation?tex=Y%3D%EF%BC%88y_%7B1%7D%2C%5Ccdots%2Cy_%7Bn%7D%EF%BC%89" alt="[公式]"> 这个的联合概率。</p><p><strong>1. 有向图</strong></p><p>对于有向图模型，这么求联合概率： <img src="https://www.zhihu.com/equation?tex=P%28x_%7B1%7D%2C+%7B%5Ccdots%7D%2C+x_%7Bn%7D+%29%3D%5Cprod_%7Bi%3D0%7DP%28x_%7Bi%7D+%7C+%5Cpi%28x_%7Bi%7D%29%29" alt="[公式]"></p><p>举个例子，对于下面的这个有向图的随机变量(注意，这个图我画的还是比较广义的)：</p><p><img src="https://pic1.zhimg.com/v2-5b3f6b4a2d905297b7f73a89e92ee618_b.jpg" alt="img"></p><p>应该这样表示他们的联合概率:</p><p><img src="https://www.zhihu.com/equation?tex=P%28x_%7B1%7D%2C+%7B%5Ccdots%7D%2C+x_%7Bn%7D+%29%3DP%28x_%7B1%7D%29%C2%B7P%28x_%7B2%7D%7Cx_%7B1%7D+%29%C2%B7P%28x_%7B3%7D%7Cx_%7B2%7D+%29%C2%B7P%28x_%7B4%7D%7Cx_%7B2%7D+%29%C2%B7P%28x_%7B5%7D%7Cx_%7B3%7D%2Cx_%7B4%7D+%29+" alt="[公式]"></p><p>应该很好理解吧。</p><p><strong>2. 无向图</strong></p><p>对于无向图，我看资料一般就指马尔科夫网络(注意，这个图我画的也是比较广义的)。</p><p><img src="https://pic4.zhimg.com/v2-1d8faeb71d690d02e110c7cd1d39eed3_b.jpg" alt="img"></p><p>如果一个graph太大，可以用因子分解将 <img src="https://www.zhihu.com/equation?tex=P%3D%28Y%29" alt="[公式]"> 写为若干个联合概率的乘积。咋分解呢，将一个图分为若干个“小团”，注意每个团必须是“最大团”（就是里面任何两个点连在了一块，具体……算了不解释，就是最大连通子图），则有：</p><p><img src="https://www.zhihu.com/equation?tex=P%28Y+%29%3D%5Cfrac%7B1%7D%7BZ%28x%29%7D+%5Cprod_%7Bc%7D%5Cpsi_%7Bc%7D%28Y_%7Bc%7D+%29+" alt="[公式]"></p><p>, 其中 <img src="https://www.zhihu.com/equation?tex=Z%28x%29+%3D+%5Csum_%7BY%7D+%5Cprod_%7Bc%7D%5Cpsi_%7Bc%7D%28Y_%7Bc%7D+%29" alt="[公式]"> ，公式应该不难理解吧，归一化是为了让结果算作概率。</p><p>所以像上面的无向图：</p><p><img src="https://www.zhihu.com/equation?tex=P%28Y+%29%3D%5Cfrac%7B1%7D%7BZ%28x%29%7D+%28+%5Cpsi_%7B1%7D%28X_%7B1%7D%2C+X_%7B3%7D%2C+X_%7B4%7D+%29+%C2%B7+%5Cpsi_%7B2%7D%28X_%7B2%7D%2C+X_%7B3%7D%2C+X_%7B4%7D+%29+%29" alt="[公式]"></p><p>其中， <img src="https://www.zhihu.com/equation?tex=+%5Cpsi_%7Bc%7D%28Y_%7Bc%7D+%29" alt="[公式]"> 是一个最大团 <img src="https://www.zhihu.com/equation?tex=C" alt="[公式]"> 上随机变量们的联合概率，一般取指数函数的：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cpsi_%7Bc%7D%28Y_%7Bc%7D+%29+%3D+e%5E%7B-E%28Y_%7Bc%7D%29%7D+%3De%5E%7B%5Csum_%7Bk%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28c%2Cy%7Cc%2Cx%29%7D" alt="[公式]"></p><p>好了，管这个东西叫做<code>势函数</code>。注意 <img src="https://www.zhihu.com/equation?tex=e%5E%7B%5Csum_%7Bk%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28c%2Cy%7Cc%2Cx%29%7D" alt="[公式]"> 是否有看到CRF的影子。</p><p>那么概率无向图的联合概率分布可以在因子分解下表示为：</p><p><img src="https://www.zhihu.com/equation?tex=P%28Y+%29%3D%5Cfrac%7B1%7D%7BZ%28x%29%7D+%5Cprod_%7Bc%7D%5Cpsi_%7Bc%7D%28Y_%7Bc%7D+%29+%3D+%5Cfrac%7B1%7D%7BZ%28x%29%7D+%5Cprod_%7Bc%7D+e%5E%7B%5Csum_%7Bk%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28c%2Cy%7Cc%2Cx%29%7D+%3D+%5Cfrac%7B1%7D%7BZ%28x%29%7D+e%5E%7B%5Csum_%7Bc%7D%5Csum_%7Bk%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28y_%7Bi%7D%2Cy_%7Bi-1%7D%2Cx%2Ci%29%7D" alt="[公式]"></p><p>注意，这里的理解还蛮重要的，注意递推过程，敲黑板，这是CRF的开端！<br>这个由<code>Hammersly-Clifford law</code>保证，具体不展开。</p><h2 id="2-1-3-马尔科夫假设-amp-马尔科夫性"><a href="#2-1-3-马尔科夫假设-amp-马尔科夫性" class="headerlink" title="2.1.3 马尔科夫假设&amp;马尔科夫性"></a><strong>2.1.3 马尔科夫假设&amp;马尔科夫性</strong></h2><p>这个也属于前馈知识。</p><p><strong>1. 马尔科夫假设</strong></p><p>额应该是齐次马尔科夫假设，这样假设：马尔科夫链 <img src="https://www.zhihu.com/equation?tex=%EF%BC%88x_%7B1%7D%2C%5Ccdots%2Cx_%7Bn%7D%29" alt="[公式]"> 里的 <img src="https://www.zhihu.com/equation?tex=+x_%7Bi%7D" alt="[公式]"> 总是只受 <img src="https://www.zhihu.com/equation?tex=+x_%7Bi-1%7D" alt="[公式]"> 一个人的影响。<br>马尔科夫假设这里相当于就是个1-gram。</p><p>马尔科夫过程呢？即，在一个过程中，每个状态的转移只依赖于前n个状态，并且只是个n阶的模型。最简单的马尔科夫过程是一阶的，即只依赖于器哪一个状态。</p><p><strong>2. 马尔科夫性</strong></p><p>马尔科夫性是是保证或者判断概率图是否为概率无向图的条件。</p><p>三点内容：a. 成对，b. 局部，c. 全局。</p><p>我觉得这个不用展开。</p><h2 id="2-2-判别式（discriminative）模型-vs-生成式-generative-模型"><a href="#2-2-判别式（discriminative）模型-vs-生成式-generative-模型" class="headerlink" title="2.2 判别式（discriminative）模型 vs. 生成式(generative)模型"></a><strong>2.2 判别式（discriminative）模型 vs. 生成式(generative)模型</strong></h2><p>在监督学习下，模型可以分为判别式模型与生成式模型。</p><p>重点来了。上面有提到，我理解了HMM、CRF模型的区别是从理解了判别式模型与生成式模型的那刻，并且瞬间对其他的模型有一个恍然大悟。我记得是一年前就开始纠结这两者的区别，但我只能说，栽在了一些烂博客上，大部分都没有自己的insightful理解，也就是一顿官话，也真是难以理解。后来在知乎上一直琢磨别人的答案，然后某日早晨终于豁然开朗，就是这种感觉。</p><p>好了，我要用自己的理解来转述两者的区别了below。</p><p>先问个问题，根据经验，A批模型（神经网络模型、SVM、perceptron、LR、DT……）与B批模型（NB、LDA……），有啥区别不？（这个问题需要一些模型使用经验）应该是这样的： </p><blockquote><p>\1. A批模型是这么工作的，他们直接将数据的Y（或者label），根据所提供的features，学习，最后画出了一个明显或者比较明显的边界（具体怎么做到的？通过复杂的函数映射，或者决策叠加等等mechanism），这一点线性LR、线性SVM应该很明显吧。 </p><p>\2. B批模型是这么工作的，他们先从训练样本数据中，将所有的数据的分布情况摸透，然后最终确定一个分布，来作为我的所有的输入数据的分布，并且他是一个联合分布 <img src="https://www.zhihu.com/equation?tex=P%28X%2CY%29" alt="[公式]"> (注意 <img src="https://www.zhihu.com/equation?tex=X" alt="[公式]"> 包含所有的特征 <img src="https://www.zhihu.com/equation?tex=x_%7Bi%7D" alt="[公式]"> ， <img src="https://www.zhihu.com/equation?tex=Y" alt="[公式]"> 包含所有的label)。然后我来了新的样本数据（inference），好，通过学习来的模型的联合分布 <img src="https://www.zhihu.com/equation?tex=P%28X%2CY%29" alt="[公式]"> ，再结合新样本给的 <img src="https://www.zhihu.com/equation?tex=X" alt="[公式]"> ，通过条件概率就能出来 <img src="https://www.zhihu.com/equation?tex=Y" alt="[公式]"> ：<br><img src="https://www.zhihu.com/equation?tex=P%28Y%7CX%29+%3D+%5Cfrac%7BP%28X%2CY%29%7D%7BP%28X%29%7D" alt="[公式]"></p></blockquote><p>好了，应该说清楚了。</p><p><strong>1. 判别式模型</strong></p><p>那么A批模型对应了判别式模型。根据上面的两句话的区别，可以知道判别模型的特征了，所以有句话说：<strong>判别模型是直接对</strong> <img src="https://www.zhihu.com/equation?tex=P%28Y%7CX%29" alt="[公式]"><strong>建模</strong>，就是说，直接根据X特征来对Y建模训练。</p><p>具体地，我的训练过程是确定构件 <img src="https://www.zhihu.com/equation?tex=P%28Y%7CX%29" alt="[公式]"> 模型里面“复杂映射关系”中的参数，完了再去inference一批新的sample。</p><p>所以判别式模型的特征总结如下：</p><ol><li>对 <img src="https://www.zhihu.com/equation?tex=P%28Y%7CX%29" alt="[公式]"> 建模</li><li>对所有的样本只构建一个模型，确认总体判别边界</li><li>观测到输入什么特征，就预测最可能的label</li><li>另外，判别式的优点是：对数据量要求没生成式的严格，速度也会快，小数据量下准确率也会好些。</li></ol><p><strong>2. 生成式模型</strong></p><p>同样，B批模型对应了生成式模型。并且需要注意的是，在模型训练中，我学习到的是X与Y的联合模型 <img src="https://www.zhihu.com/equation?tex=P%28X%2CY%29" alt="[公式]"> ，也就是说，<strong>我在训练阶段是只对</strong> <img src="https://www.zhihu.com/equation?tex=P%28X%2CY%29" alt="[公式]"><strong>建模</strong>，我需要确定维护这个联合概率分布的所有的信息参数。完了之后在inference再对新的sample计算 <img src="https://www.zhihu.com/equation?tex=P%28Y%7CX%29" alt="[公式]"> ，导出 <img src="https://www.zhihu.com/equation?tex=Y" alt="[公式]"> ,但这已经不属于建模阶段了。</p><p>结合NB过一遍生成式模型的工作流程。学习阶段，建模： <img src="https://www.zhihu.com/equation?tex=P%28X%2CY%29%3DP%28X%7CY%29P%28Y%29" alt="[公式]"> （当然，NB具体流程去隔壁参考）,然后 <img src="https://www.zhihu.com/equation?tex=P%28Y%7CX%29+%3D+%5Cfrac%7BP%28X%2CY%29%7D%7BP%28X%29%7D" alt="[公式]"> 。<br>另外，LDA也是这样，只是他更过分，需要确定很多个概率分布，而且建模抽样都蛮复杂的。</p><p>所以生成式总结下有如下特点：</p><ol><li>对 <img src="https://www.zhihu.com/equation?tex=P%28X%2CY%29" alt="[公式]"> 建模</li><li>这里我们主要讲分类问题，所以是要对每个label（ <img src="https://www.zhihu.com/equation?tex=y_%7Bi%7D+" alt="[公式]"> ）都需要建模，最终选择最优概率的label为结果，所以没有什么判别边界。（对于序列标注问题，那只需要构件一个model）</li><li>中间生成联合分布，并可生成采样数据。</li><li>生成式模型的优点在于，所包含的信息非常齐全，我称之为“上帝信息”，所以不仅可以用来输入label，还可以干其他的事情。生成式模型关注结果是如何产生的。但是生成式模型需要非常充足的数据量以保证采样到了数据本来的面目，所以速度相比之下，慢。</li></ol><p>这一点明白后，后面讲到的HMM与CRF的区别也会非常清晰。<br>最后identity the picture below:</p><p><img src="https://pic2.zhimg.com/v2-9dfdfb246a1e6922ff8835d7fdf45e05_b.jpg" alt="img"></p><h2 id="2-3-序列建模"><a href="#2-3-序列建模" class="headerlink" title="2.3 序列建模"></a><strong>2.3 序列建模</strong></h2><p>为了号召零门槛理解，现在解释如何为序列问题建模。</p><p><img src="https://pic2.zhimg.com/v2-5358abb09290b93a5642fd81702dfe41_b.jpg" alt="img"></p><p>序列包括时间序列以及general sequence，但两者无异。连续的序列在分析时也会先离散化处理。常见的序列有如：时序数据、本文句子、语音数据、等等。</p><p>广义下的序列有这些特点：</p><ul><li>节点之间有关联依赖性/无关联依赖性</li><li>序列的节点是随机的/确定的</li><li>序列是线性变化/非线性的</li><li>……</li></ul><p>对不同的序列有不同的问题需求，常见的序列建模方法总结有如下：</p><ol><li>拟合，预测未来节点（或走势分析）：</li></ol><p>​      a. 常规序列建模方法：AR、MA、ARMA、ARIMA</p><p>​      b. 回归拟合</p><p>​      c. Neural Networks</p><p> \2. 判断不同序列类别，即分类问题：HMM、CRF、General Classifier（ML models、NN models）</p><p> \3. 不同时序对应的状态的分析，即序列标注问题：HMM、CRF、RecurrentNNs</p><p>在本篇文字中，我们只关注在2. &amp; 3.类问题下的建模过程和方法。</p><h2 id="三、HMM"><a href="#三、HMM" class="headerlink" title="三、HMM"></a><strong>三、HMM</strong></h2><p>最早接触的是HMM。较早做过一个项目，关于声波手势识别，跟声音识别的机制一样，使用的正是HMM的一套方法。后来又用到了<em>kalman filter</em>,之后做序列标注任务接触到了CRF，所以整个概率图模型还是接触的方面还蛮多。</p><h2 id="3-1-理解HMM"><a href="#3-1-理解HMM" class="headerlink" title="3.1 理解HMM"></a><strong>3.1 理解HMM</strong></h2><p>在2.2、2.3中提序列的建模问题时，我们只是讨论了常规的序列数据，e.g., <img src="https://www.zhihu.com/equation?tex=%EF%BC%88X_%7B1%7D%2C%5Ccdots%2CX_%7Bn%7D%EF%BC%89" alt="[公式]"> ,像2.3的图片那样。像这种序列一般用马尔科夫模型就可以胜任。实际上我们碰到的更多的使用HMM的场景是每个节点 <img src="https://www.zhihu.com/equation?tex=+X_%7Bi%7D" alt="[公式]"> 下还附带着另一个节点 <img src="https://www.zhihu.com/equation?tex=+Y_%7Bi%7D" alt="[公式]"> ，正所谓<strong>隐含</strong>马尔科夫模型，那么除了正常的节点，还要将<strong>隐含状态节点</strong>也得建模进去。正儿八经地，将 <img src="https://www.zhihu.com/equation?tex=X_%7Bi%7D+%E3%80%81+Y_%7Bi%7D+" alt="[公式]"> 换成 <img src="https://www.zhihu.com/equation?tex=i_%7Bi%7D+%E3%80%81o_%7Bi%7D" alt="[公式]"> ,并且他们的名称变为状态节点、观测节点。状态节点正是我的隐状态。</p><p>HMM属于典型的生成式模型。对照2.1的讲解，应该是要从训练数据中学到数据的各种分布，那么有哪些分布呢以及是什么呢？直接正面回答的话，正是<strong>HMM的5要素</strong>，其中有3个就是整个数据的不同角度的概率分布：</p><ul><li><img src="https://www.zhihu.com/equation?tex=N" alt="[公式]"> ，隐藏状态集 <img src="https://www.zhihu.com/equation?tex=N+%3D+%5Clbrace+q_%7B1%7D%2C+%5Ccdots%2C+q_%7BN%7D+%5Crbrace" alt="[公式]"> , 我的隐藏节点不能随意取，只能限定取包含在隐藏状态集中的符号。</li><li><img src="https://www.zhihu.com/equation?tex=M" alt="[公式]"> ，观测集 <img src="https://www.zhihu.com/equation?tex=M+%3D+%5Clbrace+v_%7B1%7D%2C+%5Ccdots%2C+v_%7BM%7D+%5Crbrace" alt="[公式]"> , 同样我的观测节点不能随意取，只能限定取包含在观测状态集中的符号。</li><li><img src="https://www.zhihu.com/equation?tex=A" alt="[公式]"> ，状态转移概率矩阵，这个就是其中一个概率分布。他是个矩阵， <img src="https://www.zhihu.com/equation?tex=+A%3D+%5Ba_%7Bij%7D%5D_%7BN+%5Ctimes+N%7D+" alt="[公式]"> （N为隐藏状态集元素个数），其中 <img src="https://www.zhihu.com/equation?tex=a_%7Bij%7D+%3D+P%28i_%7Bt%2B1%7D%7Ci_%7Bt%7D%29%EF%BC%8C+i_%7Bt%7D" alt="[公式]"> 即第i个隐状态节点,即所谓的状态转移嘛。</li><li><img src="https://www.zhihu.com/equation?tex=B" alt="[公式]"> ，观测概率矩阵，这个就是另一个概率分布。他是个矩阵， <img src="https://www.zhihu.com/equation?tex=B+%3D+%5Bb_%7Bij%7D%5D_%7BN+%5Ctimes+M%7D" alt="[公式]"> （N为隐藏状态集元素个数，M为观测集元素个数），其中 <img src="https://www.zhihu.com/equation?tex=b_%7Bij%7D+%3D+P%28o_%7Bt%7D%7Ci_%7Bt%7D%29%EF%BC%8C+o_%7Bt%7D" alt="[公式]"> 即第i个观测节点, <img src="https://www.zhihu.com/equation?tex=+i_%7Bt%7D" alt="[公式]"> 即第i个隐状态节点,即所谓的观测概率（发射概率）嘛。</li><li><img src="https://www.zhihu.com/equation?tex=%CF%80" alt="[公式]"> ，在第一个隐状态节点 <img src="https://www.zhihu.com/equation?tex=i_%7Bt%7D" alt="[公式]"> ,我得人工单独赋予，我第一个隐状态节点的隐状态是 <img src="https://www.zhihu.com/equation?tex=N" alt="[公式]"> 中的每一个的概率分别是多少，然后 <img src="https://www.zhihu.com/equation?tex=%CF%80" alt="[公式]"> 就是其概率分布。</li></ul><p>所以图看起来是这样的：</p><p><img src="https://pic4.zhimg.com/v2-d4077c2dbd9899d8896751a28490c9c7_b.jpg" alt="img"></p><p>看的很清楚，我的模型先去学习要确定以上5要素，之后在inference阶段的工作流程是：首先，隐状态节点 <img src="https://www.zhihu.com/equation?tex=i_%7Bt%7D" alt="[公式]"> 是不能直接观测到的数据节点， <img src="https://www.zhihu.com/equation?tex=o_%7Bt%7D" alt="[公式]"> 才是能观测到的节点，并且注意箭头的指向表示了依赖生成条件关系， <img src="https://www.zhihu.com/equation?tex=i_%7Bt%7D" alt="[公式]"> 在A的指导下生成下一个隐状态节点 <img src="https://www.zhihu.com/equation?tex=i_%7Bt%2B1%7D" alt="[公式]"> ，并且 <img src="https://www.zhihu.com/equation?tex=i_%7Bt%7D" alt="[公式]"> 在 <img src="https://www.zhihu.com/equation?tex=B" alt="[公式]"> 的指导下生成依赖于该 <img src="https://www.zhihu.com/equation?tex=i_%7Bt%7D" alt="[公式]"> 的观测节点 <img src="https://www.zhihu.com/equation?tex=o_%7Bt%7D" alt="[公式]"> , 并且我只能观测到序列 <img src="https://www.zhihu.com/equation?tex=%EF%BC%88o_%7B1%7D%2C+%5Ccdots%2C+o_%7Bi%7D%29" alt="[公式]"> 。</p><p>好，举例子说明（序列标注问题，POS，标注集BES）：</p><blockquote><p>input: “学习出一个模型，然后再预测出一条指定”</p><p>expected output: 学/B 习/E 出/S 一/B 个/E 模/B 型/E ，/S 然/B 后/E 再/E 预/B 测/E ……</p><p>其中，input里面所有的char构成的字表，形成观测集 <img src="https://www.zhihu.com/equation?tex=M" alt="[公式]"> ，因为字序列在inference阶段是我所能看见的；标注集BES构成隐藏状态集 <img src="https://www.zhihu.com/equation?tex=N" alt="[公式]"> ，这是我无法直接获取的，也是我的预测任务；至于 <img src="https://www.zhihu.com/equation?tex=A%E3%80%81B%E3%80%81%CF%80" alt="[公式]"> ，这些概率分布信息（上帝信息）都是我在学习过程中所确定的参数。</p></blockquote><p>然后一般初次接触的话会疑问：为什么要这样？……好吧，就应该是这样啊，根据具有同时带着隐藏状态节点和观测节点的类型的序列，在HMM下就是这样子建模的。</p><p>下面来点高层次的理解：</p><ol><li>根据概率图分类，可以看到HMM属于有向图，并且是生成式模型，直接对联合概率分布建模 <img src="https://www.zhihu.com/equation?tex=P%28O%2CI%29+%3D+%5Csum_%7Bt%3D1%7D%5E%7BT%7DP%28O_%7Bt%7D+%7C+O_%7Bt-1%7D%29P%28I_%7Bt%7D+%7C+O_%7Bt%7D%29" alt="[公式]"> (注意，这个公式不在模型运行的任何阶段能体现出来，只是我们都去这么来表示HMM是个生成式模型，他的联合概率 <img src="https://www.zhihu.com/equation?tex=P%28O%2CI%29" alt="[公式]"> 就是这么计算的)。</li><li>并且B中 <img src="https://www.zhihu.com/equation?tex=b_%7Bij%7D+%3D+P%28o_%7Bt%7D%7Ci_%7Bt%7D%29" alt="[公式]"> ，这意味着o对i有依赖性。</li><li>在A中， <img src="https://www.zhihu.com/equation?tex=a_%7Bij%7D+%3D+P%28i_%7Bt%2B1%7D%7Ci_%7Bt%7D%29" alt="[公式]"> ，也就是说只遵循了一阶马尔科夫假设，1-gram。试想，如果数据的依赖超过1-gram，那肯定HMM肯定是考虑不进去的。这一点限制了HMM的性能。</li></ol><h2 id="3-2-模型运行过程"><a href="#3-2-模型运行过程" class="headerlink" title="3.2 模型运行过程"></a><strong>3.2 模型运行过程</strong></h2><p>模型的运行过程（工作流程）对应了HMM的3个问题。</p><h2 id="3-2-1-学习训练过程"><a href="#3-2-1-学习训练过程" class="headerlink" title="3.2.1 学习训练过程"></a><strong>3.2.1 学习训练过程</strong></h2><p>对照2.1的讲解，HMM学习训练的过程，就是找出数据的分布情况，也就是模型参数的确定。</p><p>主要学习算法按照训练数据除了观测状态序列 <img src="https://www.zhihu.com/equation?tex=%EF%BC%88o_%7B1%7D%2C+%5Ccdots%2C+o_%7Bi%7D%29" alt="[公式]"> 是否还有隐状态序列 <img src="https://www.zhihu.com/equation?tex=%EF%BC%88i_%7B1%7D%2C+%5Ccdots%2C+i_%7Bi%7D%29" alt="[公式]"> 分为：</p><ul><li>极大似然估计, with 隐状态序列</li><li>Baum-Welch(前向后向), without 隐状态序列</li></ul><p>感觉不用做很多的介绍，都是很实实在在的算法，看懂了就能理解。简要提一下。</p><p><strong>1. 极大似然估计</strong></p><p>一般做NLP的序列标注等任务，在训练阶段肯定是有隐状态序列的。所以极大似然估计法是非常常用的学习算法，我见过的很多代码里面也是这么计算的。比较简单。</p><ul><li>step1. 算A</li></ul><p><img src="https://www.zhihu.com/equation?tex=%5Chat%7Ba_%7Bij%7D%7D+%3D+%5Cfrac%7BA_%7Bij%7D%7D%7B%5Csum_%7Bj%3D1%7D%5E%7BN%7DA_%7Bij%7D%7D+" alt="[公式]"></p><ul><li>step2. 算B</li></ul><p><img src="https://www.zhihu.com/equation?tex=%5Chat%7Bb_%7Bj%7D%7D%28k%29+%3D+%5Cfrac%7BB_%7Bjk%7D%7D%7B%5Csum_%7Bk%3D1%7D%5E%7BM%7DB_%7Bjk%7D%7D" alt="[公式]"></p><ul><li>step3. 直接估计 <img src="https://www.zhihu.com/equation?tex=%CF%80" alt="[公式]"></li></ul><p>比如说，在代码里计算完了就是这样的： </p><p><img src="https://pic2.zhimg.com/v2-5343666b942f952f6c79c4fa5e2dfdd9_b.jpg" alt="img"></p><p><img src="https://pic1.zhimg.com/v2-0d695539c785fe16cfcfff3b9bd0c164_b.jpg" alt="img"></p><p><img src="https://pic2.zhimg.com/v2-443053bd4342c71bf602ac15aa7c8731_b.jpg" alt="img"></p><p><strong>2. Baum-Welch(前向后向)</strong></p><p>就是一个EM的过程，如果你对EM的工作流程有经验的话，对这个Baum-Welch一看就懂。EM的过程就是初始化一套值，然后迭代计算，根据结果再调整值，再迭代，最后收敛……好吧，这个理解是没有捷径的，去隔壁钻研EM吧。</p><p>这里只提一下核心。因为我们手里没有隐状态序列 <img src="https://www.zhihu.com/equation?tex=%EF%BC%88i_%7B1%7D%2C+%5Ccdots%2C+i_%7Bi%7D%29" alt="[公式]"> 信息，所以我先必须给初值 <img src="https://www.zhihu.com/equation?tex=a_%7Bij%7D%5E%7B0%7D%2C+b_%7Bj%7D%28k%29%5E%7B0%7D%2C+%5Cpi%5E%7B0%7D" alt="[公式]"> ，初步确定模型，然后再迭代计算出 <img src="https://www.zhihu.com/equation?tex=a_%7Bij%7D%5E%7Bn%7D%2C+b_%7Bj%7D%28k%29%5E%7Bn%7D%2C+%5Cpi%5E%7Bn%7D" alt="[公式]"> ,中间计算过程会用到给出的观测状态序列 <img src="https://www.zhihu.com/equation?tex=%EF%BC%88o_%7B1%7D%2C+%5Ccdots%2C+o_%7Bi%7D%29" alt="[公式]"> 。另外，收敛性由EM的XXX定理保证。</p><h2 id="3-2-2-序列标注（解码）过程"><a href="#3-2-2-序列标注（解码）过程" class="headerlink" title="3.2.2 序列标注（解码）过程"></a><strong>3.2.2 序列标注（解码）过程</strong></h2><p>好了，学习完了HMM的分布参数，也就确定了一个HMM模型。需要注意的是，这个HMM是对我这一批全部的数据进行训练所得到的参数。</p><p>序列标注问题也就是“预测过程”，通常称为解码过程。对应了序列建模问题3.。对于序列标注问题，我们只需要学习出一个HMM模型即可，后面所有的新的sample我都用这一个HMM去apply。</p><p>我们的目的是，在学习后已知了 <img src="https://www.zhihu.com/equation?tex=P%28Q%2CO%29" alt="[公式]"> ,现在要求出 <img src="https://www.zhihu.com/equation?tex=P%28Q%7CO%29" alt="[公式]"> ，进一步</p><p><img src="https://www.zhihu.com/equation?tex=Q_%7Bmax%7D+%3D+argmax_%7BallQ%7D%5Cfrac%7BP%28Q%2CO%29%7D%7BP%28O%29%7D" alt="[公式]"></p><p>再直白点就是，我现在要在给定的观测序列下找出一条隐状态序列，条件是这个隐状态序列的概率是最大的那个。</p><p>具体地，都是用Viterbi算法解码，是用DP思想减少重复的计算。Viterbi也是满大街的，不过要说的是，Viterbi不是HMM的专属，也不是任何模型的专属，他只是恰好被满足了被HMM用来使用的条件。谁知，现在大家都把Viterbi跟HMM捆绑在一起了, shame。</p><p>Viterbi计算有向无环图的一条最大路径，应该还好理解。如图：</p><p><img src="https://pic1.zhimg.com/v2-71f1ea9abbab357f7d9bad1138ee7344_b.jpg" alt="img"></p><p>关键是注意，每次工作热点区只涉及到t 与 t-1,这对应了DP的无后效性的条件。如果对某些同学还是很难理解，请参考<a href="https://www.zhihu.com/question/20136144" target="_blank" rel="noopener">这个答案</a>下@Kiwee的回答吧。</p><h2 id="3-2-3-序列概率过程"><a href="#3-2-3-序列概率过程" class="headerlink" title="3.2.3 序列概率过程"></a><strong>3.2.3 序列概率过程</strong></h2><p>我通过HMM计算出序列的概率又有什么用？针对这个点我把这个问题详细说一下。 </p><p>实际上，序列概率过程对应了序列建模问题2.，即序列分类。<br>在3.2.2第一句话我说，在序列标注问题中，我用一批完整的数据训练出了一支HMM模型即可。好，那在序列分类问题就不是训练一个HMM模型了。我应该这么做（结合语音分类识别例子）： </p><blockquote><p>目标：识别声音是A发出的还是B发出的。<br>HMM建模过程：<br>   \1.   训练：我将所有A说的语音数据作为dataset_A,将所有B说的语音数据作为dataset_B（当然，先要分别对dataset A ,B做预处理encode为元数据节点，形成sequences）,然后分别用dataset_A、dataset_B去训练出HMM_A/HMM_B<br>   \2.   inference：来了一条新的sample（sequence），我不知道是A的还是B的，没问题，分别用HMM_A/HMM_B计算一遍序列的概率得到 <img src="https://www.zhihu.com/equation?tex=P_%7BA%7D%EF%BC%88S%EF%BC%89%E3%80%81P_%7BB%7D%EF%BC%88S%EF%BC%89" alt="[公式]"> ，比较两者大小，哪个概率大说明哪个更合理，更大概率作为目标类别。</p></blockquote><p>所以，本小节的理解重点在于，<strong>如何对一条序列计算其整体的概率</strong>。即目标是计算出 <img src="https://www.zhihu.com/equation?tex=P%28O%7C%CE%BB%29" alt="[公式]"> 。这个问题前辈们在他们的经典中说的非常好了，比如参考李航老师整理的：</p><ul><li>直接计算法（穷举搜索）</li><li>前向算法</li><li>后向算法</li></ul><p>后面两个算法采用了DP思想，减少计算量，即每一次直接引用前一个时刻的计算结果以避免重复计算，跟Viterbi一样的技巧。</p><p>还是那句，因为这篇文档不是专门讲算法细节的，所以不详细展开这些。毕竟，所有的科普HMM、CRF的博客貌似都是在扯这些算法，妥妥的街货，就不搬运了。</p><h2 id="四、MEMM"><a href="#四、MEMM" class="headerlink" title="四、MEMM"></a><strong>四、MEMM</strong></h2><p>MEMM，即最大熵马尔科夫模型，这个是在接触了HMM、CRF之后才知道的一个模型。说到MEMM这一节时，得转换思维了，因为现在这MEMM属于判别式模型。</p><p>不过有一点很尴尬，MEMM貌似被使用或者讲解引用的不及HMM、CRF。</p><h2 id="4-1-理解MEMM"><a href="#4-1-理解MEMM" class="headerlink" title="4.1 理解MEMM"></a><strong>4.1 理解MEMM</strong></h2><p>这里还是啰嗦强调一下，MEMM正因为是判别模型，所以不废话，我上来就直接为了确定边界而去建模，比如说序列求概率（分类）问题，我直接考虑找出函数分类边界。这一点跟HMM的思维方式发生了很大的变化，如果不对这一点有意识，那么很难理解为什么MEMM、CRF要这么做。</p><p>HMM中，观测节点 <img src="https://www.zhihu.com/equation?tex=o_%7Bi%7D" alt="[公式]"> 依赖隐藏状态节点 <img src="https://www.zhihu.com/equation?tex=i_%7Bi%7D" alt="[公式]"> ,也就意味着我的观测节点只依赖当前时刻的隐藏状态。但在更多的实际场景下，观测序列是需要很多的特征来刻画的，比如说，我在做NER时，我的标注 <img src="https://www.zhihu.com/equation?tex=i_%7Bi%7D" alt="[公式]"> 不仅跟当前状态 <img src="https://www.zhihu.com/equation?tex=o_%7Bi%7D" alt="[公式]"> 相关，而且还跟前后标注 <img src="https://www.zhihu.com/equation?tex=o_%7Bj%7D%28j+%5Cneq+i%29" alt="[公式]"> 相关，比如字母大小写、词性等等。</p><p>为此，提出来的MEMM模型就是能够直接允许<strong>“定义特征”</strong>，直接学习条件概率，即 <img src="https://www.zhihu.com/equation?tex=P%28i_%7Bi%7D%7Ci_%7Bi-1%7D%2Co_%7Bi%7D%29+%28i+%3D+1%2C%5Ccdots%2Cn%29" alt="[公式]"> , 总体为：</p><p><img src="https://www.zhihu.com/equation?tex=P%28I%7CO%29+%3D+%5Cprod_%7Bt%3D1%7D%5E%7Bn%7DP%28i_%7Bi%7D%7Ci_%7Bi-1%7D%2Co_%7Bi%7D%29%2C+i+%3D+1%2C%5Ccdots%2Cn" alt="[公式]"></p><p>并且， <img src="https://www.zhihu.com/equation?tex=P%28i%7Ci%5E%7B%27%7D%2Co%29" alt="[公式]"> 这个概率通过最大熵分类器建模（取名MEMM的原因）:</p><p><img src="https://www.zhihu.com/equation?tex=P%28i%7Ci%5E%7B%27%7D%2Co%29+%3D+%5Cfrac%7B1%7D%7BZ%28o%2Ci%5E%7B%27%7D%29%7D+exp%28%5Csum_%7Ba%7D%29%5Clambda_%7Ba%7Df_%7Ba%7D%28o%2Ci%29" alt="[公式]"></p><p>重点来了，这是ME的内容，也是理解MEMM的关键： <img src="https://www.zhihu.com/equation?tex=Z%28o%2Ci%5E%7B%27%7D%29" alt="[公式]"> 这部分是归一化； <img src="https://www.zhihu.com/equation?tex=f_%7Ba%7D%28o%2Ci%29" alt="[公式]"> 是<strong>特征函数</strong>，具体点，这个函数是需要去定义的; <img src="https://www.zhihu.com/equation?tex=%CE%BB" alt="[公式]"> 是特征函数的权重，这是个未知参数，需要从训练阶段学习而得。</p><p>比如我可以这么定义特征函数：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+f_%7Ba%7D%28o%2Ci%29+%3D+%5Cbegin%7Bcases%7D+1%26+%5Ctext%7B%E6%BB%A1%E8%B6%B3%E7%89%B9%E5%AE%9A%E6%9D%A1%E4%BB%B6%7D%EF%BC%8C%5C%5C+0%26+%5Ctext%7Bother%7D+%5Cend%7Bcases%7D+%5Cend%7Bequation%7D" alt="[公式]"></p><p>其中，特征函数 <img src="https://www.zhihu.com/equation?tex=+f_%7Ba%7D%28o%2Ci%29" alt="[公式]"> 个数可任意制定， <img src="https://www.zhihu.com/equation?tex=%EF%BC%88a+%3D+1%2C+%5Ccdots%2C+n%EF%BC%89" alt="[公式]"></p><p>所以总体上，MEMM的建模公式这样：</p><p><img src="https://www.zhihu.com/equation?tex=P%28I%7CO%29+%3D+%5Cprod_%7Bt%3D1%7D%5E%7Bn%7D%5Cfrac%7B+exp%28%5Csum_%7Ba%7D%29%5Clambda_%7Ba%7Df_%7Ba%7D%28o%2Ci%29+%7D%7BZ%28o%2Ci_%7Bi-1%7D%29%7D+%2C+i+%3D+1%2C%5Ccdots%2Cn" alt="[公式]"></p><p>是的，公式这部分之所以长成这样，是由ME模型决定的。</p><p>请务必注意，理解<strong>判别模型</strong>和<strong>定义特征</strong>两部分含义，这已经涉及到CRF的雏形了。</p><p>所以说，他是判别式模型，直接对条件概率建模。 上图： </p><p><img src="https://pic4.zhimg.com/v2-cb2cc25593fcaf06e682191d551ba03b_b.jpg" alt="img"></p><p>MEMM需要两点注意：</p><ol><li>与HMM的 <img src="https://www.zhihu.com/equation?tex=o_%7Bi%7D" alt="[公式]"> 依赖 <img src="https://www.zhihu.com/equation?tex=i_%7Bi%7D" alt="[公式]"> 不一样，MEMM当前隐藏状态 <img src="https://www.zhihu.com/equation?tex=i_%7Bi%7D" alt="[公式]"> 应该是依赖当前时刻的观测节点 <img src="https://www.zhihu.com/equation?tex=o_%7Bi%7D" alt="[公式]"> 和上一时刻的隐藏节点 <img src="https://www.zhihu.com/equation?tex=i_%7Bi-1%7D" alt="[公式]"></li><li>需要注意，之所以图的箭头这么画，是由MEMM的公式决定的，而公式是creator定义出来的。</li></ol><p>好了，走一遍完整流程。</p><blockquote><p>step1. 先预定义特征函数 <img src="https://www.zhihu.com/equation?tex=f_%7Ba%7D%28o%2Ci%29" alt="[公式]"> ，<br>step2. 在给定的数据上，训练模型，确定参数，即确定了MEMM模型<br>step3. 用确定的模型做序列标注问题或者序列求概率问题。</p></blockquote><h2 id="4-2-模型运行过程"><a href="#4-2-模型运行过程" class="headerlink" title="4.2 模型运行过程"></a><strong>4.2 模型运行过程</strong></h2><p>MEMM模型的工作流程也包括了学习训练问题、序列标注问题、序列求概率问题。</p><h2 id="4-2-1-学习训练过程"><a href="#4-2-1-学习训练过程" class="headerlink" title="4.2.1 学习训练过程"></a><strong>4.2.1 学习训练过程</strong></h2><p>一套MEMM由一套参数唯一确定，同样地，我需要通过训练数据学习这些参数。MEMM模型很自然需要学习里面的特征权重λ。</p><p>不过跟HMM不用的是，因为HMM是生成式模型，参数即为各种概率分布元参数，数据量足够可以用最大似然估计。而判别式模型是用函数直接判别，学习边界，MEMM即通过特征函数来界定。但同样，MEMM也有极大似然估计方法、梯度下降、牛顿迭代发、拟牛顿下降、BFGS、L-BFGS等等。各位应该对各种优化方法有所了解的。</p><p>嗯，具体详细求解过程貌似问题不大。</p><h2 id="4-2-2-序列标注过程"><a href="#4-2-2-序列标注过程" class="headerlink" title="4.2.2 序列标注过程"></a><strong>4.2.2 序列标注过程</strong></h2><p>还是跟HMM一样的，用学习好的MEMM模型，在新的sample（观测序列 <img src="https://www.zhihu.com/equation?tex=+o_%7B1%7D%2C+%5Ccdots%2C+o_%7Bi%7D" alt="[公式]"> ）上找出一条概率最大最可能的隐状态序列 <img src="https://www.zhihu.com/equation?tex=i_%7B1%7D%2C+%5Ccdots%2C+i_%7Bi%7D" alt="[公式]"> 。</p><p>只是现在的图中的每个隐状态节点的概率求法有一些差异而已,正确将每个节点的概率表示清楚，路径求解过程还是一样，采用viterbi算法。</p><h2 id="4-2-3-序列求概率过程"><a href="#4-2-3-序列求概率过程" class="headerlink" title="4.2.3 序列求概率过程"></a><strong>4.2.3 序列求概率过程</strong></h2><p>跟HMM举的例子一样的，也是分别去为每一批数据训练构建特定的MEMM，然后根据序列在每个MEMM模型的不同得分概率，选择最高分数的模型为wanted类别。</p><p>应该可以不用展开，吧……</p><h2 id="4-3-标注偏置？"><a href="#4-3-标注偏置？" class="headerlink" title="4.3 标注偏置？"></a><strong>4.3 标注偏置？</strong></h2><p>MEMM讨论的最多的是他的labeling bias 问题。</p><p><strong>1. 现象</strong></p><p>是从街货上烤过来的…… </p><p><img src="https://pic2.zhimg.com/v2-40f9945cdffb12cfec84bebc7b7e3be5_b.jpg" alt="img"></p><p>用Viterbi算法解码MEMM，状态1倾向于转换到状态2，同时状态2倾向于保留在状态2。 解码过程细节（需要会viterbi算法这个前提）：</p><blockquote><p>P(1-&gt; 1-&gt; 1-&gt; 1)= 0.4 x 0.45 x 0.5 = 0.09 ，<br>P(2-&gt;2-&gt;2-&gt;2)= 0.2 X 0.3 X 0.3 = 0.018，<br>P(1-&gt;2-&gt;1-&gt;2)= 0.6 X 0.2 X 0.5 = 0.06，<br>P(1-&gt;1-&gt;2-&gt;2)= 0.4 X 0.55 X 0.3 = 0.066 </p></blockquote><p>但是得到的最优的状态转换路径是1-&gt;1-&gt;1-&gt;1，为什么呢？因为状态2可以转换的状态比状态1要多，从而使转移概率降低,即MEMM倾向于选择拥有更少转移的状态。</p><p><strong>2. 解释原因</strong></p><p>直接看MEMM公式：</p><p><img src="https://www.zhihu.com/equation?tex=P%28I%7CO%29+%3D+%5Cprod_%7Bt%3D1%7D%5E%7Bn%7D%5Cfrac%7B+exp%28%5Csum_%7Ba%7D%29%5Clambda_%7Ba%7Df_%7Ba%7D%28o%2Ci%29+%7D%7BZ%28o%2Ci_%7Bi-1%7D%29%7D+%2C+i+%3D+1%2C%5Ccdots%2Cn" alt="[公式]"></p><p><img src="https://www.zhihu.com/equation?tex=%E2%88%91" alt="[公式]"> 求和的作用在概率中是归一化，但是这里归一化放在了指数内部，管这叫local归一化。 来了，viterbi求解过程，是用dp的状态转移公式（MEMM的没展开，请参考CRF下面的公式），因为是局部归一化，所以MEMM的viterbi的转移公式的第二部分出现了问题，导致dp无法正确的递归到全局的最优。</p><p><img src="https://www.zhihu.com/equation?tex=+%5Cdelta_%7Bi%2B1%7D+%3D+max_%7B1+%5Cle+j+%5Cle+m%7D%5Clbrace+%5Cdelta_%7Bi%7D%28I%29+%2B+%5Csum_%7Bi%7D%5E%7BT%7D%5Csum_%7Bk%7D%5E%7BM%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28O%2CI_%7Bi-1%7D%2CI_%7Bi%7D%2Ci%29+%5Crbrace+" alt="[公式]"></p><h2 id="五、CRF"><a href="#五、CRF" class="headerlink" title="五、CRF"></a><strong>五、CRF</strong></h2><p>我觉得一旦有了一个清晰的工作流程，那么按部就班地，没有什么很难理解的地方，因为整体框架已经胸有成竹了，剩下了也只有添砖加瓦小修小补了。有了上面的过程基础，CRF也是类似的，只是有方法论上的细微区别。</p><h2 id="5-1-理解CRF"><a href="#5-1-理解CRF" class="headerlink" title="5.1 理解CRF"></a><strong>5.1 理解CRF</strong></h2><p>请看第一张概率图模型构架图，CRF上面是马尔科夫随机场（马尔科夫网络），而条件随机场是在给定的随机变量 <img src="https://www.zhihu.com/equation?tex=X" alt="[公式]"> （具体，对应观测序列 <img src="https://www.zhihu.com/equation?tex=o_%7B1%7D%2C+%5Ccdots%2C+o_%7Bi%7D" alt="[公式]"> ）条件下，随机变量 <img src="https://www.zhihu.com/equation?tex=Y" alt="[公式]"> （具体，对应隐状态序列 <img src="https://www.zhihu.com/equation?tex=i_%7B1%7D%2C+%5Ccdots%2C+i_%7Bi%7D" alt="[公式]"> 的马尔科夫随机场。<br>广义的CRF的定义是： 满足 <img src="https://www.zhihu.com/equation?tex=P%28Y_%7Bv%7D%7CX%2CY_%7Bw%7D%2Cw+%5Cneq+v%29+%3D+P%28Y_%7Bv%7D%7CX%2CY_%7Bw%7D%2Cw+%5Csim+v%29+" alt="[公式]"> 的马尔科夫随机场叫做条件随机场（CRF）。</p><p>不过一般说CRF为序列建模，就专指CRF线性链（linear chain CRF）：</p><p><img src="https://pic3.zhimg.com/v2-c5e2e782e35f6412ed65e58cdda0964e_b.jpg" alt="img"></p><p>在2.1.2中有提到过，概率无向图的联合概率分布可以在因子分解下表示为：</p><p><img src="https://www.zhihu.com/equation?tex=P%28Y+%7C+X%29%3D%5Cfrac%7B1%7D%7BZ%28x%29%7D+%5Cprod_%7Bc%7D%5Cpsi_%7Bc%7D%28Y_%7Bc%7D%7CX+%29+%3D+%5Cfrac%7B1%7D%7BZ%28x%29%7D+%5Cprod_%7Bc%7D+e%5E%7B%5Csum_%7Bk%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28c%2Cy%7Cc%2Cx%29%7D+%3D+%5Cfrac%7B1%7D%7BZ%28x%29%7D+e%5E%7B%5Csum_%7Bc%7D%5Csum_%7Bk%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28y_%7Bi%7D%2Cy_%7Bi-1%7D%2Cx%2Ci%29%7D" alt="[公式]"></p><p>而在线性链CRF示意图中，每一个（ <img src="https://www.zhihu.com/equation?tex=I_%7Bi%7D+%5Csim+O_%7Bi%7D" alt="[公式]"> ）对为一个最大团,即在上式中 <img src="https://www.zhihu.com/equation?tex=c+%3D+i" alt="[公式]"> 。并且线性链CRF满足 <img src="https://www.zhihu.com/equation?tex=P%28I_%7Bi%7D%7CO%2CI_%7B1%7D%2C%5Ccdots%2C+I_%7Bn%7D%29+%3D+P%28I_%7Bi%7D%7CO%2CI_%7Bi-1%7D%2CI_%7Bi%2B1%7D%29+" alt="[公式]"> 。</p><p><strong>所以CRF的建模公式如下：</strong></p><p><img src="https://www.zhihu.com/equation?tex=P%28I+%7C+O%29%3D%5Cfrac%7B1%7D%7BZ%28O%29%7D+%5Cprod_%7Bi%7D%5Cpsi_%7Bi%7D%28I_%7Bi%7D%7CO+%29+%3D+%5Cfrac%7B1%7D%7BZ%28O%29%7D+%5Cprod_%7Bi%7D+e%5E%7B%5Csum_%7Bk%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28O%2CI_%7Bi-1%7D%2CI_%7Bi%7D%2Ci%29%7D+%3D+%5Cfrac%7B1%7D%7BZ%28O%29%7D+e%5E%7B%5Csum_%7Bi%7D%5Csum_%7Bk%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28O%2CI_%7Bi-1%7D%2CI_%7Bi%7D%2Ci%29%7D" alt="[公式]"></p><p>我要敲黑板了，这个公式是非常非常关键的，注意递推过程啊，我是怎么从 <img src="https://www.zhihu.com/equation?tex=%E2%88%8F" alt="[公式]"> 跳到 <img src="https://www.zhihu.com/equation?tex=e%5E%7B%5Csum%7D" alt="[公式]"> 的。</p><p>不过还是要多啰嗦一句，想要理解CRF，必须判别式模型的概念要深入你心。正因为是判别模型，所以不废话，我上来就直接为了确定边界而去建模，因为我创造出来就是为了这个分边界的目的的。比如说序列求概率（分类）问题，我直接考虑找出函数分类边界。所以才为什么会有这个公式。所以再看到这个公式也别懵逼了，he was born for discriminating the given data from different classes. 就这样。不过待会还会具体介绍特征函数部分的东西。</p><p>除了建模总公式，关键的CRF重点概念在MEMM中已强调过：<strong>判别式模型</strong>、<strong>特征函数</strong>。</p><p><strong>1. 特征函数</strong></p><p>上面给出了CRF的建模公式：</p><p><img src="https://www.zhihu.com/equation?tex=P%28I+%7C+O%29%3D%5Cfrac%7B1%7D%7BZ%28O%29%7D+e%5E%7B%5Csum_%7Bi%7D%5E%7BT%7D%5Csum_%7Bk%7D%5E%7BM%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28O%2CI_%7Bi-1%7D%2CI_%7Bi%7D%2Ci%29%7D" alt="[公式]"></p><ul><li>下标<em>i</em>表示我当前所在的节点（token）位置。</li><li>下标<em>k</em>表示我这是第几个特征函数，并且每个特征函数都附属一个权重 <img src="https://www.zhihu.com/equation?tex=%5Clambda_%7Bk%7D" alt="[公式]"> ，也就是这么回事，每个团里面，我将为 <img src="https://www.zhihu.com/equation?tex=token_%7Bi%7D" alt="[公式]"> 构造M个特征，每个特征执行一定的限定作用，然后建模时我再为每个特征函数加权求和。</li><li><img src="https://www.zhihu.com/equation?tex=Z%28O%29" alt="[公式]"> 是用来归一化的，为什么？想想LR以及softmax为何有归一化呢，一样的嘛，形成概率值。</li><li>再来个重要的理解。 <img src="https://www.zhihu.com/equation?tex=P%28I%7CO%29" alt="[公式]"> 这个表示什么？具体地，表示了在给定的一条观测序列 <img src="https://www.zhihu.com/equation?tex=O%3D%28o_%7B1%7D%2C%5Ccdots%2C+o_%7Bi%7D%29" alt="[公式]"> 条件下，我用CRF所求出来的隐状态序列 <img src="https://www.zhihu.com/equation?tex=I%3D%28i_%7B1%7D%2C%5Ccdots%2C+i_%7Bi%7D%29" alt="[公式]"> 的概率，注意，这里的<em>I</em>是一条序列，有多个元素（一组随机变量），而至于观测序列 <img src="https://www.zhihu.com/equation?tex=O%3D%28o_%7B1%7D%2C%5Ccdots%2C+o_%7Bi%7D%29" alt="[公式]"> ，它可以是一整个训练语料的所有的观测序列；也可以是在inference阶段的一句sample，比如说对于序列标注问题，我对一条sample进行预测，可能能得到 <img src="https://www.zhihu.com/equation?tex=P_%7Bj%7D%28I+%7C+O%29%EF%BC%88j%3D1%2C%E2%80%A6%2CJ%EF%BC%89" alt="[公式]"><em>J</em>条隐状态<em>I</em>，但我肯定最终选的是最优概率的那条（by viterbi）。这一点希望你能理解。</li></ul><p>对于CRF，可以为他定义两款特征函数：转移特征&amp;状态特征。 我们将建模总公式展开：</p><p><img src="https://www.zhihu.com/equation?tex=P%28I+%7C+O%29%3D%5Cfrac%7B1%7D%7BZ%28O%29%7D+e%5E%7B%5Csum_%7Bi%7D%5E%7BT%7D%5Csum_%7Bk%7D%5E%7BM%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28O%2CI_%7Bi-1%7D%2CI_%7Bi%7D%2Ci%29%7D%3D%5Cfrac%7B1%7D%7BZ%28O%29%7D+e%5E%7B+%5B+%5Csum_%7Bi%7D%5E%7BT%7D%5Csum_%7Bj%7D%5E%7BJ%7D%5Clambda_%7Bj%7Dt_%7Bj%7D%28O%2CI_%7Bi-1%7D%2CI_%7Bi%7D%2Ci%29+%2B+%5Csum_%7Bi%7D%5E%7BT%7D%5Csum_%7Bl%7D%5E%7BL%7D%5Cmu_%7Bl%7Ds_%7Bl%7D%28O%2CI_%7Bi%7D%2Ci%29+%5D+%7D" alt="[公式]"></p><p>其中：</p><ul><li><img src="https://www.zhihu.com/equation?tex=t_%7Bj%7D" alt="[公式]"> 为i处的转移特征，对应权重 <img src="https://www.zhihu.com/equation?tex=%5Clambda_%7Bj%7D" alt="[公式]"> ,每个 <img src="https://www.zhihu.com/equation?tex=token_%7Bi%7D" alt="[公式]"> 都有J个特征,转移特征针对的是前后token之间的限定。</li><li><ul><li>举个例子：</li></ul></li></ul><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+t_%7Bk%3D1%7D%28o%2Ci%29+%3D+%5Cbegin%7Bcases%7D+1%26+%5Ctext%7B%E6%BB%A1%E8%B6%B3%E7%89%B9%E5%AE%9A%E8%BD%AC%E7%A7%BB%E6%9D%A1%E4%BB%B6%EF%BC%8C%E6%AF%94%E5%A6%82%E5%89%8D%E4%B8%80%E4%B8%AAtoken%E6%98%AF%E2%80%98I%E2%80%99%7D%EF%BC%8C%5C%5C+0%26+%5Ctext%7Bother%7D+%5Cend%7Bcases%7D+%5Cend%7Bequation%7D" alt="[公式]"></p><ul><li>sl为i处的状态特征，对应权重μl,每个tokeni都有L个特征</li><li><ul><li>举个例子：</li></ul></li></ul><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+s_%7Bl%3D1%7D%28o%2Ci%29+%3D+%5Cbegin%7Bcases%7D+1%26+%5Ctext%7B%E6%BB%A1%E8%B6%B3%E7%89%B9%E5%AE%9A%E7%8A%B6%E6%80%81%E6%9D%A1%E4%BB%B6%EF%BC%8C%E6%AF%94%E5%A6%82%E5%BD%93%E5%89%8Dtoken%E7%9A%84POS%E6%98%AF%E2%80%98V%E2%80%99%7D%EF%BC%8C%5C%5C+0%26+%5Ctext%7Bother%7D+%5Cend%7Bcases%7D+%5Cend%7Bequation%7D" alt="[公式]"></p><p>不过一般情况下，我们不把两种特征区别的那么开，合在一起：</p><p><img src="https://www.zhihu.com/equation?tex=P%28I+%7C+O%29%3D%5Cfrac%7B1%7D%7BZ%28O%29%7D+e%5E%7B%5Csum_%7Bi%7D%5E%7BT%7D%5Csum_%7Bk%7D%5E%7BM%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28O%2CI_%7Bi-1%7D%2CI_%7Bi%7D%2Ci%29%7D" alt="[公式]"></p><p>满足特征条件就取值为1，否则没贡献，甚至你还可以让他打负分，充分惩罚。</p><p>再进一步理解的话，我们需要把特征函数部分抠出来：</p><p><img src="https://www.zhihu.com/equation?tex=Score+%3D+%5Csum_%7Bi%7D%5E%7BT%7D%5Csum_%7Bk%7D%5E%7BM%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28O%2CI_%7Bi-1%7D%2CI_%7Bi%7D%2Ci%29+" alt="[公式]"></p><p>是的，我们为 <img src="https://www.zhihu.com/equation?tex=token_%7Bi%7D" alt="[公式]"> 打分，满足条件的就有所贡献。最后将所得的分数进行log线性表示，求和后归一化，即可得到概率值……完了又扯到了log线性模型。现在稍作解释：</p><blockquote><p>log-linear models take the following form:<br><img src="https://www.zhihu.com/equation?tex=P%28y%7Cx%3B%5Comega%29+%3D+%5Cfrac%7B+exp%28%5Comega%C2%B7%5Cphi%28x%2Cy%29%29+%7D%7B+%5Csum_%7By%5E%7B%27%7D%5Cin+Y+%7Dexp%28%5Comega%C2%B7%5Cphi%28x%2Cy%5E%7B%E2%80%98%7D%29%29+%7D" alt="[公式]"></p></blockquote><p>我觉得对LR或者sotfmax熟悉的对这个应该秒懂。然后CRF完美地满足这个形式，所以又可以归入到了log-linear models之中。</p><h2 id="5-2-模型运行过程"><a href="#5-2-模型运行过程" class="headerlink" title="5.2 模型运行过程"></a><strong>5.2 模型运行过程</strong></h2><p>模型的工作流程，跟MEMM是一样的：</p><ul><li>step1. 先预定义特征函数 <img src="https://www.zhihu.com/equation?tex=+f_%7Ba%7D%28o%2Ci%29" alt="[公式]"> ，</li><li>step2. 在给定的数据上，训练模型，确定参数 <img src="https://www.zhihu.com/equation?tex=%5Clambda_%7Bk%7D" alt="[公式]"></li><li>step3. 用确定的模型做<code>序列标注问题</code>或者<code>序列求概率问题</code>。</li></ul><p>可能还是没做到100%懂，结合例子说明：</p><blockquote><p>……</p></blockquote><h2 id="5-2-1-学习训练过程"><a href="#5-2-1-学习训练过程" class="headerlink" title="5.2.1 学习训练过程"></a><strong>5.2.1 学习训练过程</strong></h2><p>一套CRF由一套参数λ唯一确定（先定义好各种特征函数）。</p><p>同样，CRF用极大似然估计方法、梯度下降、牛顿迭代、拟牛顿下降、IIS、BFGS、L-BFGS等等。各位应该对各种优化方法有所了解的。其实能用在log-linear models上的求参方法都可以用过来。</p><p>嗯，具体详细求解过程貌似问题不大。</p><h2 id="5-2-2-序列标注过程"><a href="#5-2-2-序列标注过程" class="headerlink" title="5.2.2 序列标注过程"></a><strong>5.2.2 序列标注过程</strong></h2><p>还是跟HMM一样的，用学习好的CRF模型，在新的sample（观测序列 <img src="https://www.zhihu.com/equation?tex=o_%7B1%7D%2C+%5Ccdots%2C+o_%7Bi%7D" alt="[公式]"> ）上找出一条概率最大最可能的隐状态序列 <img src="https://www.zhihu.com/equation?tex=i_%7B1%7D%2C+%5Ccdots%2C+i_%7Bi%7D" alt="[公式]"> 。</p><p>只是现在的图中的每个隐状态节点的概率求法有一些差异而已,正确将每个节点的概率表示清楚，路径求解过程还是一样，采用viterbi算法。</p><p>啰嗦一下，我们就定义i处的局部状态为 <img src="https://www.zhihu.com/equation?tex=%5Cdelta_%7Bi%7D%28I%29" alt="[公式]"> ,表示在位置i处的隐状态的各种取值可能为<em>I</em>，然后递推位置i+1处的隐状态，写出来的DP转移公式为：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cdelta_%7Bi%2B1%7D+%3D+max_%7B1+%5Cle+j+%5Cle+m%7D%5Clbrace+%5Cdelta_%7Bi%7D%28I%29+%2B+%5Csum_%7Bi%7D%5E%7BT%7D%5Csum_%7Bk%7D%5E%7BM%7D%5Clambda_%7Bk%7Df_%7Bk%7D%28O%2CI_%7Bi-1%7D%2CI_%7Bi%7D%2Ci%29+%5Crbrace" alt="[公式]"></p><p>这里没写规范因子 <img src="https://www.zhihu.com/equation?tex=Z%28O%29" alt="[公式]"> 是因为不规范化不会影响取最大值后的比较。</p><p>具体还是不展开为好。</p><h2 id="5-2-3-序列求概率过程"><a href="#5-2-3-序列求概率过程" class="headerlink" title="5.2.3 序列求概率过程"></a><strong>5.2.3 序列求概率过程</strong></h2><p>跟HMM举的例子一样的，也是分别去为每一批数据训练构建特定的CRF，然后根据序列在每个MEMM模型的不同得分概率，选择最高分数的模型为wanted类别。只是貌似很少看到拿CRF或者MEMM来做分类的，直接用网络模型不就完了不……</p><p>应该可以不用展开，吧……</p><h2 id="5-3-CRF-分析"><a href="#5-3-CRF-分析" class="headerlink" title="5.3 CRF++分析"></a><strong>5.3 CRF++分析</strong></h2><p>本来做task用CRF++跑过baseline,后来在对CRF做调研时，非常想透析CRF++的工作原理，以identify以及verify做的各种假设猜想。当然，也看过其他的CRF实现源码。</p><p>所以干脆写到这里来，结合CRF++实例讲解过程。</p><p>有一批语料数据，并且已经tokenized好了：</p><blockquote><p>Nuclear<br>theory<br>devoted<br>major<br>efforts<br>…… </p></blockquote><p>并且我先确定了13个标注元素：</p><blockquote><p>B_MAT<br>B_PRO<br>B_TAS<br>E_MAT<br>E_PRO<br>E_TAS<br>I_MAT<br>I_PRO<br>I_TAS<br>O<br>S_MAT<br>S_PRO<br>S_TAS </p></blockquote><p><strong>1. 定义模板</strong></p><p>按道理应该是定义特征函数才对吧？好的，在CRF++下，应该是先定义特征模板，然后用模板自动批量产生大量的特征函数。我之前也蛮confused的，用完CRF++还以为模板就是特征，后面就搞清楚了：每一条模板将在每一个token处生产若干个特征函数。</p><p>CRF++的模板（template）有U系列（unigram）、B系列(bigram)，不过我至今搞不清楚B系列的作用，因为U模板都可以完成2-gram的作用。</p><blockquote><p>U00:%x[-2,0]<br>U01:%x[-1,0]<br>U02:%x[0,0]<br>U03:%x[1,0]<br>U04:%x[2,0] </p><p>U05:%x[-2,0]/%x[-1,0]/%x[0,0]<br>U06:%x[-1,0]/%x[0,0]/%x[1,0]<br>U07:%x[0,0]/%x[1,0]/%x[2,0]<br>U08:%x[-1,0]/%x[0,0]<br>U09:%x[0,0]/%x[1,0] </p><p>B </p></blockquote><p>所以，U00 - U09 我定义了10个模板。</p><p><strong>2. 产生特征函数</strong></p><p>是的，会产生大量的特征。 U00 - U04的模板产生的是状态特征函数；U05 - U09的模板产生的是转移特征函数。</p><p>在CRF++中，每个特征都会try每个标注label（这里有13个），总共将生成 <img src="https://www.zhihu.com/equation?tex=N+%2A+L+%3D+i+%2A+k%5E%7B%27%7D+%2A+L" alt="[公式]"> 个特征函数以及对应的权重出来。N表示每一套特征函数 <img src="https://www.zhihu.com/equation?tex=N%3D+i+%2A+k%5E%7B%27%7D" alt="[公式]"> ，L表示标注集元素个数。</p><p>比如训练好的CRF模型的部分特征函数是这样存储的：</p><blockquote><p>22607 B<br>790309 U00:%<br>3453892 U00:%)<br>2717325 U00:&amp;<br>2128269 U00:’t<br>2826239 U00:(0.3534<br>2525055 U00:(0.593–1.118<br>197093 U00:(1)<br>2079519 U00:(1)L=14w2−12w−FμνaFaμν<br>2458547 U00:(1)δn=∫−∞En+1ρ˜(E)dE−n<br>1766024 U00:(1.0g<br>2679261 U00:(1.1wt%)<br>1622517 U00:(100)<br>727701 U00:(1000–5000A)<br>2626520 U00:(10a)<br>2626689 U00:(10b)<br>……<br>2842814 U07:layer/thicknesses/Using<br>2847533 U07:layer/thicknesses/are<br>2848651 U07:layer/thicknesses/in<br>331539 U07:layer/to/the<br>1885871 U07:layer/was/deposited<br>……（数量非常庞大） </p></blockquote><p>其实也就是对应了这样些个特征函数：</p><blockquote><p>func1 = if (output = B and feature=”U02:一”) return 1 else return 0<br>func2 = if (output = M and feature=”U02:一”) return 1 else return 0<br>func3 = if (output = E and feature=”U02:一”) return 1 else return 0<br>func4 = if (output = S and feature=”U02:一”) return 1 else return 0 </p></blockquote><p>比如模板U06会从语料中one by one逐句抽出这些各个特征：</p><blockquote><p>一/个/人/……<br>个/人/走/……</p></blockquote><p><strong>3. 求参</strong></p><p>对上述的各个特征以及初始权重进行迭代参数学习。</p><p>在CRF++ 训练好的模型里，权重是这样的：</p><blockquote><p>0.3972716048310705<br>0.5078838237171732<br>0.6715316559507898<br>-0.4198827647512405<br>-0.4233310655891150<br>-0.4176580083832543<br>-0.4860489836004728<br>-0.6156475863742051<br>-0.6997919485753300<br>0.8309956709647820<br>0.3749695682658566<br>0.2627347894057647<br>0.0169732441379157<br>0.3972716048310705<br>0.5078838237171732<br>0.6715316559507898<br>……（数量非常庞大，与每个label的特征函数对应，我这有300W个）</p></blockquote><p><strong>4. 预测解码</strong></p><p>结果是这样的：</p><blockquote><p>Nuclear B<em>TAStheory E</em>TAS<br>devoted O<br>major O<br>efforts O<br>…… </p></blockquote><h2 id="六、总结"><a href="#六、总结" class="headerlink" title="六、总结"></a><strong>六、总结</strong></h2><p><strong>1. 总体对比</strong></p><p>应该看到了熟悉的图了，现在看这个图的话，应该可以很清楚地get到他所表达的含义了。这张图的内容正是按照生成式&amp;判别式来区分的，NB在sequence建模下拓展到了HMM；LR在sequence建模下拓展到了CRF。</p><p><img src="https://pic1.zhimg.com/v2-376fd85a490e161978130ddd759244d4_b.jpg" alt="img"></p><p><strong>2. HMM vs. MEMM vs. CRF</strong></p><p>将三者放在一块做一个总结：</p><ol><li>HMM -&gt; MEMM： HMM模型中存在两个假设：一是输出观察值之间严格独立，二是状态的转移过程中当前状态只与前一状态有关。但实际上序列标注问题不仅和单个词相关，而且和观察序列的长度，单词的上下文，等等相关。MEMM解决了HMM输出独立性假设的问题。因为HMM只限定在了观测与状态之间的依赖，而MEMM引入自定义特征函数，不仅可以表达观测之间的依赖，还可表示当前观测与前后多个状态之间的复杂依赖。</li><li>MEMM -&gt; CRF:</li></ol><ul><li>CRF不仅解决了HMM输出独立性假设的问题，还解决了MEMM的标注偏置问题，MEMM容易陷入局部最优是因为只在局部做归一化，而CRF统计了全局概率，在做归一化时考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置的问题。使得序列标注的解码变得最优解。</li><li>HMM、MEMM属于有向图，所以考虑了x与y的影响，但没讲x当做整体考虑进去（这点问题应该只有HMM）。CRF属于无向图，没有这种依赖性，克服此问题。</li></ul><p><strong>3. Machine Learning models vs. Sequential models</strong></p><p>为了一次将概率图模型理解的深刻到位，我们需要再串一串，更深度与原有的知识体系融合起来。</p><p>机器学习模型，按照学习的范式或方法，以及加上自己的理解，给常见的部分的他们整理分了分类（主流上，都喜欢从训练样本的歧义型分，当然也可以从其他角度来）：</p><pre><code class="text">一、监督：{1.1 分类算法(线性和非线性)：{    感知机    KNN    概率{        朴素贝叶斯（NB）        Logistic Regression（LR）        最大熵MEM（与LR同属于对数线性分类模型）    }    支持向量机(SVM)    决策树(ID3、CART、C4.5)    assembly learning{        Boosting{            Gradient Boosting{                GBDT                xgboost（传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）；xgboost是Gradient Boosting的一种高效系统实现，并不是一种单一算法。）            }            AdaBoost        }           Bagging{            随机森林        }        Stacking    }    ……}1.2 概率图模型：{    HMM    MEMM（最大熵马尔科夫）    CRF    ……}1.3 回归预测：{    线性回归    树回归    Ridge岭回归    Lasso回归    ……}……  }二、非监督：{2.1 聚类：{    1. 基础聚类        K—mean        二分k-mean        K中值聚类        GMM聚类    2. 层次聚类    3. 密度聚类    4. 谱聚类()}2.2 主题模型:{    pLSA    LDA隐含狄利克雷分析}2.3 关联分析：{    Apriori算法    FP-growth算法}2.4 降维：{    PCA算法    SVD算法    LDA线性判别分析    LLE局部线性嵌入}2.5 异常检测：……}三、半监督学习四、迁移学习</code></pre><p>（注意到，没有把神经网络体系加进来。因为NNs的范式很灵活，不太适用这套分法，largely, off this framework）</p><p>Generally speaking，机器学习模型，尤其是有监督学习，一般是为一条sample预测出一个label，作为预测结果。 但与典型常见的机器学习模型不太一样，序列模型（概率图模型）是试图为一条sample里面的每个基本元数据分别预测出一个label。这一点，往往是beginner伊始难以理解的。</p><p>具体的实现手段差异，就是：ML models通过直接预测得出label；Sequential models是给每个token预测得出label还没完，还得将他们每个token对应的labels进行组合，具体的话，用viterbi来挑选最好的那个组合。</p><h2 id="over"><a href="#over" class="headerlink" title="over"></a><strong>over</strong></h2>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>HMM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_SLP_6_HMM&amp;HEMM&amp;CRF的比较</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_SLP_6_HMM&amp;HEMM&amp;CRF%E7%9A%84%E6%AF%94%E8%BE%83/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_SLP_6_HMM&amp;HEMM&amp;CRF%E7%9A%84%E6%AF%94%E8%BE%83/</url>
    
    <content type="html"><![CDATA[<h1 id="HMM，HEMM-CRF的比较"><a href="#HMM，HEMM-CRF的比较" class="headerlink" title="HMM，HEMM,CRF的比较"></a>HMM，HEMM,CRF的比较</h1><p>6.6.1)CRF的优点:</p><ul><li><p>CRF没有HMM 那样严格的独立性假设条件，因此可以容纳任意的上下文内容。特征设计灵活。—— 与HMM比较。</p></li><li><p>由于CRF计算全局最优输出节点的条件概率，它还克服了最大熵马尔科夫模型标记偏置的问题（label-bias）。  —— 与MEMM的比较</p></li><li><p>CRF是在给定需要标记的观察序列的条件下，计算整个标记序列的联合概率分布。而不是在给定当前状态的条件下，定义一个状态的状态分布。 ——与ME的比较</p></li></ul><p>  CRF缺点：</p><ul><li>训练代价大，复杂度高</li></ul><p>6.6.2）CRF解决的问题：</p><p>HMM模型中存在两个假设，一是输出观察值之间严格独立，二是状态转移过程中当前状态只与前一状态有关（一阶马尔科夫模型）。</p><p>MEMM模型克服了观察值之间严格独立产生的问题，但是由于状态之间的假设理论，使得该模型存在标注偏置问题。</p><p>CRF模型解决了标注偏置问题，去除了HMM中两个不合理的假设，当然，模型相应的变得更复杂了。</p><p>6.6.3）HMM模型，最大熵模型，CRF模型的优劣</p><p><strong>HMM模型</strong>将标注看做是马尔科夫链，一阶马尔科夫链式针对相邻标注的关系进行建模，其中每个标记对应一个概率函数。HMM是一种产生式模型，定义了联合概率分布，其中x和y分别表示观察序列和相对应的标注序列的随机变量。为了能够定义这种联合概率分布，产生式模型需要枚举出所有可能的观察序列。这在实际运算过程中其实非常困难。因为我们需要将观察序列的元素看成是彼此孤立的个体，即假设每个元素彼此独立，任何时刻的观察结果只依赖于该时刻的状态。</p><p>HMM模型的这个假设的前提在比较小的数据集上是合适的，但实际上在大量真实语料的观察序列中更多的是一种多重的交互特征形式表现，观察元素之间广泛存在的长程相关性。</p><p>在命名实体识别的任务中，由于实体本身结构所具有的复杂性，利用简单的特征函数往往无法覆盖所有的特性，这时HMM的假设前提使得它无法使用复杂特征（它无法使用多于一个标记的特征）。</p><p>最大熵模型可以使用任意的复杂相关特征，在性能上最大熵分类器超过了Byaes分类器，但是，作为一种分类器，这两种方法有一个共同的缺点：</p><p>即每个单词都是单独进行分类的，标记之间的关系无法得到充分利用。具有马尔科夫链的HMM模型可以建立标记之间的马尔科夫关联性，这是最大熵模型所没有的。</p><p><strong>最大熵模型的优点</strong>：首先，最大熵模型获得的是所有满足约束条件的模型中信息熵极大的模型；</p><p>其次，最大熵统计模型可以灵活的设置约束条件，通过约束条件的多少可以调节模型对未知数据的适应度和对已知数据的拟合程度；</p><p>再其次，最大熵模型自然地解决了统计模型中参数平滑的问题。</p><p><strong>最大熵模型的不足：</strong>首先，最大熵统计模型中二值化特征只是记录特征的出现与否，而文本分类需要知道特征的强度。因此，它在分类方法中不是最优的；</p><p>其次，由于算法的收敛速度较慢，导致最大熵模型的计算代价较大，时空开销大，再次，数据稀疏的问题也比较严重。</p><p><strong>最大熵模型马尔科夫模型</strong>把HMM模型和最大熵模型的优点集成到一个产生式模型，这个模型允许状态转移概率依赖序列中彼此之间非独立的特征上，从而将上下文信息引入到模型的学习和识别中，提高了识别的精确度，召回率也大大提高，有实验证明，这个新的模型在序列标注任务上表现的比HMM和无状态的最大熵模型要好的多。</p><p><strong>CRF模型</strong>的特点：首先，CRF在给定了观察序列的情况下，对整个序列的联合概率有一个统一的指数模型。一个比较吸引人的特性是其损失函数的凸面性。其次，条件随机模型相比较于改进的马尔科夫模型可以更好地利用待识别文本中所提供的上下文信息，以得到更好的实验结果。</p><p>条件随机域在中文组块识别方面有效，避免了严格的独立性假设和数据归纳偏置问题。条件随机域模型应用到了中文命名实体识别中，并根据中文的特点，定义了多种特征模块，并且有测试结果表明：在采用相同特征集合的条件下，条件随意域模型较其他概率模型有更好的性能表现。</p><p>再者，词性标注主要面临兼词类词义消歧以及未知词标注的难题，传统隐马尔科夫问题不易融合新特征，而最大熵马尔科夫模型上存在标注偏置问题。</p><p>CRF模型有很强的推理能力，并且能够使用复杂，有重叠性的，非独立的特征进行训练和推理，能够充分地利用上下文信息作为特征，还可以任意添加其外部数据特征。同时CRF解决了最大熵模型的标签偏置问题。</p><p>CRF和最大熵模型的本质区别是，最大熵模型在每个状态都有一个概率模型，在每个状态转移时都要进行归一化。如果某个状态只要一个后续状态，那么从该状态到后续状态的跳转概率为1，这样，不管输入为任何内容，他都要向后跳转。而CRF模型在所有的状态上建立一个统一的概率模型，这样在归一化时，即使某个状态只有一个后续状态，它到该后续状态的跳转概率也不会是1，从而解决了标注偏置的问题，因此，从理论上讲，CRF非常适合中文的词性标注。</p><p><strong>CRF模型的优点</strong>：首先，CRF模型由于其自身在结合多种特征方面的优势和避免了标记偏置问题。其次，CRF的性能更好，CRF对特征的融合能力也比较强。对于示例较小的时间类ME来说，CRF的识别效果明显高于ME的识别效果。</p><p><strong>CRF模型的不足：</strong>首先，通过基于CRF的结合多种特征的方法 做英语的命名实体识别时，发现在CRF的方法中，特征的选择和优化是影响结果的关键因素。特征选择的好与坏，直接决定了系统性能的好坏，其次，训练模型的时间比ME要长，而且获得的模型很大，在一般的PC机上无法运行。</p><p>在概率归一化的时候，CRF的归一化在模型上更加合理，但是在计算的时候可能导致计算量增加，而HMM的归一化会导致标注偏置的问题。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191228_1942_00_352.png" alt=""></p><p>正文：</p><p>一般可以从两个方面理解CRF,一个是图模型（逻辑回归的拓展），另一个是线性的CRF和HMM有相似的结构：分贝是判别式模型和生成式模型。</p><p>先从无向图说起：</p><p><strong>概率无向图的定义</strong></p><p>首先我们有无向图G = （V,E），V是节点，E是边，图G中每个节点V上都有一个随机变量y，这样所有的节点上的随机变量都构成一组随机变量Y，图G上有联合概率分布P（Y）,边e表示相邻节点的变量存在某种神秘的联系。</p><p>图G上的随机变量Y满足马尔科夫性，即两个不相邻的节点上的随机变量yi，yj条件独立。</p><p>这个模型的定义就这么简单，他又叫马尔科夫随机场。</p><p><strong>最大团模型定义</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191228_2034_21_542.png" alt=""></p><p>图中{Y1,Y2,Y3}和{Y3,Y2,Y4}是最大团，包含的任何节点都两两相连被称作团。最大团就是不能再添加节点。</p><p>然后呢，有个定理叫Hammersley-Clifford定理，给出了无向图模型P(Y)的公式。</p><p><strong>Hammersley-Clifford定理</strong><br>*概率无向图模型的联合概率分布P(Y)可以表示为如下形式：</p><p><img src="https://images2015.cnblogs.com/blog/865647/201703/865647-20170326220058205-293231570.png" alt="img"></p><p>- 条件随机场(conditional random field)<br>定义：（和上面的模型比较就是多了一个X。）<br><em>设X与Y是随机变量，P(Y|X)是给定条件X的条件下Y的条件概率分布，若随机变量Y构成一个由无向图G=(V,E)表示的马尔科夫随机场。则称条件概率分布P(X|Y)为条件随机场。</em><br>虽然定义里面没有要求，我们还是默认X和Y结构一致，这是general CRF，然后看看linear chain CRF，</p><p>线性链就是X和Y都是一串序列，线性链里面呢，<strong>最大团就是相邻的两项，y_i和y_i+1。</strong><br>由Hammersley-Clifford定理写出linear chain CRF的公式。</p><p>势函数取 对数线性，就得到了第一次见让本学渣云里雾里的公式。（懒得输了贴个图）</p><p>再详细点：</p><p>就是linear chain CRF常见的两种特征函数指数和的形式。<br>注意点！！！高潮来了！！如果我们把上式中的特征函数<img src="https://www.zhihu.com/equation?tex=t_%7Bk%7D" alt="t_{k}">)去掉，得到就是自变量X关于Y的logistic回归（加上一个normalizer函数Z(x)），每个Y和X之间对数线性关系。</p><p>本学渣看到这里的时候真是amazing了一下。</p><p>好了，那么是不是可以说linear chain CRF是logistic回归，再加上了有关相邻项某种神秘联系的参数呢？看起来是这样的，我也不敢确定= =、、<br>之后呢，再从HMM的角度看。</p><p>- HMM和linear chain CRF</p><p>HMM的概率分布可以写成这样的形式：</p><p>右边取对数变成和的形式，再加上归一化的Z(x) 得到</p><p>嗯，这样一看就和前面的CRF长的很像了，就是一个是条件概率，一个是联合概率，<br>这也是discriminative model和generative model的区别。<br>注意Z(x)是遍历所有y的全局归一化，写在乘积符号里面的是local归一化，得到的是MEMM。<br>其实generative和discriminative的差别是很大的，因为假设不一样，结果和参数训练的方法都不同，</p><p>线性的CRF不需要EM算法，稍微简单一些，最大似然训练集之后，梯度下降加上vertebi算法就可以了。</p><p>嗯，所以说我们就是从这两条路走到了线性的CRF，general的CRF也是从MRF来的，公式是最大团的乘积形式，计算上麻烦一些，会用到loopy belief propagation。</p><p><strong>——————————————————————我是华丽丽的分割线———————————————————————————————————</strong></p><p>来来来，这两天正好在复习CRF，我从头给你说。。。</p><p><strong>模型——</strong><br><em>首先什么是随机场呢，一组随机变量，他们样本空间一样，那么就是随机场。当这些随机变量之间有依赖关系的时候，对我们来说才是有意义的。</em></p><p>我们利用这些随机变量之间的关系建模实际问题中的相关关系，实际问题中我们可能只知道这两个变量之间有相关关系，但并不知道具体是多少，我们想知道这些依赖关系具体是什么样的，于是就把相关关系图画出来，然后通过实际数据训练去把具体的相关关系训练出来嵌入到图里，然后用得到的这个图去进行预测、去进行reference等很多事情。</p><p><em>那么为了简化某些为问题来说，也为了这个图画出来能用，我们会在画图的时候要遵循一些假设和规则，比如马尔科夫独立性假设。**按照这个假设和规则来画图，画出来的图会满足一系列方便的性质便于使用。</em></p><p>马尔可夫独立性假设是说：对一个节点，在给定他所连接的所有节点的前提下，他与外接是独立的。就是说如果你观测到了这个节点直接连接的那些节点的值的话，那他跟那些不直接连接他的点就是独立的。形式上，我们是想把他设计成这个样子的，边可以传递信息，点与点之间通过边相互影响，如果观测到一个节点的取值或者这个节点的取值是常量，那么别的节点就无法通过这个节点来影响其他节点。所以对一个节点来说，如果用来连接外界的所有节点都被锁住了，那他跟外界就无法传递信息，就独立了。这比贝叶斯网络就直观多了，贝叶斯网络要判断两点之间独立还要看有没有v-structure，还要看边的指向。</p><p><em>呐，满足马尔可夫独立性的随机场，就叫马尔可夫随机场。它不仅具有我刚才说的那些性质，除此之外，还等价于吉布斯分布。</em></p><p>这些边具体是如何建模的呢，以什么形式记录这些概率信息的？贝叶斯网络每一条边是一个条件概率分布，P(X|Y)，条件是父节点、结果是子节点。他有一个问题，就是当我知道A、B、C三个变量之间有相关关系，但是不知道具体是谁依赖谁，或者</p><p>我不想先假设谁依赖谁，这个时候贝叶斯就画不出来图了</p><p>。因为</p><p>贝叶斯网络是通过变量之间的条件分布来建模整个网络的，相关关系是通过依赖关系（条件分布）来表达的</p><p>。而马尔可夫随机场是这样，</p><p>我不想知道这三个变量间到底是谁依赖谁、谁是条件谁是结果，我只想用联合分布直接表达这三个变量之间的关系</p><p>。比如说两个变量A、B，这两个变量的联合分布是：</p><pre><code class="text">|     A, B     | P(A, B) ||--------------+---------|| A = 0, B = 0 |   100   || A = 0, B = 1 |    10   || A = 1, B = 0 |    20   || A = 1, B = 1 |   200   |</code></pre><p>这个分布表示，这条边的功能是使它连接的两点（A和B）趋同，当A = 0的时候B更可能等于0不太可能等于1，当A = 1的时候B更可能等于1不太可能等于0。这样一来你知道了三个变量之间的联合分布，那他们两两之间的条件分布自然而然就在里面。</p><p>这样出来的图是等价于吉布斯分布的，就是说，你可以只在每个最大子团上定义一个联合分布（而不需要对每个边定义一个联合分布），整个图的联合概率分布就是这些最大子团的联合概率分布的乘积。当然这里最大子团的联合概率并不是标准的联合概率形式，是没归一化的联合概率，叫factor（因子），整个图的联合概率乘完之后下面再除一个归一化因子和就归一化了，最终是一个联合概率，每个子团记载的都是因子，是没归一化的概率，严格大于零，可以大于一。但关键是依赖关系、这些相关关系已经encode在里面了。</p><p><em>这是马尔科夫随机场。</em></p><p>条件随机场是指这个图里面一些点我已经观测到了，求，在我观测到这些点的前提下，整张图的分布是怎样的。就是given观测点，你去map inference也好你去做之类的事情，你可能不求具体的分布式什么。这里还要注意的是，<strong>马尔科夫随机场跟贝叶斯网络一样都是产生式模型，条件随机场才是判别式模型。</strong></p><p><em>这是条件随机场，NER（命名实体识别）这个任务用到的是线性链条件随机场。</em></p><p>线性链条件随机场的形式是这样的，观测点是你要标注的这些词本身和他们对应的特征，例如说词性是不是专有名词、语义角色是不是主语之类的。隐节点，是这些词的标签，比如说是不是人名结尾，是不是地名的开头这样。这些隐节点（就是这些标签），依次排开，相邻的节点中间有条边，跨节点没有边（线性链、二阶）。然后所有观测节点（特征）同时作用于所有这些隐节点（标签）。至于观测节点之间有没有依赖关系，这些已经不重要了，因为他们已经被观测到了，是固定的。</p><p><em>这是线性链条件随机场的形式。</em></p><p>呐，这些特征是怎么表达的呢？是这样，他有两种特征，一种是转移特征，就是涉及到两个状态之间的特征。另一种就是简单的状态特征，就是只涉及到当前状态的特征。特征表达形式比较简单，就是你是否满足我特征所说的这个配置，是就是1，不是就是。比如说，上一个状态是地名的中间，且当前词是’国’（假设他把中国分词 拆成两个了），且当前词的词性是专有名词、且上一个词的词性也是专有名词，如果满足这个配置、输出就是1、不满足就输出0。然后这些特征每个都有一个权 重，我们最后要学的就是这些权重。特征跟权重乘起来再求和，外面在套个exp，出来就是这个factor的形式。这是一个典型的对数线性模型的表达方式。这种表达方式非常常见，有很多好处，比如为什么要套一个exp呢？一方面，要保证每一个factor是正的，factor可以大于一也可以不归一化，但一定要是正的。另一方面，我们最后要通过最大似然函数优化的，似然值是这些 factor的累乘，对每一个最大子团累乘。这么多项相乘没有人直接去优化的，都是取log变成对数似然，然后这些累乘变成累加了嘛，然后优化这个累加。无论是算梯度用梯度下降，还是另导数为零求解析解都很方便了（这个表达形态下的目标函数是凸的）。你套上exp之后，再取对数，那么每个因子就变成一堆特征乘权重的累积，然后整个对数似然就是三级累积，对每个样本、每个团、每个特征累积。这个形式就很有利了，你是求导还是求梯度还是怎样，你面对的就是一堆项的和，每个和是一个1或者一个0乘以一个 权重。当然后面还要减一个log(Z)，不过对于map inference来说，给定Z之后log(Z)是常量，优化可以不带这一项。</p><p><strong>推断——</strong><br><strong>线性链的条件随机场跟线性链的隐马尔科夫模型一样，一般推断用的都是维特比算法。这个算法是一个最简单的动态规划。</strong></p><p>首先我们推断的目标是给定一个X，找到使P(Y|X)最大的那个Y嘛。然后这个Z(X)，一个X就对应一个Z，所以X固定的话这个项是常量，优化跟他没关系（Y的取值不影响Z）。然后 exp也是单调递增的，也不带他，直接优化exp里面。所以最后优化目标就变成了里面那个线性和的形式，就是对每个位置的每个特征加权求和。比如说两个状态的话，它对应的概率就是从开始转移到第一个状态的概率加上从第一个转移到第二个状态的概率，这里概率是只exp里面的加权和。那么这种关系下就可以用维特比了，首先你算出第一个状态取每个标签的概率，然后你再计算到第二个状态取每个标签得概率的最大值，这个最大值是指从状态一哪个标签转移到这个标签的概率最大，值是多 少，并且记住这个转移（也就是上一个标签是啥）。然后你再计算第三个取哪个标签概率最大，取最大的话上一个标签应该是哪个。以此类推。整条链计算完之后， 你就知道最后一个词去哪个标签最可能，以及去这个标签的话上一个状态的标签是什么、取上一个标签的话上上个状态的标签是什么，酱。这里我说的概率都是 exp里面的加权和，因为两个概率相乘其实就对应着两个加权和相加，其他部分都没有变。<br><strong>学习——</strong><br>这是一个典型的无条件优化问题，基本上所有我知道的优化方法都是<strong>优化似然函数</strong>。典型的就是梯度下降及其升级版（牛顿、拟牛顿、BFGS、L-BFGS），</p><p>这里版本最高的就是L-BFGS了吧，所以一般都用L-BFGS。除此之外<strong>EM算法也可以优化这个问题</strong>。</p><p><strong>——————————————————————我是华丽丽的分割线———————————————————————————————————</strong></p><p>基本符号：Y是序列的标注，X是序列的特征</p><p>两者的建模方式不同：</p><ul><li>对于CRF，直接用最大熵准则建模p(Y|X)的概率。</li><li>而HMM，是在做了markov假设下去建模p(Y,X)（即一切观察量的联合概率分布）。</li></ul><p>这里借用</p><p>@li Eta</p><p>同学答案里的公式，我把i变成t，要注意：</p><ol><li><p>CRF是用最大熵准则建模p(Y|X)，从而得到一个指数归一化的函数形式，但是分子上应该是exp(sum_i{w_i<em>f_i(Y,X)}) 而不是f_i(yt,yt+1,X). 把f(Y,X)变到f(yt,yt+1,X)其实是做了一个markov假设的CRF，*</em>而不是General形式的CRF**. 当然实际应用中，我们甚至把特征简化为只能是f(yt,X)和f(yt,yt+1)（注意此时f里面没X了)，进一步减小特征数和加快训练和推断速度。这种叫linear chain CRF。</p></li><li><p>HMM是生成模型，而</p><p><a href="https://www.cnblogs.com/people/e3f97a56de92dafc0e4740042cc06bd9" target="_blank" rel="noopener">@li Eta</a></p><p>同学贴的实际上是MEMM(最大熵马尔科夫模型),它建模的也是p(Y|X), 而不是一般我们讲的用来建模p(Y,X)的HMM生成模型。HMM生成模型，求和号里是p(yt|yt-1)p(xt|yt)而不是p(yt|yt-1,X).此时p(yt|yt-1)一般就是个多项分布，而p(xt|yt)则可以是任意你用来建模世界的分布。把</p><p><a href="https://www.cnblogs.com/people/e3f97a56de92dafc0e4740042cc06bd9" target="_blank" rel="noopener">@li Eta</a></p><p>同学里面的MEMM的X到Y的箭头倒过来指（变成Y指向X)，才是HMM.</p></li></ol><p>CRF: 最大熵准则建模条件概率<br>HMM:假设出变量间的概率分布，建模所有观察到的变量的联合分布。在Y变量间做了markov假设。</p><p>再次注意CRF跟markov没关系！linear chain CRF才和markov有关。</p><p>而linear chain CRF和MEMM的分母在求和号里面外面的区别，并不是CRF和HMM的区别。<br>至于CRF和HMM，要先把CRF约束成linear chain CRF，然后linear chain CRF和HMM的区别：<strong>是判别式模型和生成模型的区别，是函数拟合和概率模型的区别。</strong></p><p><em>这里再多说点，HMM叫hidden markov model， markov上面已经说了，而hidden是什么意思呢？在上面的例子里，Y是序列的标注，是可见的观察量。但是HMM的标准介绍里面，Y是未知量，HMM建模的是p(X) = sum_Y P(Y,X) 而不是 P(Y,X),注意要对Y求和的。搞语音识别的同学一般接触的是这个HMM，这个是带有hidden的真身，而搞自然语言处理的同学接触的多是Y是可见量的那个HMM，但其实是阉割版的HMM，因为根本没有hidden的变量啊，没有了hidden变量的hmm就不需要EM来训练，很不好玩的。学hmm的时候不学EM，很可惜，EM是ML领域最神奇的算法。</em></p><p><em>CRF叫condition random field，为啥这么叫呢，我猜呢，是一般无向图模型大家就叫markov random field，CRF呢是个二部图，一部分是Y一部分是X，最后建模的是P(Y|X)是个条件概率，所以叫condition random field. 我就这么一猜，确实有点不负责任，这个名字的来源我一直么研究过，这段要是说错了请见谅。</em></p><p><strong>——————————————————————我是华丽丽的分割线———————————————————————————————————</strong></p><p>贝叶斯是生成模型，<strong>并且假设变量之间相互独立。</strong>那么对于像NLP中NER这样的任务是肯定不行的。</p><p>HMM的出现，拓展了贝叶斯的图关系，model了观测变量的markov性，但是，始终表达能力不够，没有context。</p><p>CRF不model输出与单个变量之间的关系了，<strong>而是与LR类似，**</strong>model y 与 所有的向量x1，x2,x3,x4…..xn之间的关系，**</p><p>加上图的local function的表达能力使得context信息和feature扩展能力变强。于是CRF得到了很不错的应用。</p><p> <img src="https://images2015.cnblogs.com/blog/865647/201703/865647-20170326222700549-1918005795.png" alt="img"></p><p>之前学习过隐马尔可夫模型的话，我们知道它是一个时间序列模型。起初对这个【时间】不以为然，隐马尔可夫模型中的节点，不就是一个个状态么，何必叫时间序列模型，还不如更名为状态序列模型。</p><p>学到条件随机场，发现了HMM和CRF的一个显著区别，即节点与节点之间的边，<strong>HMM是有向的，而CRF是无向的。</strong></p><p>而时间序列，能很好的表达<strong>当前状态与之前状态有关而和后续状态无关这一特性</strong>，即在图中的有向性，因此时间序列模型相比状态模型更合适。</p><p><strong>而CRF则可以成为一个HMM的扩展，称为状态序列模型更合适，从【有向边】升级到【无向边】。</strong></p><p>CRF只需要考虑当前观测状态的特性，不像HMM有严格的独立性的要求。<br>从图的角度来看，就是观测序列的元素之间并不存在图结构。<br>从建立模型的角度来看，CRF只考虑观测序列<strong>X整体</strong>，而HMM是将<strong>每一个时刻的状态都考虑在内，</strong>并且对<strong>观测序列做出了马尔科夫独立性假设</strong>。<br>正是因为<strong>CRF不做独立性假设，这就是“条件随机”的含义。</strong></p>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>HMM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_SLP_6_HMM_马尔科夫链</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_SLP_6_HMM_%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E9%93%BE/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_SLP_6_HMM_%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E9%93%BE/</url>
    
    <content type="html"><![CDATA[<h1 id="马尔科夫链"><a href="#马尔科夫链" class="headerlink" title="马尔科夫链"></a>马尔科夫链</h1><p>马尔可夫链是一个模型，<strong>告诉我们关于随机变量序列的概率，状态。每个状态都可以从某个集合中获取值</strong>。 这些集合可以是单词，标签或代表任何东西的符号，例如天气。 下图左边是天气转换，右边是生成句子的单词序列。</p><p><img src="https://pic4.zhimg.com/v2-1f6f4ea4278bcceab330ad4f269dbc2b_b.jpg" alt="img"></p><p>考虑状态变量 <img src="https://www.zhihu.com/equation?tex=q_1%2Cq_2%2C...%2Cq_i" alt="[公式]"> ,HMM核心假设是:<strong>当预测未来时，过去并不重要，重要的是现在</strong>。</p><p><img src="https://pic2.zhimg.com/v2-f6c356f8473cba19ddc47e666a3a0ee9_b.png" alt="img"></p><p>上图显示了一个马尔可夫链，用于为一系列天气事件分配概率，其中词汇表包括HOT, COLD,和 WARM，状态表示为图中的节点，转换以其概率表示为边。 <strong>转换是概率：离开给定状态的弧的值必须总和为1</strong>。上图右边用于为单词序列 <img src="https://www.zhihu.com/equation?tex=w_1%2C...%2Cw_n" alt="[公式]"> 分配一个概率，他其实是一个bigram的语言模型，每条边表示概率 <img src="https://www.zhihu.com/equation?tex=p%28w_i%7Cw_j%29" alt="[公式]"> 。</p><p>在形式上，马尔可夫链由以下组件指定:</p><p><img src="https://pic3.zhimg.com/v2-ae800f6388fec4020030f6f9924538ca_b.jpg" alt="img"></p><ol><li><img src="https://www.zhihu.com/equation?tex=Q+%3D+q_%7B1%7D%2Cq_%7B2%7D+...q_%7Bn%7D" alt="[公式]"> ，N个状态的集合</li><li>A = <img src="https://www.zhihu.com/equation?tex=a_%7B11%7D%2Ca_%7B12%7D+...a_%7Bn1%7D+...a_%7Bnn%7D" alt="[公式]"> ,一个转移概率矩阵A，每个 <img src="https://www.zhihu.com/equation?tex=a_%7Bij%7D" alt="[公式]"> 代表从状态i移动到状态j的概率 <img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bj%3D1%7D%5E%7Bn%7D%7Ba_%7Bij%7D%3D1%7D" alt="[公式]"> , <img src="https://www.zhihu.com/equation?tex=%5Cforall+i" alt="[公式]"> </li><li><img src="https://www.zhihu.com/equation?tex=%5Cpi+%3D+%5Cpi_1%2C%5Cpi_2%2C...%2C%5Cpi_N" alt="[公式]"> ，状态上的初始概率分布(某一时刻的所有可能的隐藏状态的分布(可能性))。是马尔可夫链从状态i开始的概率。一些状态j可能有 <img src="https://www.zhihu.com/equation?tex=%5Cpi_j+%3D+0" alt="[公式]"> ，这意味着它们不可能是初始状态。 <img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7B%5Cpi+_%7Bi%7D%3D1%7D" alt="[公式]"> </li></ol><p><strong>马尔科夫链模型的状态转移矩阵收敛到的稳定概率分布与我们的初始状态概率分布无关：</strong>如果我们得到了这个稳定概率分布对应的马尔科夫链模型的状态转移矩阵，则我们可以用任意的概率分布样本开始，<strong>带入马尔科夫链模型的状态转移矩阵</strong>，这样经过一些序列的转换，最终就可以得到符合对应稳定概率分布的样本。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191226_1614_48_682.png" alt=""></p><p>使用图中的样本概率( <img src="https://www.zhihu.com/equation?tex=%5Cpi+%3D+%5B0.1%2C0.7%2C0.2%5D" alt="[公式]">) )计算下列每一个序列的概率:</p><pre><code class="text">(8.5) hot hot hot hot（0.1*0.1*0.1*0.1）(8.6) cold hot cold hot（0.7*0.1*0.7*0.1）</code></pre><pre><code>小结：1.马尔科夫链假设，预测的东西仅仅和现在的状态有关，和过去的一切状态无关2.形态上看，和FST无区别</code></pre>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>马尔科夫链</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_SLP_6_EM算法</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_SLP_6_EM%E7%AE%97%E6%B3%95/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_SLP_6_EM%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h1><h2 id="2-1-基础知识"><a href="#2-1-基础知识" class="headerlink" title="2.1 基础知识"></a>2.1 基础知识</h2><h4 id="2-1-1-凸函数"><a href="#2-1-1-凸函数" class="headerlink" title="2.1.1 凸函数"></a>2.1.1 凸函数</h4><p>设是定义在实数域上的函数，如果对于任意的实数，都有：<br> <img src="https://www.zhihu.com/equation?tex=f%27%27+%5Cge0+%5C%5C" alt="[公式]"><br>那么是凸函数。若不是单个实数，而是由实数组成的向量，此时，如果函数的 Hesse 矩阵是半正定的，即<br> <img src="https://www.zhihu.com/equation?tex=H%27%27+%5Cge+0+%5C%5C" alt="[公式]"><br>是凸函数。特别地，如果 <img src="https://www.zhihu.com/equation?tex=f%27%27+%3E+0" alt="[公式]"> 或者  <img src="https://www.zhihu.com/equation?tex=H%27%27+%3E+0" alt="[公式]"> ，称为严格凸函数。</p><h4 id="2-1-2-Jensen不等式"><a href="#2-1-2-Jensen不等式" class="headerlink" title="2.1.2 Jensen不等式"></a>2.1.2 Jensen不等式</h4><p>如下图，如果函数 <img src="https://www.zhihu.com/equation?tex=f" alt="[公式]"> 是凸函数， <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 是随机变量，有 0.5 的概率是 a，有 0.5 的概率是 b， <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]"> 的期望值就是 a 和 b 的中值了那么：<br> <img src="https://www.zhihu.com/equation?tex=E%5Bf%28x%29%5D+%5Cge+f%28E%28x%29%29+%5C%5C" alt="[公式]"><br>其中，<img src="https://www.zhihu.com/equation?tex=E%5Bf%28x%29%5D+%3D+0.5f%28a%29+%2B+0.5+f%28b%29%EF%BC%8Cf%28E%28x%29%29+%3D+f%280.5a+%2B+0.5b%29" alt="[公式]"> ，这里 a 和 b 的权值为 0.5,  <img src="https://www.zhihu.com/equation?tex=f%28a%29" alt="[公式]">  与 a 的权值相等，<img src="https://www.zhihu.com/equation?tex=f%28b%29" alt="[公式]"> 与 b 的权值相等。</p><p>特别地，如果函数 <img src="https://www.zhihu.com/equation?tex=f" alt="[公式]">  是严格凸函数，当且仅当： <img src="https://www.zhihu.com/equation?tex=p%28x+%3D+E%28x%29%29+%3D+1" alt="[公式]">  (即随机变量是常量) 时等号成立。</p><p><img src="https://pic1.zhimg.com/v2-22d1d68bb9db46d48c1a4c194477427c_b.jpg" alt="img"></p><p>注：若函数  <img src="https://www.zhihu.com/equation?tex=f" alt="[公式]">  是凹函数，Jensen不等式符号相反。</p><h4 id="2-1-3-期望"><a href="#2-1-3-期望" class="headerlink" title="2.1.3 期望"></a>2.1.3 期望</h4><p>对于离散型随机变量 X 的概率分布为  <img src="https://www.zhihu.com/equation?tex=p_i+%3D+p%5C%7BX%3Dx_i%5C%7D" alt="[公式]"> ，数学期望 <img src="https://www.zhihu.com/equation?tex=E%28X%29" alt="[公式]">  为：<br> <img src="https://www.zhihu.com/equation?tex=E%28X%29+%3D+%5Csum+%5Climits+_i+x_ip_i+%5C%5C" alt="[公式]"></p><p><img src="https://www.zhihu.com/equation?tex=p_i" alt="[公式]"> 是权值，满足两个条件 <img src="https://www.zhihu.com/equation?tex=1+%5Cge+p_i+%5Cge+0%EF%BC%8C%5Csum+%5Climits+_i+p_i+%3D+1" alt="[公式]">  。</p><p>若连续型随机变量X的概率密度函数为 <img src="https://www.zhihu.com/equation?tex=f%28x%29" alt="[公式]"> ，则数学期望 <img src="https://www.zhihu.com/equation?tex=E%28X%29" alt="[公式]"> 为：<br> <img src="https://www.zhihu.com/equation?tex=E%28X%29+%3D+%5Cint+_+%7B-%5Cinfty%7D+%5E%7B%2B%5Cinfty%7D+xf%28x%29+dx+%5C%5C" alt="[公式]"><br>设 <img src="https://www.zhihu.com/equation?tex=Y+%3D+g%28X%29" alt="[公式]">， 若 <img src="https://www.zhihu.com/equation?tex=X" alt="[公式]"> 是离散型随机变量，则：<br> <img src="https://www.zhihu.com/equation?tex=E%28Y%29+%3D+%5Csum+%5Climits+_i+g%28x_i%29p_i+%5C%5C" alt="[公式]"><br>若  <img src="https://www.zhihu.com/equation?tex=X" alt="[公式]">  是连续型随机变量，则：<br> <img src="https://www.zhihu.com/equation?tex=E%28X%29+%3D+%5Cint+_+%7B-%5Cinfty%7D+%5E%7B%2B%5Cinfty%7D+g%28x%29f%28x%29+dx+%5C%5C" alt="[公式]"> </p><h2 id="2-2-EM算法的推导"><a href="#2-2-EM算法的推导" class="headerlink" title="2.2 EM算法的推导"></a>2.2 EM算法的推导</h2><p>对于 <img src="https://www.zhihu.com/equation?tex=m" alt="[公式]"> 个相互独立的样本 <img src="https://www.zhihu.com/equation?tex=x%3D%28x%5E%7B%281%29%7D%2Cx%5E%7B%282%29%7D%2C...x%5E%7B%28m%29%7D%29" alt="[公式]"> ，对应的隐含数据 <img src="https://www.zhihu.com/equation?tex=z%3D%28z%5E%7B%281%29%7D%2Cz%5E%7B%282%29%7D%2C...z%5E%7B%28m%29%7D%29" alt="[公式]"> ，此时 <img src="https://www.zhihu.com/equation?tex=%28x%2Cz%29" alt="[公式]"> 即为完全数据，样本的模型参数为 <img src="https://www.zhihu.com/equation?tex=%CE%B8" alt="[公式]"> , 则观察数据 <img src="https://www.zhihu.com/equation?tex=x%5E%7B%28i%29%7D" alt="[公式]"> 的概率为  <img src="https://www.zhihu.com/equation?tex=P%28x%5E%7B%28i%29%7D%7C%5Ctheta%29" alt="[公式]"> ，完全数据 <img src="https://www.zhihu.com/equation?tex=%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%29" alt="[公式]"> 的似然函数为 <img src="https://www.zhihu.com/equation?tex=P%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%7C%5Ctheta%29" alt="[公式]"> 。</p><p>假如没有隐含变量 <img src="https://www.zhihu.com/equation?tex=z" alt="[公式]">，我们仅需要找到合适的 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 极大化对数似然函数即可：<br> <img src="https://www.zhihu.com/equation?tex=%5Ctheta+%3Darg+%5Cmax+%5Climits_%7B%5Ctheta%7DL%28%5Ctheta%29+%3D+arg+%5Cmax+%5Climits_%7B%5Ctheta%7D%5Csum%5Climits_%7Bi%3D1%7D%5Em+logP%28x%5E%7B%28i%29%7D%7C%5Ctheta%29+%5C%5C" alt="[公式]"> </p><p>增加隐含变量 <img src="https://www.zhihu.com/equation?tex=z" alt="[公式]"> 之后，我们的目标变成了找到合适的 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=z" alt="[公式]"> 让对数似然函数极大<em>：</em><br> <img src="https://www.zhihu.com/equation?tex=%5Ctheta%2C+z+%3D+arg+%5Cmax+%5Climits_%7B%5Ctheta%2Cz%7DL%28%5Ctheta%2C+z%29+%3D+arg+%5Cmax+%5Climits_%7B%5Ctheta%2Cz%7D%5Csum%5Climits_%7Bi%3D1%7D%5Em+log%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29+%5C%5C" alt="[公式]"> </p><p>不就是多了一个隐变量 <img src="https://www.zhihu.com/equation?tex=z" alt="[公式]"> 吗？那我们自然而然会想到分别对未知的 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=z" alt="[公式]"> 分别求偏导，这样做可行吗？</p><p>理论上是可行的，然而如果对分别对未知的 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=z" alt="[公式]"> 分别求偏导，由于<img src="https://www.zhihu.com/equation?tex=+logP%28x%5E%7B%28i%29%7D%7C%5Ctheta%29" alt="[公式]"> 是 <img src="https://www.zhihu.com/equation?tex=P%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29" alt="[公式]"> 边缘概率(建议没基础的同学网上搜一下边缘概率的概念)，转化为 <img src="https://www.zhihu.com/equation?tex=+logP%28x%5E%7B%28i%29%7D%7C%5Ctheta%29" alt="[公式]"> 求导后形式会非常复杂（可以想象下 <img src="https://www.zhihu.com/equation?tex=log%28f_1%28x%29%2B+f_2%28x%29%2B%E2%80%A6" alt="[公式]">)复合函数的求导) ，所以很难求解得到 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=z" alt="[公式]"> 。那么我们想一下可不可以将加号从 log 中提取出来呢？我们对这个式子进行缩放如下：  <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%5Csum%5Climits_%7Bi%3D1%7D%5Em+log%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29+%26+%3D+%5Csum%5Climits_%7Bi%3D1%7D%5Em+log%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D+%5Ctag%7B1%7D+%5C%5C+%26+%5Cgeq+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29log%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D+%5Ctag%7B2%7D+%5Cend%7Balign%7D" alt="[公式]"> </p><p>上面第(1)式引入了一个未知的新的分布 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]">，满足：</p><p><img src="https://www.zhihu.com/equation?tex=%5Csum+%5Climits+_z+Q_i%28z%29%3D1%2C0+%5Cle+Q_i%28z%29%5Cle+1+%5C%5C" alt="[公式]"> </p><p>第(2)式用到了 Jensen 不等式 (对数函数是凹函数)：</p><p> <img src="https://www.zhihu.com/equation?tex=log%28E%28y%29%29+%5Cge+E%28log%28y%29%29+%5C%5C" alt="[公式]"><br>其中：</p><p><img src="https://www.zhihu.com/equation?tex=E%28y%29+%3D+%5Csum%5Climits_i%5Clambda_iy_i%2C+%5Clambda_i+%5Cgeq+0%2C+%5Csum%5Climits_i%5Clambda_i+%3D1+" alt="[公式]"> </p><p><img src="https://www.zhihu.com/equation?tex=y_i+%3D+%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D" alt="[公式]"> </p><p><img src="https://www.zhihu.com/equation?tex=%5Clambda_i+%3D+Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]"> </p><p>也就是说 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D" alt="[公式]"> 为第 i 个样本<em>，</em> <img src="https://www.zhihu.com/equation?tex=+Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]"> 为第 i 个样本对应的权重，那么：</p><p><img src="https://www.zhihu.com/equation?tex=E%28log%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D%29+%3D+%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29+log%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D+%5C%5C" alt="[公式]"> </p><p>上式我实际上是我们构建了 <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%2C+z%29" alt="[公式]"> 的下界，我们发现实际上就是 <img src="https://www.zhihu.com/equation?tex=log%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D" alt="[公式]"> 的加权求和，由于上面讲过权值 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]"> 累积和为1，因此上式是 <img src="https://www.zhihu.com/equation?tex=log%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D" alt="[公式]"> 的加权平均，也是我们所说的期望，<strong>这就是Expectation的来历啦</strong>。下一步要做的就是寻找一个合适的 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%29" alt="[公式]"> 最优化这个下界(M步)。</p><p>假设 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 已经给定，那么 <img src="https://www.zhihu.com/equation?tex=logL%28%5Ctheta%29" alt="[公式]"> 的值就取决于 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%29" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=+p%28x%5E%7B%28i%29%7D%2Cz%5E%7B%28i%29%7D%29" alt="[公式]"> 了。我们可以通过调整这两个概率使下界逼近 <img src="https://www.zhihu.com/equation?tex=logL%28%5Ctheta%29" alt="[公式]"> 的真实值，当不等式变成等式时，说明我们调整后的下界能够等价于<img src="https://www.zhihu.com/equation?tex=logL%28%5Ctheta%29" alt="[公式]"> 了。由 Jensen 不等式可知，等式成立的条件是随机变量是常数，则有：  <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D+%3Dc+%5C%5C" alt="[公式]"><br>其中 c 为常数，对于任意 <img src="https://www.zhihu.com/equation?tex=i" alt="[公式]">，我们得到：<br> <img src="https://www.zhihu.com/equation?tex=%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D+%3Dc%7BQ_i%28z%5E%7B%28i%29%7D%29%7D+%5C%5C" alt="[公式]"><br>方程两边同时累加和：<br> <img src="https://www.zhihu.com/equation?tex=%5Csum%5Climits_%7Bz%7D+%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D+%3D+c%5Csum%5Climits_%7Bz%7D+%7BQ_i%28z%5E%7B%28i%29%7D%29%7D+%5C%5C" alt="[公式]"><br>由于 <img src="https://www.zhihu.com/equation?tex=%5Csum%5Climits_%7Bz%7DQ_i%28z%5E%7B%28i%29%7D%29+%3D1" alt="[公式]">。 从上面两式，我们可以得到：<br> <img src="https://www.zhihu.com/equation?tex=%5Csum%5Climits_%7Bz%7D+%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D+%3D+c+%5C%5C" alt="[公式]"> </p><p><img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29+%3D+%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7Bc%7D+%3D+%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7B%5Csum%5Climits_%7Bz%7DP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D+%3D+%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7BP%28x%5E%7B%28i%29%7D%7C%5Ctheta%29%7D+%3D+P%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%29+%5C%5C" alt="[公式]"> </p><p>其中：</p><p>边缘概率公式： <img src="https://www.zhihu.com/equation?tex=P%28x%5E%7B%28i%29%7D%7C%5Ctheta%29+%3D+%5Csum%5Climits_%7Bz%7DP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29" alt="[公式]"> </p><p>条件概率公式： <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7BP%28x%5E%7B%28i%29%7D%7C%5Ctheta%29%7D+%3D+P%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%29" alt="[公式]"> </p><p>从上式可以发现 <img src="https://www.zhihu.com/equation?tex=Q%28z%29" alt="[公式]">是已知样本和模型参数下的隐变量分布。</p><p>如果 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29+%3D+P%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%29%29" alt="[公式]"> , 则第 (2) 式是我们的包含隐藏数据的对数似然的一个下界。如果我们能极大化这个下界，则也在尝试极大化我们的对数似然。即我们需要极大化下式：  <img src="https://www.zhihu.com/equation?tex=arg+%5Cmax+%5Climits_%7B%5Ctheta%7D+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29log%5Cfrac%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D%7BQ_i%28z%5E%7B%28i%29%7D%29%7D+%5C%5C" alt="[公式]"> </p><p>至此，我们推出了在固定参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]">后分布 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]"> 的选择问题， 从而建立了 <img src="https://www.zhihu.com/equation?tex=logL%28%5Ctheta%29" alt="[公式]"> 的下界，这是 E 步，接下来的M 步骤就是固定 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]"> 后，调整 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]">)，去极大化<img src="https://www.zhihu.com/equation?tex=logL%28%5Ctheta%29" alt="[公式]">的下界。</p><p>去掉上式中常数的部分 <img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29" alt="[公式]"> ，则我们需要极大化的对数似然下界为：<br> <img src="https://www.zhihu.com/equation?tex=arg+%5Cmax+%5Climits_%7B%5Ctheta%7D+%5Csum%5Climits_%7Bi%3D1%7D%5Em+%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29log%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D+%5C%5C" alt="[公式]"> </p><h2 id="2-3-EM算法流程"><a href="#2-3-EM算法流程" class="headerlink" title="2.3 EM算法流程"></a>2.3 EM算法流程</h2><p>现在我们总结下EM算法的流程。</p><p>输入：观察数据<img src="https://www.zhihu.com/equation?tex=x%3D%28x%5E%7B%281%29%7D%2Cx%5E%7B%282%29%7D%2C...x%5E%7B%28m%29%7D%29" alt="[公式]">，联合分布 <img src="https://www.zhihu.com/equation?tex=p%28x%2Cz+%7C%5Ctheta%29" alt="[公式]"> ，条件分布 <img src="https://www.zhihu.com/equation?tex=p%28z%7Cx%2C+%5Ctheta%29" alt="[公式]">， 极大迭代次数 <img src="https://www.zhihu.com/equation?tex=J" alt="[公式]"> 。</p><p>1) 随机初始化模型参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 的初值  <img src="https://www.zhihu.com/equation?tex=%5Ctheta%5E0" alt="[公式]"> </p><p>2)  <img src="https://www.zhihu.com/equation?tex=%5Ctext%7Bfor+j+from+1+to+J%7D" alt="[公式]">：</p><ul><li>E步：计算联合分布的条件概率期望：</li></ul><p><img src="https://www.zhihu.com/equation?tex=Q_i%28z%5E%7B%28i%29%7D%29+%3A%3D+P%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%29%29+%5C%5C" alt="[公式]"> </p><ul><li>M步：极大化 <img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%29" alt="[公式]"> ,得到 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> :</li></ul><p><img src="https://www.zhihu.com/equation?tex=%5Ctheta+%3A+%3D+arg+%5Cmax+%5Climits_%7B%5Ctheta%7D%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DQ_i%28z%5E%7B%28i%29%7D%29log%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D+%5C%5C" alt="[公式]"> </p><ul><li>重复E、M步骤直到 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> 收敛</li></ul><p>输出：模型参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]"> </p><h2 id="2-4-EM算法另一种理解"><a href="#2-4-EM算法另一种理解" class="headerlink" title="2.4 EM算法另一种理解"></a>2.4 EM算法另一种理解</h2><p>坐标上升法（Coordinate ascent）(<strong>类似于梯度下降法，梯度下降法的目的是最小化代价函数，坐标上升法的目的是最大化似然函数；梯度下降每一个循环仅仅更新模型参数就可以了，EM算法每一个循环既需要更新隐含参数和也需要更新模型参数，梯度下降和坐标上升的详细分析参见</strong><a href="https://zhuanlan.zhihu.com/p/36535299" target="_blank" rel="noopener">攀登传统机器学习的珠峰-SVM (下)</a>)：</p><p><img src="https://pic4.zhimg.com/v2-389aa0ac570f105b0e3b77ed0d3cf10b_b.jpg" alt="img"></p><p>图中的直线式迭代优化的路径，可以看到每一步都会向最优值前进一步，而且前进路线是平行于坐标轴的，因为每一步只优化一个变量。</p><p>这犹如在x-y坐标系中找一个曲线的极值，然而曲线函数不能直接求导，因此什么梯度下降方法就不适用了。但固定一个变量后，另外一个可以通过求导得到，因此可以使用坐标上升法，一次固定一个变量，对另外的求极值，最后逐步逼近极值。对应到EM上，<strong>E步：</strong>固定 θ，优化Q；<strong>M步：</strong>固定 Q，优化 θ；交替将极值推向极大。</p><h2 id="2-5-EM算法的收敛性思考"><a href="#2-5-EM算法的收敛性思考" class="headerlink" title="2.5 EM算法的收敛性思考"></a>2.5 EM算法的收敛性思考</h2><p>EM算法的流程并不复杂，但是还有两个问题需要我们思考：</p><p>1） EM算法能保证收敛吗？</p><p>2） EM算法如果收敛，那么能保证收敛到全局极大值吗？　</p><p>首先我们来看第一个问题, EM 算法的收敛性。要证明 EM 算法收敛，则我们需要证明我们的对数似然函数的值在迭代的过程中一直在增大。即：</p><p><img src="https://www.zhihu.com/equation?tex=%5Csum%5Climits_%7Bi%3D1%7D%5Em+logP%28x%5E%7B%28i%29%7D%7C%5Ctheta%5E%7Bj%2B1%7D%29+%5Cgeq+%5Csum%5Climits_%7Bi%3D1%7D%5Em+logP%28x%5E%7B%28i%29%7D%7C%5Ctheta%5E%7Bj%7D%29+%5C%5C" alt="[公式]"> </p><p>由于：</p><p><img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%2C+%5Ctheta%5E%7Bj%7D%29+%3D+%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DP%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%5E%7Bj%7D%29%29log%7BP%28x%5E%7B%28i%29%7D%EF%BC%8C+z%5E%7B%28i%29%7D%7C%5Ctheta%29%7D+%5C%5C" alt="[公式]"> </p><p>令：</p><p><img src="https://www.zhihu.com/equation?tex=H%28%5Ctheta%2C+%5Ctheta%5E%7Bj%7D%29+%3D+%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DP%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%5E%7Bj%7D%29%29log%7BP%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%29%7D+%5C%5C" alt="[公式]"> </p><p>上两式相减得到：</p><p><img src="https://www.zhihu.com/equation?tex=%5Csum%5Climits_%7Bi%3D1%7D%5Em+logP%28x%5E%7B%28i%29%7D%7C%5Ctheta%29+%3D+L%28%5Ctheta%2C+%5Ctheta%5E%7Bj%7D%29+-+H%28%5Ctheta%2C+%5Ctheta%5E%7Bj%7D%29+%5C%5C" alt="[公式]"> </p><p>在上式中分别取 <img src="https://www.zhihu.com/equation?tex=%CE%B8" alt="[公式]"> 为 <img src="https://www.zhihu.com/equation?tex=%CE%B8%5Ej" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=%CE%B8%5E%7Bj%2B1%7D" alt="[公式]">，并相减得到：</p><p><img src="https://www.zhihu.com/equation?tex=%5Csum%5Climits_%7Bi%3D1%7D%5Em+logP%28x%5E%7B%28i%29%7D%7C%5Ctheta%5E%7Bj%2B1%7D%29+-+%5Csum%5Climits_%7Bi%3D1%7D%5Em+logP%28x%5E%7B%28i%29%7D%7C%5Ctheta%5E%7Bj%7D%29+%3D+%5BL%28%5Ctheta%5E%7Bj%2B1%7D%2C+%5Ctheta%5E%7Bj%7D%29+-+L%28%5Ctheta%5E%7Bj%7D%2C+%5Ctheta%5E%7Bj%7D%29+%5D+-%5BH%28%5Ctheta%5E%7Bj%2B1%7D%2C+%5Ctheta%5E%7Bj%7D%29+-+H%28%5Ctheta%5E%7Bj%7D%2C+%5Ctheta%5E%7Bj%7D%29+%5D+%5C%5C" alt="[公式]"> </p><p>要证明EM算法的收敛性，我们只需要证明上式的右边是非负的即可。</p><p>由于<img src="https://www.zhihu.com/equation?tex=%CE%B8%5E%7Bj%2B1%7D" alt="[公式]">)使得<img src="https://www.zhihu.com/equation?tex=L%28%CE%B8%2C%CE%B8%5Ej%29" alt="[公式]">极大，因此有：</p><p><img src="https://www.zhihu.com/equation?tex=L%28%5Ctheta%5E%7Bj%2B1%7D%2C+%5Ctheta%5E%7Bj%7D%29+-+L%28%5Ctheta%5E%7Bj%7D%2C+%5Ctheta%5E%7Bj%7D%29+%5Cgeq+0+%5C%5C" alt="[公式]"> </p><p>而对于第二部分，我们有：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+H%28%5Ctheta%5E%7Bj%2B1%7D%2C+%5Ctheta%5E%7Bj%7D%29+-+H%28%5Ctheta%5E%7Bj%7D%2C+%5Ctheta%5E%7Bj%7D%29+%26+%3D+%5Csum%5Climits_%7Bi%3D1%7D%5Em%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DP%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%5E%7Bj%7D%29log%5Cfrac%7BP%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%5E%7Bj%2B1%7D%29%7D%7BP%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%5Ej%29%7D+%5Ctag%7B3%7D+%5C%5C+%26+%5Cleq+%5Csum%5Climits_%7Bi%3D1%7D%5Emlog%28%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DP%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%5E%7Bj%7D%29%5Cfrac%7BP%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%5E%7Bj%2B1%7D%29%7D%7BP%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%5Ej%29%7D%29+%5Ctag%7B4%7D+%5C%5C+%26+%3D+%5Csum%5Climits_%7Bi%3D1%7D%5Emlog%28%5Csum%5Climits_%7Bz%5E%7B%28i%29%7D%7DP%28+z%5E%7B%28i%29%7D%7Cx%5E%7B%28i%29%7D%EF%BC%8C%5Ctheta%5E%7Bj%2B1%7D%29%29+%3D+0+%5Ctag%7B5%7D+%5Cend%7Balign%7D" alt="[公式]"> </p><p>其中第（4）式用到了Jensen不等式，只不过和第二节的使用相反而已，第（5）式用到了概率分布累积为1的性质。</p><p>至此，我们得到了：<img src="https://www.zhihu.com/equation?tex=%5Csum%5Climits_%7Bi%3D1%7D%5Em+logP%28x%5E%7B%28i%29%7D%7C%5Ctheta%5E%7Bj%2B1%7D%29+-+%5Csum%5Climits_%7Bi%3D1%7D%5Em+logP%28x%5E%7B%28i%29%7D%7C%5Ctheta%5E%7Bj%7D%29+%5Cgeq+0" alt="[公式]"> ，证明了EM算法的收敛性。</p><p>从上面的推导可以看出，EM 算法可以保证收敛到一个稳定点，但是却不能保证收敛到全局的极大值点，因此它是局部最优的算法，当然，如果我们的优化目标 <img src="https://www.zhihu.com/equation?tex=L%28%CE%B8%2C%CE%B8%5Ej%29" alt="[公式]"> 是凸的，则EM算法可以保证收敛到全局极大值，这点和梯度下降法这样的迭代算法相同。至此我们也回答了上面提到的第二个问题。</p><h2 id="2-6-EM算法应用"><a href="#2-6-EM算法应用" class="headerlink" title="2.6. EM算法应用"></a>2.6. EM算法应用</h2><p>如果我们从算法思想的角度来思考EM算法，我们可以发现我们的算法里已知的是观察数据，未知的是隐含数据和模型参数，在E步，我们所做的事情是固定模型参数的值，优化隐含数据的分布，而在M步，我们所做的事情是固定隐含数据分布，优化模型参数的值。EM的应用包括：</p><ul><li>支持向量机的SMO算法</li><li>混合高斯模型</li><li>K-means</li><li>隐马尔可夫模型</li></ul><h2 id="3-EM算法案例-两硬币模型"><a href="#3-EM算法案例-两硬币模型" class="headerlink" title="3. EM算法案例-两硬币模型"></a>3. <a href="https://link.zhihu.com/?target=http%3A//ai.stanford.edu/~chuongdo/papers/em_tutorial.pdf">EM算法案例-两硬币模型</a></h2><p>假设有两枚硬币A、B，以相同的概率随机选择一个硬币，进行如下的掷硬币实验：共做 5 次实验，每次实验独立的掷十次，结果如图中 a 所示，例如某次实验产生了H、T、T、T、H、H、T、H、T、H (H代表正面朝上)。a 是在知道每次选择的是A还是B的情况下进行，b是在不知道选择的是A还是B的情况下进行，问如何估计两个硬币正面出现的概率？</p><p><img src="https://pic2.zhimg.com/v2-a5b47206d802b392e0e72a23c6b7bb95_b.jpg" alt="img"></p><p><strong>CASE a</strong></p><p>已知每个实验选择的是硬币A 还是硬币 B，重点是如何计算输出的概率分布，这其实也是极大似然求导所得。<br> <img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cunderset%7B%5Ctheta+%7D%7Bargmax%7DlogP%28Y%7C%5Ctheta%29+%26%3D+log%28%28%5Ctheta_B%5E5%281-%5Ctheta_B%29%5E5%29+%28%5Ctheta_A%5E9%281-%5Ctheta_A%29%29%28%5Ctheta_A%5E8%281-%5Ctheta_A%29%5E2%29+%28%5Ctheta_B%5E4%281-%5Ctheta_B%29%5E6%29+%28%5Ctheta_A%5E7%281-%5Ctheta_A%29%5E3%29+%29+%5C%5C+%26%3D+log%5B%28%5Ctheta_A%5E%7B24%7D%281-%5Ctheta_A%29%5E6%29+%28%5Ctheta_B%5E9%281-%5Ctheta_B%29%5E%7B11%7D%29+%5D+%5Cend%7Balign%2A%7D" alt="[公式]"><br>上面这个式子求导之后发现，5 次实验中A正面向上的次数再除以总次数作为即为  <img src="https://www.zhihu.com/equation?tex=%5Chat%CE%B8_A" alt="[公式]"> ，5次实验中B正面向上的次数再除以总次数作为即为 ，即:</p><p><img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D_A+%3D+%5Cfrac%7B24+%7D%7B24%2B6%7D+%3D+0.80+%5C%5C" alt="[公式]"> </p><p><img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D_B+%3D+%5Cfrac%7B9%7D%7B+9+%2B+11%7D+%3D+0.45+%5C%5C" alt="[公式]"> </p><p><strong>CASE b</strong></p><p>由于并不知道选择的是硬币 A 还是硬币 B，因此采用EM算法。</p><p>E步：初始化<img src="https://www.zhihu.com/equation?tex=%5Chat+%CE%B8_A%5E%7B%280%29%7D+%3D+0.60" alt="[公式]">和 <img src="https://www.zhihu.com/equation?tex=%5Chat+%CE%B8_B%5E%7B%280%29%7D+%3D+0.50" alt="[公式]"> ，计算每个实验中选择的硬币是 A 和 B 的概率，例如第一个实验中选择 A 的概率为：</p><p><img src="https://www.zhihu.com/equation?tex=P%28z%3DA%7Cy_1%2C+%5Ctheta%29+%3D+%5Cfrac+%7BP%28z%3DA%2C+y_1%7C%5Ctheta%29%7D%7BP%28z%3DA%2Cy_1%7C%5Ctheta%29+%2B+P%28z%3DB%2Cy_1%7C%5Ctheta%29%7D+%3D+%5Cfrac%7B%280.6%29%5E5%2A%280.4%29%5E5%7D%7B%280.6%29%5E5%2A%280.4%29%5E5%2B%280.5%29%5E%7B10%7D%7D+%3D+0.45+%5C%5C" alt="[公式]"> </p><p><img src="https://www.zhihu.com/equation?tex=P%28z%3DB%7Cy_1%2C+%5Ctheta%29+%3D+1-+P%28z%3DA%7Cy_1%2C+%5Ctheta%29+%3D+0.55+%5C%5C" alt="[公式]"> </p><p>计算出每个实验为硬币 A 和硬币 B 的概率，然后进行加权求和。</p><p><strong>M步</strong>：求出似然函数下界 <img src="https://www.zhihu.com/equation?tex=+Q%28%5Ctheta%2C+%5Ctheta%5Ei%29" alt="[公式]">， <img src="https://www.zhihu.com/equation?tex=y_j" alt="[公式]">代表第 <img src="https://www.zhihu.com/equation?tex=j" alt="[公式]"> 次实验正面朝上的个数，<img src="https://www.zhihu.com/equation?tex=%5Cmu_j" alt="[公式]"> 代表第 <img src="https://www.zhihu.com/equation?tex=j" alt="[公式]"> 次实验选择硬币 A 的概率，<img src="https://www.zhihu.com/equation?tex=1-%5Cmu_j" alt="[公式]"> 代表第 <img src="https://www.zhihu.com/equation?tex=j" alt="[公式]"> 次实验选择硬币B的概率 。</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+Q%28%5Ctheta%2C+%5Ctheta%5Ei%29+%26%3D+%5Csum_%7Bj%3D1%7D%5E5%5Csum_%7Bz%7D+P%28z%7Cy_j%2C+%5Ctheta%5Ei%29logP%28y_j%2C+z%7C%5Ctheta%29%5C%5C%26%3D%5Csum_%7Bj%3D1%7D%5E5+%5Cmu_jlog%28%5Ctheta_A%5E%7By_j%7D%281-%5Ctheta_A%29%5E%7B10-y_j%7D%29+%2B+%281-%5Cmu_j%29log%28%5Ctheta_B%5E%7By_j%7D%281-%5Ctheta_B%29%5E%7B10-y_j%7D%29+%5Cend%7Balign%2A%7D" alt="[公式]"> </p><p>针对L函数求导来对参数求导，例如对 <img src="https://www.zhihu.com/equation?tex=%CE%B8_A" alt="[公式]">求导：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cfrac%7B%5Cpartial+Q%7D%7B%5Cpartial+%5Ctheta_A%7D+%26%3D+%5Cmu_1%28%5Cfrac%7By_1%7D%7B%5Ctheta_A%7D-%5Cfrac%7B10-y_1%7D%7B1-%5Ctheta_A%7D%29+%2B+%5Ccdot+%5Ccdot+%5Ccdot+%2B+%5Cmu_5%28%5Cfrac%7By_5%7D%7B%5Ctheta_A%7D-%5Cfrac%7B10-y_5%7D%7B1-%5Ctheta_A%7D%29+%3D+%5Cmu_1%28%5Cfrac%7By_1+-+10%5Ctheta_A%7D+%7B%5Ctheta_A%281-%5Ctheta_A%29%7D%29+%2B+%5Ccdot+%5Ccdot+%5Ccdot+%2B%5Cmu_5%28%5Cfrac%7By_5+-+10%5Ctheta_A%7D+%7B%5Ctheta_A%281-%5Ctheta_A%29%7D%29+%5C%5C+%26%3D+%5Cfrac%7B%5Csum_%7Bj%3D1%7D%5E5+%5Cmu_jy_j+-+%5Csum_%7Bj%3D1%7D%5E510%5Cmu_j%5Ctheta_A%7D+%7B%5Ctheta_A%281-%5Ctheta_A%29%7D+%5Cend%7Balign%2A%7D+%5C%5C" alt="[公式]"> </p><p>求导等于 0 之后就可得到图中的第一次迭代之后的参数值:</p><p><img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D_A%5E%7B%281%29%7D+%3D+0.71+%5C%5C" alt="[公式]"> </p><p><img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D_B%5E%7B%281%29%7D+%3D+0.58+%5C%5C" alt="[公式]"> </p><p>当然，基于Case a 我们也可以用一种更简单的方法求得：</p><p><img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D_A%5E%7B%281%29%7D+%3D+%5Cfrac%7B21.3%7D%7B21.3%2B8.6%7D+%3D+0.71+%5C%5C" alt="[公式]"> </p><p><img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D_B%5E%7B%281%29%7D+%3D+%5Cfrac%7B11.7%7D%7B+11.7+%2B+8.4%7D+%3D+0.58+%5C%5C" alt="[公式]"> </p><p><strong>第二轮迭代</strong>：基于第一轮EM计算好的 <img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D_A%5E%7B%281%29%7D%2C+%5Chat%7B%5Ctheta%7D_B%5E%7B%281%29%7D" alt="[公式]"> , 进行第二轮 EM，计算每个实验中选择的硬币是 A 和 B 的概率（E步），然后在计算M步，如此继续迭代……迭代十步之后 <img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D_A%5E%7B%2810%29%7D+%3D+0.8%2C+%5Chat%7B%5Ctheta%7D_B%5E%7B%2810%29%7D+%3D+0.52" alt="[公式]"> </p>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>EM算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_SLP_6_HMM_三个问题</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_SLP_6_HMM_%E4%B8%89%E4%B8%AA%E9%97%AE%E9%A2%98/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_SLP_6_HMM_%E4%B8%89%E4%B8%AA%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h1 id="HMM的三个核心问题的解"><a href="#HMM的三个核心问题的解" class="headerlink" title="HMM的三个核心问题的解"></a>HMM的三个核心问题的解</h1><p>本文首先对马尔科夫过程、隐形马尔科夫模型等概念做一次梳理性阐述，然后对HMM的三大问题依次给出解法。</p><h3 id="1-马尔科夫网络，马尔科夫模型，马尔科夫过程，贝叶斯网络的区别"><a href="#1-马尔科夫网络，马尔科夫模型，马尔科夫过程，贝叶斯网络的区别" class="headerlink" title="1.马尔科夫网络，马尔科夫模型，马尔科夫过程，贝叶斯网络的区别"></a>1.马尔科夫网络，马尔科夫模型，马尔科夫过程，贝叶斯网络的区别</h3><p>有如下递进定义</p><ol><li>将随机变量为节点，若两个随机变量相关或者不独立，则将两者链接一条边；若给定若干随机变量，则形成一个有向图，即构成一个<strong>网络</strong></li><li>若该网络是有向无环图，则称这个网络为<strong>贝叶斯网络</strong></li><li>如果这个图退化成线性链的方式，则得到<strong>马尔科夫模型</strong>，因为每个节点都是随机变量，将其看成各个时刻或空间的相关变化，以随机过程的视角，则可以看成是<strong>马尔科夫过程</strong>。</li><li>若上述网络是无向的，则是无向图模型，又称<strong>马尔科夫随机场或者马尔科夫网络</strong>。</li><li>若给定某些条件，研究这个马尔科夫随机场，则得到条件随机场。</li><li>如果使用条件随机场解决标注问题，并且进一步将条件随机场中的网络拓扑变成线性的，则得到线性链条件随机场</li></ol><h3 id="2-马尔科夫模型"><a href="#2-马尔科夫模型" class="headerlink" title="2.马尔科夫模型"></a>2.马尔科夫模型</h3><h4 id="2-1马尔科夫过程"><a href="#2-1马尔科夫过程" class="headerlink" title="2.1马尔科夫过程"></a>2.1马尔科夫过程</h4><p>马尔科夫过程是一类随机过程，他的原始模型马尔科夫链，由俄罗斯数学家A.A.马尔科夫提出，该过程有如下性质：在已知现在的条件下，他未来的演变过程不依赖于他过去的状态。例如森林中动物的数量演变过程，液体微粒做的布朗运动，传染病受感染的人数等，都可以视为马尔科夫过程。</p><p>每个状态的转移只依赖于之前的n个状态，这个状态被称为1个n阶的模型。其中n是转移状态的数目。最简单的马尔科夫模型是一个一阶过程。每个状态的转移只依赖于之前的那个状态。这也叫做马尔科夫性质。</p><h3 id="3-HMM的三个重要问题"><a href="#3-HMM的三个重要问题" class="headerlink" title="3.HMM的三个重要问题"></a>3.HMM的三个重要问题</h3><p>1） Likelihood，给出转移矩阵A、发射矩阵B、观测值O，求特定观测值出现的概率P（O | A，B）</p><p>​    用前向后向算法(Forward - backward Algorithm)。</p><p>2）Decoding，给出转移矩阵A、发射矩阵B、观测值O，求状态层Q</p><p>​    解码问题，用基于动态规划的维特比算法。</p><p>3）Learning，给出观测值O和状态层Q，求A,B.</p><p>​    模型参数学习问题。这个问题的求解需要用到基于EM算法的鲍姆-韦尔奇算法.</p><p><strong>例题</strong></p><p>晓明有三天假期，为了打发时间，可以在每一天选择三件事情中的一件来做，分别是散步、购物、打扫卫生。</p><p>我们在生活中的决定通常会受到天气的影响，比如晴天可能会想要去散步或购物，下午则不想出门在家打扫卫生。这里的散步、购物、整理是可观测序列。下雨和天晴是隐藏状态。用如下概率图表示:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191227_1641_40_122.png" alt=""></p><p>对应的三个问题就是：</p><p>1.likelihood：已知整个模型，我连续观察到三天做的事情是：散步-购物-收拾。求出现这些行为的概率？</p><p>2.Decoding：已知整个模型，同样观察到三天所做的事情是：散步-购物-收拾。求三天的天气是怎样？</p><p>3.Learning：我只知道三天做了：散步-购物-收拾。其他什么信息都没有，我得自己建立一个模型，计算出：晴雨转换概率、第一天天气分布情况、根据天气选择做某事的概率。</p><pre><code>找到好的实例之后理解马尔科夫模型和三大问题就直观多了。没有实例之前去想象A矩阵是什么，B矩阵是什么，h矩阵是什么的时候，自己心理清楚这样其实根本没理解。需要实例化。以后遇见太抽象的概念记得及时实例化提高认知效率。进一步分析：为什么明知概念模糊拖这么久才实例化(accidently)？因为没有摆脱高中的学习模式！！！！！！！！！！！高中习惯了老师扔出概念，然后马上feed us优质例题。这个认知过程太完美太省力，addicted，类似于婴儿吃妈妈嚼烂了又吐出来的肉糜。But you are almost 27.没有张老师，徐战胜，王汉娇们了，the road the phd,也许有的领域真就知道自己懂了。I need academic growing up.Come on,it`s time for student becoming a teacher.</code></pre><h3 id="3-1-Likelihood-遍历与前向算法"><a href="#3-1-Likelihood-遍历与前向算法" class="headerlink" title="3.1 Likelihood - 遍历与前向算法"></a>3.1 Likelihood - 遍历与前向算法</h3><p> 已知整个模型，我连续观察到三天做的事情是：散步-购物-收拾。求出现这些行为的概率？</p><p><strong>解1：</strong>遍历算法</p><p>假设第一天是晴天，要散步，直接把图上的概率乘一下就好了：0.6*0.4 = 0.24。不难列出所有情况。</p><p>第二天要做的事情就是在第一天的基础上再遍历所有情况，same goes 第三天。</p><p>这个思路简单，但是复杂度会随着观测序列和隐藏状态的增加而爆炸。</p><p>复杂度：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191227_1702_55_488.png" alt=""></p><p><strong>解2</strong>：前向算法：Forward Algorithm</p><p>假设第一天要散步，计算出包括雨天和晴天的散步概率</p><p>假设第二天要购物，在第一天的基础上计算出雨天和晴天的购物的概率</p><p>第三天同理。“带剪枝的遍历”。</p><p>复杂度：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191227_1704_58_200.png" alt=""></p><h3 id="3-2-Decoding-维特比算法"><a href="#3-2-Decoding-维特比算法" class="headerlink" title="3.2 Decoding - 维特比算法"></a>3.2 Decoding - 维特比算法</h3><p>已知整个模型，同样观察到三天所做的事情是：散步-购物-收拾。求三天的天气是什么？</p><p>也就是要求隐藏状态。用维特比算法。维特比算法是应用最广的动态规划算法。利用动态规划，可以解决任何一个图中的最短路径问题。而维特比算法是针对一个特殊的图-篱笆网络（lattice）的有向图最短路径问题而提出的。他之所以重要是因为使用HMM模型描述的问题都可以用它来解码。包括现在的数字通信，语音识别，机翻，拼音转汉字等。维特比算法一般用于模式识别，通过观测数据来反推出隐藏状态。下面是这个算法的detail。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191227_1641_40_122.png" alt=""></p><p>因为是要根据观测数据来反推，所以这里要进行一个假设，<strong>假设这三天所做的行为分别是：散步、购物、打扫卫生，</strong>那么我们要求的是这三天的天气(路径)分别是什么。</p><p>初始计算第一天下雨和第一天晴天去散步的概率值：</p><p><img src="https://pic1.zhimg.com/v2-73131ddf54508a7084f0452da924751c_b.jpg" alt="img"></p><p>表示第一天下雨的概率</p><p><img src="https://pic3.zhimg.com/v2-27da2400809669de79c6f818d124763e_b.jpg" alt="img"></p><p>表示中间的状态(下雨)s概率</p><p><img src="https://pic1.zhimg.com/v2-a1a5e88edd5711b389e5c3fc5ea3bcfc_b.jpg" alt="img"></p><p>表示下雨并且散步的概率</p><p><img src="https://pic3.zhimg.com/v2-4f2fffcb11a07760749be5870f7fada6_b.jpg" alt="img"></p><p>表示下雨天到下雨天的概率</p><p><img src="https://pic1.zhimg.com/v2-e127070538c94eb18e37f8b32d84a750_b.jpg" alt="img"></p><p><img src="https://pic2.zhimg.com/v2-2591cecffb3f93a7c1971ef65f49f5a9_b.jpg" alt="img"></p><p>   初始路径为：</p><p><img src="https://pic1.zhimg.com/v2-482826b9557199994b9ea145eb20a1c4_b.jpg" alt="img"></p><p><img src="https://pic1.zhimg.com/v2-a438ee87c87d96d651eabdd250f1d87c_b.jpg" alt="img"></p><ol><li>计算第二天下雨和第二天晴天去购物的概率值：</li></ol><p><img src="https://pic1.zhimg.com/v2-f5f0030f0ac7e28a260e2b20c5806fc8_b.jpg" alt="img"></p><p>对应路径为：</p><p><img src="https://pic4.zhimg.com/v2-fe2e21e05b0a17c5379802b44896e2e7_b.jpg" alt="img"></p><ol><li>计算第三天下雨和第三天晴天去打扫卫生的概率值：</li></ol><p><img src="https://pic4.zhimg.com/v2-f11d52d82c3010dd9155bcd0c9527a63_b.jpg" alt="img"></p><p>对应路径为：</p><p><img src="https://pic3.zhimg.com/v2-1e0431aafa9790b683f335e87999537e_b.jpg" alt="img"></p><p>比较每一步中  的概率大小，选取<strong>最大值</strong>并找到对应的路径，依次类推就能找到最有可能的<strong>隐藏状态路径</strong>。<br>第一天的概率最大值为  </p><p><img src="https://pic3.zhimg.com/v2-6a33161b94c9ce27a3ba5ea5a9f43e0a_b.jpg" alt="img"></p><p>对应路径为Sunny，<br>第二天的概率最大值为  </p><p><img src="https://pic3.zhimg.com/v2-b5b9c73fdc7c8b2db44e50f2799a51b6_b.jpg" alt="img"></p><p>对应路径为Sunny，<br>第三天的概率最大值为  </p><p><img src="https://pic2.zhimg.com/v2-e696141e8b8be5367c769538f3f2dbcd_b.jpg" alt="img"></p><p>对应路径为Rainy。</p><p>合起来的路径就是Sunny-&gt;Sunny-&gt;Rainy，这就是我们所求。</p><pre><code>维特比小结：无外乎是三天三层的树形结构。遍历这颗二叉的状态树，找到每一层的最大可能性。每一层的最大可能性就是预估结果。</code></pre><h3 id="3-3-Learning-EM算法"><a href="#3-3-Learning-EM算法" class="headerlink" title="3.3.Learning - EM算法"></a>3.3.Learning - EM算法</h3><p>我只知道三天做了：散步-购物-收拾。其他什么信息都没有，我得自己建立一个模型，计算出：晴雨转换概率、第一天天气分布情况、根据天气选择做某事的概率。</p><p>详见概念 - 6 - EM算法</p>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>HMM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_SLP_6_HMM_马尔科夫模型</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_SLP_6_HMM_%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_SLP_6_HMM_%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="隐性马尔可夫模型-HMM"><a href="#隐性马尔可夫模型-HMM" class="headerlink" title="隐性马尔可夫模型(HMM):"></a>隐性马尔可夫模型(HMM):</h1><h2 id="1-什么样的问题需要HMM模型"><a href="#1-什么样的问题需要HMM模型" class="headerlink" title="1. 什么样的问题需要HMM模型"></a>1. 什么样的问题需要HMM模型</h2><p>首先我们来看看什么样的问题可以用HMM模型。使用HMM模型时我们的问题一般有这两个特征：</p><p>​    １）我们的问题是基于序列的，比如时间序列，或者状态序列。</p><p>​    ２）我们的问题中有两类数据，一类序列数据是可以观测到的，即观测序列；而另一类数据是不能观察到        的，即隐藏状态序列，简称状态序列。（Hidden state）</p><p>有了这两个特征，那么这个问题一般可以用HMM模型来尝试解决。这样的问题在实际生活中是很多的。比如：我现在在打字写博客，我在键盘上敲出来的一系列字符就是观测序列，而我实际想写的一段话就是隐藏序列，输入法的任务就是从敲入的一系列字符尽可能的猜测我要写的一段话，并把最可能的词语放在最前面让我选择，这就可以看做一个HMM模型了。再举一个，我在和你说话，我发出的一串连续的声音就是观测序列，而我实际要表达的一段话就是状态序列，你大脑的任务，就是从这一串连续的声音中判断出我最可能要表达的话的内容。</p><h2 id="2-HMM模型的定义"><a href="#2-HMM模型的定义" class="headerlink" title="2. HMM模型的定义"></a>2. HMM模型的定义</h2><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191226_1633_56_442.png" alt=""></p><ul><li><p>Q是各个状态，图示的节点</p></li><li><p>A是状态转换矩阵，是各个状态不变/互相转换的概率，图示边上的概率</p></li><li><p>B是“发射概率”，是观测一个状态为多少的概率，图示节点外侧的所指的观测结果(根据状态也就是天气是冷还是热，观察Bob吃了1/2/3个雪糕的概率分别是多少……)</p></li><li><p>O是观测值向量，是记录状态转换的一系列观测值。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191226_1639_35_657.png" alt=""></p><h3 id="HMM的三个重要问题"><a href="#HMM的三个重要问题" class="headerlink" title="HMM的三个重要问题"></a><strong>HMM的三个重要问题</strong></h3><p>1） Likelihood，给出转移矩阵A、发射矩阵B、观测值O，求特定观测值出现的概率P（O | A，B）</p><p>​    用前向后向算法(Forward - backward Algorithm)。</p><p>2）Decoding，给出转移矩阵A、发射矩阵B、观测值O，求状态层Q</p><p>​    解码问题，用基于动态规划的维特比算法。</p><p>3）Learning，给出观测值O和状态层Q，求A,B.</p><p>​    模型参数学习问题。这个问题的求解需要用到基于EM算法的鲍姆-韦尔奇算法， </p></li></ul><h3 id="HMM模型做了两个很重要的假设如下："><a href="#HMM模型做了两个很重要的假设如下：" class="headerlink" title="HMM模型做了两个很重要的假设如下："></a><strong>HMM模型做了两个很重要的假设如下：</strong></h3><p>1） 齐次马尔科夫链假设。即任意时刻的隐藏状态只依赖于它前一个隐藏状态。如果在时刻t的隐藏状态是it=qi,在时刻t+1的隐藏状态是it+1=qj, 则从时刻t到时刻t+1的HMM状态转移概率aij可以表示为：</p><p>aij=P(it+1=qj|it=qi)</p><p>这样aij可以组成马尔科夫链的状态转移矩阵A:</p><p>A=[aij]N×N</p><p>2） 观测独立性假设。即任意时刻的观察状态只仅仅依赖于当前时刻的隐藏状态，这也是一个为了简化模型的假设。如果在时刻tt的隐藏状态是it=qj, 而对应的观察状态为ot=vk, 则该时刻观察状态vk在隐藏状态qj下生成的概率为bj(k),满足：</p><p>bj(k)=P(ot=vk|it=qj)</p><p>这样bj(k)可以组成观测状态生成的概率矩阵BB:</p><p>B=[bj(k)]N×M</p><p>除此之外，我们需要一组在时刻t=1的隐藏状态概率分布Π:</p><p>Π=[π(i)]N其中π(i)=P(i1=qi)</p><h2 id="3-一个HMM模型实例"><a href="#3-一个HMM模型实例" class="headerlink" title="3.一个HMM模型实例"></a>3.一个HMM模型实例</h2><p>　　　　下面我们用一个简单的实例来描述上面抽象出的HMM模型。这是一个盒子与球的模型，例子来源于李航的《统计学习方法》。</p><p>　　　　假设我们有3个盒子，每个盒子里都有红色和白色两种球，这三个盒子里球的数量分别是：</p><table><thead><tr><th>盒子</th><th>1</th><th>2</th><th>3</th></tr></thead><tbody><tr><td>红球数</td><td>5</td><td>4</td><td>7</td></tr><tr><td>白球数</td><td>5</td><td>6</td><td>3</td></tr></tbody></table><p>按照下面的方法从盒子里抽球，开始的时候，从第一个盒子抽球的概率是0.2，从第二个盒子抽球的概率是0.4，从第三个盒子抽球的概率是0.4。以这个概率抽一次球后，将球放回。然后从当前盒子转移到下一个盒子进行抽球。规则是：如果当前抽球的盒子是第一个盒子，则以0.5的概率仍然留在第一个盒子继续抽球，以0.2的概率去第二个盒子抽球，以0.3的概率去第三个盒子抽球。如果当前抽球的盒子是第二个盒子，则以0.5的概率仍然留在第二个盒子继续抽球，以0.3的概率去第一个盒子抽球，以0.2的概率去第三个盒子抽球。如果当前抽球的盒子是第三个盒子，则以0.5的概率仍然留在第三个盒子继续抽球，以0.2的概率去第一个盒子抽球，以0.3的概率去第二个盒子抽球。如此下去，直到重复三次，得到一个球的颜色的观测序列:</p><p>O={红，白，红}</p><p>注意在这个过程中，观察者只能看到球的颜色序列，却不能看到球是从哪个盒子里取出的。</p><p>那么按照我们上一节HMM模型的定义，我们的观察集合是:</p><p>V={红，白}，M=2V={红，白}，M=2</p><p>我们的状态集合是：</p><p>Q={盒子1，盒子2，盒子3}，N=3Q={盒子1，盒子2，盒子3}，N=3</p><p>而观察序列和状态序列的长度为3.</p><p>初始状态分布为：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191225_1926_00_172.png" alt=""></p><p>状态转移概率分布矩阵为：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191225_1926_11_063.png" alt=""></p><p>观测状态概率矩阵为：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191225_1926_37_154.png" alt=""></p><pre><code>小结;根据生动的抽屉拿球的例子，我对HMM的粗浅理解是，知道FST的跳转路径，猜各状态间跳转概率。</code></pre>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>HMM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_SLP_3_一阶逻辑</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_SLP_3_%E4%B8%80%E9%98%B6%E9%80%BB%E8%BE%91/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_SLP_3_%E4%B8%80%E9%98%B6%E9%80%BB%E8%BE%91/</url>
    
    <content type="html"><![CDATA[<h1 id="概念：一阶逻辑（first-order-logic）"><a href="#概念：一阶逻辑（first-order-logic）" class="headerlink" title="概念：一阶逻辑（first order logic）"></a>概念：一阶逻辑（first order logic）</h1><p>一阶二阶这类的词, 一是表达量化的程度, 二是表达逻辑系统多有表达能力. </p><ul><li>0阶逻辑： 每一个字母就代表一个命题, 所以命题逻辑只能表达句子之间的关系, 比如“p&amp;q”, “if p then q”等等的真值如何从p和q的真值中计算出来.</li><li>1阶逻辑则引入了两个量词, 即universal quantifier(倒A，所有)和existential quantifier(倒E，存在), 并且加入了一阶谓词和individual variables（变量）和individual constants（常量）. 这些导致一阶逻辑可以量化individuals in the domain. 比如经典的</li></ul><hr><p>三段论就可以被一阶逻辑表达：<br>For all x, Hx-&gt;Mx<br>Hs<br>-—<br>Ms</p><hr><p>其中for all x就是量化了所有individuals, 即domain里的任意一对象, 用individual variable x来表示. Hx则是表示x属于H(Human)这个谓词的extension, Mx表示x属于M(Mortal)的extension. s则是individual constant, 代表苏格拉底. 然后通过Universal Instantiation和Modus Ponens推出结论Ms(Socrates is mortal). 这里要提到一个集合论的逻辑基础, 如果逻辑学的基础是集合论的话, 那么individuals就是最小的个体对象, 一阶谓词则是包含个体的集. 那么For all x, Hx-&gt;Mx则可以“翻译”成: 对于任意个体x, 如果x属于H这个集, 那么x就属于M这个集.</p><p>但注意, 我们的量词在这里只能表达“对于任意一个individual x”, 然而这个量词的表达能力是有限的. 比如说Leibniz Law: “对于任意individual x和y, 如果x和y相等, 那么对于任意性质P, Px当且仅当Py. ” 这段话里面的“对于任意性质”, 用一阶逻辑是表达不出来的. 因为一阶逻辑只能量化个体, 而性质却是包含个体的集, 所以我们要引入二阶variable, 才能量化性质, 从而表达“对于任意包含个体的集合”. 这句话用二阶逻辑写出来会是这样：<br>∀x,y (x=y → ∀P (Px&lt;-&gt;Py))</p><p>注意看第二个量词, 量化的不是个体x或y, 而是性质P. 这个量化就叫做二阶量化.集合论上来说, 一阶量化个体, 二阶量化包含个体的集合, 三阶量化包含包含个体的集合的集合, 等等等等如此类推</p><p>引用源：知乎 - ZS Chen</p>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>一阶逻辑</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_SLP_5_FSD状态传感器</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_SLP_5_FSD%E7%8A%B6%E6%80%81%E4%BC%A0%E6%84%9F%E5%99%A8/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_SLP_5_FSD%E7%8A%B6%E6%80%81%E4%BC%A0%E6%84%9F%E5%99%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="状态传感器FST概念"><a href="#状态传感器FST概念" class="headerlink" title="状态传感器FST概念"></a>状态传感器FST概念</h1><p>状态传感器是有限状态机的一种。有限状态机适合处理正则语言，有限状态传感器适合处理正则关系。</p><p>他是一种类似于字典树/自动机的数据结构。</p><p>转自豆瓣-巴扎黑</p><p>有限状态转移机在自然语言处理上的应用</p><p>原文地址：</p><p><a href="http://infolocata.com/mirovia/finite-state-transducers-for-natural-language-processing/" target="_blank" rel="noopener">http://infolocata.com/mirovia/finite-state-transducers-for-natural-language-processing/</a></p><p>在自然语言处理中，经常会遇到一些针对某些内容法则做出修改的操作，比如说：如果c的后面紧接x的话，则把c变为b，FST则是基于这些规则上的mathematical操作，比如说把若干个规则整合成一个single pass或mega rule，这样做呢，就可以很有效的提高rule-based system的效率。</p><p>首先，先来大概的了解一下有限状态机（FSM）</p><p>有限状态机呢就是一个由一堆状态（当然啦，有限的嘛），还有一堆转移条件组成的‘鸡’。英文定义是这样的：a FSM is an abstract mathematical model of computation that is capable of storing a status or state and changing this state based on input. 我的理解大概这个FSM就是一个基于某些状态和某些规则而构造的一个模型。比如原文作者就丧心病狂的给他的小白鼠构造了下面这个“我的一天”的模型。</p><table><thead><tr><th><img src="https://img1.doubanio.com/view/note/large/public/p10828389.jpg" alt="img"></th></tr></thead><tbody><tr><td></td></tr></tbody></table><p>所以呢，我们大致可以看出来这个Mirovia的小白鼠每天的生活就是困了睡、饿了吃、无聊了就开始搞基……</p><p>在NLP中，FSM包含一个起始节点和一个终止结点，然后通过all possible combination of condition labels，就能去generate, recognize language了。</p><p>而FST呢，就是一种special type of FSM. 具体的来说，FST会有一个input string和一个output string, 对比起对于某种状态我要么接受条件做出转移，要么我就原地不动的FSM来说，FST是将这个input string转移成output string了，也就是说，it accepts a string on its input tape and generate another string on its output tape.</p><p>FST在一些NLP task里面特别有用，比如说我给出以下三个规则：</p><p>1) 当c后紧接x时，将c变为b         cx→bx</p><p>2) 当a前面是rs时，将a变为b        rsa→rsb</p><p>3) 当b前面是rs，后面是xy时,将b变为a       rsbxy→rsaxy</p><p>所以当我们的input string是rsaxyrscxy时，根据以上三个规则，我们就可以做出以下的变换：</p><p>rsaxyrscxy→rsaxyrsbxy</p><p>rsaxyrsbxy→rsbxyrsbxy</p><p>rsbxyrsbxy→rsaxyrsaxy</p><p>然后大家就会发现，第二步做的变换的第三步又变回去了！搞毛啊！浪费我们程序猿宝贵的时间和精力啊！于是FST就出现了， FST provides a path to eliminate these inefficiencies.</p><p>首先呢，我们把每一个rule都用一个FST表示出来，每个状态间的link表示接收input character以及相应的output character. 表示方式是input/output. 所以针对上面的3个rules，我们可以得到以下三个FST：</p><table><thead><tr><th><img src="https://img9.doubanio.com/view/note/large/public/p10828394.jpg" alt="FST based on rules"></th></tr></thead><tbody><tr><td>FST based on rules</td></tr></tbody></table><p>接下来，我们就要去extending 这些FST了，为什么要extending他们呢？原文是这么说的： FST would be of little use in matching against an input string as each is designed to process exactly the context window described in its corresponding rule.我的理解是这样的，比如说第一个rule，当rsaxyrscxy进来后，先判断第一个字符r，rule 1就识别不出来了，因为他只知道c和b的事嘛。所以为了让FST能适用于任何任意长度的string而且能够根据rules进行必要的操作，我们就要extend FST. 就拿刚刚的rule 1来说吧，他只是为了变换cx到bx，但是他也要能handle得了其前面的‘rsaxyrs’这些characters还有他后面的那个y啊，所以呢，我们可以简单的把这些貌似与规则暂时无关的characters列在一个its own individual edge上面, labeled with ?/?. 然后用ε表示无输出，我们来看一下刚刚那三个FST扩展后的结果先：</p><table><thead><tr><th><img src="https://img1.doubanio.com/view/note/large/public/p10828409.jpg" alt="extended FST"></th></tr></thead><tbody><tr><td>extended FST</td></tr></tbody></table><p>好的，就以第一个图来讲解一下</p><p>首先输入rsaxyrscxy，然后c前面的这一串‘rsaxyrs’，由于他们和我规则没有半毛钱关系，所以他们是什么就还是什么，然后遇到c字符了，我们就用c/ε 来表示“暂时无输出，跳入下一个字符”，所以我们看到了1那个节点，这个节点分别有三个link指出去，，x/bx表示输入x的话，那就输出bx；?/c?表示输入为非x外的任意字符，则原样输出嘛；c/c表示输入c，输出c(相当于继续判断我后面那个是什么)。</p><p>然后呢，就要去把上面这一堆合成一个single FST了，这个时候作者说了一句“A full explanation of the FST composition algorithm is beyond the scope of this write-up”，咳咳，same reason吧，大家比划比划应该能搞出下面这个composed FST:</p><table><thead><tr><th><img src="https://img3.doubanio.com/view/note/large/public/p10828432.jpg" alt="Final FST"></th></tr></thead><tbody><tr><td>Final FST</td></tr></tbody></table><p>这个final FST能达到那三个rule的效果，而且重点是the transformation required only a single pass though the FST and did not result in any efficient transformations. 原来这些rule的task耗费的时间与他们的rule number啊，窗长啊，input string字符数啊都有关系，而现在FST耗费的时间只与input string字符数有关了，perfect啊！</p><p>（完）</p>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>状态传感器</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_SLP_5_FST状态机</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_SLP_5_FST%E7%8A%B6%E6%80%81%E6%9C%BA/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_SLP_5_FST%E7%8A%B6%E6%80%81%E6%9C%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="状态机概念"><a href="#状态机概念" class="headerlink" title="状态机概念"></a>状态机概念</h1><p>FSM 解决一个输入序列，经过 FSM，最终停留在什么状态这样一个问题。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191221_1534_17_665.png" alt=""></p><p>作者：陈天</p><p>链接：<a href="https://zhuanlan.zhihu.com/p/28142401" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/28142401</a></p><p>来源：知乎</p><p>在谈论一般意义的状态机时，我们先看看有限状态机，Finite State Machine，简称 FSM。</p><p>在计算理论（Theory of computation）中，FSM 是一切的基础，也是能力最为有限的机器。在其能力之上是 CFL（Context Free Language），然后是 Turing Machine。</p><p>FSM 解决一个输入序列，经过 FSM，最终停留在什么状态这样一个问题。对于一个字符串是否以 \0 结尾（C 语言的字符串结构），FSM 可以给出答案。</p><p>CFL 是一切编程语言的基础。你写的一段 python 代码是否语法正确，CFL 能够给出答案。</p><p>Turing Machine 就是我们日常用各种算法写代码解决各种问题的基础。不较真地说，JVM 就是一个 Turing Machine。</p><p>再往上，就是未知的世界 —— Turing Machine 也解决不了的问题。</p><p>如果你工作多年，已经把 FSM 的知识还给了老师，不打紧，程序君帮你简单复习一下。</p><p>一个 FSM 首先有一系列的状态（state）。根据输入的不同，FSM 从一个状态切换到另一个状态。在这些状态中，有一些状态是特殊的状态 —— 接受状态（accept state）。如果输入处理完毕，FSM 停留在接受状态，那么 FSM 处理成功，否则失败。</p><p>我们看个例子。请听题：写一个状态机，验证一串二进制bit，包含偶数个 0 和奇数个 1。</p><p>合法的输入有：1，100，10101</p><p>不合法的输入有：10，00，1100</p><p>我们知道，写一段程序，搞定数据结构，就搞定了 80%。开发一个 FSM 也是一样，选取合适的状态是最最关键的。确定了状态之后，剩下的只是辛苦活。</p><p>对于这个简单的问题，大家一眼都能看出，可能存在四种状态。二进制串包含：</p><ol><li>偶数个 0 和偶数个 1（记作 EE）</li><li>偶数个 0 和奇数个 1（记作 EO）</li><li>奇数个 0 和偶数个 1（记作 OE）</li><li>奇数个 0 和奇数个 1（记作 OO）</li></ol><p>FSM 初始化的状态是 EE，一个 bit 都没处理，0 和 1 都是偶数个。FSM 的接受状态是 EO。如果最终到达这个状态，那么处理成功。</p><p>我们很容易能画出这样的状态机：</p><p><img src="https://pic2.zhimg.com/v2-adbd0653c0308b9cc4c3f695d877fa0d_b.png" alt="img"></p><p><strong>这是编程的内功，此状态机设计出来剩下的就是等价于写一堆if的活儿了。状态机目前我理解为自己设计的穷尽了每一步走向的图。把它设计好是一件非常需要功力的事。</strong></p><p>手起刀落，马到功成。简单地有点侮辱你的智商。</p><p>来个难的吧 —— 难到那种可能你抓破头皮喊破喉咙也找不到优雅的解法的问题。</p><p>请听题：判断一个 binary string 是否能被 3 整除。</p><p>这个问题合法的输入有：11，110，1001，1100，1111，…</p><p>不合法的输入一大堆，光看输入似乎看不出什么规律。所以你不可能用两个状态（可以整除/不可以整除）来描述。</p><p>这里要注意，FSM 很傻，对于输入，只能一个单元一个单元处理（在这里一个单元是一个 bit），你既不能吃着碗里的去看锅里的（偷看后面输入），也不能吃着碗里的，把胃里的吐出来重新咀嚼（回溯处理过的输入）。你能依赖的，只有当前所处的状态，以及当前的输入。</p><p>光说不练假把式，我们来搞点输入试一试。</p><p>如果第一个输入是 1，那么它不能被 3 整除。商 0 余 1。一个数能不能被整除，关键看余数是否为 0。除了 0 之外，这里余数可能的取值还有 1 和 2。我们试试把状态应该设置为 0，1，2，看看是否有解。这里 0 是接受状态，也是初始状态。OK，从初始状态 0 起，输入是 1，那么状态迁移到 1。</p><p>如果第二个输入是 0，也就是说现在看到的串是 10，10 和 1 的关系是什么？<strong>进位</strong>！二进制逢二进一，所以相当于 被除数 乘了 2。被除数乘 2，相当于余数乘 2 再模除数（这个我就不证明了）。</p><p>所以第二个输入是 0 时，余数 (1 * 2) % 3 = 2，状态从 1 迁移到 2。</p><p>如果第二个输入是 1，那么就乘 2 加 1 再取模，余数是 (1 * 2 + 1) % 3 = 0，状态从 1 迁移到 0。</p><p>顺着这样的思路走下去，可以绘制出如下的 FSM：</p><p><img src="https://pic2.zhimg.com/v2-a8d504bf088a26d3df7c7dca332d7b15_b.png" alt="img"></p><p>有没有想过，你苦苦思考的答案，竟然工整得如此清秀美丽，让你窒息？就像程序君第一次看到「红楼梦」对「苍井空」这样的旷世绝对一样，惊掉了半盆口水。</p><p>在构建 FSM 的过程中，不管你做了多少运算，为这个过程付出了多少脑力，最终，你得到的是一个：在 x 状态下，输入 a，得到 y 状态这样一个字典。这是 FSM 很多时候是最高效算法的原因：<strong>你已经把最艰难的部分编译进了 FSM，剩下的就是查表的操作。</strong></p><p>好了，看你做题做得那么认真，咱们再来一道：判断一个 binary string 是否能被 8 整除。</p><p>这有何难！被 8 除有 8 种余数，整 8 个状态。然后依葫芦画瓢 bla bla bla，就大功告成了。</p><p>对不？对。复杂不？挺复杂。要是判断能不能被 16 整除，这么做下来，一页 A4 纸恐怕都不够画状态变迁的。</p><p>还有没有其他方法？肯定有。</p><p>这个问题合法的输入有：1000，10000，11000，111000，101000，…</p><p>规律很明显，以 000 结尾。这也的确是 8 的倍数的定义。这个 FSM 很好写：</p><p><img src="https://pic1.zhimg.com/v2-aaf5afbb83c5de119a0f836aa7203884_b.png" alt="img"></p><p>也比按照上一种算法得出的 FSM 简单明了多了。</p><p>注：这里有个小问题，0 在上面的状态机并不被接受，但可以被 8 整除。更严谨正确的写法是这样（不过我们下文先不纠结这样的小细节）：</p><p><img src="https://pic4.zhimg.com/v2-5099ef147a86629eb5a6c59462fd954f_b.png" alt="img"></p><p>可见，<strong>同样一个问题，可以有不同的 FSM 处理。</strong> 有些复杂，有些简单。我们要努力追寻那些状态最少的 FSM。</p><p>以上描述的 FSM 都是 DFA（Deterministic Finite Automaton），确定有限自动机。就是 <strong>给定一个状态，和一个输入，你总能确定地转换到下一个状态</strong>。</p><p>DFA 有一些有意思的特性，比如补运算，只需要把接受状态和非接受状态互换，新的 DFA 就是原来 DFA 的补。比如：判断一个 binary string 不能被 8 整除。这样就可以：</p><p><img src="https://pic3.zhimg.com/v2-da89f517d6b1218521ce4791f9e5a312_b.png" alt="img"></p><p>而且，就像算术里面我们总能找到最小公约数一样，通过一些算法，我们总能将一个 DFA 转化成最小 DFA。这个就不详说了。</p><p>我们再回到整除 8 的例子。我们能不能这么表述这个 FSM 呢？</p><p><img src="https://pic3.zhimg.com/v2-1dacf86bdc6c62daf4bcdc6dade0d276_b.png" alt="img"></p><p>我不管一开始接受多少个字符，但最后三个必须是 0，0，0。这样的表述很直观。不过，一般而言，对人直观的东西对机器不直观。这是一个 NFA（Nondeterministic Finite Automaton），非确定有限自动机。给定一个状态和一个输入，我们无法确定地转换到下一个状态。比如，当状态为 E 时，输入 0，究竟该往状态 0 上迁移，还是保持目前的状态，who knows?</p><p>子曰：To transit or not to transit, this is a question.</p><p>对付 NFA，我们只能用 decision tree —— 凡是一个输入可能产生多个状态的地方，有几个输出状态就分裂出几条路径，这样当所有路径都结束（要么输入走完了，要么卡在某个状态无法处理了），只要有一条路径到达接受状态，那么这个输入就满足 NFA。比如 1000 和 1001 这两个输入：</p><p><img src="https://pic4.zhimg.com/v2-4cfd7be9900a10ff57b5679cb81fe823_b.jpg" alt="img"></p><p>前者成功，后者失败。</p><p>当然，这样的处理效率上并非最优，decision tree 上的路径会随着带有不确定性的状态的数量指数增长。所以，大部分时候，我们要把 NFA 转化成 DFA，然后再把 DFA 转化成最小 DFA。</p><p>NFA 有什么用呢？它的一个重要使用场景是 regular expression（regex）。regex 是一种简单的描述模式匹配的语言（或者表达式），大部分同学的日常工作都离不开它。上述的问题用 regex 表达是 .*000。早期的 regex 会被转化成 NFA，然后再被转化成 DFA，最终能够高效地处理输入。使用 FSM 处理 regex 的代表产品如 awk，sed，re2。</p><p>不过现在的 regex 已经不纯粹了（non-regular），尤其是对 backreference 的支持，使其不仅无法用 NFA 表述，甚至都不是 context-free，比如这个这个用来匹配回文的正则表达式：(.+)\1（它可以匹配 “tyrtyr”）。有阵子我是 regex 的迷弟 —— 就是那种拿着锤子，看什么问题都是钉子的迷弟 —— 经常写一些要么莫名其妙，要么无比复杂的 regex。复杂的就不说了，莫名其妙，但很好玩的有这个：</p><pre><code class="text">(.+)(.+)(.+)(.+)(.+).*?\5\4\3\2\1</code></pre><p>大家猜猜是干嘛的。</p><p>这样的 regex，处理起来是非常麻烦的，要能够停止，回溯，还要有额外的空间记录已经捕获的组。处理这样的已经超出了 regex 范畴的产品是：pcre。现在很多语言的 regex 库直接使用 pcre（比如 elixir，但早期的 python 不是，golang 用官方自己的 re2）。</p><p>吐槽一句，pcre 非常复杂，复杂到还有自己的 JIT。对不同方式实现 regex 的工具性能的比较，可以看看这个对照：<a href="https://link.zhihu.com/?target=http%3A//sljit.sourceforge.net/regex_perf.html">http://sljit.sourceforge.net/regex_perf.html</a>。</p><p>FSM 理论就讲这么多。戳阅读原文可以了解更多（我的大部分例子出自这里）。</p><p>FSM 的应用主要是在 event based processing。一般如果系统在某个状态下，接收某些信息，处理后产生一个新的状态，都可以用 FSM 的思路来实现。最典型的使用场景是 network protocol，比如 OSPF（rfc2328 的主要篇幅就是在描述各种场景下状态的变化），再比如 TCP 3 way handshake，FTP 的建连和各种 command 的处理。</p><p>还有些场景涉及到具体的业务，比如用户的养成体系（用户从 注册用户 -&gt; 已验证用户 -&gt; 资料完整用户 -&gt; 核心用户 的迁移），支付系统，预订系统也有 FSM 的影子。</p>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>状态机</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_总结</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_%E6%80%BB%E7%BB%93/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<p>自然语言是表达者对某状态(state)[比如<em>“饥饿”</em>]的<strong>编码(encode)</strong>[比如<em>“我饿了,I`m hungry,Я голоден”</em>],接收者会对这段音波/文字进行解码(decode)来揣测表达者的状态(state)。</p><p>自然语言处理是对state转换、code、encode/decode过程的一系列数学建模，通常基于规则+机器学习的模型效果不错。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_Dic</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_Dic/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_Dic/</url>
    
    <content type="html"><![CDATA[<table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>Phonetics</td><td>语音学</td></tr><tr><td>Phonology</td><td>音韵学</td></tr><tr><td>Morphology</td><td>形态学</td></tr><tr><td>orthographic</td><td>正字法</td></tr><tr><td>Syntax</td><td>语法</td></tr><tr><td>Pragmatics</td><td>语用学</td></tr><tr><td>Semantics</td><td>语义学</td></tr><tr><td>Ambiguity</td><td>一词多义，模棱两可</td></tr><tr><td>Dative pronoun</td><td>与格代词</td></tr><tr><td>Possessive pronoun</td><td>所有格代词</td></tr><tr><td>part-of-speech</td><td>词性</td></tr><tr><td>lexical disambiguation</td><td>词义消歧</td></tr><tr><td>speech act interpretation</td><td>言语行为解释</td></tr><tr><td>regular grammars</td><td>正则文法</td></tr><tr><td>critical role</td><td>关键角色</td></tr><tr><td>crucial</td><td>重要的</td></tr><tr><td>symbolic</td><td>象征的</td></tr><tr><td>stochastic</td><td>随机的</td></tr><tr><td>retrieval</td><td>检索</td></tr><tr><td>corpora</td><td>语料库</td></tr><tr><td>finite-state automaton</td><td>有限状态自动机</td></tr><tr><td>punctuation</td><td>标点符号</td></tr><tr><td>appendix</td><td>附录</td></tr><tr><td>Disjunction</td><td>析取</td></tr><tr><td>Precedence</td><td>优先级</td></tr><tr><td>Symmetrically</td><td>对称地</td></tr><tr><td>sketched out</td><td>勾勒出来</td></tr><tr><td>concatenation</td><td>串联</td></tr><tr><td>union</td><td>并联</td></tr><tr><td>anchor</td><td>锚</td></tr><tr><td>intersection</td><td>交叉</td></tr><tr><td>complementation</td><td>互补</td></tr><tr><td>closure</td><td>关闭</td></tr><tr><td>singular</td><td>单数的</td></tr><tr><td>plural</td><td>复数的</td></tr><tr><td>stemming</td><td>堵塞</td></tr><tr><td>nominal inflection</td><td>名词的曲折变化</td></tr><tr><td>colon</td><td>冒号</td></tr><tr><td>intuitively</td><td>直觉</td></tr><tr><td>interpretation</td><td>解释</td></tr><tr><td>notion</td><td>概念</td></tr><tr><td>estimators</td><td>估计器</td></tr><tr><td>assign</td><td>分配</td></tr><tr><td>essential</td><td>本质</td></tr><tr><td>augmentative</td><td>增强的</td></tr><tr><td>extraction</td><td>提取</td></tr><tr><td>utterance</td><td>语言</td></tr><tr><td>strip out</td><td>剥离</td></tr><tr><td>lumped</td><td>集中</td></tr><tr><td>lemma</td><td>词根</td></tr><tr><td>picnic</td><td>野餐</td></tr><tr><td>metric</td><td>米制的，标准的</td></tr><tr><td>perplexity</td><td>困惑</td></tr><tr><td>extrinsic evaluation</td><td>外在评价</td></tr><tr><td>sparse data</td><td>稀疏数据</td></tr><tr><td>recapitulates</td><td>概述</td></tr><tr><td>ration</td><td>定额</td></tr><tr><td>implicit</td><td>隐性的</td></tr><tr><td>decomposing</td><td>分解</td></tr><tr><td>transcribing</td><td>抄写</td></tr><tr><td>acoustic</td><td>声学的</td></tr><tr><td>perspective</td><td>观点</td></tr><tr><td>phones</td><td>音素</td></tr><tr><td>hoax</td><td>骗局</td></tr><tr><td>Gaussian density function,pdfs</td><td>高斯概率密度函数</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_9 - Automatic Speech Recognition</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_9%20-%20Automatic%20Speech%20Recognition/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_9%20-%20Automatic%20Speech%20Recognition/</url>
    
    <content type="html"><![CDATA[<h1 id="9-Auto-Speech-Recognition-语音转文字"><a href="#9-Auto-Speech-Recognition-语音转文字" class="headerlink" title="9. Auto Speech Recognition - 语音转文字"></a>9. Auto Speech Recognition - 语音转文字</h1><p>Spoken language understanding is a difficult task.The goal of <strong>Automatic speech recognition（ASR）</strong>is to address this this task computationally by building system  that <strong>map from an acoustic signal to a string of word</strong>.And <strong>Automatic speech understanding(ASU)</strong> extends this goal to understand more than just words,also sentences.</p><p>影响语音转文字系统性能表现的几个参数：</p><ul><li>需要转换的词汇量：如果只是判断对错/数字，容易。判断词汇库是整体英语，难。</li><li>speech的流畅度：如果是切分好的单个单词，容易。流畅长篇幅对话，难。</li><li>声音质量：apparently，噪音少识别起来容易。</li><li>讲话者的类型：apparently，native speaker识别起来容易，日本口音难</li></ul><p>本章节阐述：</p><p>主要聚焦于大词汇量连续篇幅的语音识别问题（LVCSR : Large vocabulary continuous speech recognition）。</p><p>LVCSR主要使用的范式是HMM模型。本章涉及的核心知识在之前章节均有提及：第七章的音素，第五六章的贝叶斯法则，HMM模型，维特比算法，BW前向算法，第四章的N-gram模型和困惑度。</p><p>本章将介绍HMM-based的speech-recognizer、信号处理技术如何分离MFCC特征、高斯音学模型、维特比算法训练ASR模型的步骤（embedded training）。第十章会接着介绍ASR的其他细节。</p><p><strong>9.1)语音识别系统架构</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/ApplicationFrameHost_tFCT614otB.png" alt=""></p><p><strong>9.3</strong>）<strong>从waveform中提取MFCC feature</strong></p><p>1.把音波变成观察矩阵</p><p>首先把音波分帧，每帧25ms，两帧之间重叠10ms。分帧后音波变成了一小段一小段的，因为波形在时域上几乎没有描述能力，因此必须对波形做MFCC提取。根据人耳朵的生理特征，把每一个波形变换成一个多维向量，可以简单理解为这个向量提取了这帧语音的内容信息，这个过程就是声学特征提取。</p><p>因此，声音就成了一个十二行,N列的一个矩阵，称之为观察矩阵，这里N是总帧数。如下图，每一个帧都用12维的向量表示，色块颜色的深浅表示向量值的大小。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_W2LnNViUEa.png" alt=""></p><p>如何把这个矩阵变成文本？首先介绍两个概念：</p><ul><li>音素：单词的发音由音素构成（常见的英语用音素是CMU的39音素表构成，汉语由声母+韵母构成）</li><li>状态：这里可理解为比音素更细致的语音单位。通常把音素划分成3个状态。</li></ul><p>语音识别总体是如何工作的？</p><ul><li>把帧识别成状态（难点）</li><li>状态组合成音素</li><li>音素组合成单词</li></ul><p>如下图所示，竖条代表一个帧，若干个帧组成一个状态，三个状态组合成一个音素。若干个音素组合成一个单词。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_rMS54NmxEe.png" alt=""></p><p>那么每个帧对应哪个状态呢？可以看某个帧对应哪个状态的概率最大，这个帧就属于哪个状态。如下所示，这个帧在S3上的条件概率最大，所以就判断这个帧属于S3状态。这个条件概率在“高斯声学模型”中查阅的。这个模型里边训练了一大堆参数，通过这些参数，就可以知道帧对应状态的概率。训练参数的过程（<strong>embedding training</strong>）需要用到巨大的语音数据，具体过程这里省略。</p><p>这样做有一个问题，每帧都会对应一个状态号，最后整个语音都会得到一堆乱七八糟的状态号。假设语音有1000帧，就会有1000个状态号和333个音素，111个单词，但是其实1000帧就是25秒，30个单词左右，说明这里每个帧一个状态号的分配方式有问题，前后交叠有大量重复。其实相邻帧应该有相同的状态号才合理，因为每帧很短。</p><p>解决这个合理给帧分配状态号的问题，可使用HMM模型，一共两步;</p><ul><li>构建一个状态网络</li><li>从状态网络中寻找与声音最匹配的路径</li></ul><p>这样就把输出的结果限定在了刚才搭建的状态网络中，只要网络搭建的足够大，就可以包含任意文本路径，但是这个网络搭建的越大，就会越稀释，想达到比较好的准确率就很难。所以要根据任务特点，合理选择网络大小。</p><p>搭建状态网络，就是从单词级网络展开成音素网络，再展开成状态网络，语音识别其实就是在状态网络中搜索一条最佳路径，语音对应的这条路径是概率最大的路径。这其实也就是得知<strong>输出结果</strong>“语音，求解他的<strong>隐藏状态</strong>“状态”，是HMM的decoding问题，使用维特比算法（路径搜索的维特比算法是，概率一种动态规划的剪枝算法）可解。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_TjYucmLpiT.png" alt=""></p><p>这里有三个累积概率，分别是：</p><ul><li>观察概率：每帧和每个状态转移的概率（高斯声学模型得知）</li><li>转移概率：每个状态转移到自身和下个状态的概率（高斯声学模型得知）</li><li>语言概率：根据语言统计规律得到的规律（语言模型N-gram etc得知）</li></ul>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_7 - Phonetics</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_7%20-%20Phonetics/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_7%20-%20Phonetics/</url>
    
    <content type="html"><![CDATA[<h1 id="7-Phonetics-语音学-–-本章对研究帮助不大，跳过"><a href="#7-Phonetics-语音学-–-本章对研究帮助不大，跳过" class="headerlink" title="7.Phonetics(语音学) – 本章对研究帮助不大，跳过"></a>7.Phonetics(语音学) – 本章对研究帮助不大，跳过</h1><p>A speech recognition system needs to have a pronunciation for every word it can recognize,and a text-to-speech system needs to have a pronunciation for every word it can say.</p><ol><li><p>In this chapter,we introduces <strong>phonetic alphabets</strong>音标字母 for describing these pronunciations.</p></li><li><p>Then we introduces two main areas of phonetics, <strong>articulatory phonetics</strong>发音语音学,and <strong>acoustic phonetics</strong>声学语音学。</p></li><li><p>Also briefly touch on <strong>phonology</strong>音韵学</p></li></ol><p><strong>7.1）</strong>Speech sounds and phonetic transciption</p><p>音标表格如下：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191229_1536_52_924.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Phonetics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_8 - Speech Synthesis</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_8%20-%20Speech%20Synthesis/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_8%20-%20Speech%20Synthesis/</url>
    
    <content type="html"><![CDATA[<h1 id="8-Speech-Synthesis-语音合成（文字转语音）"><a href="#8-Speech-Synthesis-语音合成（文字转语音）" class="headerlink" title="8. Speech Synthesis - 语音合成（文字转语音）"></a>8. Speech Synthesis - 语音合成（文字转语音）</h1><p>Speech Synthesis detail is as following:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191229_1557_04_707.png" alt=""></p><p>本单元讲解如何将文本转换成语音，主要分成如下四个任务：</p><ul><li>text normalization  文本标准化</li><li>phonetic analysis 语音分析（把token化的词汇转换成音标）</li><li>prosodic analysis 韵律分析( 把音标组成的集合拼凑地和谐一点)</li></ul><ul><li>waveform synthesis 波形合成（让拼凑好的音标们-IR转wave-发音）</li></ul><h3 id="8-1-Text-Normalization"><a href="#8-1-Text-Normalization" class="headerlink" title="8.1) Text Normalization"></a><strong>8.1</strong>) Text Normalization</h3><p>Text normalization was combined as follwoing:</p><ul><li>sentence tokenization</li><li>non-standard words</li><li>homograph disambiguation</li></ul><h4 id="8-1-1-Sentence-tokenization"><a href="#8-1-1-Sentence-tokenization" class="headerlink" title="8.1.1)Sentence tokenization"></a><strong>8.1.1)</strong>Sentence tokenization</h4><p>tokenization理解为把句子拆分成小块（token），token之间可以是被空格键分隔，也可以是被句号，逗号，或者单纯是被语义分隔，被token化的单词已经解决了大小写，缩写，等等问题。</p><p>做tokenzation的时候一般采用机器学习的方法，在训练集上人们用手工的方法标注各个token，用&lt;EOS&gt;符号间隔开各个token。</p><p>机器学习方法可能学习到的特征有：缩写，前缀，后缀，前边后边的单词之间的相互关系等等。</p><h4 id="8-1-2）non-standard-words"><a href="#8-1-2）non-standard-words" class="headerlink" title="8.1.2）non-standard words"></a><strong>8.1.2）</strong>non-standard words</h4><p>dealing with non-standard words need three steps:</p><ul><li>tokenization - separete and identify non-standard words.</li><li>classification - classification to label them with a type in following pictures.</li><li>expansion - convert each type into a string of standard words.</li></ul><p>the kind of non-standard words:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191229_1620_31_807.png" alt=""></p><p><strong>tokenization:</strong></p><p>大体上，我们通过空格把各个toke分隔开，然后逐一与字典对比，任何不在字典里的词语我们都理解为一个non-standard words</p><p><strong>classification：</strong></p><p>对这些non-standard words，我们需要对他们分类，分类的结果集就是上图出现的各个项。</p><p>分类的具体做法可以是desision list算法，这个算法会在第20章讲解。</p><p>我们现在在构建分类器的时候可以先使用手工标注的训练集，使用监督机器学习的方法去训练分类器。SVM,逻辑回归等等。</p><p><strong>expansion:</strong></p><p>expansion can deal with abbr problem,all the abbr can be expanded to <strong>ordinary words</strong>,by using <strong>abbr dictionary</strong>,.In which ambiguities problem can be solved by <strong>homonym disambiguation</strong></p><p>​    homonym disambiguation:</p><p>​        Most disambiguation problem can be sovled by different part-of-speech role,most disambiguation         abbr/word also different pronunciation.</p><p>​        The rest of disambiguation problem which have same POS and same pronunciation,can be solved         by <strong>word sense disambiguation algorithms</strong>,like <strong>decision list algorithm</strong>.which will be introduced         in chapter 20.</p><h3 id="8-3-Phonetic-analysis-语音分析"><a href="#8-3-Phonetic-analysis-语音分析" class="headerlink" title="8.3 Phonetic analysis - 语音分析"></a>8.3 Phonetic analysis - 语音分析</h3><p>上一小段介绍的是如何把文本内容拆成token，这一小段介绍的是如何把token转化成发音。</p><p>这一小段比较重要的一个组件是<strong>音素字典</strong>。单独使用字典效率是不高的，因为总有一些token你在字典里边查不到（比如稀奇古怪的人名等，书中举例子说在两个不同的词典里运用华尔街日报的语料库，分别有5%左右的token在字典里查不到）。</p><p>本小段分为三个小节，分别介绍：<strong>音素字典</strong>、<strong>名字</strong>、<strong>字符-音素转换规则</strong>。</p><p><strong>8.3.1</strong>）音素字典</p><p>最常用的音素字典是CMU Pronounceing Dictionary.包含了120000多个单词的读音。但是他的设计目的为为了用于语音识别，而不是为了把文字合成语音。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191230_1418_05_932.png" alt=""></p><p>另一个设计目的就是为了科研的合成语音的字典是UNISYN，他是免费的。如下是使用示例。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191230_1419_52_341.png" alt=""></p><p>​    </p><p><strong>8.3.2</strong>)名字</p><p>如本小结开头所说，有很多名字在词典里边是查不到的，因为人名/公司名等等实在太多了，仅仅在美国预估都有两百万人名，为了解决有些人名在词典里查不到，通常的做法是使用 字符-音素转换规则（下小结介绍，通常构建两套预测系统，一个是名字专用，另一个是用于其他）</p><p><strong>8.3.3</strong>）字符-音素装换规则</p><p>grapheme-to-phoneme,or g2p.核心做法就是将cake转换成[K EY K].</p><p>早期做法是letter-to-sound，or LTS rule。英语发音中比较难掌握的是重读的界定，之前都是采用复杂的rule-based的规则，现在多采用机器学习的方法对重读标注。</p><h3 id="8-4-Prosodic-Prominence-韵律"><a href="#8-4-Prosodic-Prominence-韵律" class="headerlink" title="8.4 Prosodic Prominence - 韵律"></a>8.4 Prosodic Prominence - 韵律</h3><p>在语音中，通常会有某些词比较<strong>突出</strong>，突出的方法可能是重读也可能是减慢发音速度，这样的突出的词汇会更感性，对听者有更深的印象。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191230_1443_07_979.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191230_1443_19_973.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191230_1443_40_551.png" alt=""></p><p><strong>8.4.1</strong>）文本转音波系统的输出：Internal Representation</p><p>最终，经过TOBi或者其他系统，成功将文字转换成internal representation,然后这个IR会输入下一节的双音波形合成系统，被转换为读音输出。</p><p>IR如下所示</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191230_1452_07_167.png" alt=""></p><h3 id="8-5-将上文输出的Internal-representation转换为发音（WaveForm）"><a href="#8-5-将上文输出的Internal-representation转换为发音（WaveForm）" class="headerlink" title="*8.5 *将上文输出的Internal representation转换为发音（WaveForm）"></a>*<em>8.5 *</em>将上文输出的Internal representation转换为发音（WaveForm）</h3><p>介绍两种将internal representatiton转换为音波的方法：一是双音波形合成，二是单元综合合成(unit selection synthesis)</p><p><strong>8.5.1</strong>）Diphone音波合成</p><p>粗略地讲，Diphone是一个音素的中间，到另一个音素的中间，的东西。</p><p><strong>DiPhone拼接合成模型</strong>通过查询diphone数据库，对一系列音素生成波形。</p><p>此模型可以被如下步骤表针:</p><ol><li>训练<ol><li>挨个记录每一个话筒对单个diphone的发音</li><li>把每一个diphone从一篇speech中切分出来，存到数据库中</li></ol></li><li>合成<ol><li>按需从di-phone数据库中取出一系列需要的diphone</li><li>在各个di-phone的边界进行简单的信号处理之后，串联起来</li><li>使用信号处理技术，调整基频和持续时间等。</li></ol></li></ol><p><strong>8.5.2</strong>）Unit selection(Waveform)Synthesis</p><p>刚才介绍的diphone waveform synthesis有两个毛病：</p><p>一是依赖于diphone数据库，数据库中的内容都是从speech中切分出来的，没有质量稳定性。任何时候取出来的数据都有可能听起来不自然。</p><p>二是只能使用相邻的音素，但实际应用中很多时候需要使用并不相邻的音素。</p><p>于是就有了Unit selectioin synthesis系统，可以视作上一个 DiPhone系统的新一代产品。他做了如下两个改进：</p><p>1.之前的系统中，每个音素就是单独的音素，在新系统中，每一个音素是几个小时时长的存储单元，包含了同一音素在各个场景下的细微不同的发音。</p><p>2.在之前的系统中，音素串联采用例例如PSOLA算法等，新的系统中，采用了信号处理系统处理串联。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191230_1635_42_296.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191230_1636_30_296.png" alt=""></p><p>Making choice by cepstral distance algorithm.</p><p>代价函数和分项（Target cost、Join cost）如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191230_1640_56_763.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191230_1641_13_154.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191230_1641_25_194.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_6 - HMM &amp; HEMM</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_6%20-%20HMM%20&amp;%20HEMM/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_6%20-%20HMM%20&amp;%20HEMM/</url>
    
    <content type="html"><![CDATA[<h1 id="6-Hidden-Markov-and-Maximum-Entropy-Models（HMM-MEMM）"><a href="#6-Hidden-Markov-and-Maximum-Entropy-Models（HMM-MEMM）" class="headerlink" title="6.Hidden Markov and Maximum Entropy Models（HMM,MEMM）"></a>6.Hidden Markov and Maximum Entropy Models（HMM,MEMM）</h1><p>HMM and HEMM are both sequence classifiers.</p><p>Sequence classifier or sequence labeler is a model whose job is to assign some label or class to each unit in a sequence.The FST we studied in Chapter 3 is a kind of non-probabilisitic sequence classifier.</p><p>We have seen on important sequence classification task:POS tagging.</p><p>This chapter is roughtly divided into 2 section:HMM , MEMM.</p><p><strong>6.1</strong>) Markov Chains - 马尔科夫链 - 详见概念6 - 马尔科夫链</p><p><strong>6.2</strong>)The Hidden Markov Model - 隐性马尔科夫模型 - 详见概念6-马尔科夫模型</p><p>Both were defined in appendix.</p><p><strong>6.3</strong>)HMM的三个问题 - 详见概念6-三个问题</p><ul><li>likelihood</li><li>learning</li><li>decoding</li></ul><p><strong>6.6)</strong></p><p>HEMM和CRF和HMM</p><p>【详见概念_6_HMM,HEMM,CRF的比较】</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_5 - Part-of-Speech Tagging</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_5%20-%20Part-of-Speech%20Tagging/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_5%20-%20Part-of-Speech%20Tagging/</url>
    
    <content type="html"><![CDATA[<h1 id="5-Part-of-Speech-Tagging"><a href="#5-Part-of-Speech-Tagging" class="headerlink" title="5.Part-of-Speech Tagging"></a>5.Part-of-Speech Tagging</h1><p>Introduce three algorithms:</p><ul><li>rule-based tagging</li><li>HMM tagging</li><li>transformation-based tagging</li></ul><p>词性标注是一项消歧任务，很多情况下词具有多于一个的意思，我们的工作是为这种情况找到正确的标签。</p><h2 id="5-2）Tagset-for-English"><a href="#5-2）Tagset-for-English" class="headerlink" title="5.2）Tagset for English"></a><strong>5.2</strong>）Tagset for English</h2><p>There are 3 different tagset.</p><ul><li>45-tag Penn Treebank tagset</li><li>61-tag C5 tagset</li><li>87-tag tagset </li></ul><ul><li>Small Tagset:</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191225_1326_22_350.png" alt=""></p><ul><li>Middle Tagset</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191225_1328_08_570.png" alt=""></p><ul><li>Large Tagset</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/n7NLrfur0L.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191225_1330_02_268.png" alt=""></p><h2 id="5-3-Part-of-Speech-Tagging"><a href="#5-3-Part-of-Speech-Tagging" class="headerlink" title="5.3) Part-of-Speech Tagging"></a><strong>5.3</strong>) Part-of-Speech Tagging</h2><p>Sometimes,tagging can be difficult（unambiguous）,For example ,book is ambiguous.Book can be a noun as a read book,or be a verb, as booking a hotel.Below picture shows the number of word type with different level of part-of-speech ambiguity.</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191225_1344_07_777.png" alt=""></p><p>There are three kinds of tagging:</p><ul><li>rule-based tagging</li><li>HMM tagging</li><li>transformation-based tagging</li></ul><h3 id="5-3-1-rule-based-tagging"><a href="#5-3-1-rule-based-tagging" class="headerlink" title="5.3.1 rule-based tagging"></a><strong>5.3.1 rule-based tagging</strong></h3><p>like the code:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191225_1604_00_167.png" alt=""></p><h3 id="5-3-2）HMM-tagging"><a href="#5-3-2）HMM-tagging" class="headerlink" title="5.3.2）HMM tagging:"></a><strong>5.3.2）HMM tagging</strong>:</h3><h4 id="HMM-Tagging的组件"><a href="#HMM-Tagging的组件" class="headerlink" title="HMM Tagging的组件:"></a><strong>HMM Tagging的组件:</strong></h4><p>HMM Tagging有两个组件，即A和B概率矩阵。</p><p>矩阵A包<strong>含标签转移概率</strong> <img src="https://www.zhihu.com/equation?tex=P%28t_i%7Ct_%7Bi-1%7D%29" alt="[公式]"> ，<img src="https://www.zhihu.com/equation?tex=P%28t_i%7Ct_%7Bi-1%7D%29" alt="[公式]"> <strong>表示给定前一个标签(隐藏状态)的当前标签发生的概率</strong>。例如，像will这样的情态动词很可能后面跟着一个基本形式的动词，比如race，所以我们认为这种可能性很高。<strong>我们通过计数来计算这个转移概率的最大似然估计</strong>，<strong>在我们看到标记语料库中的第一个标记的次数中，第一个标记后面紧跟着第二个标记的频率(计算第一个标记后第二个标记出现的次数再除第一个标记总的出现次数)：</strong></p><p><img src="https://pic4.zhimg.com/v2-821f5c0d8508923acb74ced907900d53_b.png" alt="img"></p><p>通俗写法：P（race|will） = (will,race)次数   / （will）次数</p><p>例如，在华尔街日报语料库中，MD(情态动词)发生13124次，紧随其后的是VB(动词原形)出现10471, MLE估计为：</p><p><img src="https://pic4.zhimg.com/v2-28d6d7976b3bf9d6526b95923f9e3653_b.png" alt="img"></p><p>在返回解码算法之前，让我们看一个示例。</p><p><img src="https://pic3.zhimg.com/v2-39d989ee5a949ae21123c1f704229416_b.png" alt="img"></p><p>分子是数据中标记为 <img src="https://www.zhihu.com/equation?tex=t_i" alt="[公式]"> 时，单词也是 <img src="https://www.zhihu.com/equation?tex=w_i" alt="[公式]"> 的出现次数，分母是 <img src="https://www.zhihu.com/equation?tex=t_i" alt="[公式]"> 出现的次数，在WSJ语料库中出现的13124个MD中，与will一起出现4046次：即，P（will是情态动词 |will ） = C(will,will情态动词)/C(所有情态动词).</p><p><img src="https://pic4.zhimg.com/v2-6f53212d4bcbe071656d95948eeba477_b.png" alt="img"></p><p>我们先前看到过这种贝叶斯模型; 回想一下，这个可能性术语并不是在问“词will最可能的标签是什么？”那将是后验 <img src="https://www.zhihu.com/equation?tex=P%28MD%7Cwill%29" alt="[公式]"> 。相反，<img src="https://www.zhihu.com/equation?tex=P%28will+%7C+MD%29" alt="[公式]"> 回答了一个有点违反直觉的问题“如果我们要生成一个MD，这个情态动词(MD)有多大可能是will？”</p><p>图8.4所示为HMM词性标记器中三种状态的A转移概率矩阵和B观察概率矩阵;完整的标记器将为每个标记提供一个状态。</p><p><img src="https://pic2.zhimg.com/v2-b0c92f0f469c96823534cc857f0c0a0d_b.jpg" alt="img"></p><p>Figure 8.4。HMM表示的两部分的一个示例:用于计算先验概率的A转换概率，以及与每个状态相关的B观察概率，每个可能的观察词的一种可能性</p><p>*<em>HMM Tagging工作原理 *</em></p><p>对于任何包含隐变量的模型，例如HMM，确定与观察序列对应的隐变量序列的任务称为解码.</p><p><strong>Tagging</strong> 给定一个HMM模型 <img src="https://www.zhihu.com/equation?tex=%5Clambda+%3D+%28A%2C+B%29" alt="[公式]"> 和一个观察序列 <img src="https://www.zhihu.com/equation?tex=O+%3D%5C%7Bo_1%2Co_2%2C...o_T%5C%7D" alt="[公式]"> ，找出最可能的状态序列 <img src="https://www.zhihu.com/equation?tex=Q+%3Dq_1q_2...q_T" alt="[公式]"> </p><p>对于词性标注，<strong>隐马尔可夫解码的目标是在给定n个单词 <img src="https://www.zhihu.com/equation?tex=w_%7B1%7D%5E%7Bn%7D" alt="[公式]"> 的观察序列的情况下，选择最可能的标记序列 <img src="https://www.zhihu.com/equation?tex=t_%7B1%7D%5E%7Bn%7D" alt="[公式]"> 作为标记序列</strong>：</p><p><img src="https://pic2.zhimg.com/v2-ed2abb5d28fcaf4cc0f3c178bf7eeb29_b.png" alt="img"></p><p>我们在HMM中做这件事的方法是用<strong>贝叶斯规则来代替计算:</strong></p><p><img src="https://pic3.zhimg.com/v2-716e8251d6c81015640c7dcedfdb46ea_b.png" alt="img"></p><p>此外，我们通过去掉分母 <img src="https://www.zhihu.com/equation?tex=P%28w_%7B1%7D%5E%7Bn%7D%29" alt="[公式]"> （分母可去对结果无影响）来简化式8.14:</p><p><img src="https://pic4.zhimg.com/v2-d300cff6cc2fc091149d5b6702054c53_b.png" alt="img"></p><p>标记器进一步简化了两个假设。首先，<strong>一个单词出现的概率只取决于它自己的标签，与相邻的单词和标签无关:</strong></p><p><img src="https://pic3.zhimg.com/v2-717897d2ccbbdef5cf21a77c841d2f1a_b.png" alt="img"></p><p><strong>第二个假设，bigram假设，一个标签的概率只依赖于前一个标签，而不是整个标签序列;</strong></p><p><img src="https://pic4.zhimg.com/v2-64b9f026b07a0b5153ec3746b1fe0c0f_b.png" alt="img"></p><p>将式8.16和式8.17的简化假设代入式8.15，得到bigram tagger中最可能的标签序列如下式：</p><p><img src="https://pic4.zhimg.com/v2-850351b4b6b59972589d018319d55153_b.png" alt="img"></p><p>式8.18的两部分与我们刚刚定义的B状态概率和转移概率完全对应。</p><p>Viterbi algo on POS Tagging<img src="http://bqlab-pic.test.upcdn.net/pic/20191225_1951_16_048.png" alt=""></p><h3 id="5-3-3-Transformation-based-Tagging"><a href="#5-3-3-Transformation-based-Tagging" class="headerlink" title="5.3.3) Transformation-based Tagging"></a>5.3.3) Transformation-based Tagging</h3><p>This kind of tagging draws inspiration from both rule-based and HMM tagging:it based on the rules and also have machine learning technique.</p><p>How TBT applied?</p><p>First,every words was initilized by most likely label.</p><p>Secondly,It exams every possible transformation and select the one that result in most improved tagging</p><p>Finally,it then re-tags the data according to this rule.</p><p>TBT will repeat last two stage until it reaches some stopping criterion.</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_3 - Word &amp; Transducers</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_3%20-%20Word%20&amp;%20Transducers/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_3%20-%20Word%20&amp;%20Transducers/</url>
    
    <content type="html"><![CDATA[<h1 id="3-Word-amp-Transducers"><a href="#3-Word-amp-Transducers" class="headerlink" title="3.Word &amp; Transducers"></a>3.Word &amp; Transducers</h1><p><strong>3.1</strong>)Word</p><p>Before processing, words in speech should be Stemming、lemmatization and tokenization.</p><p>Tokenization means put”New York”in one word,separate “I`m” into “I” and “am”。</p><p>Using FSA to build Stemming net of words</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191220_1600_40_308.png" alt=""></p><p><strong>3.2</strong>)FST : Finite state transducers.</p><p>I define it my way:</p><p>If you input “aa”,”b”will be output,and the state in still “q0”,if you input “b”,”a or b”will out put,state will be “q1”.For example,if you input “aa aa b a”，output is “b b a ba”.</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191220_1655_35_867.png" alt=""></p><p>Defination with details is in another md file.</p><p>FSA is isomorphic to <strong>regular languages</strong>, FSTs are isomorphic to <strong>regular relations</strong></p><p><strong>3.9</strong>)Tokenization</p><p>“Space空格键” is not a sufficient tool to separate words from sentences,and many languages dont have spaces(Chinese,Japanes,Thai).A example:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191220_1704_30_824.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191220_1704_51_270.png" alt=""></p><p>Tokenizer can be build with simple regular expression,which can be easily implemented in FSTs.See the script as follow:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191220_1708_03_391.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_4 - N-Grams</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_4%20-%20N-Grams/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_4%20-%20N-Grams/</url>
    
    <content type="html"><![CDATA[<h1 id="4-N-Grams"><a href="#4-N-Grams" class="headerlink" title="4 - N-Grams"></a>4 - N-Grams</h1><p>N-gram is a language model,is a N-token sequence of words.</p><h3 id="1-the-way-to-calculate-conditional-probability"><a href="#1-the-way-to-calculate-conditional-probability" class="headerlink" title="1.the way to calculate conditional probability."></a>1.the way to calculate conditional probability.</h3><p>For example,how can we calculate <strong>P(the|its water is so transparent that)</strong> ?</p><p><strong>Method1):</strong> counting 2 sentences</p><p>Counting the times of “its water is so transparent that the” and “its water is so transparent that” in the whole corpus.It works sometime,but language is creative,many sentences is not exist.</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191222_1112_48_111.png" alt=""> </p><p><strong>Method2):</strong> chain rule of probability,then use method 1.</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191222_1149_27_804.png" alt=" m"></p><p>notice that:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191222_1331_13_781.png" alt=""></p><p>means  P(w1w2w3w4……wn)</p><p>But <strong>P(its|water is) * P(water|is) * P(is) *<em>still don`t work fine.The reason is same to *</em>Method1</strong>,some sentence not exist.</p><p><strong>Method3):</strong> transform the question to <strong>P(the | that)</strong></p><p>Markov assumption: We can predict the probability of future words without looking too far away.So here is the <strong>bigram</strong> model : <strong>P(the|Walden Pond`s water is so transparent that)</strong> = <strong>P(the|that)</strong>.</p><p>How can we caclute <strong>P(the | that)</strong>?</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191222_1342_51_243.png" alt=""></p><p>As above,we count the number of <strong>（”that the“ / “that”)</strong>.</p><p>Example for method3；</p><p>mini-corpus:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191222_1355_44_172.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191222_1356_06_426.png" alt=""></p><p>例如上图第一个概率等式：分母是&lt;s&gt;出现的总次数，一共是3。分子是&lt;s&gt;后面跟着I 的次数，一共是2。</p><h3 id="2-randomly-genarate-sentences-by-n-gram"><a href="#2-randomly-genarate-sentences-by-n-gram" class="headerlink" title="2.randomly genarate sentences by n-gram"></a>2.randomly genarate sentences by n-gram</h3><p>Sentences randomly generated by using corpora of <strong>Shakespeare`s works</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191223_1438_50_162.png" alt=""></p><p>Sentences randomly generated by using corpora of <strong>Wall Street Journal</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191223_1441_18_717.png" alt=""></p><h3 id="3-How-to-evaluating-N-gram-Perplexity"><a href="#3-How-to-evaluating-N-gram-Perplexity" class="headerlink" title="3.How to evaluating N-gram : Perplexity"></a>3.How to evaluating N-gram : Perplexity</h3><p>How to evaluating a language model from good to bad?</p><ul><li><p>extrinsic evalation ：</p><p>Extrinsic evalation is applying language model to different specific question(machine translator,speech recognition,spelling corrector etc),and compare two LM the cost of time.Is`s intution,but usually cost too much time.</p></li><li><p>intrinsic evalation :</p><p>Just calculate perplexity : 当语言模型训练完后，测试集中的句子都是正常的句子，模型在测试集上验证概率越高，说明句子概率越大，困惑度越小，模型越好。</p></li></ul><p>The <strong>Perplexity</strong> was defined as follow:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191223_1504_16_492.png" alt=""></p><p>a <strong>example</strong> of calculate <strong>perplexity</strong> in a mini corpora:</p><p>​    mini corpora is just 10 digits:(0,1,2,3,4,5,6,7,8,9)</p><p>​    <img src="http://bqlab-pic.test.upcdn.net/pic/20191223_1512_43_979.png" alt=""></p><p>​    <img src="http://bqlab-pic.test.upcdn.net/pic/20191223_1513_07_960.png" alt=""></p><p>如下，另一个在同一个语料库上生成的un / bi / tri - gram模型的困 惑值有显著不同。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191223_1529_59_667.png" alt=""></p><h3 id="4-Smoothing"><a href="#4-Smoothing" class="headerlink" title="4. Smoothing"></a>4. Smoothing</h3><p>为什么需要平滑（Smoothing）？</p><p>（平滑通过）</p><p>Smoothing.Smooth provides a better way of estimating the probability of N-grams than MLE.Commonly used smoothing algorithms for N-grams rely on lower-order N-gram counts through backoff or interpolation</p><p>3.Both backoff and interpolation require discounting such as Kneser-Ney,WittenBell,or Good-Turing discounting.</p><p>假设语料库由如下三个句子构成：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1317_13_202.png" alt=""></p><p>按照最大似然估计的方法来计算p(BROWN READ A BOOK)的概率如下：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1317_57_370.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1318_14_432.png" alt=""></p><p>结果是0.06。这时候求p（David read a book）的概率是多少呢？因为David从来没有在语料库中出现过。所以如下所示，求出来的概率为0.但这与事实不符。因为语言是creative的，但语料库是有限的，一定会有一些语料没有在语料库中出现，所以需要对这部分没有出现在语料库中的creative的语料做平滑处理（也就是劫富济贫，把出现多的语料劫一部分出来分给他们）。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1321_41_820.png" alt=""></p><p><strong>Laplace Smoothing</strong></p><p>Laplace smoothing just add 1 to any tokens.It`s not a good enough smoothing method,but it`s a good start-up baseline.</p><p>Before Laplace smoothing:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1117_36_583.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1120_07_549.png" alt=""></p><p>After Laplace smoothing</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1117_06_667.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1121_21_187.png" alt=""></p><p><strong>Kneser-Ney smoothing</strong></p><p>Equation:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1340_17_404.png" alt=""></p><p>c是词语出现的次数。N(c+1)是出现次数为（c+1）次的词语的总数，如下是两个语料库的示例：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1342_40_890.png" alt=""></p><p><strong>插值与回退（Interlation and backoff）</strong></p><p>插值：interlation</p><p>在一些情况下，无3-gram example，只得用2-gram，1-gram代替之</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1513_27_145.png" alt=""></p><p>λ的其他构成</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1513_40_098.png" alt=""></p><p>回退：backoff</p><p>回退法恐怕是最好理解的一种平滑方法了，它的思路就是：如果一个n-gram的条件概率为0，则用(n-1)-gram的条件概率取代，如果(n-1)-gram的条件概率依然为0，继续回退，直到1-gram概率，如果1-gram依然为0，就直接忽略掉该词。用式子表示如下：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1525_53_830.png" alt=""></p><p><strong>交叉熵cross-entropy</strong></p><p>cross entropy of m on p is defined by</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191224_1539_52_534.png" alt=""></p><pre><code>Summary:1.N-gram probability is conditioinal probability of a word given the previous N-1 words.N-gram probabilities can be computed by simply counting in a corpus and normalizing(the maximum likelihood estimate),or they can be computed by more sophisticated algorithms.2.Smoothing.Smooth provides a better way of estimating the probability of N-grams than MLE.Commonly used smoothing algorithms for N-grams rely on lower-order N-gram counts through backoff or interpolation3.Both backoff and interpolation require discounting such as Kneser-Ney,WittenBell,or Good-Turing discounting.4.N-gram LM are evaluated by separating the corpus int a training set and a test set.training the model on the training set,and evaluating on the test set.The perplexity of LM on a test set is used to compare LM performance.</code></pre>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_24 - 对话agent</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_24%20-%20%E5%AF%B9%E8%AF%9Dagent/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_24%20-%20%E5%AF%B9%E8%AF%9Dagent/</url>
    
    <content type="html"><![CDATA[<h1 id="24-Dialogue-and-Conversational-Agents"><a href="#24-Dialogue-and-Conversational-Agents" class="headerlink" title="24 - Dialogue and Conversational Agents"></a>24 - Dialogue and Conversational Agents</h1><p>这一章介绍问答助手的基本结构+算法。Session24.1介绍人类对话的基本概念，如对话的交替，表达技巧，grounding，对话结构等。Session24.2介绍口语系统的组件和评价标准。Session24.5和Session24.6介绍信息状态架构和马尔科夫对话代理模型，以及高阶话题如BDI范式（belief - desire -intention）信念 - 渴望 - 意图范式。</p><h3 id="24-1-人类对话的Properties"><a href="#24-1-人类对话的Properties" class="headerlink" title="24.1 人类对话的Properties"></a>24.1 人类对话的Properties</h3><h4 id="24-1-1-Turns-and-turn-talking"><a href="#24-1-1-Turns-and-turn-talking" class="headerlink" title="24.1.1 Turns and turn-talking"></a>24.1.1 Turns and turn-talking</h4><p>人类对话的模式是一个人说完了之后另一人说，交替进行，通常情况下，两人对话的重叠部分不超过5%。两个人交替间的停顿时间在100ms左右。为了实现这种模式，人类对话通常有如下三个规律来规范交替的进行，非常显而易见。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_GQuE6iM6Bj.png" alt=""></p><p>通常一个比正常情况更长的听读怒会有额外的表达效果，如下所示一个长停顿代表了不想积极回应问题。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_RxTCJd6Uy7.png" alt=""></p><h4 id="24-1-2-Language-as-Action-Speech-Acts-语言是会产生后果的行动"><a href="#24-1-2-Language-as-Action-Speech-Acts-语言是会产生后果的行动" class="headerlink" title="24.1.2 Language as Action:Speech Acts 语言是会产生后果的行动"></a>24.1.2 Language as Action:Speech Acts 语言是会产生后果的行动</h4><p>语言能对现实世界产生具体影响。根据语言产生不同的影响，把语言分为如下三类：</p><ul><li>说话行为Locutionary act: 单纯说话，表达某个东西，传递某种意思。</li><li>施事行为Illocutionary act:提问，回答，许诺，通过话语影响听众，改变其观点和行动。</li><li>取效行为Perlocutionary act：故意改变听众的观点和行为，听众会采取具体的举措。</li></ul><p><strong>“外边真冷”</strong>这句话</p><ul><li><p>仅仅说出来听众不在意是说话行为</p></li><li><p>说出来了听众也觉得确实很冷想把窗户关上是施事行为</p></li><li><p>听众停了之后起身把窗户关上了是取效行为。</p></li></ul><p>speech act主要用来描述<strong>施事行为</strong>。细分为如下五类：</p><ul><li>论证:承诺某事就是这样的。（示意，论证，发誓，吹嘘，下结论等）</li><li>指令:让听众做某事。(要求，命令，请求，邀请，建议，恳求)</li><li>承诺:向听众承诺未来某事会发生（承诺，计划，发誓，打赌，不同意）</li><li>表达:向听众表达心理状态（感谢，抱歉，欢迎，谴责）</li><li>声明:为世界带来新状态（辞职，宣战，辞退，分手）</li></ul><h4 id="24-1-3-Language-as-Joint-Action-Grounding"><a href="#24-1-3-Language-as-Joint-Action-Grounding" class="headerlink" title="24.1.3 Language as Joint Action: Grounding"></a>24.1.3 Language as Joint Action: Grounding</h4><p>和独白不同，对话需要演讲者与听众保持持续的 common ground，听众需要不停给研究者反馈，代表自己听懂/没听懂。正如电梯按钮按完了会亮一样，信息发出者<strong>需要</strong>即时反馈。</p><p>以下五种反馈按照积极程度递增的顺序排序</p><ul><li>持续注意：听众显示他一直保持对演讲者的注意力</li><li>开启类似话题：听众开启类似话题</li><li>认可：听众点头，说对或者太好了之类肯定短语</li><li>示范：听众通过调整语序/套用研究者的发言，来示范给演讲者看</li><li>演示：听众逐字逐句演示给演讲者</li></ul><h4 id="24-1-4-Conversational-Struture"><a href="#24-1-4-Conversational-Struture" class="headerlink" title="24.1.4 Conversational Struture"></a>24.1.4 Conversational Struture</h4><p>了解了对话的一般结构才能设计对话系统。如下。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_lypkQUT0m9.png" alt=""></p><h4 id="24-1-5-Conversational-Implicature-会话含义"><a href="#24-1-5-Conversational-Implicature-会话含义" class="headerlink" title="24.1.5 Conversational Implicature 会话含义"></a>24.1.5 Conversational Implicature 会话含义</h4><p>目前为止我们只学习了对话系统的“基础设施”，还没有学习信息究竟是如何从研究者传递到听众的。17章有演示如何计算一句话的meaning。在对话中，通常meaning要比word所表达的本身含义再多出来（extend）一点点。如下一个例子，回答者并未直接回答问题的，但是传达出了enough info.</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_EdfV3Innth.png" alt=""></p><p>以下4个“最大”是指导会话含义理解</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_iEJLDOMIvj.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_tQ2PwVZI99.png" alt=""></p><h3 id="24-2-基础对话系统"><a href="#24-2-基础对话系统" class="headerlink" title="24.2 基础对话系统"></a>24.2 基础对话系统</h3><p>上文对人类对话有所介绍，下文将对对话系统的六个模块做逐一介绍。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_YPJpxMWo9o.png" alt=""></p><p>左一到左四好理解。右一用于控制对话domain，例如航班预定，生活信息查询等。右二是总控。</p><h4 id="24-2-1-ASR模块-语音识别模块"><a href="#24-2-1-ASR模块-语音识别模块" class="headerlink" title="24.2.1 ASR模块 -  语音识别模块"></a>24.2.1 ASR模块 -  语音识别模块</h4><p>在第九章中介绍过，语音识别模块输入的是音素，输出的是字符串。对于domain-based的对话系统，语音识别模块只需要辨识身后的自然语言理解模块能理解的特定领域语料即可。为此，商业对话系统基本都用<strong>基于有限状态语法规则的非概率语言模型</strong>。这些语法规则由人为规定，对所有系统可能understand的语言做出特定response。在Session24.3中我们会介绍适用于VoiceXML系统的人为构建的语法规则。</p><p>通常对话系统中的语言模型是要依赖<strong>对话state</strong>的。如果一个对话系统进行到提出问题“Which city are you departing from?”的时候，这个系统的语言识别模块只能识别后边这一句话的括号内内容“I (leave|depart)from [CITYNAME]”。</p><p>在对话系统期待用户说话的场景下，系统通常运用restrictive grammar，来限制对话的状态数量。</p><h4 id="24-2-2-NLU-模块-自然语言理解模块"><a href="#24-2-2-NLU-模块-自然语言理解模块" class="headerlink" title="24.2.2 NLU 模块 - 自然语言理解模块"></a>24.2.2 NLU 模块 - 自然语言理解模块</h4><p>如何实现理解自然语言？要借鉴22章提到的信息提取技术。也就是frame-and-slot技术。例如一个“帮助用户寻找合适航班”的系统，它的frame-with-slot就是如下这样的：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_YtyIywxGbT.png" alt=""></p><p>如何把句子拆解成frame需要的格式从而填补slot？使用一些手工制作的semantic grammars来parse句子。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_J9qCVHmSEk.png" alt=""></p><p>另一种办法是使用HMM的反编码技术，把句子当成发射值，待填补的slot当成隐状态。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_wmjBFvCtqm.png" alt=""></p><p>数学过程老生常谈，贝叶斯概率对换+分母可忽略+单独N概率用语言模型N-gram计算。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_IAtyar6XGU.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_O2o3TGwHWE.png" alt=""></p><h4 id="24-2-3-Generation-and-TTS-Components"><a href="#24-2-3-Generation-and-TTS-Components" class="headerlink" title="24.2.3 Generation and TTS Components"></a>24.2.3 Generation and TTS Components</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/Typora_HxU6DdyGmC.png" alt=""></p><p>输出模块，核心任务有两个，分别是<strong>what to say</strong>和<strong>how to say it</strong>。</p><p><strong>what to say</strong> 的任务用content planner实现，它决定了要给用户表达什么，是否要问问题，如何回答等等，这个<strong>content planner</strong>组件通常和<strong>Dialogue Manager</strong> merge在一起,we come back later on this.</p><p><strong>how to say it</strong>的任务用language generation模块生成。一种实现是用template - based generation,它有很多模块可被填充，如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_mJ9hbrSOcs.png" alt=""></p><p>还有一种实现how to say it的方法是使用自然语言生成。它由如下三个模块组成。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_o0136Nyaer.png" alt=""></p><h4 id="24-2-4-Dialogue-Manager-核心：对话管理模块"><a href="#24-2-4-Dialogue-Manager-核心：对话管理模块" class="headerlink" title="24.2.4 Dialogue Manager 核心：对话管理模块"></a>24.2.4 Dialogue Manager 核心：对话管理模块</h4><p>本模块接受识别模块输入的信息，把输出传递给上个session将的Generation模块。对话管理模块最常见的架构是有限状态-框架基础(finite-state,frame-based)的架构，将在本session介绍。下个session将讨论更强大的information-state dialogue manager（基于马尔科夫决策过程的概率版本的information-state manager）。最后介绍plan-based architecutes.</p><p>最简单的对话管理模块架构就是finite-state manager。如下图所示，对话由彼此相连的状态（system）完全控制，这个系统的工作模式就是连续问用户好几个问题，忽略掉用户说的其他任何内容，这种系统也叫<strong>system-initiative</strong> system。这种系统要求用户exactly回答它提出的任何问题，这会让对话非常生硬sometimes annoying。在设计这种finite-state系统的架构时，当然也可以在每个state上都链接一些subset of state，但是这样会使state数量爆炸，bad arch。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_ICuRoueyFG.png" alt=""></p><p>于是有了<strong>mixed initative</strong>，这种设计就不是完全系统主导了，而是用户和系统一起主导对话。这种系统的常见架构是frame-based,system问一些问题，然后提取用户的回答来填满slot。这种slot-用户answer的设计如下所示</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_AZioJu8Ck1.png" alt=""></p><p>这种系统会一直攒slot，直到攒够slot足够在database中做查询和返回后，返回给用户回答。如果用户同时回答两三个问题，那系统就是并行打开两三个slot来攒用户的信息。</p><h4 id="24-4-Dialogue-System-Design-and-Evaluation-对话系统设计思路"><a href="#24-4-Dialogue-System-Design-and-Evaluation-对话系统设计思路" class="headerlink" title="24.4 Dialogue System Design and Evaluation 对话系统设计思路"></a>24.4 Dialogue System Design and Evaluation 对话系统设计思路</h4><ol><li><p>研究用户和任务类型。</p><p>了解潜在用户画像，评估任务类型。常见的研究方法包括对类似系统调研，对潜在用户发调查问卷，研究类似的人类之间的对话等。</p></li><li><p>构建模拟和原型（simulationas and prototypes）</p></li><li><p>迭代地测试设计思路</p></li></ol><h3 id="24-5-Information-State-and-Dialogue-Acts"><a href="#24-5-Information-State-and-Dialogue-Acts" class="headerlink" title="24.5 Information-State and Dialogue Acts"></a>24.5 Information-State and Dialogue Acts</h3><p>目前我们介绍的frame-based 的对话系统都只能在限定的domain运行。因为语义解释和基于框架的语义生成过程都要根据slot-filling的需求开展。但是更实用的对话系统应该不仅仅是用来满足内部的插槽而已，而是能决定何时应该打断用户的话，在不清楚的地方能想用户提出更明确的问题，如何给出几个靠谱的suggestion等。所以这个session我们讲解一个更先进的架构，<strong>information - state arch</strong>下个session我们还会讲更深入的马尔科夫决策过程。本session讲解的information-state arch包括如下五个组件；</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_KJsYGsm55W.png" alt=""></p><p>第一个组件，信息状态，是一个高度抽象的概念，比上一session中介绍的有限状态机中的状态更复杂。它包含了语义上下文信息，两个对话者的共识，当下使用的模型，两个对话者的注意力等。</p><p>第二三组件一起讲，是一个对话act组件。他是speech act组件的一个扩展，以情感分析和dialogue act作为输入，以text/speech为输出。</p><p>最后，update rule组件用于更新information state。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_5ksCF31ab0.png" alt=""></p><h4 id="24-5-1-Using-Dialogue-Acts-对话行为"><a href="#24-5-1-Using-Dialogue-Acts-对话行为" class="headerlink" title="24.5.1 Using Dialogue Acts 对话行为"></a>24.5.1 Using Dialogue Acts 对话行为</h4><p>Dialogue Act是Speech Act的延伸，如下是scheduling domain（主要用于在某领域预定一个meeting）的18个Dialogue Act。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_i89KX4vp2C.png" alt=""></p><p>每个Act有如下四种大类</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_9xs5XUQMpq.png" alt=""></p><h4 id="24-5-2-解释对话行为"><a href="#24-5-2-解释对话行为" class="headerlink" title="24.5.2 解释对话行为"></a>24.5.2 解释对话行为</h4><p>我们如何解释一个对话行为？如何判断输入字段是一个Suggestion/Question/Statement ？如下是三种问题的示例。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_GfIZXDkpXs.png" alt=""></p><p>有些间接的speech act，比如一个看上去是Statement的句子实际是一个question等等。为了判断出一个句子到底是哪种act，需要引入supervised classification task，这个分类任务使用的feature是act的micro grammar,grammar 由如下三个组件构成。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_UMoQ4SAUM5.png" alt=""></p><ol><li>从单词上分析，一个“please”或者’’would you”通常一个Request的象征。“Are you”是一个YES-NO        questiond的象征，这种单词分析可以在Dialogue-Specific N-gram grammars中进行。</li><li>Prosody音韵分析:升降调/气息强弱通常可判断肯定或否定.</li><li>对话结构:yeah后边跟一个建议通常就是agreement等。</li></ol><p>总体来说，给定一个观测句子o，确定这个句子的对话act是多少的 概率由如下数学表达式定义，还是熟悉的贝叶斯对换位置公式。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_vtEvtiY9Eh.png" alt=""></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_23 - QA</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_23%20-%20QA/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_23%20-%20QA/</url>
    
    <content type="html"><![CDATA[<h1 id="23-Question-Answering-and-Summarization"><a href="#23-Question-Answering-and-Summarization" class="headerlink" title="23 - Question Answering and Summarization"></a>23 - Question Answering and Summarization</h1><p>本章介绍基于事实的回答问题系统+文章的总结系统。如果我们查找的是结构化的数据，可用上一章介绍的信息提取算法来词查找。如果查找的是非结构化的信息，则需要用本章介绍的“回答问题系统”处理，非结构化信息的查找就是一种“使用非正式的单词或句子”来表达查找需求的场景，这种场景下客户通常期望能返回一些回答or一些文本，或something in between。</p><p>第一个session介绍向量空间模型，第二个session介绍基于事实的一问一答系统。第六个session介绍基于句子的总结系统/基于注意力机制的总结系统。</p><h3 id="23-1-Information-Retrieval-信息检索"><a href="#23-1-Information-Retrieval-信息检索" class="headerlink" title="23.1 Information Retrieval 信息检索"></a>23.1 Information Retrieval 信息检索</h3><p><strong>术语定义</strong></p><p>文档document指的是一个有索引的，可被检索系统直接定位的最小单元，对应到web就是一个网页</p><p>集合collection指的是一系列用来满足用户需求的文档documents</p><p>term指的是一个词汇元素。</p><p>query指的是一系列terms</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_3w6aY5INY5.png" alt=""></p><p><strong>词向量空间</strong></p><p>词义相近的在向量空间中距也近，概念在CS224N中学过了。空间和点积的直观图如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_WZMYCVDLrg.png" alt=""></p><p>点积：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_dnt6s0Vt8D.png" alt=""></p><p><strong>Term weighting</strong>词汇权重</p><p>Tf-idf表示，概念在CS224N中学过。就是词频 - 逆文档频率系数。词频就是这个词在本文中出现的频率，频率越高越重要；逆文档系数是语料库总文数/包含该词的文树  再取对数，如果词语在各个文章中都有，例如“的”，它在本文中重要性为零，如果词语只在一篇文章中出现，也就是本文，那重要性很高，例如“对<strong>阿尔法脱氧核糖核酸逆转录蛋白</strong>的研究”中的这个BioNER。</p><p>词频数学定义</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_1dDqjUrMmt.png" alt=""></p><p>逆文档频率数学定义</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_5KlTJHKTsq.png" alt=""></p><p>总定义</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/chrome_TpTuxSwx34.png" alt=""></p><ul><li><p>词干化和构建stop list也对performance有帮助。</p></li><li><p>评价IR系统performance的benchmark无外乎是召回率和准确率，在前文有提及。</p></li></ul><h3 id="23-2）Factoid-Question-Answering"><a href="#23-2）Factoid-Question-Answering" class="headerlink" title="23.2）Factoid Question Answering"></a><strong>23.2</strong>）Factoid Question Answering</h3><p>基于事实的问答系统示例如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_8h2ysn9tiw.png" alt=""></p><p>The fundamental problem in factiod question answering is : <strong>Gap between the way that questions are posed and the way that answers are expressed in a text.</strong></p><p>问答系统架构思路是，先用开销较小的IR技术从document库中抽取一小部分相关文件，在这一步中，不使用开销大的NLP技术比如parsing或者role labeling处理海量ducuments，把这一小部分文件用IR技术抽取出来后，再用NLP技术进行处理。</p><p>下图演示了Arch，主要是三块,在接下来的session中逐一介绍</p><ul><li>Question Processing</li><li>Passage Retrieval</li><li>Answer Processing</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_plJqei3v64.png" alt=""></p><h4 id="23-2-1）Question-Processing-问题语句预处理"><a href="#23-2-1）Question-Processing-问题语句预处理" class="headerlink" title="23.2.1）Question Processing 问题语句预处理"></a><strong>23.2.1</strong>）Question Processing 问题语句预处理</h4><p>问题处理这一步分两个大目标，在架构图中可见。分别是对<strong>查询语句方程式化</strong>和对<strong>查询分类</strong>。</p><p><strong>查询语句方程式化 Query Formulation</strong></p><p>目标是从问题中提取出一系列单词which form an IR query,具体要提取成什么样的form视application而定。通常我们会移除stopword和疑问词（who,when,where,etc..）。然后对query使用基于叙词表的算法来扩充它（expansion），然后我们就能得到一个更大的keyword sets。很多系统都使用WordNet作为叙词表。</p><p>还有一种查询语句方程式化的方法是reformulation。也就是把疑问句变形为陈述句，如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_tFty9IZgjU.png" alt=""></p><p>问题分类<strong>Question Classification</strong></p><p>问题分类的依据是<strong>答案的种类</strong>。回答是“A person”和“A City”肯定不是一类。显而易见的思路是使用命名实体识别来做详细分类，但其实这种分类方式不够科学，性能表现更好的分类方式如下图，是人工设计的。而新的问题如何能按照如下的类别分好类？当然是采用反复提到的基于统计学的机器学习方法。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_stMfDtVi3V.png" alt=""></p><h4 id="23-2-2-Passage-Retrieval-段落检索"><a href="#23-2-2-Passage-Retrieval-段落检索" class="headerlink" title="23.2.2 Passage Retrieval 段落检索"></a><strong>23.2.2</strong> Passage Retrieval 段落检索</h4><p>本session介绍QA系统架构的第二大部分 段落检索</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_aAyDZJkhRN.png" alt=""></p><p>接收到上一session中的问题处理模块输出的query后，把这个query放进本模块的IR system中。也就是上图左一中，这个IR system可以是典型的IR引擎也可以是一个Web搜索引擎。总之他会输出出a set of docs.也就是开篇提到的使用IR方法而非NLP方法对文件库中的文件做一个开销较小的筛选，接下来对筛选出来的候选文件上开销大的工具精细处理。</p><p>虽然 a set of doc输出出来经过了相关度排序，但是排在首位的doc不一定就是需要的doc。所以接下来的任务是对选出来的set of doc做extract信息提取的工作。可以采用21章提到的 paragraph segmentation algorithm of type.</p><p>接下来要做的是段落检索，这一步中，我们已经把不相关的段落从doc中过滤掉了，然后把剩下的相关的段落按照相关性排序。然后根据如下的features，使用机器学习方法对剩下的段落重排序，然后挑选之。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_cBEmClxtbL.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_nMimu5gN6E.png" alt=""></p><h4 id="23-2-3-Answer-Processing-回答处理"><a href="#23-2-3-Answer-Processing-回答处理" class="headerlink" title="23.2.3 Answer Processing 回答处理"></a><strong>23.2.3</strong> Answer Processing 回答处理</h4><p>本session介绍QA系统架构的第三大部分回答处理。</p><p>![1578475614060](C:\Users\Bq Lion\AppData\Roaming\Typora\typora-user-images\1578475614060.png)</p><p>上个session已经把有用的段落排好序输进本模块了，本模块作用是提取出有用信息，组织成答案返回给用户。</p><p>两个算法可用于此模块：分别是<strong>答案类型模式提取算法answer-type pattern extraction</strong>和<strong>N-gram平铺算法</strong>。</p><p><strong>答案类型模式提取算法：</strong></p><p>利用了可能的回答类型信息 + 正则表达式。比如预计答案类型是“HUMAN”，那就用正则表达式在候选段落中搜索任何和HUMAN有关的句子并返回，如下例子中带下划线的词汇（NER和长度信息）就是期待的答案类型，被Answer Processing在candidate段落中检索到并返回。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_HaKyl4DMDB.png" alt=""></p><p>当然有些问题的期待回答并不是上述例子中的NER或者长度信息那么简单明了，而是一个较为复杂的句子，面对这种情况只能采用人工标注的正则表达式展开对candidate的搜索。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_661wCiHOAy.png" alt=""></p><p>模式提取算法和20章22章中介绍的关系抽取算法相似，模式抽取算法的目的是在学习<strong>一个特定的回答类型</strong>比如YEAR-OF-BIRTH与一个特定的问题的aspect之间的关系。可以把这个问题抽象为如下表述：</p><p>Relation between (PERSON-NAME/YEAR-OF-BIRTH)和（TERM-TO-BE-DEFINED/DEFINITION）。</p><p>这个表述和20章介绍的学习 Wordnet 句法集中的（下位词/下位词）之间的关系和</p><p>22章中介绍的学习词汇与词汇之间的ACE relations相似。</p><p>算法伪代码描述如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_hINbmRHkOK.png" alt=""></p><p>对于回答类型是BIRTH-OF-DATE的模式识别，学习到了如下两种模式</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_wVvo2x94bY.png" alt=""></p><p>其实仅借助命名实体探测和回答模式分离这两个技术是不够的，通常我们还使用采用了如下特征的分类器对candidate排序。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_kqaxyLw9Hp.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_fMOcu4fulA.png" alt=""></p><p>N-gram tiling算法通常仅用于web搜索。</p><p><strong>23.2.4</strong>对Factoid Answer系统的评价标准</p><p>采用平均导数排序，or MRR，也就是存在一个训练集里边包含了正确答案，将QA系统的回答与正确答案交叉验证。系统给的回答通常有五个，如果第一个就对了得分是1/1，第二个才对得分是1/2，第五个才对得分1/5，没有对的得分0，然后总体套上其他数学形式sum后求均值，表达式如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_DbnWMLsu2Q.png" alt=""></p><h3 id="23-3-Summarization文本总结"><a href="#23-3-Summarization文本总结" class="headerlink" title="23.3 Summarization文本总结"></a>23.3 Summarization文本总结</h3><p>目前为止，我们学习的算法都是对用户呈现整个文档（IR）或者是一个短的基于事实的回答（factoid answer phrase）。有些时候我们需要的是一个篇幅夹在整个文档/一句话 中间的文档，例如对一篇文档的总结，或者对几篇文档的总结（summarization）。</p><p>文本总结（text summarization）的正式定义：是对一个特定任务和用户，输出对一段删节版的text蒸馏出来的信息。如下几种summarization是研究热点。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_CQr6UaQhiI.png" alt=""></p><p>其实之前的IR和QA中的核心任务也包含在了文本总结里边，本质都是对信息的查找和遴选，只不过规模不同叫法不同而已。文本总结的任务通常分为三步：</p><ol><li>内容选择(content selection)：决定要从candidate文档中选择那些句子/语料做为原材料</li><li>Information ordering：如何组织排序刚才选择的原材料s</li><li>Sentence Realization：如何呈现出流利的candidate。</li></ol><h4 id="23-4）单个文件总结"><a href="#23-4）单个文件总结" class="headerlink" title="23.4）单个文件总结"></a><strong>23.4</strong>）单个文件总结</h4><p>继承紧接上文的三步骤，内容选择，信息ordering和句子实现。</p><p>总arch如下图</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_2y4pmgvOly.png" alt=""></p><p><strong>23.4.1</strong>无监督学习的内容选择任务</p><p>内容选择任务可以归类为 classification的机器学习任务。分类器classifier的任务是把candidate文档中每一句话都按照binary标签分类：important VS unimportant。</p><p>最早期的基于直觉设计的用于summary任务中选择句子的分类器就是筛选句子中是否含有大量<strong>关键词</strong>。如何界定关键词多少？建议使用TF-IDF或者对数似然ratio界定。</p><p>具体给句子打分到底有多关键的centrality(x)可以用数学表达式描述如下：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_e8jvcK6FD0.png" alt=""></p><p><strong>23.4.2</strong>基于修辞分析的无监督内容选择任务</p><p>importance veries from diff structures</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_R9MeB15UCu.png" alt=""></p><p>23.4.3监督机器学习的内容选择任务</p><p>也就是存在人手工标注的训练集，Ziff-Davis corpus for example。</p><p>​    <img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_FvEvwciSOi.png" alt=""></p><p><strong>23.4.4</strong>) 句子简化</p><p>句子简化也多是使用监督学习算法，对句子运行parser删除特定的语法单元。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_7ViipL2riN.png" alt=""></p><h3 id="23-5-多文件的总结Muti-DOC-Summarization"><a href="#23-5-多文件的总结Muti-DOC-Summarization" class="headerlink" title="23.5 多文件的总结Muti - DOC Summarization"></a>23.5 多文件的总结Muti - DOC Summarization</h3><p>想真正解决多文件总结任务距离还远得很，一个多文件总结应用场景是在对同一件新闻的各家媒体稿做总结输出。多文件的总结问题的处理架构和之前的单文件总一致，也是<strong>内容选择，信息排序，句子生成</strong>三大步骤。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_ughTbZwJTW.png" alt=""></p><p><strong>23.5.1）</strong>多文件总结中的内容选择模块</p><p>单文件总结中的内容选择，无监督学习和有监督学习都可以用。多文件的内容选择则不行，只能用无监督学习。</p><p>单文件总结与多文件总结的最大区别是<strong>冗余</strong>。所以多文件总结算法的一个重要任务就是避免冗余信息，输出一句新话时，要检查不要与其他已输出的话重复过多。</p><p>数学上的设计是generate一个冗余factor，给重复过多的句子增加罚项。一个可用的factor是MMR系统，也就是<strong>最大边际相关scoring系统。</strong>Sim（s,si）表示s句子和已经分类出来的si句子组之间有多大的相似性，λ是手动调节的重要性参数。</p><p><strong>23.5.2</strong>） 多文件总结中的信息ordering</p><p>把去除了冗余信息的句子们提取出来后，接下来要做的就是对这些信息做ordering，组织他们重新有序。放入具体的实例中就是，对同一个新闻事件的不同媒体新闻稿做总结。一个思路是使用时间序列排序（chronological ordering）。使用时间序列排序的缺点是生成的句子缺乏语序的<strong>一致性</strong>，这个不难理解，不同语篇按照时间顺序硬拼凑在一起必然缺乏一致性。为了解决硬拼缺乏一致性的问题，需要使用coreference-based algorithm.(基于共指算法)。共指算法使用了centering<strong>定心</strong>的概念。</p><p>centering<strong>定心</strong> 的概念是每个语段都有一个突出的实体，也就是<strong>focus焦点</strong>。定心的精髓是聚焦于<strong>焦点</strong>和<strong>焦点之前转换</strong>的<strong>句法实现</strong>。通常两个命名实体和实体间的转换被preferred。这样可以在ordering的时候输出更有一致性的语料。</p><p>一个常用的方法是，对每个句子分配一致性得分，一个between两句子的转换得分，通常包含了词汇的连贯性和基于实体的连贯性。有了这样的评分系统后，选择最优化所有local句子pair的distance的choosing order操作，是时分苦难的，可以转化为<strong>循环排序</strong>和<strong>旅行商问题</strong>，也就是NP完全问题。</p><p>如上文所讲，其实information ordering 和上个session的内容选择是完全分离的两个任务，有一种协同处理他们的模型是HMM模型，文件的隐状态是，内容的topics和sentences observations，for example,对于加州地震新闻报道文稿中，隐状态就是<strong>地震震级，地点，营救，伤亡</strong>。</p><p><strong>23.5.3</strong>）Sentence Realization</p><p>上个模块输出的句子，仍然存在缺乏一致性和语义不同的问题，需要做最后的调整，调整的手段是，使用一些rule，比如下文两个rule。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_LLVpr9e1yK.png" alt=""></p><p>如下一个最终调整实例</p><p>![1578563125318](C:\Users\Bq Lion\AppData\Roaming\Typora\typora-user-images\1578563125318.png)</p><h3 id="23-6-QA系统设计思路"><a href="#23-6-QA系统设计思路" class="headerlink" title="23.6 QA系统设计思路"></a>23.6 QA系统设计思路</h3><p>如何设计QA系统？首先，QA回答的内容必定是从文件库中搜索+整理出来的。按照Q搜索的工作接下来会详细介绍，是一种基于Query-focused 的Summarization；整理的工作就是上个session介绍的多文件整理的工作。</p><p>如何对着问题Q有目的性的搜索？我们可以在上个session中介绍的多文件总结系统上做一些调整，让这个系统在内容选择这个模块上，ranking各个内容的时候，偏重于让含有Q的关键词的句子，ranking的高一些。</p><p>另一个提高performance的思路是，对不同的提问分类，比如分成<strong>查询人物档案，查询药物，查询某物定义等</strong>，然后对这些不同的分类的提问，构建定制化的<strong>内容选择</strong>算法。</p><p>如下是几个常见的提问的definition</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_K4mMtos4p0.png" alt=""></p><p>Sum up，如下是基于<strong>问题导向搜索的，多文件整理的，QA系统架构</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_TDA0lvTTyZ.png" alt=""></p><p>左一模块，文件检索，L是问题长度，N是待查文件数量。返回1127个句子。</p><p>左二模块，谓词识别，这个是根据Q的导向来筛选1127个句子，输出9个根据词问题定制化的句子，和383个非定制化句子。</p><p>左三模块，数据分析，输入上模块383句子个非定制化句子，输出ordering后的句子。</p><p>左四模块，Answer生成。输入上模块的ordering之后的句子+第二个模块输出的9个定制化的句子，然后整理，输出一篇合格的answer。</p><p>具体实例如下：</p><p><strong>输入</strong>：What is a Water Spinach?</p><p><strong>输出</strong>:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_vsWyk0nwtw.png" alt=""></p><h3 id="23-7-文件总结的评估方法"><a href="#23-7-文件总结的评估方法" class="headerlink" title="23.7 文件总结的评估方法"></a>23.7 文件总结的评估方法</h3><p>收到BLEU启发，文件总结系统的评价方式多用ROUGE-2评估（Recall - Oriented Understudy For Gisting Evaluation）。数学定义如下；</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_obUcS6BCt1.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_22 - Information Extraction</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_22%20-%20Information%20Extraction/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_22%20-%20Information%20Extraction/</url>
    
    <content type="html"><![CDATA[<h1 id="22-Information-Extraction"><a href="#22-Information-Extraction" class="headerlink" title="22 - Information Extraction"></a>22 - Information Extraction</h1><p>本章主要概念：</p><ul><li>NER：命名实体识别</li><li>relation detection and classification：关系检测与分类</li><li>event detection and classification：事件检测与分类</li><li>temporal expression recognition：事件表达式识别</li><li>template filling：模板填充</li></ul><h3 id="22-1-NER-命名实体识别"><a href="#22-1-NER-命名实体识别" class="headerlink" title="22.1)NER : 命名实体识别"></a><strong>22.1</strong>)NER : 命名实体识别</h3><p>命名实体识别分类的举例</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_gAytSQ6xFK.png" alt=""></p><p><strong>22.1.2</strong>）NER as Sequence Labeling</p><p>标准的命名实体识别的步骤是使用word-by-word sequence labeling任务。其实进行NER的方法与第五章的POS tagging和十三章的syntactic chunking方法相同。</p><p>PS：提一下第五章的POS tagging：使用的还是HMM base 的 维特比算法（decoding）。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/Typora_2EY2kXShLb.png" alt=""></p><p>问题的本质如下所示，观察到句子/词语，猜测对应的词性/NER类型。也就是观察到结果，猜测其隐状态。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/Typora_mhyTE7MGA1.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/Typora_VaaEPeouLb.png" alt=""></p><p>具体的 word-by-word IOB-style tagging技术细节会在后边的session详细介绍。</p><p><strong>22.1.3</strong>） Evaluation of Named Entity Recognition</p><p>标准的<strong>训练NER system</strong>的步骤如下：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_CkvnPGXNZF.png" alt=""></p><p>首先是人工标注，构建test set，然后构建training set，然后训练之（MEMM,CRF,SVM,HMM等）。</p><p>NER系统的评价方法是F1 measure，和Chunking-System Evaluation中的F1 measure一致。F1 measure就是一个对召回率和准确率之间的取舍，控制取舍的参数是β，β&gt;1偏好召回率，β&lt;1偏好准确率，如下：</p><p>![1578297475664](C:\Users\Bq Lion\AppData\Roaming\Typora\typora-user-images\1578297475664.png)</p><p>其中P是precision，准确率，R是召回率，recall，具体定义如下：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_9o9lFPvgDg.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_SCyRkoJMHq.png" alt=""></p><p><strong>22.1.4</strong>）NER架构</p><ul><li>首先，使用高准确率的rule-based 来tag不模糊的实体。</li><li>搜索substring，which matches之前探测到的命名实体识别。这里使用的方法是19章中提到的基于概率的字符串匹配方法。</li><li>查阅特定domain的 NER list</li><li>使用基于概率的sequence labeling techs</li></ul><h3 id="22-2-关系探测和分类"><a href="#22-2-关系探测和分类" class="headerlink" title="22.2 关系探测和分类"></a>22.2 关系探测和分类</h3><p>命名实体之间必定存在关系，厘清NER之间的关系就能基本上完成语义理解。NER之间的关系如下所示，一部分，并列，树级关系等等。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_0x9NRGiPHR.png" alt=""></p><p><strong>22.2.1</strong>）关系探测中的监督学习方法</p><p>监督学习方法肯定已经不陌生，无外乎就是给定一个人工标注的test集，然后对剩下的没有标注的训练集进行监督学习（无外乎是一种统计学上的模式识别并模仿之）。</p><p>最straghtforward的关系探测的方法是：</p><p>1.列出本句子中任何两对关系 </p><p>2.遍历所有的关系，逐一作出判断</p><p>在第二步中，遍历所有关系，作出判断肯定是使用监督学习的机器学习模型。想要让模型工作的好，特征工程非常重要。构建优秀的特征分为如下三个方面：</p><ul><li>feature of the named entities<ul><li>两个命名实体的种类</li><li>两个种类的级联</li><li>参数的中心词</li><li>参数的的词袋模型</li></ul></li><li>the words in the text<ul><li>词袋和bi-gram词袋 between 两个命名实体</li><li>两个命名实体的词干</li><li>实体上下文的 词干和word</li><li>参数的词距</li><li>参数的实体数量</li></ul></li><li>the syntactic structure<ul><li>在组成结构中是否存在特定的结构</li><li>块 基本路径</li><li>块头部的 Bags</li><li>树的依赖路径</li><li>成分树路径</li><li>参数见的树距离</li></ul></li></ul><p>三个参数的实例如下所示</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_DgRYDGFRft.png" alt=""></p><h3 id="22-3-Temporal-and-Event-Processing-时间和事件处理"><a href="#22-3-Temporal-and-Event-Processing-时间和事件处理" class="headerlink" title="22.3 Temporal and Event Processing(时间和事件处理)"></a>22.3 Temporal and Event Processing(时间和事件处理)</h3><p>至此，我们了解了NER提取和分析他们之间的关系。还有一个非常重要的概念将在本session中介绍：“时间实体”。在一些NLP应用中，对时间这个概念的处理至关重要（其实想要厘清语义，尤其是有一定篇幅的语言的语义，把时间线弄清楚必不可少）。</p><p><strong>22.3.1</strong>）时间表达式的识别</p><p>关于<strong>绝对时间</strong>，<strong>相对时间</strong>，<strong>时间间隔</strong>。是表达时间的三个重要概念。也就是坐标-坐标的加减-基于坐标的距离。如下图片。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_bXYfeu8bND.png" alt=""></p><p>时间实体识别的具体方法有如下三个：</p><ul><li>rule-based system</li><li>基于统计的机器学习（sequence classifiers based on token by tokne IOB enconding）</li><li>作为语义角色标记的组成分类</li></ul><p>对于上文提到的第二个，IOB encoding，训练IOB style 的时间表达式的taggers的典型特征如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_X0B2ceJvbr.png" alt=""></p><p>对时间实体的正则化</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_2fh07ii32E.png" alt=""></p><p><strong>22.3.3</strong>）事件实体的探测和分析</p><p>在英语中，一般时间都和动词相关，几乎都是动词后边跟着介绍事件（event）。rule-based和基于统计的机器学习方法都被应用到了事件实体的探测和分析上。这两种方法都借鉴了句子中的POS tagging，和动词的分类等等。</p><p>当事件（event）和时间因素（temporal expression）都被探测到后，接下来的逻辑任务就是把这些时间信息何来地放到timebank中排序。</p><p><strong>22.3.4</strong>）TimeBank</p><p>TimeBank of Allen`s(1984) relations, as follows:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_c0fOlkkKNn.png" alt=""></p><p>一个抽取事件+时间的句例：</p><p>“Delta Ari Lines <strong>soared[e1]</strong> 33% to a record[t1] in the fiscal first quarter,<strong>bucking[e2]</strong> the industry trend toward <strong>declining[e3]</strong> profits”</p><p>事件有三个，通常用动词标注，上文中黑体标注。</p><p>时间有两个，分别是record和fiscal first quarter。</p><p>事件之间的关系：</p><ul><li>soar[e1] is <strong>before</strong> record[t1]</li><li>soar[e1] is <strong>simultaneous</strong> with the bucking[e3]</li><li>declining[e3] includes soaring[e1]</li></ul><h3 id="22-4-Template-Filling模板填充"><a href="#22-4-Template-Filling模板填充" class="headerlink" title="22.4 Template Filling模板填充"></a>22.4 Template Filling模板填充</h3><p>模板由各个slot插槽组成，插槽中填充的是命名实体。如下是一个例子</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_IiChHL8sBn.png" alt=""></p><p><strong>22.4.1</strong>）使用统计方法的模板填充</p><p>统计方法中的序列标签问题，是一个解决模板填充的有效方法。序列标签标注模型会被训练如何正确地把候选项分配给slot。（多分类问题HMM等，24章有更深入的讨论）。</p><p><strong>22.4.2</strong>）使用Finite-state有限状态机的模板填充</p><p>基于有限状态机的模板填充的典型步骤：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_6LsKRrb3em.png" alt=""></p><p>实例</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_n0n7Ao8jNm.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_ZNfJVh1wlX.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_bW2awuxgOK.png" alt=""></p><h3 id="22-5-生物医学命名实体识别"><a href="#22-5-生物医学命名实体识别" class="headerlink" title="22.5 生物医学命名实体识别"></a>22.5 生物医学命名实体识别</h3><p>BioNER的需求很大，如下所示，新词逐年稳步增长</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_xOjwRd2yNs.png" alt=""></p><p>生物医学的命名实体识别要难于一般的命名实体识别，一般的命名实体识别仅仅需要识别<strong>人名</strong>，<strong>组织名</strong>和<strong>地名</strong>，而BioNER在识别项上分类更多，如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_CWQaKmMnPk.png" alt=""></p><p>基本上所有NER领域内的武器都被应用到了BioNER上。</p><p>找到并正则化所有的生物医学命名实体，只是厘清各实体在文本中扮演着怎样角色的初级步骤。</p><p>其他的厘清实体在文本中扮演怎样角色的方法，目前热门的技术还有两个，分别是<strong>遍历各实体对关系</strong>和<strong>搞清各实体与中心事件的关系</strong>。</p><p><strong>遍历各个实体对的关系</strong>举例</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_KMvWk9IVa6.png" alt=""></p><p>如上提到的命名实体之间的关系就是<strong>疾病</strong>被<strong>药物治疗</strong>，寻找这种关系的方法上文已经提到，可以使基于规则的有限状态机，也可以是基于统计学的HMM机器学习方法。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_2 - Regular expressions and automata</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_2%20-%20Regular%20expressions%20and%20automata/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_2%20-%20Regular%20expressions%20and%20automata/</url>
    
    <content type="html"><![CDATA[<h1 id="2-Regular-expressions-and-automata"><a href="#2-Regular-expressions-and-automata" class="headerlink" title="2 - Regular expressions and automata"></a>2 - Regular expressions and automata</h1><p><strong>2.2</strong>) Relationship between FSA and RE</p><p>Any regular expression(RE) can be implemented as a finite state automata(FSA),symmetrically,any finite-state automata can be described with a regular expression.</p><p>Both RE and FSA can be used to describe regular languages:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191220_1321_39_996.png" alt=""></p><p><strong>Using FSA to understand sheep talk</strong></p><p>sheep language can be defined as any string from the following set:</p><p>baa!</p><p>baaaa!</p><p>baaaaa!</p><p>baaaaaa!</p><p>baaaaaaa!</p><p>baaaaaaaa!</p><p>baaaaaaaaa!</p><p>…</p><p>Sheep language can be described as following finite state automata(FSA):</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191220_1335_21_266.png" alt=""></p><p>Same FSA adding with fail state:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191220_1359_38_175.png" alt=""></p><p>Another example FSA of dollar changing；</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191220_1421_18_177.png" alt=""></p><h4 id="Idea-flash"><a href="#Idea-flash" class="headerlink" title="Idea flash:"></a>Idea flash:</h4><h4 id="The-RNN-model-in-CS224N-is-a-implement-of-FSA-Especially-the-circle-on-node-Somehow-I-feel-if-diving-deep-there-is-not-as-much-difficult-things-as-I-used-to-image-The-guys-who-inviting-models-and-theroy-are-also-HUMAN"><a href="#The-RNN-model-in-CS224N-is-a-implement-of-FSA-Especially-the-circle-on-node-Somehow-I-feel-if-diving-deep-there-is-not-as-much-difficult-things-as-I-used-to-image-The-guys-who-inviting-models-and-theroy-are-also-HUMAN" class="headerlink" title="The RNN model in CS224N is a implement of FSA???????????Especially the circle on node.Somehow I feel if diving deep,there is not as much difficult things as I used to image.The guys who inviting models and theroy are also HUMAN."></a>The RNN model in CS224N is a implement of FSA???????????Especially the circle on node.Somehow I feel if diving deep,there is not as much difficult things as I used to image.The guys who inviting models and theroy are also HUMAN.</h4><p><strong>Summary:</strong></p><ul><li>most important fundamental concept in NLP:  <strong>finite automaton</strong></li><li>the practical tool based on automation: <strong>regular expression</strong></li><li>Basic operations in regular expressions include concatenation of symbols, disjunction<br>of symbols ([], |, and .), counters (*, +, and {n,m}), anchors (ˆ,$) and precedence operators ((,)).</li><li>Any regular expression can be realized as a finite state automaton (FSA).</li></ul>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_10 - Automatic Speech RecongnitionAdvanced Topics</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_10%20-%20Automatic%20Speech%20RecongnitionAdvanced%20Topics/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_10%20-%20Automatic%20Speech%20RecongnitionAdvanced%20Topics/</url>
    
    <content type="html"><![CDATA[<h1 id="10-Auto-Speech-Recognition-Advanced-Topic-：语音转文字进阶话题"><a href="#10-Auto-Speech-Recognition-Advanced-Topic-：语音转文字进阶话题" class="headerlink" title="10. Auto Speech Recognition Advanced Topic ：语音转文字进阶话题"></a>10. Auto Speech Recognition Advanced Topic ：语音转文字进阶话题</h1><p>之前企图对输入的语音转换成音素的处理办法是：构建一个由全体语言组成的HMM状态网络，然后在网络中采用维特比算法进行全局搜索。这种算法太expensive了。</p><p>改进思路是采用多路编码的decoding技术，使用新的<strong>上下文相关声学模型(triphone)</strong>。本章还会介绍<strong>判别训练(discriminative training)</strong>和模型的一些变体；</p><p><strong>10.1</strong>）多路编码decoding : N-Best List and Lattices</p><p>首先，维特比算法在进行对语音输入的decoding的时候，有如下两个问题：</p><ul><li>在应对一词多音/一音多词的语言时，维特比算法表现很差</li><li>维特比算法很难take advantage of 复杂的语言模型：2-gram还行，3-gram就不行了。因为3-gram violates the <strong>dynamic programming invariant</strong></li></ul><p>改进如上两个问题的思路有：</p><ul><li>改进维特比算法，将原本只返回单一值，变成返回多值。以改进一词多音的问题。</li><li>使用其他的的decoding算法，比如<strong>stack decoder</strong>,或者<strong>A* decoder</strong>。</li></ul><p>Multiple-decoding：</p><p>N-best list：先使用一个开销小的、简单的语言模型处理语音输入，然后输出一个N-best句子，然后将这个N-best句子输入到一个开销大的、复杂的模型中去。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_ZYnDyJFty2.png" alt=""></p><p>N-best句子如下图所示，直接处理这个N-best句子也较为费劲，可以把N-best句子转换成Lattice格式。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_3eoTFxVLf8.png" alt=""></p><p>Lattice</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_CVXNZkamQs.png" alt=""></p><p>And，Lattice可以转换成有限状态机</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_4jl5MpmHUs.png" alt=""></p><p><strong>10.2</strong>) A* （Stack） Decoding</p><p><strong>10.3</strong>) 上下文声学模型：Triphones HMM</p><p>A tripphones HMM model represent a phone in a particulaar left and right context.For example,   triphone[y-eh+l] means [eh] in the middle of [y] and [l].</p><p>带了上下文的声学（音素）模型比单独的音素模型要更精细一些。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_b97JYOwns2.png" alt=""></p><p>在上图决策树中可以看到，同一个“<strong>ih</strong>”可以有不同的triphone。</p><p>下图是triphone的训练过程，（1）（2）步是将iy音素复制进triphone，（3）是连接（4）是将其扩张至GMM高斯模型。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/AcroRd32_J9c2W8kdKi.png" alt=""></p><p><strong>10.4</strong>）判别训练（discriminative training）</p><p>以下用<img src="https://www.zhihu.com/equation?tex=X" alt="[公式]">)代表训练数据中的语音信号，<img src="https://www.zhihu.com/equation?tex=W" alt="[公式]">)代表训练数据中的文本，<img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]">代表语音模型（acoustic model）的参数。语言模型（language model）是固定的。</p><p>最大似然训练法（maximum likelihood, ML）的目标函数是这样的：<br><img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D_%7B%5Ctext%7BML%7D%7D+%3D+%5Carg%5Cmax_%5Ctheta+P_%5Ctheta%28X%7CW%29" alt="[公式]"></p><p>而区分性训练（discriminative training, DT）的目标函数是这样的：<br><img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D_%7B%5Ctext%7BDT%7D%7D+%3D+%5Carg+%5Cmax_%5Ctheta+P_%5Ctheta%28W%7CX%29" alt="[公式]"></p><p>区别在于条件概率不同。ML中，只要训练文本产生训练语音的概率大就行了；而DT要求的是训练语音对应训练文本的概率大，换句话说，就是要训练文本产生训练语音的概率，与其它文本产生训练语音的概率之差大。对DT的目标函数用一次贝叶斯公式就能看出这一点：<br><img src="https://www.zhihu.com/equation?tex=%5Chat%7B%5Ctheta%7D_%5Ctext%7BDT%7D+%3D+%5Carg+%5Cmax_%5Ctheta+%5Cfrac%7BP_%5Ctheta%28X%7CW%29+P%28W%29%7D%7BP_%5Ctheta%28X%29%7D+%3D+%5Carg+%5Cmax_%5Ctheta+%5Cfrac%7BP_%5Ctheta%28X%7CW%29+P%28W%29%7D%7B%5Csum_w+P_%5Ctheta%28X%7Cw%29+P%28w%29%7D" alt="[公式]"><br>分子上的<img src="https://www.zhihu.com/equation?tex=P_%5Ctheta%28X%7CW%29" alt="[公式]">，正是ML的目标函数；而分母则是所有文本（包括训练文本和它的所有竞争者）产生训练语音的概率的（按语言模型加权的）和。</p><p>由于分母上要枚举所有可能的文本并不现实，所以实际中，一般是用一个已有的ML训练的语音系别系统对训练语音做一次解码，得到n-best list或lattice，用这里面的文本来近似分母上的求和。n-best list或lattice中包含了训练文本的足够接近的竞争者。</p><p>把DT的目标函数取个对数，可以得到：</p><p>右边第一项</p><p>是常数，可忽略；第二项是ML的目标函数的对数；第三项的形式与第二项有相似之处。</p><p>ML训练问题一般是用EM算法来解决的。DT训练多了第三项，同样有Generalized EM算法来求解。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_12 - 16,17 - 21语法学，语义语用学</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_12%20-%2016,17%20-%2021%E8%AF%AD%E6%B3%95%E5%AD%A6%EF%BC%8C%E8%AF%AD%E4%B9%89%E8%AF%AD%E7%94%A8%E5%AD%A6/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_12%20-%2016,17%20-%2021%E8%AF%AD%E6%B3%95%E5%AD%A6%EF%BC%8C%E8%AF%AD%E4%B9%89%E8%AF%AD%E7%94%A8%E5%AD%A6/</url>
    
    <content type="html"><![CDATA[<h1 id="12-16语法学，17-21语义语用学"><a href="#12-16语法学，17-21语义语用学" class="headerlink" title="12 - 16语法学，17 - 21语义语用学"></a>12 - 16语法学，17 - 21语义语用学</h1><p>以上几章主要侧重于对英语这门语言本身的讲解，对发表论文帮助不大。先跳过，回头需用再补。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_11 - Computational Phonology</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_11%20-%20Computational%20Phonology/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_11%20-%20Computational%20Phonology/</url>
    
    <content type="html"><![CDATA[<h1 id="11-Computational-Phonology"><a href="#11-Computational-Phonology" class="headerlink" title="11. Computational Phonology"></a>11. Computational Phonology</h1><p>本章较艰涩，对NLP发文帮助不大，Skip，以后需用再补。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/技术_爬虫</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/%E6%8A%80%E6%9C%AF_%E7%88%AC%E8%99%AB/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/%E6%8A%80%E6%9C%AF_%E7%88%AC%E8%99%AB/</url>
    
    <content type="html"><![CDATA[<h1 id="爬虫"><a href="#爬虫" class="headerlink" title="爬虫"></a>爬虫</h1><p>目的:掌握定向网络数据爬取和网页解析的基本能力</p><p>传达的理念:website is the API</p><p>基本内容介绍</p><ul><li>request库  自动爬取HTML页面,自动网络请求提交</li><li>robots.txt  网络爬虫排除标准</li><li>Beautiful Soup解析HTML页面</li><li>实战项目</li><li>正则表达式</li><li>Scrapy专业爬虫框架</li></ul><h2 id="1-Request库"><a href="#1-Request库" class="headerlink" title="1.Request库"></a>1.Request库</h2><ul><li><p>安装request:pip install requests</p><p>import requests<br>r = requests.get(“<a href="http://www.baidu.com&quot;" target="_blank" rel="noopener">http://www.baidu.com&quot;</a>)<br>r.status_code</p><p>r.encoding = ‘utf-8’<br>r.text</p></li><li><p>Request库的七个主要方法</p><ul><li>requests.request()    构造请求,支撑如下方法的基础方法</li><li>requests.get()              获取HTML的主要方法,对应HTTP的GET</li><li>requests.head()          获取HTML的网页头信息</li><li>requests.post()            向HTML网页提交POST请求</li><li>requests.put()              向HTML网页提交PUT请求</li><li>requests.patch()          提交网页局部修改请求</li><li>requests.delete()        提交页面删除请求</li></ul></li></ul><ul><li><p>Response对象</p><ul><li><p>Response对象</p><ul><li><p>是r.request的返回值,示范代码如下</p><ul><li>import requests</li><li>r = requests.get(“<a href="http://www.baidu.com&quot;" target="_blank" rel="noopener">http://www.baidu.com&quot;</a>)</li><li>print(r.status_code)</li></ul><p>​        200</p><ul><li><p>type(r)</p><p>&lt;class ‘requests.model.Response’&gt;</p></li></ul></li></ul></li><li><p>Response属性</p><ul><li>r.status_code 请求的返回状态,200成功404失败</li><li>r.text       url对应的页面内容</li><li>r.encoding        从header中猜测的相应内容编码方式</li><li>r.append_encoding    从内容中分析相应内容编码方式</li><li>r.content      相应内容的二进制方式</li></ul></li></ul></li></ul><ul><li>Requests库的异常<ul><li>requests.ConnectionError   网络链接错误异常,DNS查询失败,拒绝链接等等</li><li>requests.HTTPerror                HTTP错误</li><li>requests.URLRequired        URL缺失</li><li>requests.TooManyRedirects  超过最大重定向次数</li><li>requests.ConnectTimeout    链接远程服务器超时异常</li><li>requests.Timeout                     请求URL超时,产生超时异常</li></ul></li></ul><h2 id="2-爬虫引发的问题"><a href="#2-爬虫引发的问题" class="headerlink" title="2.爬虫引发的问题"></a>2.爬虫引发的问题</h2><h4 id="2-1网络爬虫的尺寸"><a href="#2-1网络爬虫的尺寸" class="headerlink" title="2.1网络爬虫的尺寸"></a>2.1网络爬虫的尺寸</h4><ul><li>小规模:Requests库   :爬取网页</li><li>中规模:Scrapy库        :爬取网站,系列网站</li><li>大规模:定制                 :爬取全网,用于搜索引擎</li></ul><h4 id="2-2爬虫的问题"><a href="#2-2爬虫的问题" class="headerlink" title="2.2爬虫的问题"></a>2.2爬虫的问题</h4><ul><li><p>对服务器资源的消耗,是一种骚扰</p></li><li><p>服务器上的数据归属权问题,造成官司</p></li><li><p>个人隐私泄露风险:爬虫具备一定的突破能力,能破解简单的访问控制</p></li></ul><p>对爬虫的限制</p><ul><li><p>来源审查:判断HTTP协议头的User-Agent域,只响应浏览器或者友好爬虫的访问</p></li><li><p>发布公告:Robots协议,告知爬虫爬取策略,要求遵守</p><ul><li><p>具体实现实在网站的根目录下放置robots.txt文件,写明白哪些资源不可以爬取</p><ul><li><p>案例,京东的robots协议</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1459_09_696.png" alt=""></p><p>解读:不允许访问/?开头的路径,不允许访问符合上边通配符的</p></li></ul></li></ul></li></ul><h2 id="3-Beautiful-Soup"><a href="#3-Beautiful-Soup" class="headerlink" title="3.Beautiful Soup"></a>3.Beautiful Soup</h2><h4 id="3-1-HTML基本格式"><a href="#3-1-HTML基本格式" class="headerlink" title="3.1 HTML基本格式"></a>3.1 HTML基本格式</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1459_37_478.png" alt=""></p><h2 id="4-信息标记的三种形式"><a href="#4-信息标记的三种形式" class="headerlink" title="4.信息标记的三种形式"></a>4.信息标记的三种形式</h2><ul><li>信息的标记<ul><li>标记后的信息能形成组织结构,增加了信息维度</li><li>标记后的信息有利于通信,存储,展示</li><li>标记的结构和信息本身一样有价值</li><li>标记后的信息更利于程序理解和运用</li></ul></li></ul><ul><li>html的信息标记<ul><li>[H]  hyper:超文本<ul><li>声音</li><li>图像</li><li>视频</li></ul></li><li>[T]text:文字</li><li>[M]make up</li><li>[L]language</li></ul></li></ul><h2 id="5-中国大学排名的定向爬取"><a href="#5-中国大学排名的定向爬取" class="headerlink" title="5.中国大学排名的定向爬取"></a>5.中国大学排名的定向爬取</h2><ul><li>功能描述<ul><li>输入:大学排名网站的url]</li><li>输出:排名信息的屏幕输出(排名,大学名称,总分)</li><li>技术路线:requests-BeautifulSoup4</li><li>定向爬虫:仅对url爬取,不对扩展爬取</li><li>目前只能爬取静态信息,动态信息是爬取不到的,动态信息的爬取手段在后文中介绍(Scrapy)</li></ul></li></ul><ul><li>步骤描述<ul><li>从网络中获取大学排名信息的网页信息</li><li>提取网页内容到[合适的数据结构]  ——-如何提取?如何选择合适的数据结构?</li><li>利于数据结构展示输出</li></ul></li></ul><h2 id="6-2Re库介绍"><a href="#6-2Re库介绍" class="headerlink" title="6.2Re库介绍"></a>6.2Re库介绍</h2><p>Re库是Python的标准库,主要用于字符串匹配</p><p>调用方法:import re</p><ul><li><p>正则表达式的表示类型</p><ul><li>raw string类型(原生字符串类型)</li><li>string类型,更繁琐</li></ul></li><li><p>Re库的主要功能函数</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1459_49_071.png" alt=""></p></li></ul><ul><li>re.research(pattern,string,flags=0)<ul><li>在一个字符串中搜索匹配正则表达式的第一个位置,返回match对象<ul><li>pattern:正则表达式 的字符串或原生字符串表示</li><li>string:待匹配字符串</li><li>flags:正则表达式使用时的控制标记</li></ul></li></ul></li></ul><h2 id="7-Scrapy"><a href="#7-Scrapy" class="headerlink" title="7.Scrapy"></a>7.Scrapy</h2><p>Scrapy爬虫框架介绍</p><p>功能强大,是优秀的python第三方库</p><p>爬虫框架</p><p>爬虫框架是实现爬虫功能的一个软件结构和功能组件集合</p><p>爬虫框架是一个半成品,能够帮助用户实现专业爬虫工具</p><ul><li>Scrapy的架构(5+2     5个组件,2个中间件)<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1459_57_505.png" alt=""></li></ul><p>用户只需要编写的是Spiders的配置和ITEM PIPELINES的配置</p><ul><li><p>engine</p><ul><li>控制所有模块之间的数据流</li><li>根据条件触发事件</li></ul></li><li><p>Downloader</p><ul><li>根据请求下载网页</li><li>不需要用户修改</li></ul></li><li><p>Scheduler</p><ul><li>对所有的爬取请求进行调度管理</li><li>不需要用户修改</li></ul></li><li><p>Downloader和engine之间的中间件:</p><ul><li>目的:实施对engine,scheduler和Downloader之间进行用户可配置的控制</li><li>功能:修改,丢弃,新增请求或者响应</li></ul></li><li><p>Item Pipelines</p><ul><li>以流水线方式处理Spider产生的爬取项</li><li>由一组操作顺序产生,类似于流水线,每个操作是一个Item pipeline类型</li><li>可能操作:清理,检验,可能查重爬取项中的HTML数据,将数据存储到数据库</li></ul></li><li><p>Spider</p><ul><li>解析Downloader返回的响应</li><li>产生爬取项(Scarpyed item)</li><li>产生额外的爬取请求(request)</li></ul></li><li><p>Spider中间件</p><ul><li>目的,对爬取项和请求进行再处理</li><li>功能:丢弃,修改,新增请求或者爬取项</li><li>用户可以编写配置代码</li></ul></li></ul><h3 id="7-1-request和Scrapy的对比"><a href="#7-1-request和Scrapy的对比" class="headerlink" title="7.1 request和Scrapy的对比"></a>7.1 request和Scrapy的对比</h3><p>相同点</p><ul><li>两者都可以进行页面请求和爬取,python爬虫的两个重要技术路线</li><li>两者可用性都好,文档丰富,入门简单</li><li>两者都没有处理js,提交表单,应对验证码等功能(可扩展)</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1500_18_344.png" alt=""></p><h3 id="7-2-Scrapy命令行"><a href="#7-2-Scrapy命令行" class="headerlink" title="7.2 Scrapy命令行"></a>7.2 Scrapy命令行</h3><p>命令行里的常用命令</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1500_28_740.png" alt=""></p><ul><li>Scrapy采用命令行而不是图形界面的逻辑<ul><li>命令行(不是图形界面)更容易自动化,适合脚本控制<ul><li>本质上,Scrapy是给程序员用的,功能而不是界面更重要</li></ul></li></ul></li></ul><h3 id="7-3爬虫的第一个实例"><a href="#7-3爬虫的第一个实例" class="headerlink" title="7.3爬虫的第一个实例"></a>7.3爬虫的第一个实例</h3><ul><li><p>[命令行输入]Scrapy startproject python123demo</p><p>新建工程之后生成的文件目录</p><ul><li>scrapy.cfg   部署Scrapy爬虫的配置文件</li><li>python123demo/          Scrapy框架的用户自定义python代码<ul><li>_<em>init_</em>.py             初始化脚本</li><li>items.py              Item代码模板(继承类)</li><li>middlewares.py       中间件代码模板(继承类)</li><li>pipelines.py      pipelines代码模板(继承类)</li><li>settings.py    Scrapy爬虫的配置文件</li><li>spiders/        spider代码模板目录<ul><li>_<strong><em>init</em></strong>__.py  初始文件,无需修改</li><li>_<strong>pycache\</strong>,缓存目录,无需修改</li></ul></li></ul></li></ul></li><li><p>[命令行输入]cd python123demo</p></li><li><p>[命令行输入]Scrapy genspider demo python123.io</p><ul><li>这个命令使得spider文件夹下增加了一个demo.py文件</li></ul></li></ul><pre><code class="python"># -*- coding: utf-8 -*-import scrapy    class DemoSpider(scrapy.Spider):    name = &quot;demo&quot;    allowed_domains = [&quot;python123.io&quot;]    start_urls = [&#39;http://python123.io/&#39;]    def parse(self, response):        pass</code></pre><p>其中domains是目标域名的意思</p><p>parse()用于处理响应,解析内容形成字典,发现新的url爬取内容</p><ul><li>爬虫产生步骤总结<ul><li>建立一个Scrapy工程</li><li>在工程中产生一个Scrapy爬虫</li><li>配置产生的spider爬虫</li><li>运行爬虫,获取网页</li></ul></li></ul><ul><li><p>[命令行输入]Scrapy crawl demo</p><p>运行,爬取html</p></li></ul><h3 id="7-4-yield关键字"><a href="#7-4-yield关键字" class="headerlink" title="7.4 yield关键字"></a>7.4 yield关键字</h3><p>yield   &lt;–&gt;   生成器</p><ul><li><p>生成器是一个不断产生值的函数</p></li><li><p>包含yield语句的函数是一个生成器</p></li><li><p>生成器在每次运行的时候产生一个值(yield语句),函数被冻结,被唤醒后再产生一个值</p><pre><code class="python">#生成器写法def gen(n):    for i in range(n):        yield i ** 2 for i in gen(5):    print(i,&quot; &quot;,end  = &quot; &quot;)</code></pre></li></ul><pre><code>  cout:0   1    4    9   16</code></pre><pre><code>* 生成器写法的优势  * 更节省存储空间  * 响应更迅速  * 使用更灵活* 生成器与普通写法的对比  ![](http://bqlab-pic.test.upcdn.net/pic/20191207_1500_50_004.png)## 8.股票Scrapy爬取实例* 数据网站的确定  * 获取股票信息    * 东方财富网:\http://quote.eastmoney.com/stocklist.tml  * 获取个股信息    * 百度股票:\https://gupiao.baidu.com/stock    * 单个股票\https://gupiao.baidu.com/stock/sz002439.html* 步骤  * 建立工程和Spider模板    * Scrapy startproject BaiduStocks    * cd BaiduStocks    * scrapy genspider stocks  baidu.com    * 进一步修改spiders/stocks.py  * 编写spider     * 配置stocks.py文件    * 修改对返回页面的处理    * 修改对新增URL爬取请求的处理  * 编写ITEM Pipelines    * 配置pipelines.py文件* 定义对爬取项(Scrapy item的处理类)    * 配置settings.py中  * ITEM_PIPELINES选项</code></pre>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>爬虫</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/SpeechAndLanguageProcessing_1 - Introdution</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_1%20-%20Introdution/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/SpeechAndLanguageProcessing_1%20-%20Introdution/</url>
    
    <content type="html"><![CDATA[<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h1><p><strong>1.1</strong>) Required knowledge for NLP:</p><ul><li>Phonetics and Phonology : knowledge about linguistic sounds</li><li>Morphology : knowledge of meaningful components of words</li><li>Syntax :knowledge of the structural relationships between words</li><li>Pragmatics :knowledge of the relationship of meaning to the goals and intentions of speaker(what is )</li><li>Semantics :knowledge of meaning(what is said)</li><li>Discourse : knowledge about linguistic units larger than a single utterance</li></ul><p><strong>1.2</strong>) Key task : Disambiguation at variety level </p><p>Because of ambiguous,A sentence “<strong>I make her duck</strong>“can have so much meanings as behind:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191219_1519_21_672.png" alt=""></p><p><strong>Models</strong> and <strong>Algorithms</strong> are introduced to do disambiguating.</p><table><thead><tr><th>Problem solved</th><th>Method</th></tr></thead><tbody><tr><td>duck is a verb or noun?</td><td>part-of-speech tagging</td></tr><tr><td>make means “create” or “cook”?</td><td>word sense disambiguation</td></tr><tr><td>“her” and “duck” same entity?</td><td>probabilistic parsing</td></tr><tr><td>sentence is a statement or quesition?</td><td>speech act interpretation</td></tr></tbody></table><p><strong>1.3</strong>）Toolkits</p><p>This behind are all useful models</p><ul><li>State machine[状态机的概念见单独文件]<ul><li>finite-state automata</li><li>finite-state transducer</li></ul></li><li>formal rule system<ul><li>regular grammars</li><li>regluar relations</li><li>context-free grammars</li><li>feature-augmented grammars</li></ul></li><li>first order logic <ul><li>lambda-calculus</li><li>features-structures</li><li>semantic-primitives</li></ul></li><li>probabilisitc model – augment_all_above<ul><li>Hidden Markov Models</li></ul></li><li>vector-space model</li><li>machine learning model<ul><li>classifiers<ul><li>decision trees</li><li>support vector machines</li><li>Gaussian Mixture Models</li><li>Logistic regression</li></ul></li><li>sequence models<ul><li>HMM</li><li>Maximum Entropy Markov Model</li><li>Conditioanl Random Fields</li></ul></li></ul></li></ul><p><strong>1.7</strong>)Summary</p><p><strong>Conference:</strong></p><ul><li>NLP:<ul><li>ACL,EACL,NAACL</li><li>SIGs</li><li>COLING</li><li>CoNLL</li><li>EMNLP</li></ul></li><li>AI work for NLP <ul><li>AAAI</li><li>IJCAI</li></ul></li><li>Speech recogniton<ul><li>ICSLP</li><li>EUROSPEECH</li><li>IEEE ICASSP</li><li>SIGDial</li></ul></li></ul><p><strong>Book</strong>:</p><ul><li><p>Manning and Sch¨utze (1999) (Foundations of Statistical Language Processing)  :</p><p> focuses on statistical models of tagging, parsing, disambiguation,collocations, and other areas.</p></li><li><p>Charniak (1993) (Statistical Language Learning) :</p><p>an accessible, though older and less-extensive, introduction to similar material.</p></li><li><p>Manning et al. (2008) focuses on information retrieval, text classification, and clustering. </p></li><li><p>NLTK,the Natural Language Toolkit (Bird and Loper, 2004):</p><p>a suite of Python modules and data for natural language processing, together with a Natural Language Processing book based on the NLTK suite</p></li><li><p>Pereira and Shieber (1987) gives a Prolog-based introduction to parsing and interpretation. </p></li><li><p>Russell and Norvig (2002) is an introduction to artificial intelligence that includes chapters on<br>natural language processing. </p></li><li><p>Partee et al. (1990) has a very broad coverage of mathematical linguistics. </p></li><li><p>Grosz et al. (1986) (Readings in Natural Language Processing)</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1.科研/课题组信息搜集</title>
    <link href="undefined2020/02/24/1.%E7%A7%91%E7%A0%94/%E8%AF%BE%E9%A2%98%E7%BB%84%E4%BF%A1%E6%81%AF%E6%90%9C%E9%9B%86/"/>
    <url>2020/02/24/1.%E7%A7%91%E7%A0%94/%E8%AF%BE%E9%A2%98%E7%BB%84%E4%BF%A1%E6%81%AF%E6%90%9C%E9%9B%86/</url>
    
    <content type="html"><![CDATA[<h1 id="NLP课题组信息搜集20191218"><a href="#NLP课题组信息搜集20191218" class="headerlink" title="NLP课题组信息搜集20191218"></a>NLP课题组信息搜集20191218</h1><p>作者：cstghitpku</p><p>链接：<a href="https://zhuanlan.zhihu.com/p/48529628" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/48529628</a></p><p>来源：知乎</p><p>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p><p>根据这几年的积累，整理了一份国内外学术界和工业界的牛人和大牛团队，供大家申请硕士、博士、博士后和找工作参考。</p><p>学校（排名不分先后）：</p><p><a href="https://link.zhihu.com/?target=http%3A//ir.hit.edu.cn/">哈工大社会计算与信息检索实验室</a>：刘挺老师坐镇，教师包括：秦兵、张宇、车万翔、赵妍妍、刘铭、张伟男、丁效等老师，实验室共7个组，另外王海峰老师也是实验室兼职博导。</p><p><a href="https://link.zhihu.com/?target=http%3A//insun.hit.edu.cn/">哈工大智能技术与自然语言处理实验室</a>：王晓龙老师坐镇，教师包括刘秉权、刘远超、孙承杰等老师</p><p><a href="https://link.zhihu.com/?target=http%3A//mitlab.hit.edu.cn/">哈工大机器智能与翻译研究室</a>：赵铁军老师坐镇，教师包括杨沐昀、郑德权、徐冰老师等，另外周明老师是实验室兼职博导。</p><p><a href="https://link.zhihu.com/?target=http%3A//icrc.hitsz.edu.cn/index.htm">哈工大深圳智能计算研究中心</a>：王晓龙老师坐镇，包括陈清才、汤步洲、徐睿峰、刘滨等老师，实力很强。</p><p><a href="https://link.zhihu.com/?target=http%3A//www.hitsz-hlt.com/">哈工大深圳人类语言技术组</a>：徐睿峰老师坐镇，情感原因发现做的比较好。</p><p>哈工大另外做NLP的老师包括：关毅、王轩等。</p><p><a href="https://link.zhihu.com/?target=http%3A//nlp.csai.tsinghua.edu.cn/site2/index.php/zh">清华大学自然语言处理与社会人文计算实验室</a>：孙茂松老师坐镇，包括刘洋、刘知远等老师。论文发的非常多。</p><p><a href="https://link.zhihu.com/?target=http%3A//coai.cs.tsinghua.edu.cn/">清华大学交互式人工智能（CoAI）课题组</a>：朱小燕老师坐镇，包括黄民烈等老师。Dialogue System做的非常好，论文非常多。</p><p><a href="https://link.zhihu.com/?target=http%3A//www.thuir.cn/">清华大学智能技术与系统国家重点实验室信息检索课题组</a>：马少平老师坐镇，包括张敏、刘奕群等老师。信息检索做的非常好，论文非常多，前段时间刚拿了CIKM唯一的最佳论文（因为一作是学生，跟最佳学生论文合二为一了）。</p><p>清华大学另外做NLP的老师还有李涓子、唐杰、朱军等老师，李老师知识图谱做得好，唐老师数据挖掘（尤其是学者画像）做得好，朱老师偏向机器学习和贝叶斯等做的很好。</p><p><a href="https://link.zhihu.com/?target=http%3A//klcl.pku.edu.cn/">北京大学计算语言学教育部重点实验室</a>：教师包括：王厚峰、万小军、常宝宝、李素建、孙栩、严睿、穗志方、吴云芳等（包含其他实验室的老师）。万老师、李老师、常老师等发论文很多。</p><p><a href="https://link.zhihu.com/?target=http%3A//www.icst.pku.edu.cn/lcwm/index.php%3Ftitle%3D%25E9%25A6%2596%25E9%25A1%25B5">北京大学语言计算与互联网挖掘研究组</a>：<a href="https://link.zhihu.com/?target=http%3A//www.icst.pku.edu.cn/lcwm/wanxj/">万小军</a>老师、孙薇薇老师。万老师主要做自动摘要、文本生成、情感分析与计算等，论文非常多。</p><p>中科院NLP组主要集中在自动化所模式识别国家重点实验室下属的<a href="https://link.zhihu.com/?target=http%3A//nlpr-web.ia.ac.cn/cip/introduction.htm">中文信息处理研究组</a>，另外计算所有刘群老师组和软件所也有孙乐老师做。具体老师包括刘群、宗成庆、赵军、孙乐、王斌、徐君、张家俊、刘康、韩先培、何世柱等老师。论文非常多。</p><p><a href="https://link.zhihu.com/?target=http%3A//nlp.fudan.edu.cn/">复旦大学自然语言处理组</a>：黄萱菁、邱锡鹏等老师，发论文很多。</p><p><a href="https://link.zhihu.com/?target=http%3A//kw.fudan.edu.cn/">复旦大学知识工场</a>：肖仰华老师知识图谱做的非常好，论文发的很多。</p><p><a href="https://link.zhihu.com/?target=http%3A//nlp.suda.edu.cn/index.html">苏州大学自然语言处理组</a>：做机器翻译、情感分析、信息抽取等，论文发的很多。教师包括张民、周国栋、姚建民、李正华、熊得意、李军辉、洪宇、陈文亮等老师。其中张老师、姚老师、李老师都是哈工大毕业的，张老师也是哈工大的兼职博导，论文很多。</p><p><a href="https://link.zhihu.com/?target=http%3A//www.nlplab.com/">东北大学自然语言处理实验室</a>：机器翻译做的非常好，还成立了自己的公司，对外合作很多。姚天顺老师是创始人，朱靖波老师坐镇，教师包括肖桐、任飞亮、张春良、王会珍等老师。</p><p>另外东北大学的王大玲老师、冯时老师情感分析做的不错。</p><p>浙江大学：陈华钧、赵洲等老师，陈老师知识图谱做的很厉害。</p><p>中国人民大学：文继荣、赵鑫、徐君、窦志成等老师。文老师现在是院长，之前在MSRA，信息检索非常厉害。</p><p>上海交大：<a href="https://link.zhihu.com/?target=http%3A//bcmi.sjtu.edu.cn/~zhaohai/">赵海</a>老师，主要做机器翻译、句法分析等。</p><p>东南大学：漆桂林老师，知识图谱做的很厉害。</p><p><a href="https://link.zhihu.com/?target=http%3A//ir.dlut.edu.cn/">大连理工信息检索实验室：</a>林鸿飞老师坐镇，包括杨志豪、王健、张绍武、孙媛媛、张冬瑜、杨亮等老师。主要做信息检索，隐喻、幽默等语料库做的非常好。</p><p>大连理工大学：<a href="https://link.zhihu.com/?target=http%3A//faculty.dlut.edu.cn/dlut_nlp/zh_CN/index.htm">黄德根</a>老师团队，擅长词法分析、命名实体自动识别、短语自动识别、中日机器翻译、社交媒体文本处理等NLP研究。近年来围绕词向量、NLP深度学习、神经网络机器翻译等开展研究。</p><p>西湖大学：张岳老师，之前在新加坡，论文发的非常非常非常多，剑桥2016年统计的全世界发论文的数量好像排第二。</p><p><a href="https://link.zhihu.com/?target=http%3A//nlp.nju.edu.cn/">南京大学自然语言处理研究组</a>：包括陈家俊、<a href="https://link.zhihu.com/?target=http%3A//cs.nju.edu.cn/daixinyu/">戴新宇</a>、黄书剑等老师。</p><p>天津大学：<a href="https://link.zhihu.com/?target=http%3A//cs.tju.edu.cn/faculty/zhangpeng/">张鹏</a>老老师，信息检索做的很好。</p><p>北京理工大学：<a href="https://link.zhihu.com/?target=http%3A//cs.bit.edu.cn/szdw/jsml/js/hhy/index.htm">黄河燕</a>老师、张华平老师。黄老师是北京理工大学计算机学院院长，主要研究机器翻译，担任好几个副理事长，享受国务院特殊津贴。<a href="https://link.zhihu.com/?target=http%3A//www.nlpir.org/">中科院的自然语言处理工具包</a>就是张老师做的，另外跟刘群老师合作发了不少论文，我之前工作时实习生开发的NER就是借鉴的他的层叠马尔可夫模型而二次开发和优化的。</p><p>武汉大学语言与信息研究中心：姬东鸿、李晨亮等老师。</p><p><a href="https://link.zhihu.com/?target=http%3A//nlp.xmu.edu.cn/">厦门大学智能科学与技术系自然语言处理实验室</a>：包括史晓东等老师，主要做机器翻译、知识图谱、信息抽取等。</p><p>昆明理工大学：<a href="https://link.zhihu.com/?target=http%3A//222.197.200.10/MHWZ/MHWQTGL/jslist.do%3Fzgh%3Dzdh001%26lmlxdm%3D02%26lmlbdm%3D0204">余正涛</a>老师团队，有100多人，主要做信息检索、机器翻译和智能系统与决策分析。</p><p>山东大学：<a href="https://link.zhihu.com/?target=http%3A//ir.sdu.edu.cn/~liqiangnie/index.html">聂礼强</a>老师，信息检索做的非常好，论文很多。之前在新加坡，新加坡发SIGIR太多了。。。</p><p>南开大学：<a href="https://link.zhihu.com/?target=http%3A//cc.nankai.edu.cn/teachers/introduce/yangzl">杨征路</a>老师，主要做信息检索。</p><p>北京邮电大学：王小捷老师。</p><p>北京语言大学：<a href="https://link.zhihu.com/?target=http%3A//xxkx.blcu.edu.cn/art/2014/10/28/art_3082_1090594.html">于东</a>老师，主要做机器翻译、人机对话等。</p><p>华东师范大学：<a href="https://link.zhihu.com/?target=https%3A//faculty.ecnu.edu.cn/s/2627/main.jspy">吴苑</a>斌老师，记得应该是复旦大学黄萱菁老师的学生。</p><p>山西大学：李茹老师，山西大学计算机学院副院长。</p><p><a href="https://link.zhihu.com/?target=http%3A//www5.zzu.edu.cn/nlp">郑州大学自然语言处理实验室</a>：教师包括昝红英、<a href="https://link.zhihu.com/?target=http%3A//www5.zzu.edu.cn/nlp/info/1004/1166.htm">柴玉梅</a>、<a href="https://link.zhihu.com/?target=http%3A//www5.zzu.edu.cn/nlp/info/1004/1163.htm">穆玲玲</a>、<a href="https://link.zhihu.com/?target=http%3A//www5.zzu.edu.cn/nlp/info/1004/1151.htm">赵丹</a>、<a href="https://link.zhihu.com/?target=http%3A//www5.zzu.edu.cn/nlp/info/1004/1626.htm">钱晓捷</a>、牛桂玲、马玉汴等。</p><p><a href="https://link.zhihu.com/?target=https%3A//nlp.heida.me/">黑龙江大学</a>：付国宏、张梅山等老师。</p><p>以下是港澳台地区的高校（排名不分先后）：</p><p><a href="https://link.zhihu.com/?target=http%3A//nlg.csie.ntu.edu.tw/">台湾大学自然语言处理实验室</a>：主要研究方向包括知识图谱、机器翻译、问答、自动摘要、信息检索等，论文非常多。</p><p><a href="https://link.zhihu.com/?target=http%3A//www.cse.ust.hk/~hltc/">香港科技大学人类语言技术中心</a>：论文非常多，牛人也不少。</p><p><a href="https://link.zhihu.com/?target=http%3A//www1.se.cuhk.edu.hk/~textmine/">香港中文大学文本挖掘组</a>：主要研究方向包括文本挖掘和信息检索。</p><p><a href="https://link.zhihu.com/?target=http%3A//nlp2ct.cis.umac.mo/">澳门大学自然语言处理与中葡翻译实验室</a>：主要做机器翻译，做的非常好，论文也非常多。</p><p><a href="https://link.zhihu.com/?target=http%3A//www4.comp.polyu.edu.hk/~cswjli/Group.html">香港理工大学社会媒体挖掘组</a>：主要研究方向包括社会影响力建模、社会媒体分析、观点摘要、观点追踪、跨语言情感分析等，这个实验室对外合作很多，比如北大李素建老师、MSRA的韦福如老师等。</p><p>国内工业界（排名不分先后）：</p><p>百度王海峰老师以及带领的自然语言处理部+百度研究院做NLP的一些组，内部NLPC平台集成了几十个NLP算子，一些算子每天调用量都能上亿次（不要问我为啥知道这么详细，因为15-16年我参与开发过2个算子，被加到开发者组了，经常有群邮件告知各算子的调用情况），几乎涵盖所有的NLP任务，部分技术在<a href="https://link.zhihu.com/?target=http%3A//ai.baidu.com/">百度AI开放平台-全球领先的人工智能服务平台-百度AI开放平台</a>开放API，少量技术在<a href="https://link.zhihu.com/?target=https%3A//github.com/baidu">github</a>开源代码；</p><p>MSRA周明老师带领的NLC组，组内论文发的非常多，尤其是几个高级研究员、主管研究员；</p><p>哈工大和科大讯飞联合实验室：实验室主任是刘挺老师，阅读理解做的非常好；</p><p><a href="https://link.zhihu.com/?target=http%3A//www.noahlab.com.hk/">华为诺亚方舟</a>的刘群老师以及带领的团队；</p><p>小米：王斌老师坐镇，王老师翻译的书相信大部分人都看过吧。</p><p><a href="https://link.zhihu.com/?target=https%3A//ailab.bytedance.com/">今日头条</a>的李航老师；</p><p>阿里巴巴达摩院语言技术实验室，在全球6个地点（杭州，北京，西雅图，硅谷，纽约，新加坡）组建了100多人的国际化团队。包括司罗，黄非，骆卫华，陈博兴，刘晓钟等，内部搭建了平台，也做了一些技术评测取得不俗成绩。</p><p><a href="https://link.zhihu.com/?target=https%3A//ai.tencent.com/ailab/nlp/">腾讯NLP</a>做的也不少，只是个人感觉都是很多团队在做，比较分散，没有集中到一起。如果有人总结比较好的话，欢迎告知。</p><p>其他很多创业公司也有大牛坐镇，只是太多、太分散了，不再赘述，感兴趣的可以私聊。</p><p>国外学术界：</p><p><a href="https://link.zhihu.com/?target=http%3A//nlp.csail.mit.edu/">麻省理工学院</a>：<a href="https://link.zhihu.com/?target=http%3A//people.csail.mit.edu/regina/"> Regina Barzilay</a> ,<a href="https://link.zhihu.com/?target=https%3A//people.csail.mit.edu/tommi/"> Tommi S. Jaakkola</a>。</p><p><a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~nasmith/nlp-cl.html">卡内基梅隆大学</a>：<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~jgc"> Jaime Carbonell</a> ,<a href="https://link.zhihu.com/?target=http%3A//www.justinecassell.com/"> Justine Cassell</a> ,<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~wcohen"> William Cohen</a>（主要做信息抽取）,<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~cdyer"> Chris Dyer</a>（主要做机器翻译）,<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~sef"> Scott Fahlman</a> ,（主要做只是表示和知识推理），<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~ref"> Robert Frederking</a> ,<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~hovy"> Eduard Hovy</a> ,<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~alavie"> Alon Lavie</a> ,<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~lsl"> Lori Levin</a> ,<a href="https://link.zhihu.com/?target=http%3A//psyling.psy.cmu.edu/brian"> Brian MacWhinney</a> ,（做的比较杂），<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~teruko"> Teruko Mitamura</a> ,（主要做QA），<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~tom"> Tom Mitchell</a> ,<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~ehn"> Eric Nyberg</a>，<a href="https://link.zhihu.com/?target=http%3A//www.andrew.cmu.edu/user/ko"> Kemal Oflazer</a> ,<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~cprose"> Carolyn Penstein Rosé</a> ,（主要做聊天），<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~roni"> Roni Rosenfeld</a> ,<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~nasmith"> Noah Smith</a> ,<a href="https://link.zhihu.com/?target=http%3A//www.cs.cmu.edu/~epxing"> Eric Xing</a>。</p><p><a href="https://link.zhihu.com/?target=https%3A//www.clsp.jhu.edu/">约翰·霍普金斯大学</a>：Andreas Andreou，Raman Arora，Jason Eisner, Sanjeev Khudanpur, David Yarowsky, Hynek Hermansky，Mark Dredze, Tom Lippincott，Philipp Koehn，Najim Dehak，Ben van Durme。绝对的NLP领域顶级牛校，研究几乎涵盖所有NLP任务，而且做的都非常好，如果非要说主要研究内容的话：句法分析、机器翻译。</p><p><a href="https://link.zhihu.com/?target=https%3A//www.cs.princeton.edu/research/areas/nlp">普林斯顿大学</a>：<a href="https://link.zhihu.com/?target=https%3A//www.cs.princeton.edu/people/profile/arora"> Sanjeev Arora</a>，<a href="https://link.zhihu.com/?target=https%3A//www.cs.princeton.edu/people/profile/karthikn"> Karthik Narasimhan</a>。</p><p><a href="https://link.zhihu.com/?target=https%3A//nlp.stanford.edu/">斯坦福大学</a>： Christopher Manning, Daniel Jurafsky, Percy Liang，这几个人不用赘述了吧，实验室做的很广泛，句法分析和词性标注的工具很有名。</p><p><a href="https://link.zhihu.com/?target=http%3A//nlp.seas.harvard.edu/">哈弗大学</a>：Stuart Shieber，Alexander Rush，主要做MT、自动摘要和文本生成。</p><p><a href="https://link.zhihu.com/?target=https%3A//www.cl.cam.ac.uk/research/nl/">剑桥大学</a>：Edward J. Briscoe，Ann Copestake，Simone Teufel，Paula Buttery，Andreas Vlachos，摘要、文本生成、NLU、句法分析、IR做的都不错。</p><p><a href="https://link.zhihu.com/?target=http%3A//www.clg.ox.ac.uk/">牛津大学</a>：Stephen Pulman，Phil Blunsom（MT非常非常厉害）。</p><p><a href="https://link.zhihu.com/?target=http%3A//nlp.cs.berkeley.edu/">加州大学伯克利分校</a>：<a href="https://link.zhihu.com/?target=http%3A//www.eecs.berkeley.edu/~klein"> Dan Klein</a>（主要做IE和MT）。</p><p><a href="https://link.zhihu.com/?target=http%3A//nlp.cs.illinois.edu/">伊利诺伊大学香槟分校</a>：<a href="https://link.zhihu.com/?target=http%3A//www.cs.uiuc.edu/homes/mfleck/"> Margaret M. Fleck</a> (<a href="https://link.zhihu.com/?target=http%3A//www.cs.uiuc.edu/"> CS</a> )，<a href="https://link.zhihu.com/?target=https%3A//netfiles.uiuc.edu/girju/index.html"> Roxana Girju</a> (<a href="https://link.zhihu.com/?target=http%3A//www.linguistics.uiuc.edu/"> Linguistics</a> )，<a href="https://link.zhihu.com/?target=http%3A//www.ifp.uiuc.edu/~hasegawa/"> Mark Hasegawa-Johnson</a> (<a href="https://link.zhihu.com/?target=http%3A//www.ece.uiuc.edu/"> ECE</a> )，<a href="https://link.zhihu.com/?target=http%3A//www.cs.uiuc.edu/homes/juliahmr/"> Julia Hockenmaier</a> (<a href="https://link.zhihu.com/?target=http%3A//www.cs.uiuc.edu/"> CS</a> )，<a href="https://link.zhihu.com/?target=http%3A//l2r.cs.uiuc.edu/~danr"> Dan Roth</a> (<a href="https://link.zhihu.com/?target=http%3A//www.cs.uiuc.edu/"> CS</a> )，<a href="https://link.zhihu.com/?target=http%3A//www-faculty.cs.uiuc.edu/~czhai"> ChengXiang Zhai</a> (<a href="https://link.zhihu.com/?target=http%3A//www.cs.uiuc.edu/"> CS</a> )。</p><p><a href="https://link.zhihu.com/?target=http%3A//nlp.cis.upenn.edu/">宾夕法尼亚大学</a>：<a href="https://link.zhihu.com/?target=http%3A//www.cis.upenn.edu/~mitch/"> Mitch Marcus</a>，<a href="https://link.zhihu.com/?target=http%3A//www.cis.upenn.edu/~danroth/"> Dan Roth</a>，<a href="https://link.zhihu.com/?target=http%3A//www.seas.upenn.edu/~ungar/"> Lyle Ungar</a>，<a href="https://link.zhihu.com/?target=http%3A//www.cis.upenn.edu/~nenkova/"> Ani Nenkova</a>，<a href="https://link.zhihu.com/?target=http%3A//www.cis.upenn.edu/~ccb/"> Chris Callison-Burch</a>，句法分析做的非常屌，LTAG、Penn Treebank不用过多解释了吧。</p><p>芝加哥大学：John Lafferty（CRF发明人，机器翻译做的也不错）, John Goldsmith</p><p>哥伦比亚大学：Kathy McKeown, Julia Hirschberg，Owen Rambow。</p><p>康奈尔大学：Lillian Lee（主要做SA（情感分析））, Thorsten Joachims（深入学习SVM的话应该知道他）, Claire Cardie, Yoav Artzi。</p><p><a href="https://link.zhihu.com/?target=https%3A//u.osu.edu/cllt/">俄亥俄州立大学（OSU）</a>：Eric Fosler-Lussier(我是因为做对话知道的他), Michael White（主要做NLG）, William Schuler（主要做句法分析和MT）, Micha Elsner, Alan Ritter, Wei Xu（社交媒体）。</p><p><a href="https://link.zhihu.com/?target=http%3A//www.isp.pitt.edu/about">匹兹堡大学</a>：Ashley Kevin， Brusilovsky Peter, Lewis Michael。</p><p><a href="https://link.zhihu.com/?target=https%3A//www.cs.toronto.edu/compling/">多伦多大学</a>：Graeme Hirst, Gerald Penn，Frank Rudzic，Suzanne Stevenson，主要做句法分析、语义分析。</p><p><a href="https://link.zhihu.com/?target=http%3A//rl.cs.mcgill.ca/people.html">麦吉尔大学</a>：<a href="https://link.zhihu.com/?target=http%3A//www.cs.mcgill.ca/~dprecup"> Doina Precup</a>，<a href="https://link.zhihu.com/?target=http%3A//www.cs.mcgill.ca/~jcheung/"> Jackie Chi Kit Cheung</a>，<a href="https://link.zhihu.com/?target=http%3A//www.cs.mcgill.ca/~jpineau"> Joelle Pineau</a>，<a href="https://link.zhihu.com/?target=http%3A//www.cs.mcgill.ca/~prakash"> Prakash Panangaden</a></p><p>蒙特利尔大学：Yoshua Bengio，不过多解释。</p><p>佐治亚理工：Eric Gilbert（社会计算领域很有名）。</p><p><a href="https://link.zhihu.com/?target=https%3A//cl.usc.edu/">南加州大学</a>：Jerry Hobbs，Ron Artstein，David DeVault，Kallirroi Georgila，Panayiotis (Panos) Georgiou， Andrew Gordon，Jerry Hobbs，Khalil Iskarous，Kevin Knight，Sungbok Lee， Anton Leuski，Jonathan May，Prem Natarajan，MT、IE、关系挖掘、对话做的都不错。</p><p><a href="https://link.zhihu.com/?target=https%3A//www.cs.washington.edu/research/nlp">华盛顿大学</a>：<a href="https://link.zhihu.com/?target=http%3A//www.timalthoff.com/"> Tim Althoff</a>，<a href="https://link.zhihu.com/?target=http%3A//melodi.ee.washington.edu/people/bilmes/pgs/index.html"> Jeffrey Bilmes</a>，Yejin Choi，Pedro Domingos，Oren Etzioni，<a href="https://link.zhihu.com/?target=https%3A//homes.cs.washington.edu/~hannaneh/"> Hannaneh Hajishirzi</a>，<a href="https://link.zhihu.com/?target=https%3A//homes.cs.washington.edu/~nasmith"> Noah Smith</a>，Daniel S. Weld，Luke Zettlemoyer，主要做句法分析、MT、对话、IR等。</p><p><a href="https://link.zhihu.com/?target=http%3A//edinburghnlp.inf.ed.ac.uk/">爱丁堡大学</a>：Shay Cohen（句法分析），Sharon Goldwater，Kenneth Heafield（MT），Frank Keller（句法分析），Mirella Lapata（NLU、NLG），Adam Lopez，Walid Magdy（IR、DM、社会计算），Rico Sennrich （句法分析、MT），Mark Steedman（对话），Ivan Titov（句法分析、NLU），Bonnie Webber（QA）。</p><p><a href="https://link.zhihu.com/?target=https%3A//www.comp.nus.edu.sg/~nlp/">新加坡国立大学</a>：NG Hwee Tou（主要做MT和句法纠错）。</p><p>马里兰大学：Philip Resnik, Naomi Feldman，Marine Carpuat,Hal Daumé, 主要做MT和IR。</p><p><a href="https://link.zhihu.com/?target=https%3A//nlp.ccis.northeastern.edu/">东北大学</a>：David A. Smith, Byron Wallace, Lu Wang。</p><p>加州大学伯克利分校：Dan Klein，主要做NLP和ML交叉研究。</p><p>加州大学圣巴巴拉分校：William Wang, 主要做IE（信息抽取）和ML。</p><p>加州大学圣克鲁兹分校：Marilyn Walker，主要做dialogue。</p><p><a href="https://link.zhihu.com/?target=http%3A//nlpatcuny.cs.qc.cuny.edu/">纽约市立学院（CUNY）</a>：Martin Chodorow，Liang huang，Andrew Rosenberg，William Sakas，Virginia Teller。</p><p>University of Massachusetts Amherst：<a href="https://link.zhihu.com/?target=https%3A//people.cs.umass.edu/~mccallum/">Andrew McCallum</a>（CRF、主题模型）、Bruce Croft、James Allan（IR做的非常屌）。</p><p><a href="https://link.zhihu.com/?target=https%3A//wp.nyu.edu/ml2/">纽约大学</a>：Sam Bowman, Kyunghyun Cho，NLU做的非常好。</p><p><a href="https://link.zhihu.com/?target=https%3A//nlp.cs.unc.edu/">北卡罗来纳大学教堂山分校</a>（UNC）：Mohit Bansal, Tamara Berg,主要做句法分析、多模态对话。</p><p>罗切斯特大学：Len Schubert, James Allen（篇章分析、对话做的很好），Dan Gildea（句法分析、MT）。</p><p><a href="https://link.zhihu.com/?target=https%3A//www.sheffield.ac.uk/dcs/research/groups/nlp">谢菲尔德大学</a>：<a href="https://link.zhihu.com/?target=http%3A//www.dcs.shef.ac.uk/~robertg"> Rob Gaizauskas</a> (Head of Group），<a href="https://link.zhihu.com/?target=http%3A//www.dcs.shef.ac.uk/~hepple"> Mark Hepple</a>，<a href="https://link.zhihu.com/?target=http%3A//staffwww.dcs.shef.ac.uk/people/L.Specia/"> Lucia Specia</a>（MT很厉害），<a href="https://link.zhihu.com/?target=http%3A//www.dcs.shef.ac.uk/~marks"> Mark Stevenson</a>（主要做IR和IE），<a href="https://link.zhihu.com/?target=http%3A//www.dcs.shef.ac.uk/~yorick"> Yorick Wilks</a>（ACL终身成就奖）。</p><p>还有很多学校、很多学术界大佬没整理了，后续再补充吧。另外美国工业界的NLP大牛也很多，比如google、facebook、microsoft、amazon、IBM等公司。</p>]]></content>
    
    
    <categories>
      
      <category>1.科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>课题组信息</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/修改conda源_下载问题</title>
    <link href="undefined2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E4%BF%AE%E6%94%B9conda%E6%BA%90_%E4%B8%8B%E8%BD%BD%E9%97%AE%E9%A2%98/"/>
    <url>2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E4%BF%AE%E6%94%B9conda%E6%BA%90_%E4%B8%8B%E8%BD%BD%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h4 id="首次改为国内源："><a href="#首次改为国内源：" class="headerlink" title="首次改为国内源："></a>首次改为国内源：</h4><p>打开cmd</p><p>conda config –add channels <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</a><br>conda config –add channels <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge</a><br>conda config –add channels <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/</a><br>conda config –add channels <a href="https://mirrors.ustc.edu.cn/anaconda/pkgs/main/" target="_blank" rel="noopener">https://mirrors.ustc.edu.cn/anaconda/pkgs/main/</a><br>conda config –add channels <a href="https://mirrors.ustc.edu.cn/anaconda/pkgs/free/" target="_blank" rel="noopener">https://mirrors.ustc.edu.cn/anaconda/pkgs/free/</a><br>conda config –add channels <a href="https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/" target="_blank" rel="noopener">https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/</a><br>conda config –add channels <a href="https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/" target="_blank" rel="noopener">https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/</a><br>conda config –add channels <a href="https://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/" target="_blank" rel="noopener">https://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/</a><br>conda config –add channels <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</a><br>conda config –add channels <a href="https://mirrors.ustc.edu.cn/anaconda/cloud/menpo/" target="_blank" rel="noopener">https://mirrors.ustc.edu.cn/anaconda/cloud/menpo/</a><br>conda config –set show_channel_urls yes</p><p>C:\Users&lt;你的用户名&gt; 下就会生成配置文件.condarc</p><p>删除第三行-default保存</p><p>conda info检查修改是否生效</p><h4 id="改回默认源："><a href="#改回默认源：" class="headerlink" title="改回默认源："></a>改回默认源：</h4><p>打开C:\Users&lt;bqlion&gt;.condarc文件，改成:</p><pre><code>channels:  - defaultsshow_channel_urls: true</code></pre><p>保存</p><h4 id="再改回国内源"><a href="#再改回国内源" class="headerlink" title="再改回国内源;"></a>再改回国内源;</h4><p>打开C:\Users&lt;bqlion&gt;.condarc文件，改成:</p><pre><code>channels:  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/  - https://mirrors.ustc.edu.cn/anaconda/cloud/menpo/  - https://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/  - https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/  - https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/  - https://mirrors.ustc.edu.cn/anaconda/pkgs/free/  - https://mirrors.ustc.edu.cn/anaconda/pkgs/main/  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/show_channel_urls: truessl_verify: false</code></pre><h4 id="CondaHTTPError-HTTP-000-CONNECTION-FAILED的问题："><a href="#CondaHTTPError-HTTP-000-CONNECTION-FAILED的问题：" class="headerlink" title="CondaHTTPError: HTTP 000 CONNECTION FAILED的问题："></a>CondaHTTPError: HTTP 000 CONNECTION FAILED的问题：</h4><p>conda config –set ssl_verify false</p><p>若还不行就输入：</p><p>conda config –set ssl_verify no</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>1.科研/WebScience讲座笔记</title>
    <link href="undefined2020/02/24/1.%E7%A7%91%E7%A0%94/WebScience%E8%AE%B2%E5%BA%A7%E7%AC%94%E8%AE%B0/"/>
    <url>2020/02/24/1.%E7%A7%91%E7%A0%94/WebScience%E8%AE%B2%E5%BA%A7%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="Web-Science讲座笔记"><a href="#Web-Science讲座笔记" class="headerlink" title="Web Science讲座笔记"></a>Web Science讲座笔记</h1><ul><li><p>写论文步骤</p><p>在感兴趣的方向提出问题和假设</p><p>验证前人是否已经做过？</p><p>设计实验</p><p>做实验,收集数据</p><p>写论文</p></li></ul><h3 id="开题案例"><a href="#开题案例" class="headerlink" title="开题案例"></a>开题案例</h3><hr><p>案例:人工智能</p><p>阅读中科院，工程院前沿报告,阅读各种综述，阅读下文专项报告</p><p>两个月更新一次:ESI Reaserach front专项报告</p><p>在web产品里找到,网页右边可以关键词聚类</p><p>AI子课题:自然语言处理,主页可显示高被引用论文(数据结构网络的核心)</p><p>能顺便找到本学科的raising star</p><ul><li><p>在web science里写检索式,如何寻找文章引用网络</p><p><a href="https://www.clarivate.com.cn/e-clarivate/wos_video_wos_research.htm" target="_blank" rel="noopener">https://www.clarivate.com.cn/e-clarivate/wos_video_wos_research.htm</a></p></li></ul><ul><li><p>如何在上行的检索之后筛选自己需要的信息?(上个结果返回30w+文章)</p><ul><li><p>检索结果:</p><p>这个功能能找到哪些基金在资助哪些项目</p><p>哪些导师在做,找导师/合伙人利器</p><ul><li><p>分析机构:</p><p>能找到哪些机构近几年发表了哪些文章</p><ul><li>分析机构中的作者</li></ul></li></ul></li></ul></li></ul><ul><li><p>几个产品按钮</p><p>在网页上方有个文章被引用次数的功能,发布在哪个期刊</p></li></ul><ul><li><p>左侧功能栏:</p><p>review是学科综述</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>1.科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Web_of_Science</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1.科研/深度学习论文集[Done]</title>
    <link href="undefined2020/02/24/1.%E7%A7%91%E7%A0%94/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%9B%86%5BDone%5D/"/>
    <url>2020/02/24/1.%E7%A7%91%E7%A0%94/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%9B%86%5BDone%5D/</url>
    
    <content type="html"><![CDATA[<h2 id="1-深度学习历史和基础"><a href="#1-深度学习历史和基础" class="headerlink" title="1 深度学习历史和基础"></a>1 深度学习历史和基础</h2><h3 id="1-0-书籍"><a href="#1-0-书籍" class="headerlink" title="1.0 书籍"></a>1.0 书籍</h3><p>█[0] Bengio, Yoshua, Ian J. Goodfellow, and Aaron Courville. <strong>“Deep learning</strong>.” An MIT Press book. (2015). [pdf] (Ian Goodfellow 等大牛所著的教科书，乃深度学习圣经。你可以同时研习这本书以及以下论文) ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//github.com/HFTrader/DeepLearningBook/raw/master/DeepLearningBook.pdf">https://github.com/HFTrader/DeepLearningBook/raw/master/DeepLearningBook.pdf</a></p><h3 id="1-1-调查"><a href="#1-1-调查" class="headerlink" title="1.1 调查"></a>1.1 调查</h3><p>█[1] LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. “<strong>Deep learning</strong>.” Nature 521.7553 (2015): 436-444. [pdf] (三巨头做的调查)  ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%7Ehinton/absps/NatureDeepReview.pdf">http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf</a></p><h3 id="1-2-深度置信网络-DBN，深度学习前夜的里程碑"><a href="#1-2-深度置信网络-DBN，深度学习前夜的里程碑" class="headerlink" title="1.2 深度置信网络 (DBN，深度学习前夜的里程碑)"></a>1.2 深度置信网络 (DBN，深度学习前夜的里程碑)</h3><p>█[2] Hinton, Geoffrey E., Simon Osindero, and Yee-Whye Teh. “<strong>A fast learning algorithm for deep belief nets</strong>.” Neural computation 18.7 (2006): 1527-1554. [pdf] (深度学习前夜) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%7Ehinton/absps/ncfast.pdf">http://www.cs.toronto.edu/~hinton/absps/ncfast.pdf</a></p><p>█[3] Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. “<strong>Reducing the dimensionality of data with neural networks.</strong>“ Science 313.5786 (2006): 504-507. [pdf] (里程碑，展示了深度学习的前景) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%7Ehinton/science.pdf">http://www.cs.toronto.edu/~hinton/science.pdf</a></p><h3 id="1-3-ImageNet-的进化（深度学习从此萌发）"><a href="#1-3-ImageNet-的进化（深度学习从此萌发）" class="headerlink" title="1.3 ImageNet 的进化（深度学习从此萌发）"></a>1.3 ImageNet 的进化（深度学习从此萌发）</h3><p>█[4] Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. “<strong>Imagenet classification with deep convolutional neural networks.</strong>“ Advances in neural information processing systems. 2012. [pdf] (AlexNet, 深度学习突破)  ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-    networks.pdf</a></p><p>█[5] Simonyan, Karen, and Andrew Zisserman. “<strong>Very deep convolutional networks for large-scale image recognition.</strong>“ arXiv preprint arXiv:1409.1556 (2014). [pdf] (VGGNet，神经网络变得很深层) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1409.1556.pdf">https://arxiv.org/pdf/1409.1556.pdf</a></p><p>█[6] Szegedy, Christian, et al. “<strong>Going deeper with convolutions.</strong>“ Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015. [pdf] (GoogLeNet) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf">http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf</a></p><p>█[7] He, Kaiming, et al. “<strong>Deep residual learning for image recognition.</strong>“ arXiv preprint arXiv:1512.03385 (2015). [pdf](ResNet，特别深的神经网络, CVPR 最佳论文)  ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1512.03385.pdf">https://arxiv.org/pdf/1512.03385.pdf</a></p><h3 id="1-4-语音识别的进化"><a href="#1-4-语音识别的进化" class="headerlink" title="1.4 语音识别的进化"></a>1.4 语音识别的进化</h3><p>█[8] Hinton, Geoffrey, et al. “<strong>Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups.</strong>“ IEEE Signal Processing Magazine 29.6 (2012): 82-97. [pdf] (语音识别的突破) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//cs224d.stanford.edu/papers/maas_paper.pdf">http://cs224d.stanford.edu/papers/maas_paper.pdf</a></p><p>█[9] Graves, Alex, Abdel-rahman Mohamed, and Geoffrey Hinton. “<strong>Speech recognition with deep recurrent neural networks.</strong>“ 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013. [pdf] (RNN) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1303.5778.pdf">http://arxiv.org/pdf/1303.5778.pdf</a></p><p>█[10] Graves, Alex, and Navdeep Jaitly. “<strong>Towards End-To-End Speech Recognition with Recurrent Neural Networks.</strong>“ ICML. Vol. 14. 2014. [pdf] ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.jmlr.org/proceedings/papers/v32/graves14.pdf">http://www.jmlr.org/proceedings/papers/v32/graves14.pdf</a></p><p>█[11] Sak, Haşim, et al. “<strong>Fast and accurate recurrent neural network acoustic models for speech recognition.</strong>“ arXiv preprint arXiv:1507.06947 (2015). [pdf] (谷歌语音识别系统) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1507.06947">http://arxiv.org/pdf/1507.06947</a></p><p>█[12] Amodei, Dario, et al. “<strong>Deep speech 2: End-to-end speech recognition in english and mandarin</strong>.” arXiv preprint arXiv:1512.02595 (2015). [pdf] (百度语音识别系统) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1512.02595.pdf">https://arxiv.org/pdf/1512.02595.pdf</a></p><p>█[13] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, G. Zweig “<strong>Achieving Human Parity in Conversational Speech Recognition.</strong>“ arXiv preprint arXiv:1610.05256 (2016). [pdf] (最前沿的语音识别, 微软) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1610.05256v1">https://arxiv.org/pdf/1610.05256v1</a></p><p>研读以上论文之后，你将对深度学习历史、模型的基本架构（包括 CNN, RNN, LSTM）有一个基础的了解，并理解深度学习如何应用于图像和语音识别问题。接下来的论文，将带你深入探索深度学习方法、在不同领域的应用和前沿尖端技术。我建议，你可以根据兴趣和工作/研究方向进行选择性的阅读。</p><h2 id="2-深度学习方法"><a href="#2-深度学习方法" class="headerlink" title="2 深度学习方法"></a>2 深度学习方法</h2><h3 id="2-1-模型"><a href="#2-1-模型" class="headerlink" title="2.1 模型"></a>2.1 模型</h3><p>█[14] Hinton, Geoffrey E., et al. “<strong>Improving neural networks by preventing co-adaptation of feature detectors.</strong>“ arXiv preprint arXiv:1207.0580 (2012). [pdf] (Dropout) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1207.0580.pdf">https://arxiv.org/pdf/1207.0580.pdf</a></p><p>█[15] Srivastava, Nitish, et al. “<strong>Dropout: a simple way to prevent neural networks from overfitting.</strong>“ Journal of Machine Learning Research 15.1 (2014): 1929-1958. [pdf] ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf">http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf</a></p><p>█[16] Ioffe, Sergey, and Christian Szegedy. “<strong>Batch normalization: Accelerating deep network training by reducing internal covariate shift.</strong>“ arXiv preprint arXiv:1502.03167 (2015). [pdf] (2015 年的杰出研究) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1502.03167">http://arxiv.org/pdf/1502.03167</a></p><p>█[17] Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. “<strong>Layer normalization.</strong>“ arXiv preprint arXiv:1607.06450 (2016). [pdf] (Batch Normalization 的更新) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1607.06450.pdf%3Futm_source%3Dsciontist.com%26utm_medium%3Drefer%26utm_campaign%3Dpromote">https://arxiv.org/pdf/1607.06450.pdf?utm_source=sciontist.com&amp;utm_medium=refer&amp;utm_campaign=promote</a></p><p>█[18] Courbariaux, Matthieu, et al. “<strong>Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to+ 1 or−1.</strong>“ [pdf] (新模型，快) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//pdfs.semanticscholar.org/f832/b16cb367802609d91d400085eb87d630212a.pdf">https://pdfs.semanticscholar.org/f832/b16cb367802609d91d400085eb87d630212a.pdf</a></p><p>█[19] Jaderberg, Max, et al. “<strong>Decoupled neural interfaces using synthetic gradients.</strong>“ arXiv preprint arXiv:1608.05343 (2016). [pdf] (训练方法的创新，研究相当不错) ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1608.05343">https://arxiv.org/pdf/1608.05343</a></p><p>█[20] Chen, Tianqi, Ian Goodfellow, and Jonathon Shlens. “<strong>Net2net: Accelerating learning via knowledge transfer.</strong>“ arXiv preprint arXiv:1511.05641 (2015). [pdf] (改进此前的训练网络，来缩短训练周期) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1511.05641">Accelerating Learning via Knowledge Transfer</a></p><p>█[21] Wei, Tao, et al. “<strong>Network Morphism.</strong>“ arXiv preprint arXiv:1603.01670 (2016). [pdf] (改进此前的训练网络，来缩短训练周期) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1603.01670">[1603.01670] Network Morphism</a></p><h3 id="2-2-优化-Optimization"><a href="#2-2-优化-Optimization" class="headerlink" title="2.2 优化 Optimization"></a>2.2 优化 Optimization</h3><p>█[22] Sutskever, Ilya, et al. “<strong>On the importance of initialization and momentum in deep learning.</strong>“ ICML (3) 28 (2013): 1139-1147. [pdf] (Momentum optimizer) ★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.jmlr.org/proceedings/papers/v28/sutskever13.pdf">http://www.jmlr.org/proceedings/papers/v28/sutskever13.pdf</a></p><p>█[23] Kingma, Diederik, and Jimmy Ba. “<strong>Adam: A method for stochastic optimization.</strong>“ arXiv preprint arXiv:1412.6980 (2014). [pdf] (Maybe used most often currently) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1412.6980">http://arxiv.org/pdf/1412.6980</a></p><p>█[24] Andrychowicz, Marcin, et al. “<strong>Learning to learn by gradient descent by gradient descent.”</strong> arXiv preprint arXiv:1606.04474 (2016). [pdf] (Neural Optimizer,Amazing Work) ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1606.04474">https://arxiv.org/pdf/1606.04474</a></p><p>█[25] Han, Song, Huizi Mao, and William J. Dally. <strong>“Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding.</strong>“ CoRR, abs/1510.00149 2 (2015). [pdf] (ICLR best paper, new direction to make NN running fast,DeePhi Tech Startup) ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//pdfs.semanticscholar.org/5b6c/9dda1d88095fa4aac1507348e498a1f2e863.pdf">https://pdfs.semanticscholar.org/5b6c/9dda1d88095fa4aac1507348e498a1f2e863.pdf</a></p><p>█[26] Iandola, Forrest N., et al. “<strong>SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&lt; 1MB model size.</strong>“ arXiv preprint arXiv:1602.07360 (2016). [pdf] (Also a new direction to optimize NN,DeePhi Tech Startup) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1602.07360">http://arxiv.org/pdf/1602.07360</a></p><h3 id="2-3-无监督学习-深度生成模型"><a href="#2-3-无监督学习-深度生成模型" class="headerlink" title="2.3 无监督学习/深度生成模型"></a>2.3 无监督学习/深度生成模型</h3><p>█[27] Le, Quoc V. “<strong>Building high-level features using large scale unsupervised learning.</strong>“ 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013. [pdf] (里程碑, 吴恩达, 谷歌大脑, Cat) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1112.6209.pdf%26embed">http://arxiv.org/pdf/1112.6209.pdf&amp;embed</a></p><p>█[28] Kingma, Diederik P., and Max Welling. “<strong>Auto-encoding variational bayes.</strong>“ arXiv preprint arXiv:1312.6114 (2013). <a href="VAE">pdf</a> ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1312.6114">http://arxiv.org/pdf/1312.6114</a></p><p>█[29] Goodfellow, Ian, et al. “<strong>Generative adversarial nets.</strong>“ Advances in Neural Information Processing Systems. 2014. <a href="GAN，很酷的想法">pdf</a> ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5423-generative-adversarial-nets.pdf">http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf</a></p><p>█[30] Radford, Alec, Luke Metz, and Soumith Chintala. “<strong>Unsupervised representation learning with deep convolutional generative adversarial networks.</strong>“ arXiv preprint arXiv:1511.06434 (2015). [pdf] (DCGAN) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1511.06434">http://arxiv.org/pdf/1511.06434</a></p><p>█[31] Gregor, Karol, et al. “<strong>DRAW: A recurrent neural network for image generation.</strong>“ arXiv preprint arXiv:1502.04623 (2015). [pdf] (VAE with attention, 很出色的研究) ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v37/gregor15.pdf">http://jmlr.org/proceedings/papers/v37/gregor15.pdf</a></p><p>█[32] Oord, Aaron van den, Nal Kalchbrenner, and Koray Kavukcuoglu. “<strong>Pixel recurrent neural networks.</strong>“ arXiv preprint arXiv:1601.06759 (2016). [pdf] (PixelRNN) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1601.06759">http://arxiv.org/pdf/1601.06759</a></p><p>█[33] Oord, Aaron van den, et al. <strong>“Conditional image generation with PixelCNN decoders.</strong>“ arXiv preprint arXiv:1606.05328 (2016). [pdf] (PixelCNN) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1606.05328">https://arxiv.org/pdf/1606.05328</a></p><h3 id="2-4-递归神经网络（RNN）-Sequence-to-Sequence-Model"><a href="#2-4-递归神经网络（RNN）-Sequence-to-Sequence-Model" class="headerlink" title="2.4 递归神经网络（RNN） / Sequence-to-Sequence Model"></a>2.4 递归神经网络（RNN） / Sequence-to-Sequence Model</h3><p>█[34] Graves, Alex. “<strong>Generating sequences with recurrent neural networks.</strong>“ arXiv preprint arXiv:1308.0850 (2013). [pdf] (LSTM, 效果很好，展示了 RNN 的性能) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1308.0850">http://arxiv.org/pdf/1308.0850</a></p><p>█[35] Cho, Kyunghyun, et al. “<strong>Learning phrase representations using RNN encoder-decoder for statistical machine translation.</strong>“ arXiv preprint arXiv:1406.1078 (2014). [pdf] (第一篇 Sequence-to-Sequence 的论文) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1406.1078">http://arxiv.org/pdf/1406.1078</a></p><p>█[36] Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. “<strong>Sequence to sequence learning with neural networks.</strong>“ Advances in neural information processing systems. 2014. [pdf] (杰出研究) ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5346-information-based-learning-by-agents-in-unbounded-state-spaces.pdf">http://papers.nips.cc/paper/5346-information-based-learning-by-agents-in-unbounded-state-spaces.pdf</a></p><p>█[37] Bahdanau, Dzmitry, KyungHyun Cho, and Yoshua Bengio. “<strong>Neural Machine Translation by Jointly Learning to Align and Translate.</strong>“ arXiv preprint arXiv:1409.0473 (2014). [pdf] ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1409.0473v7.pdf">https://arxiv.org/pdf/1409.0473v7.pdf</a></p><p>█[38] Vinyals, Oriol, and Quoc Le. “<strong>A neural conversational model.</strong>“ arXiv preprint arXiv:1506.05869 (2015). [pdf] (Seq-to-Seq 聊天机器人) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1506.05869.pdf%2520%28http%3A//arxiv.org/pdf/1506.05869.pdf%29">http://arxiv.org/pdf/1506.05869.pdf%20(http://arxiv.org/pdf/1506.05869.pdf)</a></p><h3 id="2-5-神经网络图灵机"><a href="#2-5-神经网络图灵机" class="headerlink" title="2.5 神经网络图灵机"></a>2.5 神经网络图灵机</h3><p>█[39] Graves, Alex, Greg Wayne, and Ivo Danihelka. “<strong>Neural turing machines.</strong>“ arXiv preprint arXiv:1410.5401 (2014). [pdf] (未来计算机的基础原型机) ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1410.5401.pdf">http://arxiv.org/pdf/1410.5401.pdf</a></p><p>█[40] Zaremba, Wojciech, and Ilya Sutskever. “<strong>Reinforcement learning neural Turing machines.</strong>“ arXiv preprint arXiv:1505.00521 362 (2015). [pdf] ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//pdfs.semanticscholar.org/f10e/071292d593fef939e6ef4a59baf0bb3a6c2b.pdf">https://pdfs.semanticscholar.org/f10e/071292d593fef939e6ef4a59baf0bb3a6c2b.pdf</a></p><p>█[41] Weston, Jason, Sumit Chopra, and Antoine Bordes. “<strong>Memory networks.</strong>“ arXiv preprint arXiv:1410.3916 (2014). [pdf] ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1410.3916">http://arxiv.org/pdf/1410.3916</a></p><p>█[42] Sukhbaatar, Sainbayar, Jason Weston, and Rob Fergus. “<strong>End-to-end memory networks.</strong>“ Advances in neural information processing systems. 2015. [pdf] ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf">http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf</a></p><p>█[43] Vinyals, Oriol, Meire Fortunato, and Navdeep Jaitly. “<strong>Pointer networks.</strong>“ Advances in Neural Information Processing Systems. 2015. [pdf] ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5866-pointer-networks.pdf">http://papers.nips.cc/paper/5866-pointer-networks.pdf</a></p><p>█[44] Graves, Alex, et al. “<strong>Hybrid computing using a neural network with dynamic external memory.</strong>“ Nature (2016). [pdf] (里程碑，把以上论文的想法整合了起来) ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//www.dropbox.com/s/0a40xi702grx3dq/2016-graves.pdf">https://www.dropbox.com/s/0a40xi702grx3dq/2016-graves.pdf</a></p><h3 id="2-6-深度强化学习"><a href="#2-6-深度强化学习" class="headerlink" title="2.6 深度强化学习"></a>2.6 深度强化学习</h3><p>█[45] Mnih, Volodymyr, et al. “<strong>Playing atari with deep reinforcement learning</strong>.” arXiv preprint arXiv:1312.5602 (2013). [pdf]) (第一个以深度强化学习为题的论文)  ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1312.5602.pdf">http://arxiv.org/pdf/1312.5602.pdf</a></p><p>█[46] Mnih, Volodymyr, et al. “<strong>Human-level control through deep reinforcement learning</strong>.” Nature 518.7540 (2015): 529-533. [pdf] (里程碑) ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf">https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf</a></p><p>█[47] Wang, Ziyu, Nando de Freitas, and Marc Lanctot. “<strong>Dueling network architectures for deep reinforcement learning.</strong>“ arXiv preprint arXiv:1511.06581 (2015). [pdf] (ICLR 最佳论文，很棒的想法)  ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1511.06581">http://arxiv.org/pdf/1511.06581</a></p><p>█[48] Mnih, Volodymyr, et al. “<strong>Asynchronous methods for deep reinforcement learning.</strong>“ arXiv preprint arXiv:1602.01783 (2016). [pdf] (前沿方法) ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1602.01783">http://arxiv.org/pdf/1602.01783</a></p><p>█[49] Lillicrap, Timothy P., et al. “<strong>Continuous control with deep reinforcement learning.</strong>“ arXiv preprint arXiv:1509.02971 (2015). [pdf] (DDPG)  ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1509.02971">http://arxiv.org/pdf/1509.02971</a></p><p>█[50] Gu, Shixiang, et al. “<strong>Continuous Deep Q-Learning with Model-based Acceleration.</strong>“ arXiv preprint arXiv:1603.00748 (2016). [pdf] (NAF)  ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1603.00748">http://arxiv.org/pdf/1603.00748</a></p><p>█[51] Schulman, John, et al. “<strong>Trust region policy optimization.</strong>“ CoRR, abs/1502.05477 (2015). [pdf] (TRPO)  ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.jmlr.org/proceedings/papers/v37/schulman15.pdf">http://www.jmlr.org/proceedings/papers/v37/schulman15.pdf</a></p><p>█[52] Silver, David, et al. “<strong>Mastering the game of Go with deep neural networks and tree search.</strong>“ Nature 529.7587 (2016): 484-489. [pdf] (AlphaGo) ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//willamette.edu/%7Elevenick/cs448/goNature.pdf">http://willamette.edu/~levenick/cs448/goNature.pdf</a></p><h3 id="2-7-深度迁移学习-终生学习-强化学习"><a href="#2-7-深度迁移学习-终生学习-强化学习" class="headerlink" title="2.7 深度迁移学习 /终生学习 / 强化学习"></a>2.7 深度迁移学习 /终生学习 / 强化学习</h3><p>█[53] Bengio, Yoshua. “<strong>Deep Learning of Representations for Unsupervised and Transfer Learning</strong>.” ICML Unsupervised and Transfer Learning 27 (2012): 17-36. [pdf] (这是一个教程) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.jmlr.org/proceedings/papers/v27/bengio12a/bengio12a.pdf">http://www.jmlr.org/proceedings/papers/v27/bengio12a/bengio12a.pdf</a></p><p>█[54] Silver, Daniel L., Qiang Yang, and Lianghao Li. “<strong>Lifelong Machine Learning Systems: Beyond Learning Algorithms.</strong>“ AAAI Spring Symposium: Lifelong Machine Learning. 2013. [pdf] (对终生学习的简单讨论) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//citeseerx.ist.psu.edu/viewdoc/download%3Fdoi%3D10.1.1.696.7800%26rep%3Drep1%26type%3Dpdf">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.696.7800&amp;rep=rep1&amp;type=pdf</a></p><p>█[55] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. “<strong>Distilling the knowledge in a neural network.</strong>“ arXiv preprint arXiv:1503.02531 (2015). [pdf] (大神们的研究)  ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1503.02531">http://arxiv.org/pdf/1503.02531</a></p><p>█[56] Rusu, Andrei A., et al. “<strong>Policy distillation.</strong>“ arXiv preprint arXiv:1511.06295 (2015). [pdf] (RL 领域) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1511.06295">http://arxiv.org/pdf/1511.06295</a></p><p>█[57] Parisotto, Emilio, Jimmy Lei Ba, and Ruslan Salakhu★★★tdinov. “<strong>Actor-mimic: Deep multitask and transfer reinforcement learning.</strong>“ arXiv preprint arXiv:1511.06342 (2015). [pdf] (RL 领域) ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1511.06342">http://arxiv.org/pdf/1511.06342</a></p><p>█[58] Rusu, Andrei A., et al. “<strong>Progressive neural networks.</strong>“ arXiv preprint arXiv:1606.04671 (2016). [pdf] (杰出研究, 很新奇的想法) ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1606.04671">https://arxiv.org/pdf/1606.04671</a></p><h3 id="2-8-One-Shot-深度学习"><a href="#2-8-One-Shot-深度学习" class="headerlink" title="2.8 One Shot 深度学习"></a>2.8 One Shot 深度学习</h3><p>█[59] Lake, Brenden M., Ruslan Salakhutdinov, and Joshua B. Tenenbaum. “<strong>Human-level concept learning through probabilistic program induction.</strong>“ Science 350.6266 (2015): 1332-1338. [pdf] (不含深度学习但值得一读) ★★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//clm.utexas.edu/compjclub/wp-content/uploads/2016/02/lake2015.pdf">http://clm.utexas.edu/compjclub/wp-content/uploads/2016/02/lake2015.pdf</a></p><p>█[60] Koch, Gregory, Richard Zemel, and Ruslan Salakhutdinov. “<strong>Siamese Neural Networks for One-shot Image Recognition.</strong>“(2015) [pdf] ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//www.cs.utoronto.ca/%7Egkoch/files/msc-thesis.pdf">http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf</a></p><p>█[61] Santoro, Adam, et al. “<strong>One-shot Learning with Memory-Augmented Neural Networks.</strong>“ arXiv preprint arXiv:1605.06065 (2016). [pdf] (one shot 学习的基础一步) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1605.06065">http://arxiv.org/pdf/1605.06065</a></p><p>█[62] Vinyals, Oriol, et al. “<strong>Matching Networks for One Shot Learning.</strong>“ arXiv preprint arXiv:1606.04080 (2016). [pdf] ★★★</p><p>地址：<a href="http://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1606.04080">https://arxiv.org/pdf/1606.04080</a></p><p>█[63] Hariharan, Bharath, and Ross Girshick. “<strong>Low-shot visual object recognition.</strong>“ arXiv preprint arXiv:1606.02819 (2016). [pdf] (通向更大规模数据的一步) ★★★★</p><p>地址：<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1606.02819">http://arxiv.org/pdf/1606.02819</a></p><h2 id="3-应用"><a href="#3-应用" class="headerlink" title="3 应用"></a>3 应用</h2><h3 id="3-1-自然语言处理-NLP"><a href="#3-1-自然语言处理-NLP" class="headerlink" title="3.1 自然语言处理 (NLP)"></a>3.1 自然语言处理 (NLP)</h3><p>█[1] Antoine Bordes, et al. “<strong>Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing.</strong>“ AISTATS(2012) [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//www.hds.utc.fr/~bordesan/dokuwiki/lib/exe/fetch.php%3Fid%3Den%253Apubli%26cache%3Dcache%26media%3Den%3Abordes12aistats.pdf">https://www.hds.utc.fr/~bordesan/dokuwiki/lib/exe/fetch.php?id=en%3Apubli&amp;cache=cache&amp;media=en:bordes12aistats.pdf</a></p><p>█[2] Mikolov, et al. “<strong>Distributed representations of words and phrases and their compositionality.</strong>“ ANIPS(2013): 3111-3119 [pdf] (word2vec) ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf</a></p><p>█[3] Sutskever, et al. ““<strong>Sequence to sequence learning with neural networks.</strong>“ ANIPS(2014) [pdf] ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf</a></p><p>█[4] Ankit Kumar, et al. ““<strong>Ask Me Anything: Dynamic Memory Networks for Natural Language Processing.</strong>“ arXiv preprint arXiv:1506.07285(2015) [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1506.07285">https://arxiv.org/abs/1506.07285</a></p><p>█[5] Yoon Kim, et al. “<strong>Character-Aware Neural Language Models.</strong>“ NIPS(2015) arXiv preprint arXiv:1508.06615(2015) [pdf] ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1508.06615">[1508.06615] Character-Aware Neural Language Models</a></p><p>█[6] Jason Weston, et al. “<strong>Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks.</strong>“ arXiv preprint arXiv:1502.05698(2015) [pdf] (bAbI tasks) ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1502.05698">A Set of Prerequisite Toy Tasks</a></p><p>█[7] Karl Moritz Hermann, et al. “<strong>Teaching Machines to Read and Comprehend.</strong>“ arXiv preprint arXiv:1506.03340(2015) <a href="CNN/每日邮报完形填空风格的问题">pdf</a> ★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1506.03340">[1506.03340] Teaching Machines to Read and Comprehend</a></p><p>█[8] Alexis Conneau, et al. “<strong>Very Deep Convolutional Networks for Natural Language Processing.</strong>“ arXiv preprint arXiv:1606.01781(2016) [pdf] (文本分类的前沿技术) ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1606.01781">[1606.01781] Very Deep Convolutional Networks for Text Classification</a></p><p>█[9] Armand Joulin, et al. “<strong>Bag of Tricks for Efficient Text Classification.</strong>“ arXiv preprint arXiv:1607.01759(2016) [pdf] (比前沿技术稍落后, 但快很多) ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1607.01759">[1607.01759] Bag of Tricks for Efficient Text Classification</a></p><h3 id="3-2-物体检测"><a href="#3-2-物体检测" class="headerlink" title="3.2 物体检测"></a>3.2 物体检测</h3><p>█[1] Szegedy, Christian, Alexander Toshev, and Dumitru Erhan. “<strong>Deep neural networks for object detection</strong>.” Advances in Neural Information Processing Systems. 2013. [pdf] ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf">http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf</a></p><p>█[2] Girshick, Ross, et al. “<strong>Rich feature hierarchies for accurate object detection and semantic segmentation.</strong>“ Proceedings of the IEEE conference on computer vision and pattern recognition. 2014. [pdf] (RCNN) ★★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf">http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf</a></p><p>█[3] He, Kaiming, et al. “<strong>Spatial pyramid pooling in deep convolutional networks for visual recognition.</strong>“ European Conference on Computer Vision. Springer International Publishing, 2014. [pdf] (SPPNet) ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1406.4729">http://arxiv.org/pdf/1406.4729</a></p><p>█[4] Girshick, Ross. “<strong>Fast r-cnn.</strong>“ Proceedings of the IEEE International Conference on Computer Vision. 2015. [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//pdfs.semanticscholar.org/8f67/64a59f0d17081f2a2a9d06f4ed1cdea1a0ad.pdf">https://pdfs.semanticscholar.org/8f67/64a59f0d17081f2a2a9d06f4ed1cdea1a0ad.pdf</a></p><p>█[5] Ren, Shaoqing, et al. “<strong>Faster R-CNN: Towards real-time object detection with region proposal networks.</strong>“ Advances in neural information processing systems. 2015. [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5638-analysis-of-variational-bayesian-latent-dirichlet-allocation-weaker-sparsity-than-map.pdf">http://papers.nips.cc/paper/5638-analysis-of-variational-bayesian-latent-dirichlet-allocation-weaker-sparsity-than-map.pdf</a></p><p>█[6] Redmon, Joseph, et al. “<strong>You only look once: Unified, real-time object detection.</strong>“ arXiv preprint arXiv:1506.02640 (2015). [pdf] (YOLO，杰出研究，非常具有使用价值） ★★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//homes.cs.washington.edu/~ali/papers/YOLO.pdf">http://homes.cs.washington.edu/~ali/papers/YOLO.pdf</a></p><p>█[7] Liu, Wei, et al. “<strong>SSD: Single Shot MultiBox Detector.</strong>“ arXiv preprint arXiv:1512.02325 (2015). [pdf] ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1512.02325">http://arxiv.org/pdf/1512.02325</a></p><p>█[8] Dai, Jifeng, et al. “<strong>R-FCN: Object Detection via Region-based Fully Convolutional Networks.</strong>“ arXiv preprint arXiv:1605.06409 (2016). [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1605.06409">Object Detection via Region-based Fully Convolutional Networks</a></p><h3 id="3-3-视觉追踪"><a href="#3-3-视觉追踪" class="headerlink" title="3.3 视觉追踪"></a>3.3 视觉追踪</h3><p>█[1] Wang, Naiyan, and Dit-Yan Yeung. “<strong>Learning a deep compact image representation for visual tracking.</strong>“ Advances in neural information processing systems. 2013. [pdf] (第一篇使用深度学习做视觉追踪的论文，DLT Tracker) ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/5192-learning-a-deep-compact-image-representation-for-visual-tracking.pdf">http://papers.nips.cc/paper/5192-learning-a-deep-compact-image-representation-for-visual-tracking.pdf</a></p><p>█[2] Wang, Naiyan, et al. “<strong>Transferring rich feature hierarchies for robust visual tracking.</strong>“ arXiv preprint arXiv:1501.04587 (2015). [pdf] (SO-DLT) ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1501.04587">http://arxiv.org/pdf/1501.04587</a></p><p>█[3] Wang, Lijun, et al. “<strong>Visual tracking with fully convolutional networks.</strong>“ Proceedings of the IEEE International Conference on Computer Vision. 2015. [pdf] (FCNT) ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//www.cv-foundation.org/openaccess/content_iccv_2015/papers/Wang_Visual_Tracking_With_ICCV_2015_paper.pdf">http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Wang_Visual_Tracking_With_ICCV_2015_paper.pdf</a></p><p>█[4] Held, David, Sebastian Thrun, and Silvio Savarese. “<strong>Learning to Track at 100 FPS with Deep Regression Networks.</strong>“ arXiv preprint arXiv:1604.01802 (2016). [pdf] (GOTURN，在深度学习方法里算是非常快的，但仍比非深度学习方法慢很多) ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1604.01802">http://arxiv.org/pdf/1604.01802</a></p><p>█[5] Bertinetto, Luca, et al. “<strong>Fully-Convolutional Siamese Networks for Object Tracking.</strong>“ arXiv preprint arXiv:1606.09549 (2016). [pdf] (SiameseFC，实时物体追踪领域的最新前沿技术) ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1606.09549">https://arxiv.org/pdf/1606.09549</a></p><p>█[6] Martin Danelljan, Andreas Robinson, Fahad Khan, Michael Felsberg. “<strong>Beyond Correlation Filters: Learning Continuous Convolution Operators for Visual Tracking.</strong>“ ECCV (2016) [pdf] (C-COT) ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/C-COT_ECCV16.pdf">http://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/C-COT_ECCV16.pdf</a></p><p>█[7] Nam, Hyeonseob, Mooyeol Baek, and Bohyung Han. “<strong>Modeling and Propagating CNNs in a Tree Structure for Visual Tracking.</strong>“ arXiv preprint arXiv:1608.07242 (2016). [pdf] (VOT2016 获奖论文,TCNN) ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1608.07242">https://arxiv.org/pdf/1608.07242</a></p><h3 id="3-4-图像标注"><a href="#3-4-图像标注" class="headerlink" title="3.4 图像标注"></a>3.4 图像标注</h3><p>█[1] Farhadi,Ali,etal. “<strong>Every picture tells a story: Generating sentences from images</strong>“. In Computer VisionECCV 2010. Springer Berlin Heidelberg:15-29, 2010. [pdf] ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//www.cs.cmu.edu/~afarhadi/papers/sentence.pdf">https://www.cs.cmu.edu/~afarhadi/papers/sentence.pdf</a></p><p>█[2] Kulkarni, Girish, et al. “<strong>Baby talk: Understanding and generating image descriptions</strong>“. In Proceedings of the 24th CVPR, 2011. [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//tamaraberg.com/papers/generation_cvpr11.pdf">http://tamaraberg.com/papers/generation_cvpr11.pdf</a></p><p>█[3] Vinyals, Oriol, et al. “<strong>Show and tell: A neural image caption generator</strong>“. In arXiv preprint arXiv:1411.4555, 2014. [pdf] ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1411.4555.pdf">https://arxiv.org/pdf/1411.4555.pdf</a></p><p>█[4] Donahue, Jeff, et al. “<strong>Long-term recurrent convolutional networks for visual recognition and description</strong>“. In arXiv preprint arXiv:1411.4389 ,2014. [pdf]</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1411.4389.pdf">https://arxiv.org/pdf/1411.4389.pdf</a></p><p>█[5] Karpathy, Andrej, and Li Fei-Fei. “<strong>Deep visual-semantic alignments for generating image descriptions</strong>“. In arXiv preprint arXiv:1412.2306, 2014. [pdf] ★★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//cs.stanford.edu/people/karpathy/cvpr2015.pdf">https://cs.stanford.edu/people/karpathy/cvpr2015.pdf</a></p><p>█[6] Karpathy, Andrej, Armand Joulin, and Fei Fei F. Li. “<strong>D**</strong>eep fragment embeddings for bidirectional image sentence mapping**”. In Advances in neural information processing systems, 2014. [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1406.5679v1.pdf">https://arxiv.org/pdf/1406.5679v1.pdf</a></p><p>█[7] Fang, Hao, et al. “<strong>From captions to visual concepts and back</strong>“. In arXiv preprint arXiv:1411.4952, 2014. [pdf] ★★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1411.4952v3.pdf">https://arxiv.org/pdf/1411.4952v3.pdf</a></p><p>█[8] Chen, Xinlei, and C. Lawrence Zitnick. “<strong>Learning a recurrent visual representation for image caption generation</strong>“. In arXiv preprint arXiv:1411.5654, 2014. [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1411.5654v1.pdf">https://arxiv.org/pdf/1411.5654v1.pdf</a></p><p>█[9] Mao, Junhua, et al. “<strong>Deep captioning with multimodal recurrent neural networks (m-rnn)</strong>“. In arXiv preprint arXiv:1412.6632, 2014. [pdf] ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1412.6632v5.pdf">https://arxiv.org/pdf/1412.6632v5.pdf</a></p><p>█[10] Xu, Kelvin, et al. “<strong>Show, attend and tell: Neural image caption generation with visual attention</strong>“. In arXiv preprint arXiv:1502.03044, 2015. [pdf] ★★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1502.03044v3.pdf">https://arxiv.org/pdf/1502.03044v3.pdf</a></p><h3 id="3-5-机器翻译"><a href="#3-5-机器翻译" class="headerlink" title="3.5 机器翻译"></a>3.5 机器翻译</h3><p>部分里程碑研究被列入 RNN / Seq-to-Seq 版块。</p><p>█[1] Luong, Minh-Thang, et al. “<strong>Addressing the rare word problem in neural machine translation.</strong>“ arXiv preprint arXiv:1410.8206 (2014). [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1410.8206">http://arxiv.org/pdf/1410.8206</a></p><p>█[2] Sennrich, et al. <strong>“Neural Machine Translation of Rare Words with Subword Units</strong>“. In arXiv preprint arXiv:1508.07909, 2015. [pdf] ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1508.07909.pdf">https://arxiv.org/pdf/1508.07909.pdf</a></p><p>█[3] Luong, Minh-Thang, Hieu Pham, and Christopher D. Manning. “<strong>Effective approaches to attention-based neural machine translation</strong>.” arXiv preprint arXiv:1508.04025 (2015). [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1508.04025">http://arxiv.org/pdf/1508.04025</a></p><p><strong>█</strong>[4] Chung, et al. “<strong>A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation</strong>“. In arXiv preprint arXiv:1603.06147, 2016. [pdf] ★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1603.06147.pdf">https://arxiv.org/pdf/1603.06147.pdf</a></p><p>█[5] Lee, et al. “<strong>Fully Character-Level Neural Machine Translation without Explicit Segmentation</strong>“. In arXiv preprint arXiv:1610.03017, 2016. [pdf] ★★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1610.03017.pdf">https://arxiv.org/pdf/1610.03017.pdf</a></p><p>█[6] Wu, Schuster, Chen, Le, et al. “<strong>Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</strong>“. In arXiv preprint arXiv:1609.08144v2, 2016. [pdf] (Milestone) ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1609.08144v2.pdf">https://arxiv.org/pdf/1609.08144v2.pdf</a></p><h3 id="3-6-机器人"><a href="#3-6-机器人" class="headerlink" title="3.6 机器人"></a>3.6 机器人</h3><p>█[1] Koutník, Jan, et al. “<strong>Evolving large-scale neural networks for vision-based reinforcement learning.</strong>“ Proceedings of the 15th annual conference on Genetic and evolutionary computation. ACM, 2013. [pdf] ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//repository.supsi.ch/4550/1/koutnik2013gecco.pdf">http://repository.supsi.ch/4550/1/koutnik2013gecco.pdf</a></p><p>█[2] Levine, Sergey, et al. “<strong>End-to-end training of deep visuomotor policies.</strong>“ Journal of Machine Learning Research 17.39 (2016): 1-40. [pdf] ★★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//www.jmlr.org/papers/volume17/15-522/15-522.pdf">http://www.jmlr.org/papers/volume17/15-522/15-522.pdf</a></p><p>█[3] Pinto, Lerrel, and Abhinav Gupta. “<strong>Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours.</strong>“ arXiv preprint arXiv:1509.06825 (2015). [pdf] ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1509.06825">http://arxiv.org/pdf/1509.06825</a></p><p>█[4] Levine, Sergey, et al. “<strong>Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection</strong>.” arXiv preprint arXiv:1603.02199 (2016). [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1603.02199">http://arxiv.org/pdf/1603.02199</a></p><p>█[5] Zhu, Yuke, et al. “<strong>Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning.</strong>“ arXiv preprint arXiv:1609.05143 (2016). [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1609.05143">https://arxiv.org/pdf/1609.05143</a></p><p>█[6] Yahya, Ali, et al. “<strong>Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search.</strong>“ arXiv preprint arXiv:1610.00673 (2016). [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1610.00673">https://arxiv.org/pdf/1610.00673</a></p><p>█[7] Gu, Shixiang, et al. “<strong>Deep Reinforcement Learning for Robotic Manipulation.</strong>“ arXiv preprint arXiv:1610.00633 (2016). [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1610.00633">https://arxiv.org/pdf/1610.00633</a></p><p>█[8] A Rusu, M Vecerik, Thomas Rothörl, N Heess, R Pascanu, R Hadsell.”<strong>Sim-to-Real Robot Learning from Pixels with Progressive Nets.</strong>“ arXiv preprint arXiv:1610.04286 (2016). [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1610.04286.pdf">https://arxiv.org/pdf/1610.04286.pdf</a></p><p>█[9] Mirowski, Piotr, et al. “<strong>Learning to navigate in complex environments.</strong>“ arXiv preprint arXiv:1611.03673 (2016). [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1611.03673">https://arxiv.org/pdf/1611.03673</a></p><h3 id="3-7-艺术"><a href="#3-7-艺术" class="headerlink" title="3.7 艺术"></a>3.7 艺术</h3><p>█[1] Mordvintsev, Alexander; Olah, Christopher; Tyka, Mike (2015). “<strong>Inceptionism: Going Deeper into Neural Networks</strong>“. Google Research. [html] (Deep Dream) ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html">https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html</a></p><p>█[2] Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. “<strong>A neural algorithm of artistic style.</strong>“ arXiv preprint arXiv:1508.06576 (2015). [pdf] (杰出研究，迄今最成功的方法) ★★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1508.06576">http://arxiv.org/pdf/1508.06576</a></p><p>█[3] Zhu, Jun-Yan, et al. “<strong>Generative Visual Manipulation on the Natural Image Manifold.</strong>“ European Conference on Computer Vision. Springer International Publishing, 2016. [pdf] (iGAN) ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1609.03552">https://arxiv.org/pdf/1609.03552</a></p><p>█[4] Champandard, Alex J. “<strong>Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks.</strong>“ arXiv preprint arXiv:1603.01768 (2016). [pdf] (Neural Doodle) ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1603.01768">http://arxiv.org/pdf/1603.01768</a></p><p>█[5] Zhang, Richard, Phillip Isola, and Alexei A. Efros. “<strong>Colorful Image Colorization</strong>.” arXiv preprint arXiv:1603.08511 (2016). [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1603.08511">http://arxiv.org/pdf/1603.08511</a></p><p>█[6] Johnson, Justin, Alexandre Alahi, and Li Fei-Fei. “<strong>Perceptual losses for real-time style transfer and super-resolution</strong>.” arXiv preprint arXiv:1603.08155 (2016). [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1603.08155.pdf">https://arxiv.org/pdf/1603.08155.pdf</a></p><p>█[7] Vincent Dumoulin, Jonathon Shlens and Manjunath Kudlur. “<strong>A learned representation for artistic style.</strong>“ arXiv preprint arXiv:1610.07629 (2016). [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1610.00633">https://arxiv.org/pdf/1610.00633</a></p><p>█[8] Gatys, Leon and Ecker, et al.”<strong>Controlling Perceptual Factors in Neural Style Transfer.</strong>“ arXiv preprint arXiv:1611.07865 (2016). [pdf] (control style transfer over spatial location,colour information and across spatial scale) ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1610.04286.pdf">https://arxiv.org/pdf/1610.04286.pdf</a></p><p>█[9] Ulyanov, Dmitry and Lebedev, Vadim, et al. “<strong>Texture Networks: Feed-forward Synthesis of Textures and Stylized Images.</strong>“ arXiv preprint arXiv:1603.03417(2016). [pdf] (纹理生成和风格变化) ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1611.03673">https://arxiv.org/pdf/1611.03673</a></p><h3 id="3-8-目标分割-Object-Segmentation"><a href="#3-8-目标分割-Object-Segmentation" class="headerlink" title="3.8 目标分割 Object Segmentation"></a>3.8 目标分割 Object Segmentation</h3><p>█[1] J. Long, E. Shelhamer, and T. Darrell, “<strong>Fully convolutional networks for semantic segmentation.</strong>” in CVPR, 2015. [pdf] ★★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1411.4038v2.pdf">https://arxiv.org/pdf/1411.4038v2.pdf</a></p><p>█[2] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. “<strong>Semantic image segmentation with deep convolutional nets and fully connected crfs.</strong>“ In ICLR, 2015. [pdf] ★★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1606.00915v1.pdf">https://arxiv.org/pdf/1606.00915v1.pdf</a></p><p>█[3] Pinheiro, P.O., Collobert, R., Dollar, P. “<strong>Learning to segment object candidates.</strong>“ In: NIPS. 2015. [pdf] ★★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1506.06204v2.pdf">https://arxiv.org/pdf/1506.06204v2.pdf</a></p><p>█[4] Dai, J., He, K., Sun, J. <strong>“Instance-aware semantic segmentation via multi-task network cascades.</strong>“ in CVPR. 2016 [pdf] ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1512.04412v1.pdf">https://arxiv.org/pdf/1512.04412v1.pdf</a></p><p>█[5] Dai, J., He, K., Sun, J. “<strong>Instance-sensitive Fully Convolutional Networks.</strong>“ arXiv preprint arXiv:1603.08678 (2016). [pdf] ★★★</p><p>地址：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1603.08678v1.pdf">https://arxiv.org/pdf/1603.08678v1.pdf</a></p>]]></content>
    
    
    <categories>
      
      <category>1.科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文集</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1.科研/语言学荐书</title>
    <link href="undefined2020/02/24/1.%E7%A7%91%E7%A0%94/%E8%AF%AD%E8%A8%80%E5%AD%A6%E8%8D%90%E4%B9%A6/"/>
    <url>2020/02/24/1.%E7%A7%91%E7%A0%94/%E8%AF%AD%E8%A8%80%E5%AD%A6%E8%8D%90%E4%B9%A6/</url>
    
    <content type="html"><![CDATA[<h1 id="语言学方向书籍推荐–Serena-Gao"><a href="#语言学方向书籍推荐–Serena-Gao" class="headerlink" title="语言学方向书籍推荐–Serena Gao"></a>语言学方向书籍推荐–Serena Gao</h1><p>首先，做nlp不一定要很懂语言学，也不一定要跟语言学扯上关系。nlp可以仅是data mining，features engineering, 也的确有很多work目前在用文本或者对话做为数据集，然后用统计学方法实现目的，比如deep learning 。在某些任务上统计学模型功不可没，比如machine translation, speech recognition, question answering, etc. </p><p>如果题主只是对nlp的应用感兴趣，想泛泛了解一下目前进展的话，以上几个回答已经非常详细了，我接下来的回答可以不用看。许多主流大公司目前的力度都在deep learning, 学好nlp基本知识，做工程就够了(当然你还需要cs的background)， 语言学的东西不用太深入研究。</p><p>————————————-3.17 update——————————-</p><p>看了一下其他答案，大家的讨论和见解都很有趣，上来更新一点。</p><p>大多数人对nlp和语言学联系的了解，在于认为rule-based的nlp就是基于语言学。的确rule-based是语言学里广泛使用的，尤其是语法(syntax, syntactic structure)。现在machine learning的发展已经可以将rules转换为hidden states,人不用去操心提出大量rules来做exhaustive search。 </p><p><strong>但computational linguistics所包含的，远远大于rules。人类语言是漫长历史进化的高级产物，远不是成千上万个rules能描述清楚的。能被nlp利用的语言学，除了枚举rules外还有很多很多。</strong></p><p>比如定义。个人认为，在研究任何问题前，都必须要想清楚你的问题是什么，怎么定义。许许多多nlp research都是基于语言学上的定义，像我下文会提到的semantics, grammar。可是如果没有从沿用语言学的定义到nlp，这个0到1的过程，最早做researchers的人该如何想明白他们的research question？</p><p>做对话系统的同学应该很熟悉dialogue acts. 现在的对话系统发展迅猛，很多新应用都基于reinforcement learning, 并且取得显著成就。尤其是某些task-oriented dialogue generator, 早就不是十多年前的rule-based system了。但任何一个系统在设计之初都要采用dialouge acts定义（当然还有其他定义），来明确该系统的目的。不然该系统如何区分wh-question, yes-no question, greetings, 还有其他？（如果觉得见到“wh-”开头，问号结尾，就是一个wh-question rule, 那我不知道该说什么好了）</p><p><strong>以上讨论不在于反驳其他答主</strong>。很开心看到这么多人对nlp有热情并且愿意分享。<strong>只是做为一个看过很多nlp research，并且投身于natural language understanding(还稍微有点爱较真)的科研工作者，想要澄清人们对nlp和computational linguistics的一些误解做的小贡献。</strong> </p><p>明确自己的research task并且贯彻到底是好事，如果要做language modeling，基于machine learning/deep learning, 那真的不用费时间在语言学上。但觉得语言学是rule based已经过时了被淘汰了，这个锅语言学真的背的有点冤呀。</p><p>———————-(我真的不能再答题了该赶due了….)—————————</p><p>接下来的回答是，给真正对computational linguistics和nlp本身感兴趣的，对某些语言现象感兴趣，并打算在这条路上开始钻研的同学的一些建议。（想忽略细节的同学请直接拉到答案最后找reference）</p><p>=========================枯燥理论高能预警========================</p><p>人大脑工作不是靠probablistic language modeling，咱们谁的脑袋里都不会听到一个词然后跑一遍hidden markov，毕竟也进化了这么多年了不是。</p><p>与nlp相关，跟概率论并进的，除了传统的语言学，还有logic呢，Lofti Zadeh老爷爷研究了一辈子的fuzzy logic，也是在探究semantics&amp;world knowledge (再次感谢老爷爷的贡献，r.i.p)。</p><p>我也并不是在强调概率模型不重要，概率模型和现在很火的deep learning architecture像是基本功一样，而且是很好用的工具，其他答主已经强调很多，我就不再重复了。 除了这些，还有很多知识可以深入了解。</p><p>另外，语言学自身是个很大又很宽泛，又互相交叉的学科。有很多研究是跟literatures and arts有关，有的是跟cognitive science有关，还有neuroscience, mathematics, education, psychology, etc。我涉猎有限，在此只能回答跟computational linguistics有关(“to the best of my knowledge”)。</p><p>回归正题。语言学方面的书籍有很多，我接下来谈一下应该如何选和如何看。以下讨论只限英文，中文的工作我不了解，希望其他答主可以帮忙。</p><p><strong>Grammar</strong>是我会首先推荐的方向。Grammar分为morphology&amp;syntax. 在这里我主要指syntax.细节可以看Chomsky, Michael Colins, Jason Eisner等人的工作。现在大家用的最多的应该是stanford的syntactic parsing吧。这方面的工作已经很成熟，要处理语言基本是拿来就能用了。 但是语法树到底是什么，怎么构建，syntatic parsing优势，如何处理ambiguity,  想要做computational linguistics的话，这些很有必要知道。最基本的例子是，当用parser来处理你的句子，你起码要能看懂这个parser output是否make sense. </p><p>接着我会推荐<strong>Semantics</strong>. 这个部分是我做最多的，感觉也是被误解最多的。尤其推荐 “Meaning in language: An introduction to Semantics and Pragmatics.” 我并没有看完。Semantics是个很复杂的研究，可以涉及到语法，句法，world knowledge, 但最终还是回归semantics自身。目前nlp里很火的有distributional semantic representation (word embedding, phrase embedding, sentence embedding, etc), semantic parsing (logical form, etc), 等等等等。同一句话可以表达的意思太多了，同一个意思带来的表达形式也太多了。一个简单句子里包含的意思会涉及到当下对话双方的情景，以前或者以后会发生的事，等等。举个个人很喜欢的例子：</p><ul><li>2016年美国大选first presidential debate, Clinton vs Trump, 当trump被问到 “does the public’s right to know outweigh your personal .. (taxes)”, Trump: “… I will release my tax returns – against my lawyer’s wishes – when she releases her 33000 emails that have been deleted. <strong>As soon as she releases them, I will release</strong>. “. 最后一句话（粗体）包含的语意有：</li></ul><ol><li>等Hilary公开邮件记录之后，我就公开我的税务信息(动作和时间点)；</li><li>Hilary没公开，我也没公开(当下既定事实)；</li><li>Hilary不愿公开，我也不愿公开(sentiment)。</li><li>She – Clinton, I, my, – Trump, them – 33000 emails (co-reference).</li></ol><p>第一层意思是直观semantics, 能够被目前的semantic representation捕捉到。第二层是presupposition, 代表着在说话当下当事人双方默认已经发生的事情，是semantics研究中的难点；第三层包含了sentiment, 做情感分析的同学应该很了解，能否被目前的classifier捕捉到我不清楚。第四层是现在也很火的coreference resolution, 虽然原文里没有明确指代每个人称代词，但听众和当事人很直接能把每个人物代入，甚至包括Trump省略的”I will release (<strong>my taxes</strong>)”. 目前的co-reference resolution,e.g. stanford corenlp,  可以解决前三个代词，但省略的部分似乎还做不到。</p><p>（我还有很多例子，有空再上来更新）</p><p>对Semantic要求最高也是最难的，在nlp中应该是在natural language understanding相关应用了。Semantics里包含了太多太多的现象，如果能稍微研究并且model其中一小部分，对downstream application来说都会是一个很大的boost。前段时间有个shared task，叫 “hedge detection”,目的是找出文本信息中的hedges and cues。大部分人会关注这个shared  task下哪个模型做的最好，个人认为难点是在定义。有“but”,”however”出现语意就一定转折了么？如果被转折，是所在句子，还是段落还是一个小phrase呢？有dependency存在么？ 另一个相似shared task是negation detection. 想要理解这些问题本身和其难点所在，computational linguistics的前期知识储备是并不可少的。</p><p>以上两个方面应该可以展现一个big picture：前者代表语言结构是如何构建的，后者代表meaning是如何被赋予到某种结构里面的。</p><p>————小分割线————-</p><p>除了大框架外，小的方向取决于你的兴趣和目标所在。对话？文本？natural language understanding or natural language generation? </p><p>另外提两个我觉得必看的，很重要的理论，是computational pragmatics范畴里的：<strong>Grice’s maxims,</strong> 和<strong>Rational Speech Act(RSA)</strong>. 这两个理论其实紧密相关。 前者理论关于谈话双方为了有效沟通会有意识的遵守的一些principle, (同时可见“cooperative principle”), 后者关于为了达到这种有效沟通，对话当中存在的一种recursive process, 并且是bayesian inference. 如果你的工作跟 inference, reasoning相关，请一定要阅读。做对话系统的应该已经很熟悉了。</p><p>最后一个比较偏门的方向是我前面提到的fuzzy logic。目前还是有researcher继承Zadeh老爷爷的衣钵，并且用fuzzy logic做出了很多natural language generation, information extraction方面的成就。个人经验而言，我博士第一年(2014)一直在关注deep learning/machine learning方面，当时觉得它们是万能的。直到第二年夏天在忙一个project, 阅读了Zadeh老爷爷的大量工作，才感觉自己一直在以很片面的眼光看research。当时真的做了满满一本笔记。</p><p><img src="https://pic2.zhimg.com/v2-2d38d8c27831d6ceec69edf6535b8e19_b.jpg" alt="img">)<img src="https://pic2.zhimg.com/80/v2-2d38d8c27831d6ceec69edf6535b8e19_hd.jpg" alt="img"></p><p>===========================好累先写到这=========================</p><p>最后，如果兴趣在建modeling，deep learning architecture, 语言学方面的part-of-speech也好，parsing也好，都只是你的工具； </p><p>同样，如果兴趣在computational linguistics,语言现象，deep learning/machine learning都是你的工具。</p><p>取决与你的任务是什么，取决于你有没有完全dedicated的信心。毕竟巴菲特和Geff Hinton是少数，大多数人都无法预测20年后火的适合什么。</p><p>感谢阅读。希望能给在犹豫是否开始computational linguistics和nlp研究同学们一些帮助。</p><p>(任何不准确的地方还请大家指正)</p><p>=============================================================</p><p><strong>Reference:</strong></p><p>（大方向书籍，我要是能全部买下来就好了…并没有全部看完，有的只是看过某一章节。Grammar和syntax知乎里面有很多问答跟这方面有关，在此不重复了。）</p><p>Cruse, Alan. “Meaning in language: An introduction to semantics and pragmatics.” (2011).</p><p>Karttunen, Lauri (1974) <a href="https://link.zhihu.com/?target=http%3A//www.stanford.edu/~laurik/publications/archive/presupplingcontext.pdf">[1]</a>. <em>Theoretical Linguistics</em> 1 181-94. Also in Pragmatics: A Reader, Steven Davis (ed.), pages 406-415, Oxford University Press, 1991.</p><p>Kadmon, Nirit. “Formal pragmatics semantics, pragmatics, presupposition, and focus.” (2001).</p><p>Levinson, Stephen C. <em>Pragmatics.</em>Cambridge: Cambridge University Press, 1983, pp. 181-184.</p><p>Wardhaugh, Ronald. <em>An introduction to sociolinguistics</em>. John Wiley &amp; Sons, 2010. (这本书的影响力很大，有很多跟social science的讨论)</p><p>(具体其他上面提到的，每一篇我都仔细读过的)</p><p><a href="https://link.zhihu.com/?target=https%3A//www.sas.upenn.edu/~haroldfs/dravling/grice.html">Grice’s Maxims</a></p><p>Monroe, Will, and Christopher Potts. “Learning in the rational speech acts model.” <em>arXiv preprint arXiv:1510.06807</em> (2015).(这篇是关于rsa如何被用于具体task上的)</p><p>Farkas, Richárd, et al. “The CoNLL-2010 shared task: learning to detect hedges and their scope in natural language text.” <em>Proceedings of the Fourteenth Conference on Computational Natural Language Learning—Shared Task</em>. Association for Computational Linguistics, 2010. (上文提到的hedge and cues shared task,关于linguistics里的现象是如何被formulate成nlp问题的)</p><p>Morante, Roser, and Eduardo Blanco. “* SEM 2012 shared task: Resolving the scope and focus of negation.” <em>Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation</em>. Association for Computational Linguistics, 2012. (negation 的shared task)</p><p>最后附上两篇老爷爷对我影响最大的：</p><p>Zadeh, Lotfi Asker. “The concept of a linguistic variable and its application to approximate reasoning—I.” <em>Information sciences</em> 8.3 (1975): 199-249.</p><p>Zadeh, Lotfi A. “The concept of a linguistic variable and its application to approximate reasoning—II.” <em>Information sciences</em> 8.4 (1975): 301-357.（ 这系列work分两部。）</p><p>Zadeh, Lotfi A. “Toward a theory of fuzzy information granulation and its centrality in human reasoning and fuzzy logic.” <em>Fuzzy sets and systems</em> 90.2 (1997): 111-127.</p><p><img src="https://pic2.zhimg.com/v2-2b2d9b7cd97948eb565daa7adececf15_b.jpg" alt="img">)<img src="https://pic2.zhimg.com/80/v2-2b2d9b7cd97948eb565daa7adececf15_hd.jpg" alt="img"></p><p><a href="">编辑于 2018-03-18</a></p>]]></content>
    
    
    <categories>
      
      <category>1.科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>语言学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1.科研/4 - 模型设计</title>
    <link href="undefined2020/02/24/1.%E7%A7%91%E7%A0%94/4%20-%20%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1/"/>
    <url>2020/02/24/1.%E7%A7%91%E7%A0%94/4%20-%20%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1/</url>
    
    <content type="html"><![CDATA[<h1 id="模型设计"><a href="#模型设计" class="headerlink" title="模型设计"></a>模型设计</h1><h4 id="确定进入-领域后，如何快速学会第一个技能？"><a href="#确定进入-领域后，如何快速学会第一个技能？" class="headerlink" title="确定进入____领域后，如何快速学会第一个技能？"></a><strong>确定进入_<strong>___</strong>领域后，如何快速学会第一个技能？</strong></h4><p>仔细研究一般现有的主要工具，流派和方法，先入门。<br>我的建议是：找到一个开源项目，比如机器翻译或者深度学习的项目。理解开源项目的任务，编译通过该项目发布的示范程序，得到与项目示范程序一致的结果。然后再深入理解开源项目示范程序的算法。自己编程实现一下这个示范程序的算法。再按照项目提供的标准测试集测试自己实现的程序。如果输出的结果与项目中出现的结果不一致，就要仔细查验自己的程序，反复修改，直到结果与示范程序基本一致。如果还是不行，就大胆给项目的作者写信请教。在此基础上，再看看自己能否进一步完善算法或者实现，取得比示范程序更好的结果。</p><h4 id="如何改进别人的模型"><a href="#如何改进别人的模型" class="headerlink" title="如何改进别人的模型"></a><strong>如何改进别人的模型</strong></h4><ul><li><p>反复阅读本领域最新发表的文章，多阅读本领域牛人发表的文章。在深入了解已有工作的基础上，探讨还有没有一些地方可以推翻、改进、综合、迁移。注意做实验的时候，不要贪多，每次实验只需要验证一个想法。</p></li><li><p>每次实验之后必须要进行分析存在的错误，找出原因。</p></li></ul><ul><li>对成功的实验，进一步探讨如何改进算法。注意实验数据必须是业界公认的数据。</li><li>与已有的算法进行比较，体会能够得出比较一般性的结论。如果有，则去写一篇文章，否则，应该换一个新的选题。</li></ul><h4 id="如何发明自己的模型"><a href="#如何发明自己的模型" class="headerlink" title="如何发明自己的模型"></a>如何发明自己的模型</h4><ol><li><p>定义你的任务，例如summarization</p></li><li><p>定义数据集，最好使用现成的数据集，因为他们已经有baselines</p></li><li><p>建立baseline，他可以是一个非常简单的一元线性回归，然后在你的训练数据集上计算你的评价标准，看看模型是过拟合还是欠拟合</p></li><li><p>遍历现有模型，选好模型，在baseline上改进.</p></li><li><p>自己创造新模型（Opitional）</p><ol><li><p>首先，你需要做好以上说的几个步骤.</p></li><li><p>然后你需要知道已经存在的模型上有哪些问题。然后你就可以设计出自己的模型。如果你想要这样做的话，<strong>你真的需要和你的导师和其他研究者保持沟通，除非你自己就是研究者并且已经获得了博士文凭</strong>。</p></li><li><p>你需要实现你的模型，然后根据你的新点子去对它快速<strong>迭代</strong>。（也许在某个位置新加一层？然后看看他起不起作用？），<strong>这种迭代的思想平滑了难度曲线，是版本控制的思想，类似于先搭起框架再往里边慢慢实现不同功能的API接口。</strong></p></li><li><p>那么在迭代的过程中，拥有足够多的的软件工程技能来配置高效的实验框架，从而能收集到这些结果就很重要。<strong>软工能力难顶，只能见招拆招了</strong></p></li><li><p>建议从一个和你的真实想法比起来相对容易很多的模型做起。先把简单模型建立起来。然后逐步尝试更复杂的模型。</p></li></ol></li><li><p>对于开始提出的终极任务：summarization。</p><ol><li>一开始你可能尝试一些非常简单的模型。比如对自然段中的所有单词向量求平均，然后用贪心搜索一个接一个地生成单词，或者用贪心搜索对维基百科中的现有文章寻找一些片段，然后把合适的片段复制过去。</li><li>然后升级你的目标，尝试某些真正让你生产整段总结的方法。</li></ol></li></ol><p><img src="http://q0u9fsub0.bkt.clouddn.com///20191202/pO3C6CoWauYN.png?imageslim" alt="mark"></p><p>CS224N课程大作业的一些关于模型选择的idea（包括了在kaggle上打比赛）</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1420_37_463.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>1.科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文写法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1.科研/7 - 论文结构</title>
    <link href="undefined2020/02/24/1.%E7%A7%91%E7%A0%94/7%20-%20%E8%AE%BA%E6%96%87%E7%BB%93%E6%9E%84/"/>
    <url>2020/02/24/1.%E7%A7%91%E7%A0%94/7%20-%20%E8%AE%BA%E6%96%87%E7%BB%93%E6%9E%84/</url>
    
    <content type="html"><![CDATA[<h1 id="7-论文结构"><a href="#7-论文结构" class="headerlink" title="7.论文结构"></a>7.论文结构</h1><p>学术研究是一项系统工程</p><p>在这个系统工程中，论文的作用则是，向学术界同行清晰准确地描述成果的创新点、技术思路、算法细节和验证结果。明白这一点，才能正确的对待论文写作：一项乏善可陈的工作，很难通过写作变得众星捧月；一项充满创新的成果，却有可能因为糟糕的写作而无法向审稿人准确传递重要价值所在，延误成果发表。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1301_14_107.png" alt=""></p><hr><p><strong>一篇NLP论文的典型结构</strong></p><p>NLP学术会议（甚至包括期刊）论文已经形成比较固定的结构。绝大部分论文由以下六大部分构成：摘要（Abstract）、介绍（Introduction）、相关工作（Related Work）、方法（Method）、实验（Experiment）、结论（Conclusion）。少数论文会根据创新成果形式不同而略有不同，例如提出新数据集的论文，可能会把Method部分调整为Dataset的标注与分析，但不影响论文整体构成。每个部分作用不同：</p><ul><li>摘要：用100-200词简介研究任务与挑战、解决思路与方法、实验效果与结论。</li><li>介绍：用1页左右篇幅，比摘要更详细地介绍研究任务、已有方法、主要挑战、解决思路、具体方法、实验结果。</li><li>相关工作：用0.5-1页左右篇幅介绍研究任务的相关工作，说明本文工作与已有工作的异同。</li><li>方法：用2-3页篇幅介绍本文提出的方法模型细节。</li><li>实验：用2-3页篇幅介绍验证本文方法有效性的实验设置、数据集合、实验结果、分析讨论等。</li><li>结论：简单总结本文主要工作，展望未来研究方向。</li></ul><p>如前所说，论文的作用是向学术界同行清晰准确地描述成果的创新点、技术思路、算法细节和验证结果。由于学术界的<strong>同行评审</strong>制度，贯穿全文的线索和目标就是要论证这份工作的<strong>创新价值</strong>，每个部分都要各司其职为这个目标而服务。为了实现这个目标，需要作者特别注意以下几点：</p><p><strong>（1）学会换位思考。</strong>要始终站在审稿人或读者的角度审视论文，思考如何更清晰地表达。这是初学者最容易忽视的问题：作为研究成果的亲历者，论文作者掌握所有细节，如果不多加留意，写作中就会出现新概念没有被明确定义就被使用等情况，很多描述和分析缺少逻辑衔接。对作者而言，这些省去的东西并不影响他对这些文字的理解；但对并不了解这份工作的读者而言，这无疑是一场噩梦，因为他们并没有作者脑中的那套背景信息。因此，写作时要时时留神，读者读这句时能否理解，所需要的背景知识前文是否已经介绍。</p><p><strong>（2）注意逻辑严谨</strong>。严谨是学术论文的底色，从引用格式、公式符号到谋章造句，虽不至于美国法学期刊的Bluebook那么变态，都力求风格统一，行文严谨。引用、公式、拼写等方面都容易学，初学者更需要注意行文严谨，力求全文从章节、段落、句子等不同级别都逻辑严密，争取做到没有一句话没来由，没有一句话没呼应：</p><ul><li>章节层面，Introduciton提到已有方法面临的几个挑战，就要对应本文提出的几个创新思路，对应Method中的几个具体算法，对应Experiment中的几个实验验证。</li><li>段落和句子层面，段间要注意照应，是并列、递进、转折还是总分关系，需要谋划妥当，要有相应句子或副词衔接。段内各句，有总有分，中心思想句和围绕论述句分工协作。</li></ul><p>除了整体结构上的建议外，每个部分也各有定式，下面按各部分提供一些写作建议，同时用我们最近发表的一篇ACL 2018论文 [2] 作为例子。</p><p><img src="https://qph.fs.quoracdn.net/main-qimg-bbaa8a2bb6839bae078ad00526fd55e8.webp" alt="img"></p><hr><p><strong>Abstract和Introduction怎么写</strong></p><p>Abstract可以看做对Introduction的提要，所以我们先介绍Introduction的写法，然后再说如何写Abstract。Introduction是对整个工作的全面介绍，是决定一篇论文能否被录用的关键。一般Introduction这么写：起手介绍<strong>研究任务</strong>和意义；随后简介面向这个任务的<strong>已有方法</strong>；接着说明已有方法面临的<strong>关键挑战</strong>；针对这些挑战，本文提出什么<strong>创新思路</strong>和具体方法；最后介绍<strong>实验结果</strong>证明本文提出方法的有效性。这几个部分各挡一面，同时又有严密的内在逻辑。每个部分也各有章法，下面分别介绍对各部分的建议：</p><p><strong>（1）研究任务</strong>。介绍本文的研究任务及其在该研究领域的重要价值和意义。如果是领域公认的重要任务的话，则可以不用详细论述其研究价值/意义；如果是新提出的研究任务，则需要花费比较多篇幅论证该任务的价值。如下所示论文[2]的第1段集中说明阅读理解研究任务。</p><p><img src="https://pic3.zhimg.com/v2-fbc75968b4f84069cf96d9f93502a7ca_b.jpg" alt="img"></p><p><strong>（2）已有方法</strong>。从研究任务递进一步，介绍这个任务的已有代表方法。如下所示论文[2]的第2段，开始介绍DS-QA。需要注意，这个已有方法需要是目前最好、最具代表性的，也是本文工作准备改进的。所谓站在巨人的肩膀上，一篇值得发表的论文需要找到那个最高的巨人。</p><p><img src="https://pic2.zhimg.com/v2-e452aa66a7d8ef7d443c2530ee7f30fd_b.jpg" alt="img"></p><p><strong>（3）面临挑战</strong>。已有方法一定仍然存在某些不足或挑战，才需要进一步研究改进。因此，需要总结已有方法面临的挑战。这是Introduction的关键部分，起着承上启下的作用。初学者特别注意，这部分涉及对已有工作的评价，务必保证精准客观。要知道，当论文投稿至NLP国际会议后，是通过同行评审决定是否录用发表，评审人一般是小同行，有很大概率是已有工作的作者。所以这部分论述一定要做到客观公正，让这些工作作者本人也能信服。</p><p>如下所示论文[2]的第3、4段，先介绍DS-QA的noisy labeling挑战，并且通过举例直观呈现。面对这个挑战，已有一些相关工作，还需说明他们各自有什么不足和挑战，为引出本文创新思路做好铺垫。</p><p><img src="https://pic3.zhimg.com/v2-18c8049ebac37a3fc1cefce247598c5a_b.jpg" alt="img"></p><p><strong>（4）创新思路</strong>。水来土掩，兵来将挡，既然已有方法有这些不足和挑战，就需要有新的创新思路和方法。这部分需要注意与上面的”挑战“部分严丝合缝，密切呼应，让读者清楚领会到这些创新思路与方法的确能够解决或缓解这些挑战问题。</p><p>如下所示论文[2]的第5段，就是介绍创新思路和方法。可以看到，一般”面临挑战“和”创新思路“部分还配图示，更直观地展示本文要解决的挑战问题和创新思路。例如论文[2]这张丑丑的图，比较直观地展示了创新方法包括Selector和Reader两个模块和作用。也可以随便看我们的其他论文[3]，大部分论文都会在Introduction中提供图示。</p><p><img src="https://pic4.zhimg.com/v2-2123386d4237163c56104af1ae38090f_b.jpg" alt="img"></p><p><strong>（5）实验结论</strong>。除了在”创新思路“部分图文两开花地说明本文创新工作外，还要通过合理的实验验证方法的有效性。一般要得到”our method achieves significant and consistent improvement as compared to other baselines“的结论，从而验证本文工作的创新性。</p><p><img src="https://pic3.zhimg.com/v2-f378a4cd00c8d1ee2b2024b27d85cb92_b.jpg" alt="img"></p><p>有些论文最后还会体贴的总结本文的主要贡献，一般说”In summary, the key contributions are x-fold: (1)…(2)…(3)…“。这样做的好处是，可以帮助审稿人总结本文的创新点放在审稿意见中，节省不少工作量。但需要注意，这些创新点要简洁明了，不能是前文的简单重复，也不能overclaim。如果要说”首次“提出或发现，一般也要前置”to the best of our knowledge“。此外还有论文最后一段会介绍接下来几个Section结构，个人感觉对一篇8页论文可能并不需要。</p><p>对于Abstract，可以看做对Introduction的简介，最简单的做法是，以上每部分都精简为1-2句话组成Abstract皆可。如下是论文[2]的Abstract内容，可以看出与Introduction的对应关系。</p><p><img src="https://pic2.zhimg.com/v2-bddf467858b05eed8d4c447bdc101831_b.jpg" alt="img"></p><hr><p><strong>Method怎么写</strong></p><p>这部分要详细介绍本文创新方法的具体细节，由于涉及非常艰涩的细节，要采用”总-分“结构来介绍。</p><p>这部分起手”总“的部分要介绍本文任务的符号定义，以及本文方法的框架组成，或者按步骤来介绍或者按模块来写，让读者对本文方法有全景式的理解。如下所示论文[2]的Methodology”总“的部分，就先介绍一些符号，然后分别介绍了Selector和Reader两个模块的主要功能。</p><p><img src="https://pic1.zhimg.com/v2-60fe75241b9ccf1217382a58f7abd19c_b.jpg" alt="img"></p><p>然后进入”分“的部分，则需对应”总“中的框架，分别介绍各关键模块/步骤。例如，论文[2]的Methodology”分“的部分，就包括3.1 Paragraph Selector、3.2 Paragraph Reader、3.3 Learning and Prediction。读者在”总“的部分已经对方法有全景式的了解，有的放矢，就比较容易理解每个模块的具体细节。而每个”分“的部分中，又可以进一步采用”总-分“结构进行介绍，例如3.1小节做完总体介绍后，又会按照Paragraph Encoding和Question Encoding分别介绍。为了更清晰地体现”总-分“结构，可以将各“分”的部分命名并加粗。</p><p><img src="https://pic2.zhimg.com/v2-a977707522216ffaf6f40905fce1c33d_b.jpg" alt="img"></p><p>初学者特别注意，（1）Introduction中对创新思路与方法的介绍，不要在Method中简单重复，否则会让认真通读全文的审稿人颇感厌烦。要做到前后照应，有所递进，前略后详，不妨使用“as mentioned in Section 1”来做关联。（2）Method部分往往包含大量公式，需要保证公式风格和符号使用前后统一，新符号使用均需显式解释。</p><hr><p><strong>Experiment怎么写</strong></p><p>这部分要详细介绍与实验相关的具体细节。一般先介绍实验数据、评测标准和比较方法等基本信息。以论文[2]为例，实验部分首先介绍实验数据与评测标准（4.1 Datasets and Evaluation Metrics）、实验比较的已有代表方法（4.2 Baselines）、实验方法的参数设置（4.3 Experimental Settings）等基本信息。</p><p>在介绍完实验基本信息后，主要开展两种实验：</p><p><strong>（1）主实验</strong>。目的是证明本文方法与已有方法相比的有效性。一般需要选取业界公认的数据集合或已有工作采用的实验验证方式，提升实验的可信性。对于学术论文而言，并不需要比该任务上最好的方法相比，只要证明采用本文创新方法与不采用本文方法相比更有效即可，也就是说，实验中尽量控制其他变量，只聚焦于本文关注的挑战问题即可。当然，如果能够因为本文创新思路，得到该任务上的最好效果，会更有吸引力，但不必总是强求。</p><p>一般实验结果用图表展示，然后在正文进行观察分析。例如，论文[2]的主实验部分先介绍不同Selector和Reader对实验效果的影响（4.4 Effect of Different Paragraph Selectors、4.5 Effect of Different Paragraph Readers），接着介绍主实验结果和观察分析（4.6 Overall Results）。其中表格中会把最好效果加粗显示，一般应大部分位于本文提出的方法；为了更加清晰明了，观察分析结论可用（1）（2）（3）列出，其中第1条一般要得出主要结论，即本文方法要显著优于已有方法。</p><p><img src="https://pic3.zhimg.com/v2-015ca9ae9ca2d0717c0398dc26abd6b6_b.jpg" alt="img"></p><p>主实验结果</p><p><img src="https://pic4.zhimg.com/v2-6766e2505e9c73ed3b2627080199fc2b_b.jpg" alt="img"></p><p>主实验分析</p><p><strong>（2）辅助实验</strong>。目的是展示本文创新方法的优势和特点。例如，不同超参数对本文方法的影响（Hyper-Parameter Effect），不同模块对本文方法效果的贡献（Ablation Test），不同数据划分对本文方法的影响（如Few-shot Learning相关工作比较常见），本文方法的主要错误类型（Error Analysis），本文方法能够改进效果的典型样例（Case Study）等。这些实验需要根据论文创新工作特点而有针对性的设计，一切要为体现本文的创新价值而服务。</p><p>例如，论文[2]的辅助实验包括4.7 Paragraph Selector Performance Analysis、4.8 Performance with different numbers of paragraphs、4.9 Potential improvement、4.10 Case study等，从各方面呈现本文提出方法的特点。</p><p>Experiment部分的特点是要图文并茂，注重通过多个表格和图示来呈现本文方法的优势和特点，需要注意图表风格统一。初学者特别注意，要做到仅凭图表下方的说明文字就可以理解每张图表内容，不要让读者还要到跑到正文寻找相关说明。因为，很多有经验的审稿人在看完Introduction后，会直接跳到Experiment图表中寻找对比效果。</p><hr><p><strong>Related Work怎么写</strong></p><p>这部分主要是介绍本文任务和方法的相关工作，目标是通过对已有工作的梳理，凸显本文工作的创新价值。对已有工作的梳理，不应是对每个工作的简单介绍，而应当注意汇总、分类、分析，或者按照时间发展顺序，或者按照技术路线划分，例如论文[2]就是按照时间脉络介绍。</p><p>在对相关工作的介绍中，要注意暗合本文创新思路要解决的挑战，不应是单纯的介绍，而是夹叙夹议，时刻注意与本文工作的照应。在Related Work的最后，应该落脚到本文工作与已有工作相比，有什么新的思路，解决了什么挑战问题。</p><p>初学者特别注意，Introduction和Related Work部分是特别需要导师或其他有经验学者帮助把关的。一是，不能遗漏重要相关工作，这点需要论文作者对相关领域工作保持跟踪；二是，与Introduction要求类似，对已有工作的评述务必精准客观。</p><p>Related Work一般放在Introduction之后，或者Conclusion之前，这一般取决于论文工作的特点。对于那些与已有工作联系紧密、创新精微的工作，一般建议放在Introduction之后，方便读者全面了解本文工作与已有工作的关系，然后开始在Method介绍本文方法。而对于有些框架性创新工作，如果主要是对已有方法的组合，一般建议Related Work放在Method、Experiment之后即可。这点并无成法，完全根据行文方便来定。</p><p><img src="https://pic1.zhimg.com/v2-2b2283b5f5e1f44ad6f14a4abf1eb920_b.jpg" alt="img"></p><hr><p><strong>Conclusion怎么写</strong></p><p>在论文最后会有总结展望，一般用一段来再次总结和强调本文的创新思路和实验结果，然后说明未来建议的研究方向和开放问题。这部分相对来讲比较固定。稍微留意的是，在准备论文最后阶段，如果发现论文有哪些应当做还没来得及做的，可以写作本文的未来工作。至少可以向审稿人表明你也想到这个问题了，赢得一点同情分。</p><p><img src="https://pic2.zhimg.com/v2-94326afefce671e98b92087a792ed3ad_b.jpg" alt="img"></p><hr><p><strong>其他建议</strong></p><p>要想写出一篇合格的NLP论文，首先是<strong>态度问题</strong>，只有态度重视，才有可能不厌其烦地反复修改，才会“不择手段”地寻找各种办法来尽力改进论文（找学长找外教借助Grammarly工具等）。其次是<strong>动手问题</strong>，只有写下来，才可能不断改，只要改就能不断进步。最后是<strong>经验问题</strong>，要写得精彩可能需要天赋，而要写得合格，只要坚持写，不断根据评阅人和其他人的意见进行思考和修改，就可以进步。总之，坚持就是胜利。</p><p>实际上，我觉得论文写作，是对思维模式的训练。也许未来你并不会从事学术研究，但通过论文写作锻炼的凝练工作创新价值的能力、清晰传递复杂信息的表达能力，对未来工作中无论是工作沟通、成果展示等，都有重要帮助。所以还希望大家都能重视这个科研道路上难得的锻炼机会。加油！</p>]]></content>
    
    
    <categories>
      
      <category>1.科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文写法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1.科研/3 - 选题</title>
    <link href="undefined2020/02/24/1.%E7%A7%91%E7%A0%94/3%20-%20%E9%80%89%E9%A2%98/"/>
    <url>2020/02/24/1.%E7%A7%91%E7%A0%94/3%20-%20%E9%80%89%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h1 id="论文选题"><a href="#论文选题" class="headerlink" title="论文选题"></a>论文选题</h1><ol><li>经过调研，已对本领域有基本认识，具备了得到idea的条件。</li><li>idea要新颖，要能推动科学的发展，同时要有可复现性和可实现性。</li><li>把握好与现存结果之间的delta</li><li>要因时而动，像语音识别和人脸识别这种已经落地的项目，可能已经没有什么突破的空间了，现在在业界拼的是数据和算力。而常识，知识推理，复杂语境，跨模态理解，可解释智能。这些点目测不能通过数据驱动的方式解决。更高层次的联想、创造、顿悟等认知能力，更是连边还没碰到。这些问题是有远见的研究者应该关注的方向。</li></ol><p>补全了相关知识，阅读了大量的文献，走访了各位前辈，观察了各圈风向标，调整首文合理预期后，</p><p><strong>我的idea是：</strong>_<strong><strong><strong><strong><strong><strong><strong>____</strong></strong></strong></strong></strong></strong></strong>.</p><p><strong>建议2：如何选择第一个好题目？</strong></p><h3 id="什么算是好的idea"><a href="#什么算是好的idea" class="headerlink" title="什么算是好的idea"></a>什么算是好的idea</h3><p>作者：刘知远 </p><p>2015年，我在微博上写过一个调侃的小段子：</p><blockquote><p>ML派坐落美利坚合众山中，百年来武学奇才辈出，隐然成江湖第一大名门正派，门内有三套入门武功，曰：图模型加圈，神经网加层，优化目标加正则。有童谣为证：熟练ML入门功，不会作文也会诌。</p></blockquote><p>到了2018年，我又续了一小段：</p><blockquote><p>不期数年，北方DL神教异军突起，内修表示学习，外练神经网络，心法众多，曰门，曰注意，曰记忆，曰对抗，曰增强。经ImageNet一役威震武林，豢Alpha犬一匹无人可近。一时家家筑丹炉，人人炼丹忙，门徒云集，依附者众，有一统江湖之势。有童谣为证：左手大数据，右手英伟达，每逢顶会炼丹忙。</p></blockquote><p>这里面提到的图模型加圈、神经网络加层、优化目标加正则，神经网络中的门、注意、记忆等，都是一些改进模型性能的创新思路，被各大NLP任务广泛使用并发表论文，也许就是因为被不同NLP任务的重复使用和发表，多少有些审美疲劳而缺少更深的创新思想，被有些网友和学者诟为“灌水”，似乎都不算好的想法。</p><p>那么什么才是好的想法呢？我理解这个”好“字，至少有两个层面的意义。</p><h3 id="学科发展角度的”好“"><a href="#学科发展角度的”好“" class="headerlink" title="学科发展角度的”好“"></a>学科发展角度的”好“</h3><p>学术研究本质是对未知领域的探索，是对开放问题的答案的追寻。所以从推动学科发展的角度，评判什么是好的研究想法的标准，首先就在一个“<strong>新</strong>”字。</p><p>过去有个说法，人工智能学科有个魔咒，凡是人工智能被解决（或者有解决方案）的部分，就不再被认为代表“人类智能”。计算机视觉、自然语言处理、机器学习、机器人之所以还被列为人工智能主要方向，也许正是因为它们尚未被解决，尚能代表“人类智能”的尊严。而我们要开展创新研究，就是要提出新的想法解决这些问题。这其中的”新“字，可以体现在提出新的问题和任务，探索新的解决思路，提出新的算法技术，实现新的工具系统等。</p><p>在保证”新“的基础上，研究想法好不好，那就看它<strong>对推动学科发展的助力有多大</strong>。深度学习之所以拥有如此显赫的影响力，就在于它对于人工智能自然语言处理、语音识别、计算机视觉等各重要方向都产生了革命性的影响，彻底改变了对无结构信号（语音、图像、文本）的语义表示的技术路线。</p><h3 id="研究实践角度的”好“"><a href="#研究实践角度的”好“" class="headerlink" title="研究实践角度的”好“"></a>研究实践角度的”好“</h3><p>那是不是想法只要够”新“就好呢？是不是越新越好呢？我认为应该还不是。因为，只有<strong>能做得出来的想法</strong>才有资格被分析好不好。所以，从研究实践角度，还需要考虑研究想法的<strong>可实现性</strong>和<strong>可验证性。</strong></p><p>可实现性，体现在该想法是否有足够的数学或机器学习工具支持实现。可验证性，体现在该想法是否有合适的数据集合和广泛接受的评价标准。很多民间科学家的想法之所以得不到学术界的认同，就是因为这些想法往往缺乏可实现性和可验证性，只停留在天马行空的纸面，只是些虚无缥缈的理念。</p><h2 id="好的研究想法从哪里来"><a href="#好的研究想法从哪里来" class="headerlink" title="好的研究想法从哪里来"></a>好的研究想法从哪里来</h2><p>想法好还是不好，并不是非黑即白的二分问题，而是像光谱一样呈连续分布，因时而异，因人而宜。计算机科技领域的发展既有积累的过程，也有跃迁的奇点，积累量变才会产生质变，吃第三个馒头饱了，也是因为前面两个馒头打底。</p><p>现在的学术研究已经成为高度专业化的职业，有庞大的研究者群体。”Publish or Perish“，是从事学术职业（如教授、研究员、研究生）的人必须做好平衡的事情，不能要求研究者的每份工作都是“诺贝尔奖”或“图灵奖”级的才值得发表。只要对研究领域的发展有所助力，就值得发表出来，帮助同行前进。鲁迅说：天才并不是自生自长在深林荒野里的怪物，是由可以使天才生长的民众产生，长育出来的，所以没有这种民众，就没有天才。这个庞大研究者群体正是天才成长的群众基础。同时，学术新人也是在开展创新研究训练中，不断磨砺寻找好想法能力，鲁迅也说：即使天才，在生下来的时候的第一声啼哭，也和平常的儿童的一样，决不会就是一首好诗。</p><p>那么，好的研究想法从哪里来呢？我总结，首先要有区分研究想法好与不好的能力，这需要<strong>深入全面了解所在研究方向的历史与现状</strong>，具体就是对学科文献的全面掌握。人是最善于学习的动物，完全可以将既有文献中不同时期研究工作的想法作为学习对象，通过了解它们提出后对学科发展的影响——具体体现在论文引用、学术评价情况等各方面——建立对研究想法好与不好的评价模型。我们很难条分缕析完美地列出区分好与不好想法的所有特征向量，但人脑强大的学习能力，只要给予足够的输入数据，就可以在神经网络中自动学习建立判别的模型，鉴古知今，见微知著，这也许就是常说的学术洞察力。</p><p>做过一些研究的同学会有感受，仅阅读自己研究方向的文献，新想法还是不会特别多。这是因为，读到的都是该研究问题已经完成时的想法，它们本身无法启发新的想法。如何产生新的想法呢？我总结有三种可行的基本途径：</p><p><strong>实践法</strong>。即在研究任务上实现已有最好的算法，通过分析实验结果，例如发现这些算法计算复杂度特别高、训练收敛特别慢，或者发现该算法的错误样例呈现明显的规律，都可以启发你改进已有算法的思路。现在很多自然语言处理任务的Leaderboard上的最新算法，就是通过分析错误样例来有针对性改进算法的 [1]。</p><p><strong>类比法</strong>。即将研究问题与其他任务建立类比联系，调研其他相似任务上最新的有效思想、算法或工具，通过合理的转换迁移，运用到当前的研究问题上来。例如，当初注意力机制在神经网络机器翻译中大获成功，当时主要是在词级别建立注意力，后来我们课题组的林衍凯和沈世奇提出建立句子级别的注意力解决关系抽取的远程监督训练数据的标注噪音问题 [2]，这就是一种类比的做法。</p><p><strong>组合法</strong>。即将新的研究问题分解为若干已被较好解决的子问题，通过有机地组合这些子问题上的最好做法，建立对新的研究问题的解决方案。例如，我们提出的融合知识图谱的预训练语言模型，就是将BERT和TransE等已有算法融合起来建立的新模型 [3]。</p><p>正如武侠中的最高境界是无招胜有招，好的研究想法并不拘泥于以上的路径，很多时候是在研究者对研究问题深刻认知的基础上，综合丰富的研究阅历和聪明才智产生”顿悟“的结果。这对初学者而言恐怕还很难一窥门径，需要从基本功做起，经过大量科研实践训练后，才能有登堂入室之感。</p><p>在科研实践过程中，除了通过大量文献阅读了解历史，通过深入思考总结产生洞察力外，还有一项必不可少的工作，那就是主动开放的学术交流和合作意识。不同研究领域思想和成果交流碰撞，既为创新思想提供了新的来源，也为”类比“和”顿悟“提供了机会。了解一下历史就可以知晓，人工智能的提出，就是数学、计算机科学、控制论、信息论、脑科学等学科交叉融合的产物。而当红的深度学习的起源，1980年代的Parallel Distributed Processing （PDP），也是计算机科学、脑认知科学、心理学、生物学等领域研究者通力合作的产物。下面是1986年出版的名著《Parallel Distributed Processing: Explorations in the Microstructure of Cognition》第一卷的封面。</p><p><img src="https://pic2.zhimg.com/80/v2-a8d3f6e553f9f279cdafea5a3e218701_hd.jpg" alt="img"></p><p>作者在前言中是这么讲他们的合作过程的，在最初长达六个月的时间里，它们每周见面交流两次讨论研究进展。</p><blockquote><p>We expected the project to take about <strong>six months</strong>. We began in January 1982 by bringing a number of our colleagues together to form a discussion group on these topics. <strong>During the first six months we met twice weekly</strong> and laid the foundation for most of the work presented in these volumes.</p></blockquote><p>而书中提供的PDP研究组的成员名单，40年后的今天仍让我惊叹其高度的跨机构、跨学科的交叉特点。所以，特别建议同学们在科研训练中，在专注研究问题的前提下，保持主动的学术交流意识，无论是听讲座报告，参加学术会议，还是选修课程，都有意识地扩宽学术交流的广度，不仅与小同行打成一片，更有看似八竿子打不着的研究领域的学术伙伴。随着研究经历的丰富，会越来越强烈地感受到，越是大跨度交叉的学术报告，越让你受到更大的启发，产生更多让自己兴奋的研究想法。</p><p><img src="https://pic4.zhimg.com/80/v2-404a752001300a69baabd40fb3d78b99_hd.jpg" alt="img"></p><h2 id="初学者应该怎么做"><a href="#初学者应该怎么做" class="headerlink" title="初学者应该怎么做"></a>初学者应该怎么做</h2><p>与阅读论文、撰写论文、设计实验等环节相比，如何产生好的研究想法，是一个不太有章可循的环节，很难总结出固定的范式可供遵循。像小马过河，需要通过大量训练实践，来积累自己的研究经验。不过，对于初学者而言，仍然有几个简单可行的原则可以参考。</p><p><strong>一篇论文的可发表价值，取决于它与已有最直接相关工作间的Delta</strong>。我们大部分研究工作都是站在前人工作的基础上推进的。牛顿说：如果说我看得比别人更远些，那是因为我站在巨人的肩膀上。在我看来，评判一篇论文研究想法的价值，就是看它站在了哪个或哪些巨人的肩膀上，以及在此基础上又向上走了多远。反过来，在准备开始一项研究工作之前，在形成研究想法的时候，也许要首先明确准备站在哪个巨人的肩膀上，以及计划通过什么方式走得更远。与已有最直接相关工作之间的Delta，决定了这个研究想法的价值有多大。</p><p><strong>兼顾摘果子和啃骨头</strong>。人们一般把比较容易想到的研究想法，叫做Low Hanging Fruit（低垂果实）。低垂果实容易摘，但同时摘的人也多，选择摘果子就容易受到想法撞车的困扰。例如，2018年以BERT为首的预训练语言模型取得重大突破，2019年中就出现大量改进工作，其中以跨模态预训练模型为例，短短几个月里<a href="http://link.zhihu.com/?target=http%3A//arxiv.org">http://arxiv.org</a>上挂出了超过六个来自不同团队的图像与文本融合的预训练模型 [4]。设身处地去想，进行跨模态预训练模型研究，就是一个比较容易想到的方向，你一定需要有预判能力，知道世界上肯定会有很多团队也同时开展这方面研究，这时你如果选择入场，就一定要做得更深入更有特色，有自己独特的贡献才行。相对而言，那些困难的问题，愿意碰的人就少，潜下心来啃硬骨头，也是不错的选择，当然同时就会面临做不出来的风险，或者做出来也得不到太多关注的风险。同学需要根据自身特点、经验和需求，兼顾摘果子和啃骨头两种类型的研究想法。</p><p><img src="https://pic2.zhimg.com/80/v2-d71aaf2b86116e3ea1e891bf9230a2c4_hd.jpg" alt="img"></p><p><strong>注意多项研究工作的主题连贯性</strong>。同学的研究训练往往持续数年，需要注意前后多项研究工作的主题连贯性，保证内在逻辑统一。需要考虑，在个人简历上，在出国申请Personal Statement中，或者在各类评奖展示中，能够将这些研究成果汇总在一起，讲出自己开展这些研究工作的总目标、总设想。客观上讲，人工智能领域研究节奏很快，技术更新换代快，所以成果发表也倾向于小型化、短平快。我有商学院、社科的朋友，他们一项研究工作往往需要持续一年甚至数年以上；高性能计算、计算机网络方向的研究周期也相对较长。人工智能这种小步快跑的特点，决定了很多同学即使本科毕业时，也会有多篇论文发表，更不用说硕士生、博士生。在这种情况下，就格外需要在研究选题时，注意前后工作的连贯性和照应关系。几项研究工作放在一起，到底是互相割裂说不上话，还是在为一个统一的大目标而努力，格外反映研究的大局意识和布局能力。例如，下图是我们课题组涂存超博士2018年毕业时博士论文《面向社会计算的网络表示学习》的章节设置，整体来看就比《社会计算的若干重要问题研究》等没有内在关联的写法要更让人信服一些。当然，对于初学者而言，一开始就想清楚五年的研究计划，根本不可能。但想，还是不去想，结果还是不同的。</p><p><img src="https://pic4.zhimg.com/80/v2-9fbee2d16f9c05fa1cb1ec86a27d265c_hd.jpg" alt="img"></p><p><strong>注意总结和把握研究动态和趋势，因时而动</strong>。2019年在知乎上有这样一个问题：”2019年在NLP领域，资源有限的个人/团队能做哪些有价值有希望的工作？“ 我当时的回答如下：</p><blockquote><p>我感觉，产业界开始集团化搞的问题，说明其中主要的开放性难题已经被解决得差不多了，如语言识别、人脸识别等，在过去20年里面都陆续被广泛商业应用。看最近的BERT、GPT-2，我理解更多的是将深度学习对大规模数据拟合的能力发挥到极致，在深度学习技术路线基本成熟的前提下，大公司有强大计算能力支持，自然可以数据用得更多，模型做得更大，效果拟合更好。<br>成熟高新技术进入商用竞争，就大致会符合摩尔定律的发展规律。现在BERT等训练看似遥不可及，但随着计算能力等因素的发展普及，说不定再过几年，人人都能轻易训练BERT和GPT-2，大家又会在同一个起跑线上，把目光转移到下一个挑战性难题上。<br>所以不如提前考虑，哪些问题是纯数据驱动技术无法解决的。NLP和AI中的困难任务，如常识和知识推理，复杂语境和跨模态理解，可解释智能，都还没有可行的解决方案，我个人也不看好数据驱动方法能够彻底解决。更高层次的联想、创造、顿悟等认知能力，更是连边还没碰到。这些正是有远见的研究者们应该开始关注的方向。</p></blockquote><p>需要看到，不同时期的研究动态和趋势不同。把握这些动态和趋势，就能够做出研究社区感兴趣的成果。不然的话，即使研究成果没有变化，只是简单早几年或晚几年投稿，结果也会大不相同。例如，2013年word2vec发表，在2014-2016年之间开展词表示学习研究，就相对比较容易得到ACL、EMNLP等会议的录用；但到了2017-2018年，ACL等会议上的词表示学习的相关工作就比较少见了。</p>]]></content>
    
    
    <categories>
      
      <category>1.科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文写法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1.科研/1 - B学科调研结论</title>
    <link href="undefined2020/02/24/1.%E7%A7%91%E7%A0%94/1%20-%20B%E5%AD%A6%E7%A7%91%E8%B0%83%E7%A0%94%E7%BB%93%E8%AE%BA/"/>
    <url>2020/02/24/1.%E7%A7%91%E7%A0%94/1%20-%20B%E5%AD%A6%E7%A7%91%E8%B0%83%E7%A0%94%E7%BB%93%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="1-学科调研结论"><a href="#1-学科调研结论" class="headerlink" title="1.学科调研结论"></a>1.学科调研结论</h1><h3 id="简要结论"><a href="#简要结论" class="headerlink" title="简要结论"></a><strong>简要结论</strong></h3><p>经过对上文阅读列表+论文库的调研，得出如下结论，可以开始选题。</p><ul><li>近年来,我感兴趣的关于_<strong>____</strong>的研究方向,全球呈现_<strong><strong>_趋势,其中较多的论文来自\</strong></strong>国家\地区,发表相关论文的研究机构有_<strong>__</strong>.蓝海领域比较新，容易出成果，我选择的这个方向是否是蓝海领域_<strong>____</strong>.</li><li>为了充分了解这个领域目前的发展状况，需要如下几个方面的调研：方法方面，是否有一套比较清晰的数学体系和机器学习体系_<strong><strong>；数据方面，有没有一个大家公认的标准训练集和测试集__</strong></strong>；研究团队，是否有著名团队和人士参加_<strong>__</strong>。如果以上几个方面的调研结论不是太清晰，作为初学者可能不要轻易进入。</li><li>全球的研究人员主要从_<strong>___</strong>等领域对课题进行研究,同时我们也注意到_<strong>__</strong>等领域的研究可能会给我们带来不一样的视角和灵感</li><li>相关课题的研究成果目前主要发表在_<strong>___</strong>等期刊上,在相关研究领域中,_____等几位学者有较多的论文产出.</li><li>影响力较高的几篇论文分别来自于_<strong>__</strong>(国家/地区)的_<strong><em>(机构)的___</em></strong>学者</li><li>近半年来_<strong><strong>__</strong></strong>方向引起了较多科研人员的关注</li><li>选择_<strong>___</strong>综述文章作为快速了解这个课题的切入点</li><li>最新的研究进展指出,该研究方向_<strong>_____</strong></li></ul><h3 id="BackGround："><a href="#BackGround：" class="headerlink" title="BackGround："></a><strong>BackGround：</strong></h3><p>自然语言处理（简称NLP），是研究计算机处理人类语言的一门技术，包括：</p><p>1.<strong>句法语义分析</strong>：对于给定的句子，进行分词、词性标记、命名实体识别和链接、句法分析、语义角色识别和多义词消歧。</p><p>2.<strong>信息抽取</strong>：从给定文本中抽取重要的信息，比如，时间、地点、人物、事件、原因、结果、数字、日期、货币、专有名词等等。通俗说来，就是要了解谁在什么时候、什么原因、对谁、做了什么事、有什么结果。涉及到实体识别、时间抽取、因果关系抽取等关键技术。</p><p>3.<strong>文本挖掘</strong>（或者文本数据挖掘）：包括文本聚类、分类、信息抽取、摘要、情感分析以及对挖掘的信息和知识的可视化、交互式的表达界面。目前主流的技术都是基于统计机器学习的。</p><p>4.<strong>机器翻译</strong>：把输入的源语言文本通过自动翻译获得另外一种语言的文本。根据输入媒介不同，可以细分为文本翻译、语音翻译、手语翻译、图形翻译等。机器翻译从最早的基于规则的方法到二十年前的基于统计的方法，再到今天的基于神经网络（编码-解码）的方法，逐渐形成了一套比较严谨的方法体系。</p><p>5.<strong>信息检索</strong>：对大规模的文档进行索引。可简单对文档中的词汇，赋之以不同的权重来建立索引，也可利用1，2，3的技术来建立更加深层的索引 。在查询的时候，对输入的查询表达式比如一个检索词或者一个句子进行分析，然后在索引里面查找匹配的候选文档，再根据一个排序机制把候选文档排序，最后输出排序得分最高的文档。</p><p>6.<strong>问答系统</strong>： 对一个自然语言表达的问题，由问答系统给出一个精准的答案。需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别，形成逻辑表达式，然后到知识库中查找可能的候选答案并通过一个排序机制找出最佳的答案。</p><p>7.<strong>对话系统</strong>：系统通过一系列的对话，跟用户进行聊天、回答、完成某一项任务。涉及到用户意图理解、通用聊天引擎、问答引擎、对话管理等技术。此外，为了体现上下文相关，要具备多轮对话能力。同时，为了体现个性化，要开发用户画像以及基于用户画像的个性化回复。</p>]]></content>
    
    
    <categories>
      
      <category>1.科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文写法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1.科研/1 - A学科调研资料库</title>
    <link href="undefined2020/02/24/1.%E7%A7%91%E7%A0%94/1%20-%20A%E5%AD%A6%E7%A7%91%E8%B0%83%E7%A0%94%E8%B5%84%E6%96%99%E5%BA%93/"/>
    <url>2020/02/24/1.%E7%A7%91%E7%A0%94/1%20-%20A%E5%AD%A6%E7%A7%91%E8%B0%83%E7%A0%94%E8%B5%84%E6%96%99%E5%BA%93/</url>
    
    <content type="html"><![CDATA[<h1 id="1-阅读列表-文献库"><a href="#1-阅读列表-文献库" class="headerlink" title="1.阅读列表+文献库"></a>1.阅读列表+文献库</h1><p>想当GrandMaster，<strong>以下提到的所有材料都绕不过去</strong>。在还没入行时间紧迫的情况下，须有所取舍。具体取舍规则办法见2 - 计划.md</p><h2 id="阅读列表"><a href="#阅读列表" class="headerlink" title="阅读列表"></a>阅读列表</h2><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>CS224N - 自然语言处理</td><td></td></tr><tr><td>CS229 - 机器学习（核心课）</td><td></td></tr><tr><td>CS229A - 机器学习应用课，数学少，应用多</td><td></td></tr><tr><td>CS231N - 计算机视觉</td><td></td></tr><tr><td>CS230 - 专注深度学习，只包含一点点机器学习（which最难的那一部分）</td><td></td></tr><tr><td>CS221 - AI导论</td><td></td></tr><tr><td>CS228 - 概率图模型</td><td></td></tr><tr><td><a href="https://www.zhihu.com/org/ji-qi-zhi-xin-65/activities" target="_blank" rel="noopener">机器之心</a></td><td></td></tr><tr><td><a href="https://zhuanlan.zhihu.com/paperweekly" target="_blank" rel="noopener">PaperWeekly</a></td><td></td></tr><tr><td><a href="https://mp.weixin.qq.com/s/B2ocJ1Y2evLwVkG5FrUz5A" target="_blank" rel="noopener">NeurIPS 2019公布获奖论文</a></td><td></td></tr><tr><td><a href="https://mp.weixin.qq.com/s/SxOKHRbTJdzlBNfHos-TOA" target="_blank" rel="noopener">深度学习所需数学知识</a></td><td></td></tr><tr><td><a href="https://mp.weixin.qq.com/s/RlgX2GErySe5rAUAvveLdQ" target="_blank" rel="noopener">EE转CS成功案例</a></td><td></td></tr><tr><td><a href="https://mp.weixin.qq.com/s/F4zQHesEGWLcBLGwYrmg3w" target="_blank" rel="noopener">Jeff Dean谈2020机器学习趋势</a></td><td></td></tr><tr><td><a href="https://mp.weixin.qq.com/s/P0MXhMBjt41WflAMGtFr-g" target="_blank" rel="noopener">从Word2Vec到BERT </a>   done</td><td></td></tr><tr><td><a href="https://mp.weixin.qq.com/s/uaBRgRp0Yue4MtC2B-8VJA" target="_blank" rel="noopener">kaggle竞赛宝典</a></td><td></td></tr><tr><td><a href="https://mp.weixin.qq.com/s/bSsmaOzQYzJ_7RjnzS804g" target="_blank" rel="noopener">NIPS2019</a></td><td></td></tr><tr><td><a href="https://mp.weixin.qq.com/s/-rrj3jusoFAqt6gjuP9y6w" target="_blank" rel="noopener">斯坦福2019全球AI报告</a></td><td></td></tr><tr><td><a href="https://mp.weixin.qq.com/s/XMcX1FQ3yDIAgvwsfEJ3yQ" target="_blank" rel="noopener">2020学术会议list</a></td><td></td></tr><tr><td><a href="https://mp.weixin.qq.com/s/E04x_tqWPaQ4CSWfGVbnTw" target="_blank" rel="noopener">因果推演</a></td><td></td></tr><tr><td><a href="https://zhuanlan.zhihu.com/p/77357304" target="_blank" rel="noopener">ACL2019知识图谱总结</a></td><td></td></tr><tr><td>概率论与数理统计</td><td></td></tr><tr><td>线性代数</td><td></td></tr><tr><td>高等数学</td><td></td></tr><tr><td><strong>优化理论</strong></td><td></td></tr><tr><td><strong>Bubeck：《Convex Optimization: Algorithms and Complexity》</strong></td><td></td></tr><tr><td>Bottou、Curtis和Nocedal：《Optimization Methods for Large-Scale Machine Learning》</td><td></td></tr><tr><td>知识图谱</td><td></td></tr><tr><td>Ian Goodfellow&amp;Yoshua Bengio&amp;Aaron Courville：DeepLearning</td><td></td></tr><tr><td><strong>nlp基石之书：speech+and+language+processing</strong></td><td></td></tr><tr><td>Manning：Introduction to Information Retrieval</td><td></td></tr><tr><td>Neural Network Methods for Natural Language Processing</td><td></td></tr><tr><td><strong>PRML：模式识别与机器学习</strong></td><td></td></tr><tr><td>周志华：《机器学习》</td><td></td></tr><tr><td><strong>李航：《统计学习方法》</strong></td><td></td></tr><tr><td>人工智能：一种现代的方法</td><td></td></tr><tr><td>数学之美</td><td></td></tr><tr><td>网络、群体与市场（中文版）</td><td></td></tr><tr><td>EMNLP2018_如何写NLP代码</td><td></td></tr><tr><td>刘洋_写论文的技术细节</td><td></td></tr><tr><td></td><td></td></tr><tr><td>理想国</td><td></td></tr><tr><td>苏格拉底自辩篇</td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table><h2 id="对阅读列表的阐述-刘洋"><a href="#对阅读列表的阐述-刘洋" class="headerlink" title="对阅读列表的阐述 - 刘洋"></a>对阅读列表的阐述 - 刘洋</h2><p><strong>1、了解NLP的最基本知识</strong>：Jurafsky和Martin的Speech and Language Processing是领域内的经典教材，里面包含了NLP的基础知识、语言学扫盲知识、基本任务以及解决思路。阅读此书会接触到很多NLP的最基本任务和知识，比如tagging, 各种parsing，coreference, semantic role labeling等等等等。这对于全局地了解NLP领域有着极其重要的意义。书里面的知识并不需要烂熟于心，但是刷上一两遍，起码对于NLP任务有基本认识，下次遇到了知道去哪里找还是非常有意义的。另外 Chris Manning 的 introduction to information retrieval 也是一本可以扫一下盲的书，当然我认为依然不需要记住所有细节，但轮廓需要了解。IR里面的很多基本算法跟NLP有不少的重合。说说我自己曾经走过的弯路。Stanford NLP的qualification考试的一部分就是选一些jurafsky 和 manning书里面的一些chapter来读，然后老师来问相关问题。开始我一直对里面的东西懒得看，所以qualification考试一拖再拖。但博士最后一年没办法拖的时候，才发现如果早知道这些东西，博士早年可以少走很多弯路。</p><p>为什么了解NLP基础知识的重要，我给大家举几个例子。<br>最近跟同学一起做语言模型 language modeling相关的事情，很多同学用LSTM或者transformers做language model随手就能实现，但是实现一个 bigram 或者 trigram的language model（LM）却因为里面的OOV的平滑问题卡了大半天（熟悉的同学可能知道，需要拉普拉斯平滑或者更sophisticated的Kneser-Ney平滑）。为什么bigram 或者 trigram的LM很重要呢？去做一个语言模型的问题，实现深度模型之前，第一步其实就要去写一个 bigram 或者 trigram的LM。为什么呢？ 因为这些N-gram模型实现简单，并且robust。通过这样简单的实现，可以告诉你这个数据集的LM模型的下限。这样我们心里会有数，神经网络模型至少不应该比这个模型差的。神经网络模型因为其超参数、梯度爆炸等问题，有时候我们不太容易决定是真的模型不行、参数没调好还是代码有bug。那么通过N-gram LM的给出的下限，我们就可以直观地知道神经网络是有bug还是没调好参数。</p><p>第二个例子就是涉及发文章了，不知道有没有同学想过，BERT里面训练LM的随机替换为什么就使结果变好，随机替换是什么鬼，怎么结果就好了。其实在BERT之前，斯坦福的吴恩达组的Ziang Xie的 Data Noising as Smoothing in Neural Network Language Models ICLR2017（<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1703.02573.pdf">https://arxiv.org/pdf/1703.02573.pdf</a>） 就首次提出了此方法，而且给出了理论解释。这种random替换其实本质上属于language modeling里面基于interpolation的平滑方式， 而基于interpolation的LM平滑，就躺在jurafsky那本书的第3.4.3节。</p><p>\2. <strong>了解早年经典的NLP模型以及论文</strong>：相比简单粗暴的神经网络模型，早年的NLP算法确实比较繁琐复杂，但里面确实有很多早年学者在硬件条件艰苦情况下的智慧结晶。熟悉了这些模型，可以在现在神经网络里面融会贯通。去年在人民大学做seminar。Seminar有大概30-40位同学参加。Seminar中，我问了一个问题，有谁知道机器翻译中的IBM模型大概是干嘛的，举手的同学大概有五分之一。我再问，谁能来手写（或者大概手写）一下IBM model1，一个人都没有。仅仅从基于IBM模型的Hierarchical Phrase-based MT, 近几年就有很多篇引用量很高的文章是基于里面的思想的。例子数不胜数： </p><p>1) chris dyer 组的<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1601.01085">Incorporating structural alignment biases into an attentional neural translation model</a> (NAACL16) 提出用双向attention做neural机器翻译的约束项，意思是如果在英语翻译法语生成的target中的一个法语词attend到了一个source中的英语词，那么反过来，法语翻译英文 target中相同这个英语词应该也attend到source中的这个英语词。其实这个思想就是完完全全相似 Percy Liang 曾经的成名作之一，早在NAACL06年 Alignment by Agreement，大家通过题目的意思就可以猜到文章的内容，正向翻译与反向翻译中的 对齐(alignment) 要 一致(agree)。如今做neural MT的同学，有多少同学读过Percy的这篇大作呢 （大家知道Percy最多的应该是Squad吧）。</p><p>2) 处理对话系统的无聊回复，用反向概率p(source|target)做reranking现在应该已经是标配。再比如Rico Sennrich的成名作之一将Monolingual data 跟seq2seq 模型结合。其实这连个思想在phrase-base MT 里面早就被广发的使用。Neural之前的MT，需要对一个大的N-best list用MERT做 reranking， 反向概率 p(source|target) 以及语言模型概率 p(target)是reranking中feature的标配。</p><p>3) Harvard NLP组, Sam Wiseman 和Alex 发表的EMNLP16 best paper runner-up, Sequence-to-Sequence Learning as Beam-Search Optimization, 基本上传承了Daume´ III and Daniel Marcu 2005年的 LaSO模型，将其思想adapt到neural里面。</p><p>如果再准本溯源，诞生于neural MT的attention，不就是IBM模型的神经网络版本嘛。</p><p>\3. <strong>了解机器学习的基本模型：</strong>神经网络的简单暴力并且有效。但是从科研的角度讲，熟悉基本的机器学习算法是必修课。比如吴恩达的 machine learning就是必要之选。记得前段时间我面试一个小伙子，一看就是很聪明的同学，而且很短的时间就有fvg一篇NAACL在投。我就问小伙子，EM算法是什么，小伙子说没有听说过EM，而且自己的科研也用不到EM。我认为这其实是一个挺大的误区。当我想起我自己，曾经就吃过很多类似的亏。因为早期数学基础偏弱，也没有决心恶补一下数学，所以早年每次看到跟variational inference相关的算法就头大，这种偏科持续了很久，限制了科研的广度。相比粗暴的神经网络，CRF等模型的inference确实相对复杂（当年我自己也看了很多次才彻底搞明白）。但搞懂这些，是一个NLP researcher的基本素养。Pattern Recognition and Machine Learning那本书，尤其是某些小节确实比较难（又暴露了数学基础差的事实），即便是只是为了过一遍，也需要很强的耐力才能看完，更不用说完全看懂了。我自己也曾经半途而废很多次，如今依然有很多章节是不太懂的。但是其中的很多基础chapter，我认为还是很值得一读的。其实可以组成那种两三个人的学习小组，不需要有太雄伟的目标，用个一年哪怕两年的时间，把几个重要的chapter 过一遍。</p><p>NLP相对是应用科学，并不是特别的数学。但是我们天天用的算法的基本数学逻辑我认为还是需要搞懂，比如dropout, 比如天天用到的优化(SGD, momentum, adaboost, adagrad)，比如各种 batch, layer normalization。这样其实可以省去很多浪费的时间，磨刀不误砍柴工。这些年来，在帮同学调bug的过程中，我至少遇见过3-5个同学 training 的时候开dropout, test 的时候没有对每个cell用 (1-dropout)去 scale （大家不要笑，这是真的）。然后画出dropout曲线就是 dropout 值越大，结果越差。在讨论的时候，同学一脸茫然并且不清楚test时候需要scale。其实本质就是并不了解dropout背后的数学原理。</p><p><strong>4. 多看NLP其他子领域的论文</strong>：NLP有很多子领域，MT，信息抽取，parsing，tagging，情感分析，MRC等等。多多熟悉其他子领域的进展是必要的。其实不同子领域所运用的模型不会相差太大。但是最开始看不熟悉领域的问题可能会有一点难，原因是对问题的formalization不是很了解。这可能就需要多花一些时间，多找懂的同学去问。其实了解不同问题的formalization也是对领域知识最好的扩充。</p><p><strong>5. 了解 CV和data mining领域的基本重大进展</strong>：当熟悉了上面所说的点之后（当然可能至少也需要一年的时间）。熟悉CV领域的基本任务、基本算法我认为对于打开科研视野很重要。但是不可否认，因为领域不用，写作风格、术语表达相差很大，又因为缺乏背景知识（文章中会省略一些基础知识，默认大家都懂。但是跨领域的人可能不懂），第一次想读懂跨领域的文章其实并不容易。我就出现过竟然在讨论班上直接把faster-RCNN讲错了的情况，以为自己看懂了，然后就讲错了（至今昱先天天还在因为这个事情调侃我）。不过重要的是，NLP领域里面一些重要的文章其实或多或少借鉴了CV里面的思想，当然也同样出现CV借鉴NLP的情况。NLP神经网络可视化、可解释性的研究，时间上还是落后于CV里面对CNN的可视化。所以很多工作大量借鉴了CV里面的类似工作。NLP运用GAN其实也是借鉴CV的。其实两个领域很多是很相通的。比如，如果不考虑question query, vision里面detection中的 region proposal（在一个大的图片背景下找一个特定区域）, 大家想是不是跟MRC里面的 span extraction （在一大堆文字里面找一个span）有异曲同工之妙。更不用说image caption generation与sequence-to-sequence模型了，本质上几乎没什么太大的区别。强化学习在生成领域generation，发完了MT(Ranzato et al., ICLR2016)再发 image caption generation, 再回到summarization. Actor-critic 模型也是类似的，还是很多做generation diversity的文章。因为跨领域不好懂，所以第一次推荐看tutorial, 如果有 sudo code 的tutorial那就更好了。另外看看扫盲课的视频，比如Stanford CS231n也是个好办法。另外，一个NLP组里面有一个很懂CV的人也很重要（拜谢昱先）， and vise versa。<br>graph embedding近两年崛起于data mining领域。目测会在（或者已经在）NLP的不少任务得到广泛应用。想到几年前，deep walk借鉴了word2vec, 开始在data mining领域发迹，然后似乎又要轮转回NLP了。</p><p><strong>6.如何快速了解某个领域研究进展</strong></p><p>最后简单说一下快速了解某领域研究进展的经验。你会发现，搜索引擎是查阅文献的重要工具，尤其是谷歌提供的Google Scholar，由于其庞大的索引量，将是我们披荆斩棘的利器。</p><p>当需要了解某个领域，如果能找到一篇该领域的最新研究综述，就省劲多了。最方便的方法还是在Google Scholar中搜索“领域名称 + survey / review / tutorial / 综述”来查找。也有一些出版社专门出版各领域的综述文章，例如NOW Publisher出版的Foundations and Trends系列，Morgan &amp; Claypool Publisher出版的Synthesis Lectures on Human Language Technologies系列等。它们发表了很多热门方向的综述，如文档摘要、情感分析和意见挖掘、学习排序、语言模型等。</p><p>如果方向太新还没有相关综述，一般还可以查找该方向发表的最新论文，阅读它们的“相关工作”章节，顺着列出的参考文献，就基本能够了解相关研究脉络了。当然，还有很多其他办法，例如去<a href="https://link.zhihu.com/?target=http%3A//videolectures.net">http://videolectures.net</a>上看著名学者在各大学术会议或暑期学校上做的tutorial报告，去直接咨询这个领域的研究者，等等。</p><h2 id="文献库在Endnote中构建完成"><a href="#文献库在Endnote中构建完成" class="headerlink" title="文献库在Endnote中构建完成"></a>文献库在Endnote中构建完成</h2>]]></content>
    
    
    <categories>
      
      <category>1.科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文写法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1.科研/2 - 计划</title>
    <link href="undefined2020/02/24/1.%E7%A7%91%E7%A0%94/2%20-%20%E8%AE%A1%E5%88%92/"/>
    <url>2020/02/24/1.%E7%A7%91%E7%A0%94/2%20-%20%E8%AE%A1%E5%88%92/</url>
    
    <content type="html"><![CDATA[<h1 id="计划"><a href="#计划" class="headerlink" title="计划"></a>计划</h1><p>比赛 -  改进模型 - 得奖 - 阅读论文 - 写作论文</p><p>比赛得奖同时也是进入行业的必要条件</p><p>进入行业是了解行业和理解科研的必要条件</p>]]></content>
    
    
    <categories>
      
      <category>1.科研</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文写法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/技术_Git</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/%E6%8A%80%E6%9C%AF_Git/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/%E6%8A%80%E6%9C%AF_Git/</url>
    
    <content type="html"><![CDATA[<h1 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h1><h4 id="1-四个区域"><a href="#1-四个区域" class="headerlink" title="1.四个区域"></a>1.四个区域</h4><p>Workplace : 工作区，自己的电脑存放代码的地方</p><p>Index，Stage ： 暂存区，存放临时的改动，事实上它是一个文件，保存即将提交到文件列表的信息</p><p>Repository  :  仓库区（版本库），存放数据的地方，有所有提交版本的地方，其中head指向最新放入仓库的版本</p><p>Remote ：远程仓库，托管代码的服务器，可以简单理解为项目组的一台用于远程数据交换的电脑</p><p>四者关系如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1204_45_779.png" alt=""></p><h4 id="2-常用命令"><a href="#2-常用命令" class="headerlink" title="2.常用命令"></a>2.常用命令</h4><p>常用</p><pre><code># 本地推至暂存区git add .# 删除暂存区/分支，但本地保留文件（不被版本控制）git rm --cached file_path# 为本地代码切换版本git checkout# 暂存区代码推至版本库git commit -m &quot;提交说明&quot;# 移除暂存区文件git reset HEAD 文件名# 去掉上次的提交(变成add前状态)git reset HEAD^# 去掉上次的提交(变成commit前状态)git reset --soft HEAD^# 显示当前git配置git config --lsit# 编辑git配置文件git config -e[--global]# 初次commit前，需要配置用户邮箱和用户名git config --global user.email &quot;you@example.com&quot;git config --global user.name &quot;Your Name&quot;</code></pre><p>略复杂的pull和push命令</p><pre><code># 远程指定分支拉取到本地指定分支git pull origin &lt;远程分支名&gt;:&lt;本地分支名&gt;# 将远程指定分支拉取到本地当前分支上git pull origin &lt;远程分支名&gt;# 将远程与本地同名分支拉取到本地当前分支上（需要先关联远程分支）git pull origin# 将本地分支推送到远程指定分支上git push origin &lt;本地分支名&gt;:&lt;远程分支名&gt;# 将本地当前分支推送到远端同名分支上git push origin &lt;本地分支名&gt;</code></pre>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1.科研/0 - 总体思路</title>
    <link href="undefined2020/02/24/1.%E7%A7%91%E7%A0%94/0%20-%20%E6%80%BB%E4%BD%93%E6%80%9D%E8%B7%AF/"/>
    <url>2020/02/24/1.%E7%A7%91%E7%A0%94/0%20-%20%E6%80%BB%E4%BD%93%E6%80%9D%E8%B7%AF/</url>
    
    <content type="html"><![CDATA[<h1 id="0-总体思路"><a href="#0-总体思路" class="headerlink" title="0 - 总体思路"></a>0 - 总体思路</h1><ul><li>基础 ： Speech and Language Processing（完成）</li><li>早年经典NLP论文：遍历论文库（进行中）</li><li>了解机器学习的基本模型：<ul><li>CS229（完成）</li><li>Pattern Recognition and Machine Learning</li></ul></li><li>了解NLP其他子领域(MT,信息抽取,parsing,tagging,情感分析,MRC等)（进行中）</li><li>了解CV和数据挖掘领域的进展</li></ul><ol><li>学科调研：构建阅读列表和论文库；读近五年survey，近三年顶会，感兴趣方向热门论文和经典书单；输出互引DAG图，输出专有名词词典，输出调研文档。</li></ol><p>   *<em>构建文献库时遇到的困难：flood。目前资料：知识图谱80 + 深度学习100 + 生物医学CRF70 + ACL660 = 910篇论文，以及7本大部头。这还不包括所谓的5年survery100 + 三年顶会3000共约3100篇论文。 *</em></p><p>   <strong>我需要的：迅速了解整个学科发展大致现状，选择自己感兴趣的+有前途做的人少的领域迅速构建论文库精读切入</strong></p><p>   <strong>初步解决方案：控制工作量，看优质survey，大部头只挑一本看，其余当工具书，迅速对学科整体建模。积极寻求所有论文的互引网络图，然后只阅读高引论文 + 感兴趣细分领域的施引文献，根据CheBio论文list启发，面向“方法”构建相似论文库。</strong></p><p>   <strong>当下任务：阅读speech and language processing</strong></p><ol start="2"><li><p>计划：工作示意图 + excel详表。</p></li><li><p>选题：刚入门不要有开创道路的想法，第一篇论文可新颖度略低。</p><p>最好能找到一起讨论的同学/小老板（科研伙伴），在前辈的带领下快速从一个小方向切入进去。</p></li><li><p>模型设计：模型设计和下边的代码、实验、调参、写文都可以从模仿起步。把和你工作最相关的文章 好好读几遍，从结构到段落到句子都可以模型。当年王鸿伟的第一篇论文就是吃透并模仿了好几篇参考文献而已，得到了老师的表扬。</p></li><li><p>代码实现</p></li><li><p>实验环节：设计实验，做实验，调参，记录结果</p></li><li><p>了解科研论文的组成结构。[Done]</p></li><li><p>开始写文，确定发文目标:列出顶会清单。查看其字数和格式限制，论文一旦超长很难缩短。</p></li><li><p>逻辑第一，这种逻辑是贯穿全文的，段落层面的逻辑、句子层面的逻辑、甚至一句话里的逻辑，都是非常关键的。一篇好的论文要循循善诱，有理有据，让人读起来不要废太多脑子，就觉得你说的很有道理。这里面要着重注意各种句子层面的关系：转折、因果、递进等。一句话可以有无数种表达方式，你要做的就是在脑子里把各种方式过滤一遍，选择最流畅的那一种。</p></li><li><p>简单构造：不要试图在一篇论文中提出太多的概念。专注于简单的构造，并围绕它构建您的论点。</p></li><li><p>使其有趣，学术论文不必很无聊。如果您想让自己的知识传播开来，请花些时间以使读者易于理解的方式来构造您的叙述。介绍您的概念，告诉它为什么很有趣，这个想法，它如何工作以及如何解决问题。</p></li><li><p>请一位导师</p></li><li><p>注意格式：始终以标准格式的单词以及双倍间距书写。</p></li><li><p>版本控制：每天另存为新草稿。您可以使用以下命名约定“ yyyymmdd-xxx work-draft”。</p></li><li><p>摘要最后写。当您完成研究论文时，您将更好地掌握所谈论的内容。在最后写摘要比在开始写摘要要容易得多。用您的文字来质疑自己的假设和推论。这样，您的写作可以使您的内容变得更好。</p></li><li><p>查重，语法检测</p></li><li><p>与已经发布的人联系，了解发布需要什么。与您信任的审稿人分享您的论文。特别是如果这是您的第一篇论文，您可能会花更多时间在不重要的论文方面。优秀的审稿人可以帮助您使您的论文格式更好。</p></li><li><p>善用工具</p><table><thead><tr><th>工具名称</th><th>描述</th><th>备注</th></tr></thead><tbody><tr><td>Paperpile</td><td>在线研究工作流</td><td><a href="https://paperpile.com/app" target="_blank" rel="noopener">https://paperpile.com/app</a></td></tr><tr><td>sci-hub</td><td>黑文献</td><td><a href="http://tool.yovisun.com/scihub/" target="_blank" rel="noopener">http://tool.yovisun.com/scihub/</a>    ww1.sci-hub.tv</td></tr><tr><td>Grammerly</td><td>语法矫正</td><td></td></tr><tr><td>Turnitin</td><td>查重</td><td></td></tr><tr><td>Inkscape</td><td>工作示意图</td><td></td></tr><tr><td>Power Point</td><td>工作示意图</td><td></td></tr><tr><td>Adobe illustrator</td><td>工作示意图</td><td></td></tr><tr><td>Readcube</td><td>Ref Control</td><td></td></tr><tr><td>Endnote</td><td>Ref Control</td><td>找投稿官网;自动匹配8个推荐投稿地;可按照期刊自动调整style;Endnote online免费，可绑定web of science账号</td></tr><tr><td>Zotero</td><td>Ref Control</td><td><a href="https://www.zotero.org" target="_blank" rel="noopener">https://www.zotero.org</a></td></tr><tr><td>Mendeley</td><td>Ref Control</td><td><a href="https://www.mendeley.com/" target="_blank" rel="noopener">https://www.mendeley.com/</a></td></tr><tr><td>Google Scholar</td><td>查阅论文</td><td><a href="https://scholar.google.co.in/" target="_blank" rel="noopener">https://scholar.google.co.in/</a></td></tr><tr><td>LaTeX</td><td>插入图片、数学公式等</td><td><a href="https://www.latex-project.org/" target="_blank" rel="noopener">https://www.latex-project.org/</a></td></tr><tr><td>TypeSet</td><td>多人协作，各期刊格式管理</td><td><a href="https://typ.st/2st78LG" target="_blank" rel="noopener">https://typ.st/2st78LG</a></td></tr><tr><td>ShareLaTexX</td><td>多人协作</td><td><a href="https://www.sharelatex.com/" target="_blank" rel="noopener">https://www.sharelatex.com/</a></td></tr><tr><td>OverLeaf</td><td>各期刊模板和格式</td><td></td></tr><tr><td>web of science</td><td>学科跟踪服务</td><td>使用邮箱创建学科跟踪服务，新动态可自动发送</td></tr><tr><td>mjl.clarivate</td><td>查看被期刊收录情况</td><td><a href="https://mjl.clarivate.com" target="_blank" rel="noopener">https://mjl.clarivate.com</a></td></tr><tr><td>publons/publons academy</td><td>寻找审稿人/同行评议</td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table></li><li><p>注意事项</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>保证有效工作时间8h/day。</td><td>工作低效的标志：老看时间，察觉到时间的流逝。这时不可硬看，通过散步/睡觉/健身及时调整。</td></tr><tr><td>张弛有度</td><td>保证work-life-balance。每天必须要高质量地玩，天天紧绷很快就疲了，拉低生命体验本末倒置，且走不远。</td></tr><tr><td>冷静思考，摒弃急躁和蛮力</td><td>科研是优雅的脑力劳动，是艺术创作。不是体育比赛，不是谁力气大谁能忍受痛苦谁就厉害。遇到困难(which一定会)心情焦躁的时候时往回拉一拉，想想Andrew Ng,Manning,Percy Liang,Einstein这些偶像会怎么做。</td></tr><tr><td>长期solo，注意maintain精神状态</td><td>多健身，尝试回到绿茵场。心怀善念，别忘了初心是make the world a better place。</td></tr><tr><td></td><td></td></tr></tbody></table></li><li><p>Times</p><p>ACL2019 : 7.28</p><p>EMNLP : 11.3</p><p>NAACL : 6.2</p></li><li></li></ol>]]></content>
    
    
    <categories>
      
      <category>1.科研</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>4.安装调试记录/快捷键</title>
    <link href="undefined2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E5%BF%AB%E6%8D%B7%E9%94%AE/"/>
    <url>2020/02/24/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E5%BF%AB%E6%8D%B7%E9%94%AE/</url>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h1 id="PyCharm"><a href="#PyCharm" class="headerlink" title="PyCharm"></a>PyCharm</h1><p>CTRL + -  = 折叠本行 </p><p>CTRL + + = 打开本行</p><p>CTRL + SHIFT + -  = 全部折叠</p><p>CTRL + SHIFT + + = 全部打开</p><h1 id="Chrome"><a href="#Chrome" class="headerlink" title="Chrome"></a>Chrome</h1><p>CTRL + SHIFT + N = 打开匿名窗口</p><p>CTRL + SHIFT + I =  检查</p><p>CTRL + W  = 关闭当前标签页</p><p>CTRL + fn  + PgUp/PgDn = 上翻/下翻当前标签页</p><h1 id="IDEA"><a href="#IDEA" class="headerlink" title="IDEA"></a>IDEA</h1><p>CTRL + SHIFT + F12 =  编程窗口最大化</p><p>CTRL + SHIFT+ N = 查找文件</p><p>SHIFT + F6 = 全局替换</p><p>CTRL + ALT + L = 格式化代码</p><p>CTRL + ALT + V =快速赋值</p><p>Cookie[] cookeis数组遍历便捷写法  = cookies.for = for(Cookie cookie : cookies)</p><p>SHIFT + F6  = 文件重命名</p><p>CTRL + SHIFT + C = 拷贝文件路径</p><p>CTRL + D = 拷贝一行</p><p>SHIFT + ENTER = 换行</p><p>CTRL + P = 查看参数</p><p>CTRL + N = 查找代码行例:QutsitonMapper.java:31  可直接定位到该文件31行</p><p>CRTL + E 切换到最近页面</p><p>数据库consolo:CTRL+ENTER:执行当前mysql    </p><p>CTRL+SHIFT+上箭头 = 本行上移</p><p>ALT+1=显示项目目录</p><p>CTRL + ALT + O = 删除无用import</p><p>CTRL + F12 =展示所有方法</p><p>CTRL + ALT + 左/右  = 前进或后退</p><p>CTRL+F6 =  替换函数参数位置，如果有依赖会自动一起替换</p><p>CTRL+A = 选中当页所有</p><p>CTRL+SHIFT+U = 大小写转换    </p><p>ALT+INSERT  = 自动生成getter setter等</p><h1 id="HTML"><a href="#HTML" class="headerlink" title="HTML"></a>HTML</h1><p>CTRL + W = 逐层扩散选中</p><p>span + TABLE = <span>|</span></p><h1 id="TYPORA"><a href="#TYPORA" class="headerlink" title="TYPORA"></a>TYPORA</h1><p>CTRL + 1 = 一级标题</p><p>三个* = 分割线</p><p>三个` = 代码块</p><p>无序列表：以*开头S</p><p>有序列表：1.开头</p><h1 id="GIT"><a href="#GIT" class="headerlink" title="GIT"></a>GIT</h1><p>SHIFT + : + x  = 退出并保存</p><p>Q = 终止当前对话</p><p>git checkout [序列号] [文件地址] =文件版本回滚</p><h1 id="CMD"><a href="#CMD" class="headerlink" title="CMD"></a>CMD</h1><p>​    Keyboard shortcuts</p><h4 id="Tab-manipulation"><a href="#Tab-manipulation" class="headerlink" title="Tab manipulation"></a>Tab manipulation</h4><ul><li><p>Ctrl + ` : <strong>Global</strong> Summon from taskbar</p></li><li><p><code>Win + Alt + p</code> : Preferences (Or right click on title bar)</p></li><li><p><code>Ctrl + t</code> : New tab dialog (maybe you want to open cmd as admin?)</p></li><li><p><code>Ctrl + w</code> : Close tab</p></li><li><pre><code>Shift + Alt + number</code></pre><p>Fast new tab:</p><ol><li><code>1.</code> CMD</li><li><code>2.</code> PowerShell</li></ol></li><li><p><code>Alt + Enter</code> : Fullscreen</p></li></ul><h4 id="Shell"><a href="#Shell" class="headerlink" title="Shell"></a>Shell</h4><ul><li><code>Ctrl + Alt + u</code> : Traverse up in directory structure (lovely feature!)</li><li><code>End, Home, Ctrl</code> : Traverse text as usual on Windows</li><li><code>Ctrl + r</code> : History search</li><li><code>Shift + mouse</code> : Select and copy text from buffer</li><li><code>Right click / Ctrl + Shift + v</code> : Paste text</li></ul><h4 id="VIM"><a href="#VIM" class="headerlink" title="VIM"></a>VIM</h4><hr><p>CTRL+O = 返回上一个文件</p><h3 id="Linux"><a href="#Linux" class="headerlink" title="Linux:"></a>Linux:</h3><hr><p>chmod -R 777 /路径  ：让锁定文件可读可写</p><p>ps -ef   查看当前进程</p><p>ps  -p 6176 -o etime查看运行时间</p><p>netstat -ntlp:查看所有端口占用情况</p><h3 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h3><p>build-target# ./bin/start-cluster.sh :拉起flink web 注意目录是bin之前那个target文件夹</p>]]></content>
    
    
    <categories>
      
      <category>4.安装调试记录</category>
      
    </categories>
    
    
    <tags>
      
      <tag>快捷键</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.文章/Java_5.索引,调优</title>
    <link href="undefined2020/02/24/0.%E6%96%87%E7%AB%A0/Java_5.%E7%B4%A2%E5%BC%95,%E8%B0%83%E4%BC%98/"/>
    <url>2020/02/24/0.%E6%96%87%E7%AB%A0/Java_5.%E7%B4%A2%E5%BC%95,%E8%B0%83%E4%BC%98/</url>
    
    <content type="html"><![CDATA[<h1 id="5-sql索引-调优"><a href="#5-sql索引-调优" class="headerlink" title="5.sql索引,调优"></a>5.sql索引,调优</h1><p>索引相关概念</p><ul><li>索引原理<ul><li>DBMS索引一般用b tree或者b+tree实现</li></ul></li></ul><p>带主键的数据库表的存储结构(正常查找)</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1201_50_181.png" alt=""></p><p>构建索引后从非聚集索引直接查找</p><ul><li>Sql语句执行流程<ul><li>create index index_birthday on user_info(birthday)   //构建索引</li><li>select user_name from user_info where birthday = ‘1991-01-01’      //正常查找数据</li><li>create index index_birthday_and_user_name on user_info(birthday,user_name);   //构建双字段的覆盖索引</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>文章</category>
      
    </categories>
    
    
    <tags>
      
      <tag>索引</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.文章/Java_6.jetty和tomcat</title>
    <link href="undefined2020/02/24/0.%E6%96%87%E7%AB%A0/Java_6.jetty%E5%92%8Ctomcat/"/>
    <url>2020/02/24/0.%E6%96%87%E7%AB%A0/Java_6.jetty%E5%92%8Ctomcat/</url>
    
    <content type="html"><![CDATA[<h1 id="7-jetty和tomcat"><a href="#7-jetty和tomcat" class="headerlink" title="7.jetty和tomcat"></a>7.jetty和tomcat</h1><p>1.Jetty是什么</p><p>​    jetty是一个开源的HTTP服务器和Servlet引擎,可以为JSP和Servlet提供运行时环境,相对与Tomcat,,jetty更加轻量级,更加简易和灵活</p><p>2.jetty特点</p><ul><li>异步,支持更高的并发量</li><li>灵活,更加轻量,更容易定制,更高的资源利用率</li><li>Jetty采用默认的NIO模型,jetty很好地支持长链接</li></ul><p>3.应用场景</p><ul><li>企业级应用tomcat占据了绝对优势</li><li>jetty默认使用NIO,在轻量级的,保持长连接的场景下使用很有优势,比如客服的聊天</li></ul><p>4.jetty原理</p><ul><li>提供了两种handlder<ul><li>handlerWrapper<ul><li>可以将一个Handler委托给另一个类执行,将handler加载到jetty中就是通过handler委托给server执行的</li></ul></li><li>handlerCollection<ul><li>将多个handler组装成handler链,可以方便地做扩展</li></ul></li></ul></li></ul><p>2.tomcat</p><p>为了服务器生成动态页面,需要运行java Servlet,那么就需要提供Servlet容器</p><p>Tomcat正式支持运行Servlet/JSP应用程序的容器(Container),运行在JVM中    </p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1202_11_172.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>文章</category>
      
    </categories>
    
    
    <tags>
      
      <tag>jetty</tag>
      
      <tag>tomcat</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.文章/Java_7.反射</title>
    <link href="undefined2020/02/24/0.%E6%96%87%E7%AB%A0/Java_7.%E5%8F%8D%E5%B0%84/"/>
    <url>2020/02/24/0.%E6%96%87%E7%AB%A0/Java_7.%E5%8F%8D%E5%B0%84/</url>
    
    <content type="html"><![CDATA[<h1 id="反射"><a href="#反射" class="headerlink" title="反射"></a>反射</h1><ul><li><p>反射的定义</p><ul><li>行状态中,能知道任何一个类的属性和方法</li><li>能调用任何一个对象的属性和方法</li><li>这种动态获取信息以及动态调用对象方法的功能称谓java的反射机制</li></ul></li><li><p>反射的用途</p><ul><li>第三方应用开发时,会遇到某个类的变量或方法是私有的,只对系统应用开放,这时候就利用java的反射机制通过反射来获取所需要的私有成员或者方法.</li></ul></li><li><p>反射的相关类</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1204_15_056.png" alt=""></p><ul><li><p>class类</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1204_22_961.png" alt=""></p></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>文章</category>
      
    </categories>
    
    
    <tags>
      
      <tag>java</tag>
      
      <tag>反射</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.文章/Java_3.泛型</title>
    <link href="undefined2020/02/24/0.%E6%96%87%E7%AB%A0/Java_3.%E6%B3%9B%E5%9E%8B/"/>
    <url>2020/02/24/0.%E6%96%87%E7%AB%A0/Java_3.%E6%B3%9B%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="泛型"><a href="#泛型" class="headerlink" title="泛型"></a>泛型</h1><p>泛型就是扩展了方法的适用范围:</p><p>例子:</p><p>原本有一个SUM(y) = (a + b)的两元素相加的方法</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1157_49_295.png" alt=""></p><p>在两数字相加或两字符串拼接的场景下本来都可以用它</p><p><img src="!%5B%5D(http://bqlab-pic.test.upcdn.net/pic/20191207_1159_04_998.png)" alt=""></p><p>但是如果y提前规定了数据类型就不能用了,两个场景必有其一要重写</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1159_13_951.png" alt=""></p><p>这时候使用泛型,&lt;T&gt;当做占位符,就可以实现代码复用了 </p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1159_22_800.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1159_32_541.png" alt=""></p><p>之所以不用Object实现参数的任意化是因为要做显式的强制转换,这种转换是要求开发者对实际参数可预知的情况下进行的,而很多时候开发者不能预知程序运行时有哪样的类型需要强转</p><p>如果强转错误,程序员也无从得知,是一个安全隐患</p><p>引入泛型不用object后所有任意化的参数类型都是隐式自动进行的,保证了效率和安全性</p>]]></content>
    
    
    <categories>
      
      <category>文章</category>
      
    </categories>
    
    
    <tags>
      
      <tag>java</tag>
      
      <tag>泛型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.文章/Java_4.表链接</title>
    <link href="undefined2020/02/24/0.%E6%96%87%E7%AB%A0/Java_4.%E8%A1%A8%E9%93%BE%E6%8E%A5/"/>
    <url>2020/02/24/0.%E6%96%87%E7%AB%A0/Java_4.%E8%A1%A8%E9%93%BE%E6%8E%A5/</url>
    
    <content type="html"><![CDATA[<h1 id="5-表链接"><a href="#5-表链接" class="headerlink" title="5.表链接"></a>5.表链接</h1><h3 id="5-1外链接"><a href="#5-1外链接" class="headerlink" title="5.1外链接"></a>5.1外链接</h3><p>外链接就是A B两个表,以左右链接的方式选择一个主表,然后附表加入主表的过程</p><p>AB两个表的图示</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1200_09_795.png" alt=""></p><ul><li><p>左外链接</p><ul><li><p>select * from TableA left join TableB on TableA.id=TableB.id</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1200_22_998.png" alt=""></p></li></ul></li></ul><ul><li><p>右外链接</p><ul><li><p>select * from TableA right join TableB on TableA.id=TableB.id</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1200_37_538.png" alt=""></p></li></ul></li></ul><ul><li><p>全外链接</p><ul><li><p>select * from TableA full join TableB on TableA.id=TableB.id</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1200_52_073.png" alt=""></p></li></ul></li><li><p>内链接</p><ul><li><p>select * from TableA JOIN TableB on TableA.id=TableB.id</p><p>结果是只链接两者共有的数据</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1201_00_326.png" alt=""></p></li></ul></li></ul><ul><li><p>交叉链接</p><ul><li><p>select * from TableA cross join TableB</p></li><li><p>结果是两个表以基础序号为乘积的笛卡尔积的排列</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1201_26_633.png" alt=""></p></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>文章</category>
      
    </categories>
    
    
    <tags>
      
      <tag>表连接</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.文章/Java_2.ClassLoader</title>
    <link href="undefined2020/02/24/0.%E6%96%87%E7%AB%A0/Java_2.ClassLoader/"/>
    <url>2020/02/24/0.%E6%96%87%E7%AB%A0/Java_2.ClassLoader/</url>
    
    <content type="html"><![CDATA[<h1 id="2-ClassLoader类加载器"><a href="#2-ClassLoader类加载器" class="headerlink" title="2.ClassLoader类加载器"></a>2.ClassLoader类加载器</h1><p>classloader作用:</p><ul><li>负责将class加载到JVM中</li><li>审查每个类由谁加载</li><li>将class字节码重新编译成JVM统一要求的对象格式</li></ul><h3 id="2-1类加载时机与过程"><a href="#2-1类加载时机与过程" class="headerlink" title="2.1类加载时机与过程"></a>2.1类加载时机与过程</h3><p>类从被加载到虚拟机内存开始,到卸载出内存为止,整个生命周期包括了七个部分:</p><p>加载,验证,准备,解析,初始化,使用,卸载</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1156_50_703.png" alt=""></p><p>如下几种情况会对类进行初始化</p><ul><li>创建类的实例</li><li>对类进行反射调用</li><li>当初始化类,发现父类没有没初始化</li><li>jvm启动,用户指定一个要执行的主类,虚拟机会先初始化这个主类</li><li>java.lang.invoke.MethodHandle实例最后的解析结果REF_getstatic,REF_putstatic,REF_invokeStatic的方法句柄，并且这个方法句柄所对应的类没有进行初始化，则需要先出触发其初始化。</li></ul><p>2.2类加载器的双亲委派模型</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1157_09_601.png" alt=""></p><p>双亲委派模型好处</p><ul><li>避免重复加载,当父类已经加载了该类的时候,就没有必要classloader再加载一次考虑到安全因素，我们试想一下，如果不使用这种委托模式，那我们就可以随时使用自定义的String来动态替代java核心api中定义的类型，这样会存在非常大的安全隐患，而双亲委托的方式，就可以避免这种情况，因为String已经在启动时就被引导类加载器（Bootstrcp ClassLoader）加载，所以用户自定义的ClassLoader永远也无法加载一个自己写的String，除非改变JDK中ClassLoader搜索类的默认算法。</li></ul>]]></content>
    
    
    <categories>
      
      <category>文章</category>
      
    </categories>
    
    
    <tags>
      
      <tag>java</tag>
      
      <tag>classLoader</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.文章/Java_1.JVM,方法区,堆</title>
    <link href="undefined2020/02/24/0.%E6%96%87%E7%AB%A0/Java_1.JVM,%E6%96%B9%E6%B3%95%E5%8C%BA,%E5%A0%86/"/>
    <url>2020/02/24/0.%E6%96%87%E7%AB%A0/Java_1.JVM,%E6%96%B9%E6%B3%95%E5%8C%BA,%E5%A0%86/</url>
    
    <content type="html"><![CDATA[<h1 id="JVM"><a href="#JVM" class="headerlink" title="JVM"></a>JVM</h1><ul><li>JAVA虚拟机的生命周期<ul><li>java虚拟机用于执行java程序,一个java程序对应一个虚拟机</li><li>java虚拟机总是开始于一个main方法,返回void,接受一个args[]参数</li><li>main()方法是程序的起点,他被执行的线程初始化为程序的初始线程</li><li>java中的线程分两种<ul><li>守护线程daemon:java虚拟机自己使用的,比如垃圾回收</li><li>非守护线程:non-daemon:包含main()方法的初始线程不是守护线程</li></ul></li></ul></li></ul><ul><li><p>运行时数据区域</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1156_03_058.png" alt=""></p><ul><li><h4 id="1-程序计数器"><a href="#1-程序计数器" class="headerlink" title="1.程序计数器"></a>1.程序计数器</h4><ul><li>内存空间小,线程私有,字节码解释器就是依赖程序计数器工作的:改变计数器的值来选取下一条需要执行的指令,分支,循环,跳转,异常处理等.</li><li>如果线程正在执行一个java方法,这个计数器就记录正在执行的虚拟机字节码地址.<ul><li>如果这个方法是native方法,则计数器的值为underfined</li></ul></li><li>程序计数器这个区域是java虚拟机中唯一没有规定任何OutOfMemoryError情况的区域</li></ul></li><li><h4 id="2-java虚拟机栈"><a href="#2-java虚拟机栈" class="headerlink" title="2.java虚拟机栈"></a>2.java虚拟机栈</h4><ul><li>线程私有,生命周期和线程一致<ul><li>线程(thread)<ul><li>是操作系统能够进行运算调度的最小单位</li><li>被包含在进程中,是进程的实际运作单位</li><li>一条线程是进程中一个单一顺序的控制流,一个进程可以并发多个线程</li><li>线程是独立调度和分派的基本单位</li></ul></li></ul></li><li>java虚拟机栈的作用:<ul><li>描述:java方法执行的内存模型</li><li>每个方法执行都会创建一个帧栈(Stack Frame)用于存储局部变量表,操作数栈,动态链接,方法出口<ul><li>局部变量表:存放了编译期的基本类型(boolean,int,char等,还有对象的引用)</li></ul></li><li>每个方法从调用到执行结束,都对应一个帧栈从虚拟机中入栈到出栈的过程</li></ul></li><li>java虚拟机的error<ul><li>StackOverflowError：线程请求的栈深度大于虚拟机所允许的深度</li><li>OutOfMemoryError：如果虚拟机栈可以动态扩展，而扩展时无法申请到足够的内存</li></ul></li></ul></li><li><h4 id="3-本地方法栈"><a href="#3-本地方法栈" class="headerlink" title="3.本地方法栈"></a>3.本地方法栈</h4><ul><li>本地方法栈为JVM使用到的native 方法服务</li></ul></li><li><h4 id="4-java堆"><a href="#4-java堆" class="headerlink" title="4.java堆"></a>4.java堆</h4><ul><li>对大部分应用,java堆是JVM所管理的内存中最大的一块,线程共享.</li><li>功能:存放对象实例和数组.</li></ul></li><li><h4 id="5-方法区"><a href="#5-方法区" class="headerlink" title="5.方法区"></a>5.方法区</h4><ul><li><p>属于共享内存区域,存储已经被虚拟机加载的类信息,常量,静态变量,即时编译器编译后的代码等数据</p></li><li><p>下图介绍了每个区域存储的内容</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1156_14_806.png" alt=""></p></li></ul></li><li><h4 id="6-运行时常量池"><a href="#6-运行时常量池" class="headerlink" title="6.运行时常量池"></a>6.运行时常量池</h4><ul><li>方法区一部分,存放编译期的各种字面量和符号引用,编译器和运行期都可以常量放进池中</li></ul></li><li><h4 id="7-直接内存"><a href="#7-直接内存" class="headerlink" title="7.直接内存"></a>7.直接内存</h4><ul><li>非虚拟机运行时数据区的部分</li></ul></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>文章</category>
      
    </categories>
    
    
    <tags>
      
      <tag>java</tag>
      
      <tag>JVM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_知识图谱和推荐系统</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%92%8C%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%92%8C%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="1-知识图谱"><a href="#1-知识图谱" class="headerlink" title="1.知识图谱"></a>1.知识图谱</h1><p>知识图谱是语义网络semantic network的知识库，可理解为理解为多关系图，多关系图一般包含多种类型的节点和边。</p><p>知识图谱的场景</p><ul><li>社交网络，人和公司都可以是实体，人是节点，公司是节点的集合，人与人之间的关系是边，边可以是朋友也可是同事等，关系可以是单向也可以是双向的</li></ul><p>数据库是结构化数据，网页是非结构化数据，处理非结构化数据是信息抽取的难点。</p><p>四个难点</p><ul><li>实体命名识别  </li><li>关系抽取()</li><li>实体统一(武汉,江城)</li><li>指代消解  (it)</li></ul><p>知识图谱的存储方式</p><p>RDF</p><ul><li>存储三元组(triple)</li><li>推理引擎</li><li>W3C标准</li><li>易于发布数据,多为学术界场景</li></ul><h3 id="2-推荐系统"><a href="#2-推荐系统" class="headerlink" title="2.推荐系统"></a>2.推荐系统</h3><p>评分预测:系统预测用户对电影的评分,根据此给用户推荐,这种是显示反馈</p><p>还有一种是点击率预测,新闻类应用中,根据用户点击某概率来优化推荐方案,这种场景下用户反馈信息的行为特征,而不能反映用户的喜爱成都,这是隐式反馈</p><p>传统的推系统只能使用用户和物品的历史交互信息,作为输入,有两个问题:</p><p>用户和物品之间的交互信息是非常稀疏的,几万个电影只看了几个可能过拟合,</p><p>或者新用户没有看过电影,这问题也叫冷启动问题</p><p>解决冷启动问题的思路一般是额外加入一些辅助信息,作为如</p><p>社交网路,把朋友的额东西推荐你给你</p><p>用户物品的属性,同属性的用户可能有同样的兴趣</p><p>各种辅助信息中,知识图谱是一种新型类型的辅助信息,是一种无语义网络,</p><p>精确性</p><p>知识图谱为物品引入了更多的而语义关系,可以深层次发现了用户兴趣</p><p>多样性</p><p>知识图谱提供了关系链接种类,有利于推荐结果的发散</p><p>可解释性</p><p>知识图谱可以链接用用户的历史记录和推荐结果,提高用户对对剑结果的满意度和接受度</p><p>知识图谱特征学习</p><p>知识图谱特征学习为知识图谱中的每个实体和关系学习得到一个低维度向量,同时保持图中原有的结构或语义信息</p><p>知识图谱特征学习是网络特征学习的一个子领域,</p><p>基于距离的翻译模型,这类模型使用基于距离的评分函数</p><p>还有一种是基于语义的匹配模型SME,NTN 等等</p><p>将知识图谱引入各种推荐系统算法，可以降低知识图谱高维性和异构性，增加知识图谱应用的灵活性</p><p>知识图谱特征学习的推荐系统</p><p>依次训练:deep knowledge-aware network(DKN)</p><p>联合训练:ripple etwork</p><p>交替训练”multi - task”思路:multi-task learning for KG enhanced recommendation(MKR)</p>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>知识图谱</tag>
      
      <tag>推荐系统</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/技术_Docker</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/%E6%8A%80%E6%9C%AF_Docker/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/%E6%8A%80%E6%9C%AF_Docker/</url>
    
    <content type="html"><![CDATA[<h1 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h1><p>Docker是个虚拟机，主要运行在linux上，和VMware Workstation Pro有很多相似的地方。</p><ul><li>镜像，是创建虚拟机之前需要下载的系统镜像文件，比如iso和img文件等</li><li>容器，是正在运行中的虚拟机</li><li>tar文件，就是镜像的压缩文件，压缩传送解压缩安装</li><li>dockerfile，配置文件，写完后通过docker bulid指令将dockerfile构建成一个镜像</li><li>仓库，类似于github，和镜像是pull和push的关系，里边有做好的ubuntu，mysql，tomcat镜像等</li></ul><h4 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h4><p>安装docker</p><p>docker pull nginx : 从仓库下载nginx</p><p>docker images : 查看本地镜像</p><p>docker run -d -p 80:80 nginx 后台将镜像运行为容器nginx，端口映射80-80</p><p>docker ps查看正在运行的容器有哪些，例如它会输出正在运行的容器是nginx，id是92a68b3fe02e</p><p>docker exec -it 92,进入运行的id开头是92的容器</p><p>cd /usr/share/nginx/html/,进入nginx默认的index目录,cat index.html就可编辑index文件</p>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_D3NER</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_D3NER/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_D3NER/</url>
    
    <content type="html"><![CDATA[<h1 id="D3NER-biomedical-named-entity-recognition-using-CRF-biLSTM-improved-with-fine-tuned-embedding-of-various-linguistic-information"><a href="#D3NER-biomedical-named-entity-recognition-using-CRF-biLSTM-improved-with-fine-tuned-embedding-of-various-linguistic-information" class="headerlink" title="D3NER:biomedical named entity recognition using CRF-biLSTM improved with fine-tuned embedding of various linguistic information"></a>D3NER:biomedical named entity recognition using CRF-biLSTM improved with fine-tuned embedding of various linguistic information</h1><p>找有代码的论文读，复现作者的工作</p><h2 id="1-标题"><a href="#1-标题" class="headerlink" title="1.标题"></a>1.标题</h2><p>D3NER:一种使用[被精细调整过”多种语言信息embedding”所提升性能的条件随机场-双向长短时记忆网络]的生物医学命名实体识别工具</p><ul><li><p>embedding</p></li><li><p>CRF</p></li><li><p>biLSTM</p><p>均另起文章讨论</p></li></ul><h2 id="2-abstract"><a href="#2-abstract" class="headerlink" title="2.abstract"></a>2.abstract</h2><p>2.1Motivation</p><p>生物医学命名实体识别技术,是从生物医学文本信息(unstructured text)中提取知识的先决条件,最近LSTM网络被应用到这个问题上,表现很好,不过我们有更改进的地方.</p><p>2.2Result</p><p>我们使用D3NER,一种使用了CRF+biLSTM网络+精调语言信息embedding的生物医学命名实体识别技术</p><p>D3NER和同方向的七种实体识别技术做了充分对比,结论是性能确实有提高</p><h2 id="3-Introduction"><a href="#3-Introduction" class="headerlink" title="3.Introduction"></a>3.Introduction</h2><ul><li><p>特征工程做NER</p><p>Named Entity Recognition(NER)是提取文本文档中知识的先决条件</p><p>Traditionally,表现好的NER模型都是用耗时长/易产生不完整不满足(incomplete,non-satisfactory)set的特征工程实现的.</p><p>代表性的模型有DNorm,TmChem,TaggerOne,UET-CAM</p></li></ul><ul><li><p>LSTM做NER</p><ul><li><p>有两种流行的embedding和我们新加的embedding</p><ul><li><p>pretrained work/token embedding</p><p>现在,一种先进的深度神经网络(biLSTM)被应用在了生物医学NER问题上,效果拔群.</p><p>代表性的模型是[L,C,2016],[M,H,2016],[W,2016b],[L,2018],[H,2017](见原文)</p><p>这些模型用biLSTM网络 学习每个语言单元(词语,符号token)的最佳上下文向量表示,这些[语言单元]    所在的句子将被喂给 序列标记模型”CRF(条件随机场)”</p><p>这些[语言单元]用低维连续向量表示形式(embedding)进行初始化,数量巨大的未标记文本会预训练这些表现形式</p></li><li><p>character-level word embedding</p><p>有些模型[H,2017][L,2018][V,2017]把输入字符级别的词汇embedding运用在biLSTM网络中,使性能进一步提升</p></li><li><p>其他embeddings</p><p>还有一些重要的与语义信息相关的特征:POS(part of speech)和chunking(断句),这两个embedding被论证为也是可以提升NER性能的</p></li><li><p>我们的模型就是加入了POS embedding和 abberviation embedding.提升了性能</p></li></ul></li></ul></li></ul><h2 id="4-Materials-and-methods"><a href="#4-Materials-and-methods" class="headerlink" title="4 Materials and methods"></a>4 Materials and methods</h2><h4 id="4-1-DataSets"><a href="#4-1-DataSets" class="headerlink" title="4.1 DataSets"></a>4.1 DataSets</h4><p>在三个benchmark上评估:BC5 CDR,NCBI,FSU-PRGE</p><h4 id="4-2-Data-pre-processing"><a href="#4-2-Data-pre-processing" class="headerlink" title="4.2 Data pre-processing"></a>4.2 Data pre-processing</h4><p>用python 的 spaCy做数据的分割,tokenization(标签?),以及POS(part of speech)标记</p><p>两项额外预处理工作:</p><ul><li>去除了连字符” - “ </li><li>把所有的数字都换成0,因为数字太特殊又容易过拟合</li></ul><h3 id="4-3模型架构"><a href="#4-3模型架构" class="headerlink" title="4.3模型架构"></a>4.3模型架构</h3><p>D3NER 包含了四层,分别是</p><ul><li>TPAC embedding</li><li>文本表示biLSTM</li><li>project层</li><li>NER层</li></ul><h4 id="4-3-1-TPAC-embedding层"><a href="#4-3-1-TPAC-embedding层" class="headerlink" title="4.3.1  TPAC embedding层"></a>4.3.1  TPAC embedding层</h4><p>总体架构</p><p>input   :由带有POS tag 的n个tokens t1,t2,t3组成的句子</p><p>output:携带embedding向量的token</p><p>每个token都被如下embedding级联:   </p><ul><li>Token itself</li><li>POS</li><li>information about its Abbreviation status </li><li>information about its Character</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1426_07_159.png" alt=""></p><ul><li>Token-level embedding<ul><li>tE用于捕捉形态上不近似,但是语义近似的词汇,比如‘Grippe’ and ‘Influenza’都是感冒</li><li>运用的是200维的 pre-trained word embedding Pyysalo</li></ul></li></ul><ul><li>POS embedding<ul><li>pE用于捕捉单词文法上的异同点相同POS tag的单词一般有相近的意思<ul><li>不同的POS tag哪怕有相近的形态也有不同的含义</li></ul></li></ul></li></ul><ul><li><p>Abbreviation embedding</p><ul><li><p>aE 判断一个token是不是一个缩写,如果是,找出缩写与 在FUS-PRGE,MeSH数据库的命名实体之间最大相似处的有关信息</p></li><li><p>首先对缩写使用AB3P,然后字符级-gram TF-IDF 生成FUS-PRGE,MeSH数据库中缩写的完整形式和每个概念名的向量,用于测量成对余弦相似度得分</p></li><li><p>aE(aD,aC,aGP)是一个三维向量</p><ul><li><p>aD是缩写与MeSH disease的最大相似性</p></li><li><p>aC是缩写与MeSH chemical的最大相似性</p></li><li><p>aGP是FUS-PRGE gene/protein name的最大相似性</p></li><li><p>为了让这个向量和其他embedding相适应,我们把他放进一个完全链接层,生成最终的5维向量</p></li></ul></li></ul></li></ul><ul><li><p>Character-level embedding</p><ul><li><p>eC代表了词汇在形态上的含义,例如(effective 和 effectiveness)</p></li><li><p>一个Character-level embedding模型,和Character to word embedding模型相似,被biLSTM建立,我们叫他CE - biLSTM模型</p><ul><li>Character set:76个实体,分别是26个字母的大小写和各种标点符号</li><li>每一个属于Character set的字符cj都被一个C lookup table所初始化为cej向量</li></ul></li><li><p>CE-biLSTM 把单个的token”ti”(有序的队列)作为输入,把Character-level的token ti 的embedding</p><p>ei作为输出,ei由forward LSTM和 backward LSTM构成</p></li></ul></li></ul><h4 id="4-3-2-CR-BiLSTM-layer-上下文表示-双向长短时记忆层"><a href="#4-3-2-CR-BiLSTM-layer-上下文表示-双向长短时记忆层" class="headerlink" title="4.3.2 CR-BiLSTM layer(上下文表示-双向长短时记忆层)"></a>4.3.2 CR-BiLSTM layer(上下文表示-双向长短时记忆层)</h4><p>Context Represent-biLSTM层,对于每个token ti,前向的LSTM 处理了前向的上下文向量表示的句子,后向的LSTM 处理了后向的上下文向量表示的句子</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1426_15_558.png" alt=""></p><h3 id="4-3-3-Project-层"><a href="#4-3-3-Project-层" class="headerlink" title="4.3.3 Project 层"></a>4.3.3 Project 层</h3><p>project 层接受CR-biLSTM层传进来的数据</p><p>project 第一层把550维的数据转换成275维</p><p>project第二层把275维的数据转换成9维(chemical/disease NER)/或5维(gene/protein NER)</p><p>在完全连接这一层之前,我们应用了批的正则化技术,这对性能的提升很大</p><h3 id="4-3-4-NER层"><a href="#4-3-4-NER层" class="headerlink" title="4.3.4 NER层"></a>4.3.4 NER层</h3><p>最上这一层使用的是条件随机字段技术(CRF),这一层用维克比算法给整句打上标签,</p><p>我们运用的打标签schema叫做 IOBES(inside,outside,beginning,ending,Singleton(独身))</p><p>一共有13个标签,分别是关于 diseases,chemicals,genes/proteins,    标签O表示不属于他们之间的任何一种</p><h3 id="4-3-5-超参数-hyper-parameters-和模型训练"><a href="#4-3-5-超参数-hyper-parameters-和模型训练" class="headerlink" title="4.3.5 超参数(hyper parameters)和模型训练"></a>4.3.5 超参数(hyper parameters)和模型训练</h3><p>本模型使用了</p><ul><li>TensorFlow</li><li>LSTM和D3NER都使用了RMSProp optimizer来优化学习率和动量值(momentum value):分别为0.0005和0.9</li><li>tanh function被用于所有的LSTM units的output </li><li>batch - padding using for pad the length of all tokens</li><li>mini batch training size被设置为128</li><li>预训练的TE(token embedding)设置为200个维</li><li>Character 被 initialized 为50维的向量</li><li>PE(part of speech embedding) – 25维</li><li>CE(character level embedding) –100维</li><li>AE(abbr embedding) – 5维</li></ul><p>避免过拟合,应用了dropout[S,2014]</p><ul><li>CE-biLSTM(character-level embedding)的final hidden layer : 0.5</li><li>CR-biLSTM(context-representing embedding)的final hidden layer : 0.15</li></ul><h3 id="4-4-LSTM网络简单释义"><a href="#4-4-LSTM网络简单释义" class="headerlink" title="4.4 LSTM网络简单释义"></a>4.4 LSTM网络简单释义</h3><p>LSTM网络是RNN的一个特定变体,它比RNN先进在它不受梯度爆炸或梯度消失的困扰,所以LSTM可以对长期依赖按序建模</p><p>LSTM之所以有这种性能,是因为通过带自适应选通机制的记忆细胞(memory cell)”ct” 获得了记忆能力</p><p>典型的LSTM有三个gate:</p><ul><li>forget gate ft</li><li>input gate it</li><li>output gate ot</li></ul><p>三个gate控制了LSTM之前留存的状态/现在输入和存储的data的特征</p><p>LSTM的隐藏层 在 时间 t的时候计算使用的公式如下所示<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1426_28_932.png" alt=""></p><h3 id="4-5CRF简单释义"><a href="#4-5CRF简单释义" class="headerlink" title="4.5CRF简单释义"></a>4.5CRF简单释义</h3><p>conditianl random field条件随机场是一个歧视的非直接的概念的图形化模型,把两个出色的标签序列化模型(隐性马尔科夫(HMMs)和最大熵马尔科夫模型(MeMM))的优点聚集在一起的模型</p><p>CRF学会:在给定了输入队列的情况下,最大化待打标签的对数似然的条件概率</p><p>公式表示如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1426_36_682.png" alt=""></p><h2 id="5-模型评价"><a href="#5-模型评价" class="headerlink" title="5.模型评价"></a>5.模型评价</h2><ul><li>与其他模型对比</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1426_44_245.png" alt=""></p><ul><li>各个embedding的影响<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1426_54_022.png" alt=""></li></ul><h2 id="6-errors"><a href="#6-errors" class="headerlink" title="6. errors"></a>6. errors</h2><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1427_49_849.png" alt=""></p><h2 id="7-结论"><a href="#7-结论" class="headerlink" title="7.结论"></a>7.结论</h2><p>D3NER被用在了3个标准测试数据集上,与现存的七个模型多了对比,表现可圈可点</p><p>提供了单独的可执行文件</p>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>D3NER</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_CRF条件随机场</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_CRF%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_CRF%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="条件随机场"><a href="#条件随机场" class="headerlink" title="条件随机场"></a>条件随机场</h1><ul><li><p>概率统计图概览</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1424_32_796.png" alt=""></p><ul><li><p>马尔科夫假设/马尔科夫性</p><ul><li>马尔科夫假设<ul><li>马尔科夫链 <img src="https://www.zhihu.com/equation?tex=%EF%BC%88x_%7B1%7D%2C%5Ccdots%2Cx_%7Bn%7D%29" alt="[公式]"> 里的 <img src="https://www.zhihu.com/equation?tex=+x_%7Bi%7D" alt="[公式]"> 总是只受 <img src="https://www.zhihu.com/equation?tex=+x_%7Bi-1%7D" alt="[公式]"> 一个人的影响。<br>马尔科夫假设这里相当于就是个2-gram。</li><li>马尔科夫过程<ul><li>在一个过程中，每个状态的转移只依赖于前n个状态</li></ul></li></ul></li><li>马尔科夫性<ul><li>马尔科夫性是保证或者判断概率图是否为概率无向图的条件<ul><li>成对性</li><li>局部性</li><li>全局性</li></ul></li></ul></li></ul></li><li><p>条件随机场定义</p><ul><li>条件随机场是在给定的随机变量<img src="https://www.zhihu.com/equation?tex=X" alt="[公式]"> （具体，对应观测序列 <img src="https://www.zhihu.com/equation?tex=o_%7B1%7D%2C+%5Ccdots%2C+o_%7Bi%7D" alt="[公式]"> ）条件下，随机变量 <img src="https://www.zhihu.com/equation?tex=Y" alt="[公式]"> （具体，对应隐状态序列 <img src="https://www.zhihu.com/equation?tex=i_%7B1%7D%2C+%5Ccdots%2C+i_%7Bi%7D" alt="[公式]"> 的马尔科夫随机场。</li><li>广义的CRF的定义是： 满足 <img src="https://www.zhihu.com/equation?tex=P%28Y_%7Bv%7D%7CX%2CY_%7Bw%7D%2Cw+%5Cneq+v%29+%3D+P%28Y_%7Bv%7D%7CX%2CY_%7Bw%7D%2Cw+%5Csim+v%29+" alt="[公式]"> 的马尔科夫随机场叫做条件随机场（CRF）</li><li>条件随机场是一种特殊的马尔科夫随机场<ul><li>马尔科夫随机场</li><li>首先我们有无向图G=(V,E)， 图G中每个节点v上都有一个随机变量y，这样所有的节点上的随机变量就构成一组随机变量Y，图G上有联合概率分布P(Y)。边e表示相邻节点的变量存在某种神秘的联系。</li><li>图G上的随机变量Y满足马尔科夫性，即两个不相邻的节点上的随机变量yi，yj条件独立。<br>这就是马尔科夫随机场。</li></ul></li></ul></li><li><p>CRF建模公式</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1424_50_420.png" alt=""></p><ul><li><p>CRF特征函数</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1425_01_425.png" alt=""></p></li><li><p><img src="http://q0u9fsub0.bkt.clouddn.com///20191112/FOwbPeI6IjxG.png?imageslim" alt="mark"></p></li><li><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1425_47_134.png" alt=""></p></li></ul></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CRF</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_词向量_n-gram</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_%E8%AF%8D%E5%90%91%E9%87%8F_n-gram/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_%E8%AF%8D%E5%90%91%E9%87%8F_n-gram/</url>
    
    <content type="html"><![CDATA[<h1 id="n-gram语言模型"><a href="#n-gram语言模型" class="headerlink" title="n-gram语言模型"></a>n-gram语言模型</h1><p>1.Statistical Language Model</p><p>在自然语言处理中的一个基本问题,如何计算一段文本序列在某某种语言下出现的概率?</p><p>例子</p><ul><li>我经常会去图书馆＿＿＿？<ul><li>预测该句后面的词，我们通常会根据已有的语料的上下文，来统计预测这句话可以填某个词汇的概率，将最大的概率作为结果返回</li></ul></li><li>机器翻译中,I like  Tomc so much,将单词逐个翻译—-{我,喜欢,汤姆,非常},这个集合中的字词排列组合成句子,然后用语言模型去计算组成句子概率的大小,概率越大越流畅</li></ul><p>2.n-gram语言模型</p><p>理解:</p><p>n-gram语言模型的思想,可以追溯到香农的问题:给定一串字母,比如”for ex”,下一个最可能出现的字母是什么?从训练语料中,我们可以通过极大似然估计的方法,得到N个概率分布,是”a”的概率是0.4,是”b”的概率是0.0001,是c的概率是….,and 别忘记约束条件:所有N个概率的分布总和为1</p><p>如下图,运用条件概率和乘法公式推倒:<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1439_24_435.png" alt=""></p><p>直接这么计算比较困难,需要引入马尔科夫假设,即,一个item的出现,只与前m个items有关,m = 0时,就是unigram</p><p>m=1时,就是bigram模型</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1439_40_759.png" alt=""></p><p>3.n-gram的数据格式</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1439_57_114.png" alt=""></p>]]></content>
    
    
    
    <tags>
      
      <tag>词向量</tag>
      
      <tag>n-gram</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2.比赛/指导_三个集成学习模型</title>
    <link href="undefined2020/02/24/2.%E6%AF%94%E8%B5%9B/%E6%8C%87%E5%AF%BC_%E4%B8%89%E4%B8%AA%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/"/>
    <url>2020/02/24/2.%E6%AF%94%E8%B5%9B/%E6%8C%87%E5%AF%BC_%E4%B8%89%E4%B8%AA%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="三个常用的集成学习模型"><a href="#三个常用的集成学习模型" class="headerlink" title="三个常用的集成学习模型"></a>三个常用的集成学习模型</h1><h4 id="常见集成学习模型一览图"><a href="#常见集成学习模型一览图" class="headerlink" title="常见集成学习模型一览图"></a>常见集成学习模型一览图</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200525_1241_36_530.png" alt=""></p><h4 id="集成学习的优点"><a href="#集成学习的优点" class="headerlink" title="集成学习的优点"></a>集成学习的优点</h4><p>采用多个分类器对数据集预测,提高整体分类器的泛化能力</p><h4 id="三种常见的集成学框架"><a href="#三种常见的集成学框架" class="headerlink" title="三种常见的集成学框架"></a>三种常见的集成学框架</h4><ul><li>bagging 装袋</li><li>boosting 提升</li><li>stacking 堆栈</li></ul><h2 id="bagging–装袋"><a href="#bagging–装袋" class="headerlink" title="bagging–装袋"></a>bagging–装袋</h2><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1436_45_170.png" alt=""></p><p>子训练集一般是各不相同的</p><p>基模型一般采用SVM或者朴素贝叶斯(大家一般采用同一种模型)</p><p>测试集扔给基模型们,然后各个基模型投票表决,简单多数为最终结果</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1437_03_239.png" alt=""></p><h2 id="Boosting提升"><a href="#Boosting提升" class="headerlink" title="Boosting提升"></a>Boosting提升</h2><p>第一次训练得到返回结果,然后给每一个结果分配权值,分类正确的权值降低,错误的权值上升</p><p>分类错误权值升高,在第二次训练时被重点关照</p><p>测试–测试集扔给各个样本,最后根据投票权值分配投票权,最终得到分配结果</p><h2 id="Stacking–堆叠"><a href="#Stacking–堆叠" class="headerlink" title="Stacking–堆叠"></a>Stacking–堆叠</h2><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1437_12_514.png" alt=""></p><ul><li>训练集分出n个基模型</li><li>集成方法:<ul><li>基础模型比如有100个,每个输出三维向量,一共就输出300维的向量</li><li>这个向量在堆叠模型那里训练</li><li>测试集也有三百维,最后生成测试数据让模型训练,得到最终结果</li></ul></li></ul><p>集成模型的偏差与方差</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1437_30_634.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>2.比赛</category>
      
    </categories>
    
    
    <tags>
      
      <tag>集成学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2.比赛/指导_数据探索&amp;预处理_CrowdFlower</title>
    <link href="undefined2020/02/24/2.%E6%AF%94%E8%B5%9B/%E6%8C%87%E5%AF%BC_%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2&amp;%E9%A2%84%E5%A4%84%E7%90%86_CrowdFlower/"/>
    <url>2020/02/24/2.%E6%AF%94%E8%B5%9B/%E6%8C%87%E5%AF%BC_%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2&amp;%E9%A2%84%E5%A4%84%E7%90%86_CrowdFlower/</url>
    
    <content type="html"><![CDATA[<h1 id="以CrowdFlower比赛为例讲解数据探索与预处理"><a href="#以CrowdFlower比赛为例讲解数据探索与预处理" class="headerlink" title="以CrowdFlower比赛为例讲解数据探索与预处理"></a>以CrowdFlower比赛为例讲解数据探索与预处理</h1><h4 id="比赛目标"><a href="#比赛目标" class="headerlink" title="比赛目标"></a>比赛目标</h4><p>​    衡量搜索结果的相关性</p><ul><li>比赛数据集<ul><li>CrowdFlower平台丰富的查询结果配对创建的</li><li>为了评估搜索相关性,CrowdFlower已经将261个搜索词与产品列表放在一起,要求人群对每个搜索结果评分,1,2,3,4分别表示搜索结果从完全不相关到完全相关</li></ul></li></ul><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><ul><li>train.csv训练集数据<ul><li>id 产品id</li><li>query 搜索词语</li><li>product_title 产品标题</li><li>product_description 产品描述文本</li><li>median_relevance 三位评分员的相关性评分中位数</li><li>relevance_variance 评分员的相关性评分方差</li></ul></li><li>test.csv<ul><li>id 产品id</li><li>query 搜索词语</li><li>product_description 产品描述文本</li></ul></li><li>目标变量<ul><li>median_relevance</li></ul></li></ul><h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1435_22_447.png" alt=""></p><ul><li>首先本数据以文字为主,文字只能输入进分类树模型,所以首先要把文字转换成数字</li><li>Dropping HTML标签</li><li>Word Replacement:然后要把拼写错误的单词替换掉</li><li>stemming:词干化:把过去时,进行时等等还原成词干 </li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1436_13_722.png" alt=""></p><ul><li>Counting Features :频率特征</li><li>DIstance features:距离特征</li><li>TF - IDF features:对频率特征的改进,如果频率太高可以通过取对数等方式进行改进</li><li>Query ID:查询的编号</li></ul><h3 id="3-1数据探索"><a href="#3-1数据探索" class="headerlink" title="3.1数据探索"></a>3.1数据探索</h3><ul><li><p>查看一眼文件头</p></li><li><p>查看列信息</p></li><li><p>查看有多少行</p></li><li><p>查看感兴趣的列信息去重后有多少项</p></li><li><p>查看感兴趣的信息去重后前一百项是什么</p></li><li><p>查看两个文件中,感兴趣的同一列有多少个是相同的,多少是不同的</p></li><li><p>对感兴趣的某一列信息,根据重复数画出柱状图</p></li></ul><pre><code class="python"># -*- coding: utf-8 -*-import pandas as pdimport numpy as npimport nltktrain = pd.read_csv(&quot;filePath&quot;)test = pd.read_csv(&quot;filePath&quot;)train.head()test.head()#查看文件头train.columnstest.columns#查看文件的列信息len(train)len(test)#查看文件有多少行train[&#39;query&#39;].unique()[0:100]#查看train这个文件中query的去重的前一百个数据train[&#39;product_title&#39;].unique()[0:10]#查看train这个文件中product_title的去重前十个元素len(train[&#39;query&#39;].unique())    len(test[&#39;query&#39;].unique())#查看train和test两个文件,去重之后一共有多少个数据len(train[&#39;product_title&#39;].unique())len(test[&#39;product_title&#39;].unique())#查看两个文件去重product_title的长度len(np.setdiff1d(test[&#39;product_title&#39;].unique(),train[&#39;product_title&#39;]))#查看两个文件中的product_title有多少个是不一样的len(np.intersect1d(test[&#39;product_title&#39;].unique(),train[&#39;product_title&#39;]))#查看两个文件中的product_title有多少个是一样的query = train[&#39;query&#39;].map(nltk.tokenize.word_tokenize)from nltk.corpus import stopwordsstopset = set(stopwords.words(&#39;english&#39;))def key_plot(data,col,top_num = 10):    s = data[col].map(nltk.tokenize.word_tokenize)    fdisk = nltk.FreqDist(words.lower() for x in s                          for words in x if words.lower()not in stopset)    #让数据集token化,统一小写,把stopwords(in at之类)去掉    top = pd.DataFrame(fdisk.most_common(top_num),columns=[&#39;query&#39;,&#39;time&#39;])    top = top.set_index(&#39;query&#39;)    top.plot(kind=&#39;bar&#39;) #按照query从大到小顺序排列key_plot(train,&#39;query&#39;)key_plot(test,&#39;query&#39;)#然后画图</code></pre><h3 id="3-2预处理"><a href="#3-2预处理" class="headerlink" title="3.2预处理"></a>3.2预处理</h3><ul><li><p>剔除HTML标签</p><ul><li>通过beautifulSoup4库提取HTML的文本信息</li></ul></li><li><p>单词替换</p><ul><li><p>拼写错误检查</p></li><li><p>同义词替换</p></li><li><p>其他替换</p></li><li><p>如下图</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1438_18_617.png" alt="">词干化</p></li><li><p>stemming:</p><ul><li>选择是porter还是snowball</li></ul></li></ul></li><li><p>数据清洗</p><ul><li>clean text:</li><li>drop html</li><li>repalce dic</li></ul></li></ul><pre><code class="python">#!/usr/bin/env python3# -*- coding: utf-8 -*-&quot;&quot;&quot;Created on Thu Oct 31 14:34:00 2019@author: bq____file____    nlp_utils.py____description___    The file provides functions to perform  NLP task,e.g.,TF-IDF and POS tagging__author__    BqLion copy from Chenglong Chen For Study use&quot;&quot;&quot;import reimport sysimport nltkfrom bs4 import BeautifulSoupfrom replacer import CsvWordReplacerfrom nltk import pos_tagfrom sklearn.feature_extraction.text import TfidVectorizer,CountVectorizersys.path.append(&quot;../&quot;)from param_config import config######stop words####stopwords  = nltk.corpus.stopwords.word(&quot;english&quot;)stopwords = set(stopwords)############stemming###########if config.stemmer_type == &quot;porter&quot;:    english_stemmer = nltk.stem.PorterStemmer()elif config.stemmer_type == &quot;snowball&quot;:    english_stemmer = nltk.stem.SnowballStemmer(&#39;english&#39;)def stem_tokens(tokens,stemmer):    stemmed = []    for token in tokens:        stemmed.append(stemmer.stem(token))    return stemmed##############POS tag###########token_pattern = r&quot;(?u)\b\w\w+\b&quot;# \b :bound # \w :any char# \+ :once or more def pos_tag_text(line,            token_pattern=token_pattern,            exclude_stopword=config.cooccurrence_word_exclude_stopword,            encode_digit=False):    token_pattern = re.compile(token_pattern,flags = re.UNICODE | re.LOCATE)    for name in [&quot;query&quot;,&quot;product_title&quot;,&quot;product_description&quot;]:        l = line[name]        ##tokenize        tokens = [x.lower()for x in token_pattern.findall(l)]        ##stem        tokens = stem_tokens(tokens,english_stemmer)        if exclude_stopword:            tokens = [x for x in tokens if x not in stopwords]        tags = pos_tag(tokens)        tags_list = [t for w,t in tags]        tags_str = &quot; &quot;.join(tags_list)        #print tags_str        line[name] = tags_str        return line###########TF-IDF#########class StemmedTfidfVectorizer(TfidVectorizer):    def build_analyzer(self):        analyzer = super(TfidVectorizer,self).build_analyzer()        return lambda doc:(english_stemmer.stem(w)for w in analyzer(doc))token_pattern = r&quot;(?u)\b\w\w+\b&quot;tfidf__norm = &quot;l2&quot;tfidf__max_df = 0.75tfidf__min_df = 3def getTFV(token_pattern = token_pattern,           norm = tfidf__norm,           max_df = tfidf__max_df,           min_df = tfidf__min_df,           ngram_range = (1, 1),           vocabulary = None,           stop_words = &#39;english&#39;):    tfv = StemmedTfidfVectorizer(min_df=min_df, max_df=max_df, max_features=None,                                  strip_accents=&#39;unicode&#39;, analyzer=&#39;word&#39;, token_pattern=token_pattern,                                 ngram_range=ngram_range, use_idf=1, smooth_idf=1, sublinear_tf=1,                                 stop_words = stop_words, norm=norm, vocabulary=vocabulary)    return tfv########### BOW ###########class StemmedCountVectorizer(CountVectorizer):    def build_analyzer(self):        analyzer = super(CountVectorizer, self).build_analyzer()        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))token_pattern = r&quot;(?u)\b\w\w+\b&quot;#token_pattern = r&#39;\w{1,}&#39;#token_pattern = r&quot;\w+&quot;#token_pattern = r&quot;[\w&#39;]+&quot;bow__max_df = 0.75bow__min_df = 3def getBOW(token_pattern = token_pattern,           max_df = bow__max_df,           min_df = bow__min_df,           ngram_range = (1, 1),           vocabulary = None,           stop_words = &#39;english&#39;):    bow = StemmedCountVectorizer(min_df=min_df, max_df=max_df, max_features=None,                                  strip_accents=&#39;unicode&#39;, analyzer=&#39;word&#39;, token_pattern=token_pattern,                                 ngram_range=ngram_range,                                 stop_words = stop_words, vocabulary=vocabulary)    return bow       ################## Text Clean #################### synonym replacerreplacer = CsvWordReplacer(&#39;%s/synonyms.csv&#39; % config.data_folder)## other replace dict## such dict is found by exploring the training datareplace_dict = {    &quot;nutri system&quot;: &quot;nutrisystem&quot;,    &quot;soda stream&quot;: &quot;sodastream&quot;,    &quot;playstation&#39;s&quot;: &quot;ps&quot;,    &quot;playstations&quot;: &quot;ps&quot;,    &quot;playstation&quot;: &quot;ps&quot;,    &quot;(ps 2)&quot;: &quot;ps2&quot;,    &quot;(ps 3)&quot;: &quot;ps3&quot;,    &quot;(ps 4)&quot;: &quot;ps4&quot;,    &quot;ps 2&quot;: &quot;ps2&quot;,    &quot;ps 3&quot;: &quot;ps3&quot;,    &quot;ps 4&quot;: &quot;ps4&quot;,    &quot;coffeemaker&quot;: &quot;coffee maker&quot;,    &quot;k-cups&quot;: &quot;k cup&quot;,    &quot;k-cup&quot;: &quot;k cup&quot;,    &quot;4-ounce&quot;: &quot;4 ounce&quot;,    &quot;8-ounce&quot;: &quot;8 ounce&quot;,    &quot;12-ounce&quot;: &quot;12 ounce&quot;,    &quot;ounce&quot;: &quot;oz&quot;,    &quot;button-down&quot;: &quot;button down&quot;,    &quot;doctor who&quot;: &quot;dr who&quot;,    &quot;2-drawer&quot;: &quot;2 drawer&quot;,    &quot;3-drawer&quot;: &quot;3 drawer&quot;,    &quot;in-drawer&quot;: &quot;in drawer&quot;,    &quot;hardisk&quot;: &quot;hard drive&quot;,    &quot;hard disk&quot;: &quot;hard drive&quot;,    &quot;harley-davidson&quot;: &quot;harley davidson&quot;,    &quot;harleydavidson&quot;: &quot;harley davidson&quot;,    &quot;e-reader&quot;: &quot;ereader&quot;,    &quot;levi strauss&quot;: &quot;levi&quot;,    &quot;levis&quot;: &quot;levi&quot;,    &quot;mac book&quot;: &quot;macbook&quot;,    &quot;micro-usb&quot;: &quot;micro usb&quot;,    &quot;screen protector for samsung&quot;: &quot;screen protector samsung&quot;,    &quot;video games&quot;: &quot;videogames&quot;,    &quot;game pad&quot;: &quot;gamepad&quot;,    &quot;western digital&quot;: &quot;wd&quot;,    &quot;eau de toilette&quot;: &quot;perfume&quot;,}     def clean_text(line,drop_html_flag=False):    names = [&quot;query&quot;,&quot;product_title&quot;,&quot;product_description&quot;]    for name in names:        l = line[name]        if drop_html_flag:            l = drop_html(l)        l = l.lower()        ## replace gb        for vol in[16,32,64,128,500]:            l = re.sub(&quot;%d gb&quot;%vol,&quot;%dgb&quot;%vol,l)            l = re.sub(&quot;%d g&quot;%vol,&quot;%dgb&quot;%vol,l)            l = re.sub(&quot;%dg&quot;%vol,&quot;%dgb&quot;%vol,l)        ## replace tb        for vol in [2]:            l = re.sub(&quot;%d tb&quot;%vol,&quot;%dtb&quot;%vol,l)        ## replace other words        for k,v in repalce_dict.item():            l = re.sub(k,v,l)            l = l.split(&quot; &quot;)        ## replace synonyms        l = replacer.replace(l)        l = &quot; &quot;.join(l)        line[name] = l    return line    ##########    ##drop html tag    ##########    def drop_html(html):        return BeautifulSoup(html).get(text(separator = &quot; &quot;))</code></pre>]]></content>
    
    
    <categories>
      
      <category>2.比赛</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据预处理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2.比赛/指导_常见算法与工具</title>
    <link href="undefined2020/02/24/2.%E6%AF%94%E8%B5%9B/%E6%8C%87%E5%AF%BC_%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95%E4%B8%8E%E5%B7%A5%E5%85%B7/"/>
    <url>2020/02/24/2.%E6%AF%94%E8%B5%9B/%E6%8C%87%E5%AF%BC_%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95%E4%B8%8E%E5%B7%A5%E5%85%B7/</url>
    
    <content type="html"><![CDATA[<h1 id="比赛常用算法与工具"><a href="#比赛常用算法与工具" class="headerlink" title="比赛常用算法与工具"></a>比赛常用算法与工具</h1><h3 id="1-1-本文提纲"><a href="#1-1-本文提纲" class="headerlink" title="1.1 本文提纲"></a>1.1 本文提纲</h3><ul><li>机器学习应用领域</li><li>机器学习常见算法</li><li>常用工具</li><li>建模与问题解决流程<ul><li>数据处理</li><li>特征工程</li><li>模型选择</li><li>寻找最佳超参数:交叉验证</li><li>模型分析与模型融合</li></ul></li><li>kaggle wiki</li><li>简单案例讲解</li></ul><h3 id="1-2机器学习常见算法"><a href="#1-2机器学习常见算法" class="headerlink" title="1.2机器学习常见算法"></a>1.2机器学习常见算法</h3><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1432_54_439.png" alt=""></p><h3 id="1-3机器学习常见工具"><a href="#1-3机器学习常见工具" class="headerlink" title="1.3机器学习常见工具"></a>1.3机器学习常见工具</h3><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1433_04_073.png" alt=""></p><ul><li>scikit - learn :速度不快,但是全面,封装的好,只需要造出来基本参数就可以自动去跑</li><li>gensim - 自然语言处理会用</li><li>NUmPy - 科学计算(封装到其他工具里了)</li><li>matplotlib - 绘图</li><li>pandas - 数据清洗,产出特征,缺省值,填充等</li><li>xgboost - 基于boost的库,分类和回归都可以完成</li><li>Natural Language Toolkit多用于英文的自然语言处理,中文用的很少</li><li>Jieba - 多用于中文语言处理</li><li>TensorFlow - 深度学习库,对显存的占用较高,速度不算太快</li><li>Caffe -深度学习库, 图像用的很多</li><li>Keras - 深度学习库,接口简单,本视频deep learning部分用Keras</li></ul><h3 id="1-4解决问题流程"><a href="#1-4解决问题流程" class="headerlink" title="1.4解决问题流程"></a>1.4解决问题流程</h3><ul><li>了解场景和目标</li><li>了解评估准则</li><li>认识数据</li><li>数据预处理(清洗,调权)</li><li>特征工程</li><li>模型调参</li><li>模型状态分析</li><li>模型融合</li></ul><h3 id="1-5-个重要的问题"><a href="#1-5-个重要的问题" class="headerlink" title="1.5 个重要的问题"></a>1.5 个重要的问题</h3><ul><li><p>拿到数据后怎么了解数据(可视化)</p><ul><li><p>肉眼看数据,尤其是维度较高的情况下是看不出什么东西的,而我们对图像的理解程度比数据高多了</p></li><li><p>例如数据可视化工具Seaborn,例如数据散列分布图和柱状图,这个可以使用Seanborn的pairplot完成</p></li><li><p>各个维度之间可以排列组合,然后看出哪些维度比较有区分度,如下图就11维和14维区分度较高</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1434_01_156.png" alt=""></p></li></ul></li><li><p>选择对贴切的机器学习算法</p><ul><li><p>数据大概看一眼,确定一些特征维度之后,可以用机器学习算法做一个baseline的系统出来</p></li><li><p>根据图谱选择</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1434_20_615.png" alt=""></p></li><li><p>确定算法的超参数</p><ul><li>例子:线性回归:多项式回归<ul><li>多项式的次数很高,就会过拟合<ul><li>过拟合就是模型太努力地去记住训练样本的分布状况,解决办法是增大样本量,或增强正则化作用</li></ul></li><li>次数很低,就会欠拟合<ul><li>解决办法是可以降低样本量,或减少正则化作用</li></ul></li></ul></li></ul></li></ul></li></ul><ul><li><p>定位模型状态,是过拟合还是欠拟合,以及解决办法</p></li><li><p>大量级的数据特征分析,和可视化</p></li><li><p>各种损失函数loss function的优缺点以及取舍</p></li></ul><h3 id="1-6通用机器学习流程与问题解决架构模板"><a href="#1-6通用机器学习流程与问题解决架构模板" class="headerlink" title="1.6通用机器学习流程与问题解决架构模板"></a>1.6通用机器学习流程与问题解决架构模板</h3><ul><li><p>数据转换</p><ul><li><p>在应用机器学习模型之前,所有的数据都必须转换为表格形式,如下图所示,这个过程最耗时,也最困难</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1434_30_463.png" alt=""></p></li><li><p>转换完成后,可以将数据灌入机器学习模型中,表格数据是机器学习中对常见的数据表示形式</p></li><li><p>一般用x表示数据,y表示标签</p></li></ul></li><li><p>标签的种类</p><ul><li>单列 - 分类问题,一个样本只属于一个类,而且一共有两个类</li><li>单列 - 实数制,回归问题,只预测一个值</li><li>多列 - 二进制,分类问题,一个样本只有一个类,总共有多个类</li><li>多列 - 实数制,回归问题,多个值的预测</li><li>多个标签 - 分类问题,一个样本可以属于几个类</li></ul></li><li><p>评估指标</p><ul><li>对任何类型的机器学习,我们都一定要知道如何评估结果<ul><li>例如不均衡的二进制分布问题,我们通常选择受试者工作特征曲线线下面积ROC AUC</li><li>对于多标签的分类问题,通常选择分类交叉熵或多类对数损失</li><li>对于回归问题,会选择均方差</li></ul></li></ul></li><li><p>库</p><ul><li>安装各种库</li><li>pandas,scikit - learn</li><li>xgboost</li><li>keras</li></ul></li><li><p>机器学习处理框架</p></li></ul><ul><li><p>第一步是识别问题,可以通过观察标签解决,一定要知道这个问题是二元分类,还是多种类或多标签分类,还是一个回归问题,当识别了问题,就可以把问题分类训练集和测试集两个部分,如下图所示</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1434_39_647.png" alt=""></p></li></ul><ul><li><p>数据预处理</p><ul><li>数据清洗<ul><li>不可信的样本丢掉</li><li>缺省值极多的字段考虑不用</li></ul></li><li>数据采样<ul><li>上/下采样</li><li>保证样本均衡</li></ul></li><li>工具<ul><li>hive sql</li><li>sparl sql</li><li>pandas</li></ul></li></ul></li><li><p>特征工程</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1434_50_075.png" alt=""></p></li><li><p>特征处理</p><ul><li>数值型</li><li>类别形</li><li>时间类</li><li>文本型</li><li>统计型</li><li>组合特征</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>2.比赛</category>
      
    </categories>
    
    
    <tags>
      
      <tag>比赛</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_极大似然估计</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/</url>
    
    <content type="html"><![CDATA[<h1 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h1><p><strong>极大似然估计是确定机器学习模型的参数的一种办法</strong>。确定参数值的过程，是找到能最大化“模型产生真实观察数据可能性“的那一组参数，略抽象，如下是一个例子：</p><p>从某过程观察了如下十个数据点，每个数据点代表了学生回答问题使用的秒数。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1155_08_403.png" alt=""></p><p>这些数据的生成过程可以使用高斯分布（正态分布）进行充分描述。高斯分布有两个参数，西格玛和μ，如何确定参数？如下示意图表示了使用不同参数的不同高斯分布（方差大的中心函数更扁平）。</p><p>注：蓝色曲线是正确曲线N(10,2.25)</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1157_24_376.png" alt=""></p><p>OK，如何反编译确定正确参数？我们把这个例子再次简化，同样的情境，这次只存在三个数据点：9,9.5,11</p><p>如何使用最大似然估计确定这个高斯分布的参数？</p><p>高斯分布中，单个数据点x的边缘概率如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1255_11_275.png" alt=""></p><p>同时观察到上边所提三个点（9,9.5,11）的联合概率是带入上边三个数据的连乘积：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1256_15_165.png" alt=""></p><p>我们只要能找到最大化上述连乘积的参数μ和西格玛就ok了。也就是说，最大似然估计是一个通过确定参数得到函数最大值的优化问题。</p><p>那么，如何求出上述函数的最大值？</p><p>easy，二元函数求偏导标准步骤，加以两边套上对数等数学小技巧就完事儿了。</p><p>SOP:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1300_07_942.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1300_26_097.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>极大似然估计</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_梯段下降算法</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_%E6%A2%AF%E6%AE%B5%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_%E6%A2%AF%E6%AE%B5%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h1 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h1><p>对于优化问题，机器学习的目标是使得某个损失函数最小。也就是找到一个x = min f(x)。但并不是所有的问题都能找到解析解，部分问题只能通过数值计算的方法逼近最优解 —— 一阶导数的梯度下降算法和二阶导数的牛顿方法。</p><ul><li>问题描述:有一个代价函数，它有两个参数，想让这个代价函数的值最小化。</li><li>做法：持续把这个两个参数向着梯度下降最快的方向迭代。<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1454_04_590.png" alt=""></li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1454_31_853.png" alt=""></p><ul><li>梯度下降算法的学习率α设置:<ul><li>α过小,收敛太慢</li><li>α过大,在最小值附近震荡</li></ul></li><li>梯度下降缺点:可能求的是局部最优解，解决办法是多次随机初始化起点。</li></ul><p>1.多特征值的回归问题</p><ul><li><p>单特征回归:只有房子面积一个特征,求预测房价</p><ul><li>单特征回归的假设函数:h(x) = θ0 + θ1x</li></ul></li><li><p>多特征回归:有房子面积,卧室数量,几层高,使用年限四个特征,求预测房价</p><ul><li>四特征回归的假设函数:h(x) = θ0 + θ1x1+θ2x2+θ3x3+θ4x4    —-&gt;   缩写h = θ(XT) :向量θ乘以向量X的转置</li><li>只考虑最简单的一次线性多项式</li></ul></li><li><p>多特征回归的梯度下降算法:</p><ul><li>每次对一个参数求偏导,并对其迭代.</li><li>所有参数都迭代这么一圈算完成一个回合.<ul><li>多参数迭代过程如下图</li><li><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1454_43_345.png" alt=""></li></ul></li></ul></li></ul><h4 id="2-梯度下降实用技巧–特征缩放-feature-scaling"><a href="#2-梯度下降实用技巧–特征缩放-feature-scaling" class="headerlink" title="2.梯度下降实用技巧–特征缩放(feature scaling)"></a>2.梯度下降实用技巧–特征缩放(feature scaling)</h4><ul><li><p>如果参数 θ1和θ2大小相差太远,那么θ1的轻微改变都可能让代价函数变化很大</p><ul><li><p>例如x1 = size(0 - 2000feet)</p></li><li><p>​         x2 = number of bedroom(1 - 5)</p></li><li><p>特征缩放:</p><ul><li><p>x1 / 2000</p></li><li><p>x2 / 5</p></li><li><p>让两个特征在一个数量级上</p></li><li><p>缩放前</p></li><li><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1456_02_307.png" alt=""></p></li><li><p>缩放后</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1456_12_601.png" alt=""></p></li></ul></li></ul></li><li><p>缩放后梯度下降曲线更加平滑,下降的步数更少</p></li><li><p>特征缩放让两个特征大小接近</p></li></ul><h4 id="3-梯度下降实用技巧–标准化"><a href="#3-梯度下降实用技巧–标准化" class="headerlink" title="3.梯度下降实用技巧–标准化"></a>3.梯度下降实用技巧–标准化</h4><ul><li><p>标准化:让每个特征数据都减去数据的平均值,除以数据的范围</p></li><li><p>如下图,u1就是平均值,S1就是[Xmax - Xmin]</p></li><li><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1456_39_454.png" alt=""></p></li></ul><ul><li>标准化等于移动了坐标轴,让特征均匀分布在[-0.5,0.5]区间</li></ul><h4 id="4-梯度下降实用技巧—设置学习率α"><a href="#4-梯度下降实用技巧—设置学习率α" class="headerlink" title="4.梯度下降实用技巧—设置学习率α"></a>4.梯度下降实用技巧—设置学习率α</h4><ul><li>梯度下降:特征值在导数乘以学习率的方向上的迭代</li><li>如下图</li><li><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1456_49_542.png" alt=""></li><li>学习率过大可能冲过最优解,在附近震荡</li><li>学习率过小可能收敛速度过慢</li><li>具体如何设置学习率吴恩达建议用等比数列尝试:[1,0.1,0.01,0.001,0.0001,etc]，有点玄学调参的感觉。</li></ul><h4 id="5-BGD：批量梯度下降算法"><a href="#5-BGD：批量梯度下降算法" class="headerlink" title="5.BGD：批量梯度下降算法"></a>5.BGD：批量梯度下降算法</h4><p>特点：每次迭代把所以样本都送入，做的是全局最优化，缺点是速度慢</p><p>代码如下</p><pre><code class="python">#-*- coding: utf-8 -*-  import random  #用y = Θ1*x1 + Θ2*x2来拟合下面的输入和输出  #input1  1   2   5   4  #input2  4   5   1   2  #output  19  26  19  20  input_x = [[1,4], [2,5], [5,1], [4,2]]  #输入  y = [19,26,19,20]   #输出  theta = [1,1]       #θ参数初始化  loss = 10           #loss先定义一个数，为了进入循环迭代  step_size = 0.01    #步长  eps =0.0001         #精度要求  max_iters = 10000   #最大迭代次数  error =0            #损失值  iter_count = 0      #当前迭代次数  err1=[0,0,0,0]      #求Θ1梯度的中间变量1  err2=[0,0,0,0]      #求Θ2梯度的中间变量2  while( loss &gt; eps and iter_count &lt; max_iters):   #迭代条件      loss = 0      err1sum = 0      err2sum = 0      for i in range (4):     #每次迭代所有的样本都进行训练          pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1]  #预测值          err1[i]=(pred_y-y[i])*input_x[i][0]          err1sum=err1sum+err1[i]          err2[i]=(pred_y-y[i])*input_x[i][1]          err2sum=err2sum+err2[i]      theta[0] = theta[0] - step_size * err1sum/4  #对应5式      theta[1] = theta[1] - step_size * err2sum/4  #对应5式      for i in range (4):          pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1]   #预测值          error = (1/(2*4))*(pred_y - y[i])**2  #损失值          loss = loss + error  #总损失值      iter_count += 1      print (&quot;iters_count&quot;, iter_count)  print (&#39;theta: &#39;,theta )  print (&#39;final loss: &#39;, loss)  print (&#39;iters: &#39;, iter_count)  </code></pre><h4 id="6-SGD-随机梯度下降算法"><a href="#6-SGD-随机梯度下降算法" class="headerlink" title="6.SGD:随机梯度下降算法"></a>6.SGD:随机梯度下降算法</h4><p>针对BGD训练速度过慢的情况，提出了SGD算法。这种算法每次从样本中随机抽出一组，训练后按梯度更新一次，然后重新抽取一组，再更新一次。在样本量很大时，能得到一个可损失值和耗时都在接受范围内的结果。</p><p>代码如下</p><pre><code class="python">#-*- coding: utf-8 -*-  import random  #用y = Θ1*x1 + Θ2*x2来拟合下面的输入和输出  #input1  1   2   5   4  #input2  4   5   1   2  #output  19  26  19  20  input_x = [[1,4], [2,5], [5,1], [4,2]]  #输入  y = [19,26,19,20]   #输出  theta = [1,1]       #θ参数初始化  loss = 10           #loss先定义一个数，为了进入循环迭代  step_size = 0.01    #步长  eps =0.0001         #精度要求  max_iters = 10000   #最大迭代次数  error =0            #损失值  iter_count = 0      #当前迭代次数  while( loss &gt; eps and iter_count &lt; max_iters):    #迭代条件      loss = 0      i = random.randint(0,3)  #每次迭代在input_x中随机选取一组样本进行权重的更新      pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1] #预测值      theta[0] = theta[0] - step_size * (pred_y - y[i]) * input_x[i][0]      theta[1] = theta[1] - step_size * (pred_y - y[i]) * input_x[i][1]      for i in range (3):          pred_y = theta[0]*input_x[i][0]+theta[1]*input_x[i][1] #预测值          error = 0.5*(pred_y - y[i])**2          loss = loss + error      iter_count += 1      print (&#39;iters_count&#39;, iter_count)  print (&#39;theta: &#39;,theta )  print (&#39;final loss: &#39;, loss)  print (&#39;iters: &#39;, iter_count)  </code></pre>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>梯度下降算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_判定模型和生成模型</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_%E5%88%A4%E5%AE%9A%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_%E5%88%A4%E5%AE%9A%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="判定模型和生成模型的区别"><a href="#判定模型和生成模型的区别" class="headerlink" title="判定模型和生成模型的区别"></a>判定模型和生成模型的区别</h1><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1126_11_341.png" alt=""></p><p>机器学习的任务是从属性X预测标记Y，即是求概率P（Y|X）</p><ul><li>判别式模型是上图左边示例，有个明显的边界，新来一个值需要判断他属于哪一类的时候直接算出他的score，当score大于threshold时为正类，反之为负类。线性回归，SVM模型都是典型的判别式模型</li><li>生成式模型是上图右边示例，无明显边界，新来一个值要判断他是哪一类的时候，首先求该值与两个不同标记的不同联合概率分布，然后大的获胜。朴素贝叶斯模型，HMM模型都是生成式模型。</li></ul><p>一个生动的例子说明两者的区别:</p><ul><li>判别式模型：要确定一个羊是山羊还是绵羊，用判别式的方法是从历史数据中学到模型（运行同一个模型得到确定的结果），然后通过提取这只羊的特征来预测出羊的类型。</li><li>生成式模型：根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模型。然后提取这只待判定羊的特征，放到山羊模型中看看概率是多少，再放到绵羊模型中看看概率是多少。哪个大就是哪个。（两个模型，两个结果，最后比比数值大小得出结论）</li></ul>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>判定模型</tag>
      
      <tag>生成模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_支持向量机</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="支持向量机SVM"><a href="#支持向量机SVM" class="headerlink" title="支持向量机SVM"></a>支持向量机SVM</h1><h3 id="9-1SVM的优化目标"><a href="#9-1SVM的优化目标" class="headerlink" title="9.1SVM的优化目标"></a>9.1SVM的优化目标</h3><p>从逻辑回归展示如何一点点修改得到本质上的支持向量机</p><ul><li><p>逻辑回归的假设函数<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1448_37_663.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1449_13_463.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1449_43_262.png" alt=""></p></li></ul><h3 id="9-2核函数"><a href="#9-2核函数" class="headerlink" title="9.2核函数"></a>9.2核函数</h3><p>对支持向量机算法做一些修改,以构造复杂的非线性分类器</p><p>我们用”核函数”来达到此目的</p><p>问题的提出:</p><p>使用高级数的多项式模型来解决无法使用直线进行分割的分类问题,如何确定模式中的每一项的参数?<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1450_05_097.png" alt=""></p><p>支持向量机的假设函数和代价函数</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1450_25_811.png" alt=""></p><h3 id="9-3-SVM的使用"><a href="#9-3-SVM的使用" class="headerlink" title="9.3 SVM的使用"></a>9.3 SVM的使用</h3><p>不建议自己写代码求解参数θ,就像没有人会写代码自己去求解平方根一样,可以直接调用现有的库</p><p>除了高斯核函数之外还有其他核函数可以用:</p><p>多项式核函数（<strong>Polynomial Kerne</strong>l）</p><p>字符串核函数（<strong>String kernel</strong>）</p><p>卡方核函数（ <strong>chi-square kernel</strong>）</p><p>直方图交集核函数（<strong>histogram intersection kernel</strong>）</p><p>等等</p><p>SVM模型 和 逻辑回归模型之间的取舍:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1450_36_327.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SVM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS229_8.应用机器学习的建议</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_8.%E5%BA%94%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BB%BA%E8%AE%AE/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_8.%E5%BA%94%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BB%BA%E8%AE%AE/</url>
    
    <content type="html"><![CDATA[<h1 id="8-应用机器学习的建议"><a href="#8-应用机器学习的建议" class="headerlink" title="8.应用机器学习的建议"></a>8.应用机器学习的建议</h1><h4 id="8-1如何提高一个算法-机器学习模型的性能"><a href="#8-1如何提高一个算法-机器学习模型的性能" class="headerlink" title="8.1如何提高一个算法/机器学习模型的性能"></a>8.1如何提高一个算法/机器学习模型的性能</h4><p>问题:预测房价的机器学习模型性能遇见瓶颈,如何提升性能?</p><ul><li><p>很多人是凭感觉去解决问题,他们有如下猜想并随便选一个去做</p></li><li><p>随便某个猜想都是耗时耗力巨大的项目,人们常常选择的是一条不归路</p></li><li><p>如何尽量排除无效的道路?</p></li></ul><ol><li>获得更多的训练样本——解决高方差   </li><li>尝试减少特征的数量——解决高方差</li><li>尝试获得更多的特征——解决高偏差</li><li>尝试增加多项式特征——解决高偏差</li><li>尝试减少正则化程度λ——解决高偏差</li><li>尝试增加正则化程度λ——解决高方差</li></ol><h3 id="8-2如何评估一个模型表现"><a href="#8-2如何评估一个模型表现" class="headerlink" title="8.2如何评估一个模型表现"></a>8.2如何评估一个模型表现</h3><ul><li><p>模型的代价函数非常小未必是好事,因为可能存在过拟合的现象</p><p>为了检验模型是否过拟合,可以采用交叉验证的方法</p><p>可以把数据集分为训练集和测试集,然后重复洗牌交叉验证</p></li><li><p>高偏差与高方差</p><ul><li><p>高偏差是欠拟合</p></li><li><p>高方差是过拟合</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1448_17_292.png" alt=""></p></li></ul></li><li><p>增加模型的多项式次数导致的过/欠拟合问题</p><ul><li>训练集上:<ul><li>增加模型多项式的次数会导致代价函数单调递减</li></ul></li><li>交叉验证集上:<ul><li>增加模型多项式的次数会导致代价函数先减后增</li><li>先减是改善了欠拟合的问题</li><li>后增就是过拟合越来越严重</li></ul></li><li>结论:<ul><li>交叉验证是评估过/欠拟合的有效手段</li></ul></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>评估模型表现</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS229_5.过拟合和正则化</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_5.%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_5.%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AD%A3%E5%88%99%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h3 id="1-过拟合和欠拟合问题"><a href="#1-过拟合和欠拟合问题" class="headerlink" title="1.过拟合和欠拟合问题"></a>1.过拟合和欠拟合问题</h3><p>两张插图说明白</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1444_50_140.png" alt=""></p><p><img src="http://q0u9fsub0.bkt.clouddn.com///20191112/wVpA8KOfcEdf.png?imageslim" alt="mark"></p><p>解决办法:</p><p>过拟合就丢弃不能正确预测的特征,欠拟合就加特征</p><h3 id="2-代价函数的惩罚项"><a href="#2-代价函数的惩罚项" class="headerlink" title="2.代价函数的惩罚项"></a>2.代价函数的惩罚项</h3><p>由下图可以看出,过拟合由于高维项造成</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1445_01_605.png" alt=""></p><p><img src="http://q0u9fsub0.bkt.clouddn.com///20191112/tmeD1LOk8k1Q.png?imageslim" alt="mark"></p><ul><li><p>给高维项设置惩罚项</p><ul><li><p>代价函数里给三次项和四次项增加一些重量,梯度下降的时候系统就会更急切想把三次项和四次项降下来</p></li><li><p>增加了惩罚项的代价函数<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1445_12_545.png" alt=""></p></li></ul></li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1445_57_673.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS229_6.神经网络(表述)</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_6.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(%E8%A1%A8%E8%BF%B0)/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_6.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(%E8%A1%A8%E8%BF%B0)/</url>
    
    <content type="html"><![CDATA[<p>#</p><h1 id="6-神经网络-表述"><a href="#6-神经网络-表述" class="headerlink" title="6.神经网络(表述)"></a>6.神经网络(表述)</h1><h3 id="6-1问题的由来"><a href="#6-1问题的由来" class="headerlink" title="6.1问题的由来"></a>6.1问题的由来</h3><ul><li>特征爆炸无法处理<ul><li>例如,智能识别一个图片是不是汽车的问题</li><li>假设使用50*50小黑白照片</li><li>那么就有2500个像素灰度作为特征</li><li>2500个特征两两组合就是3000000项,这的多项式处理不了</li><li>这时候就需要神经网络</li></ul></li></ul><h3 id="6-2直观理解和逻辑运算符的构建"><a href="#6-2直观理解和逻辑运算符的构建" class="headerlink" title="6.2直观理解和逻辑运算符的构建"></a>6.2直观理解和逻辑运算符的构建</h3><ul><li><p>直观理解</p><ul><li>神经网络能够通过自身学习得到一些列特征,这些特征是使用数据的原始特征经过拆分组合和一系列逻辑运算得到的</li><li>这样的一系列特征比普通的逻辑回归的表层特征要深刻的多</li><li>最后做决定的神经元依据上一层传进来的特征,这些特征已经经过多层传导,内化为自己的特征了</li></ul></li><li><p>神经元可以通过增加一个固定权重神经元构建逻辑运算符(与,非,或,抑或等)</p><ul><li>如下图</li></ul></li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1446_26_856.png" alt=""></p><h3 id="6-3神经网络的形态"><a href="#6-3神经网络的形态" class="headerlink" title="6.3神经网络的形态"></a>6.3神经网络的形态</h3><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1446_40_056.png" alt=""></p><h4 id="6-4神经网络基本概念的定义from-CS224N"><a href="#6-4神经网络基本概念的定义from-CS224N" class="headerlink" title="6.4神经网络基本概念的定义from CS224N"></a>6.4神经网络基本概念的定义from CS224N</h4><p>如图，神经网络有输入，有偏置单元，有激活函数，以及对后续神经网络的输出。</p><p>输入x，w是乘以输入的权重，b是偏置值，f是sigmod函数。</p><p>输入乘以权重加上偏置值后，进入sigmod函数，输出一个分类值。</p><p>有了神经网络，让我们再回到之前的单窗口分类器。代替之前直接将softmax应用在词向量，现在我们在词向量和输入之间有了中间隐藏层，这正是我们真正获得准确性和表现力的开端。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1418_52_588.png" alt=""></p><p>如上图，x是输入，是有20维的窗口，输出是8维，所以W参数矩阵时8 * 20的规模。</p><p>然后s最终的分类结果是一个单独的数字，所以U矩阵是 8*1的规模</p><p><strong>最大间距损失函数</strong></p><p>最大间距损失函数比softmax的交叉熵误差更具有鲁棒性。softmax得出的决策边界如下图所示，再数据集加入一些靠近边界的数据时，本边界会显得鲁棒性不足</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1419_00_261.png" alt=""></p><p>最大间距损失函数和svm的边界类似，它在计算边界时得出的是距离两类数据都最远的边界。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1419_09_277.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS229_7.神经网络(学习)</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_7.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(%E5%AD%A6%E4%B9%A0)/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_7.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(%E5%AD%A6%E4%B9%A0)/</url>
    
    <content type="html"><![CDATA[<h1 id="7-神经网络-学习"><a href="#7-神经网络-学习" class="headerlink" title="7.神经网络(学习)"></a>7.神经网络(学习)</h1><h3 id="7-1神经网络的代价函数"><a href="#7-1神经网络的代价函数" class="headerlink" title="7.1神经网络的代价函数"></a>7.1神经网络的代价函数</h3><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1447_04_343.png" alt=""></p><ul><li>第一项是在K个输出项上(比如四分类就是K=4),累加逻辑回归的代价函数</li><li>第二项就是更高维的罚函数</li></ul><p>代价函数的目的:找到使得J(θ)最小化的θ</p><h3 id="7-2单个训练数据如何通过神经网络"><a href="#7-2单个训练数据如何通过神经网络" class="headerlink" title="7.2单个训练数据如何通过神经网络"></a>7.2单个训练数据如何通过神经网络</h3><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1447_17_262.png" alt="">    </p><ul><li><p>z(2) = θ(1)a(1) </p><ul><li>z(2)是第二层神经元的输入项,共五项,是a(1)的三项经过θ(1)矩阵拆分后的结果</li></ul></li><li><p>a(2) = g[z(2)]</p><ul><li>a(2)是第二层神经元的输出项,共五项,是z(2)五项经过第二层神经元的sigmod函数变换后的结果</li></ul></li></ul><p>后边的几层同理</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_PCA主成分分析</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h1 id="PCA主成分分析"><a href="#PCA主成分分析" class="headerlink" title="PCA主成分分析"></a>PCA主成分分析</h1><h4 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h4><p>机器学习有一类问题是有损压缩，即我们对原数据进行一些压缩以减少存储空间，这个过程不可避免的会损失一些原有的信息，希望能尽量减少信息丢失。</p><p>假设有三个三维空间的点x1，x2，x3，希望找到降维后二维空间的点t1，t2，t3。</p><p>将三个点从三维空间映射到二维空间的是<strong>压缩函数f（x）= t</strong></p><p>将三个点从二维空间映射到回三维空间的是<strong>解压函数g（t）= x</strong>，对应的，若使用矩阵乘法来解压缩，则g(t) = <strong>D</strong>t = x</p><p>其中<strong>D</strong>就是解压缩矩阵。</p><p>例如                           </p><pre><code>x1   [1,2,3                t1  [1,2x2    1,2,3     =   D  *   t2   1,2x3    1,2,3]               t3   1,2]</code></pre><p>​              ↑压缩前数据   ↑解压缩矩阵    ↑压缩后数据</p><p>​           【  ↑ X      】      【               g(t)               】              </p><p>为了简化问题，PCA规定解压缩矩阵D必须是正交归一的。</p><p>有损压缩的问题的代价函数：</p><pre><code>C = min ( || x - g(t) || )^2C = min ( x - g(t) ) * 【( x - g(t) )T】 </code></pre><p>也就是最小化 <strong>原始数据</strong>和<strong>压缩后又解压缩数据</strong> 之间的二范数的平方。</p><h4 id="从表征空间理解PCA"><a href="#从表征空间理解PCA" class="headerlink" title="从表征空间理解PCA"></a>从表征空间理解PCA</h4><p>PCA还可理解为一种学习数据表征的无监督学习方法。</p><p>原数据<strong>x矩阵</strong>，其<strong>无偏的协方差矩阵Var[x]</strong>可表示为：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200517_1527_39_450.png" alt=""></p><p>而PCA的目的就是要找到一个线性变换的方式z：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200517_1528_25_776.png" alt=""></p><p>使得<strong>z压缩后数据矩阵</strong>的协方差矩阵<strong>Var[z]</strong>是对角的，从而在新的表征空间的元素间是无关的。</p><p>对原始数据x矩阵做奇异值分解：</p><pre><code>X = W * Σ * WT,其中W和WT都是正交矩阵，Σ是对角矩阵。</code></pre><p>要求x的无偏协方差矩阵：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200517_1527_39_450.png" alt=""></p><p>其中的 XT * X为：</p><p><img src="https://www.zhihu.com/equation?tex=X%5ETX%3D%28U%5CSigma+W%5ET%29%5ETU%5CSigma+W%5ET%3DW%5CSigma+%5E2+W%5ET" alt="[公式]"></p><p>所以x的无偏协方差矩阵是：<img src="https://www.zhihu.com/equation?tex=Var%5Bx%5D%3D%5Cfrac%7B1%7D%7Bm-1%7DW%5CSigma+%5E2+W%5ET" alt="[公式]"></p><p>那么z压缩后数据的无偏协方差矩阵是<img src="https://www.zhihu.com/equation?tex=Var%28z%29%3D%5Cfrac%7B1%7D%7Bm-1%7DZ%5ETZ+%3D+%5Cfrac%7B1%7D%7Bm-1%7DW%5ETX%5ETXW+%3D+%5Cfrac%7B1%7D%7Bm-1%7DW%5ETW%5CSigma+%5E2+W%5ETW%3D%5Cfrac%7B1%7D%7Bm-1%7D%5CSigma+%5E2" alt="[公式]"></p><p>也就是说<strong>z压缩后数据的无偏协方差矩阵</strong>是<strong>x原始数据无偏协方差矩阵</strong>的<strong>奇异值分解并刨除左右两边的W矩阵</strong></p><p>总结：可以看出经过PCA转换后，协方差只有对角上才有值，所以z的元素间是线性无关的，相当于对原空间做了一个旋转（通过矩阵W)，使得方差大的方向落在新空间的各个主轴上，如下图所示，在原空间中，方差最大的方向并不是在轴线上，经过PCA的线性转换后，在新空间 <img src="https://www.zhihu.com/equation?tex=z%3DW%5ETx" alt="[公式]"> 中方差最大的方向沿着 <img src="https://www.zhihu.com/equation?tex=z_1" alt="[公式]"> 轴方向，而次大的沿着 <img src="https://www.zhihu.com/equation?tex=z_2" alt="[公式]"> 轴方向。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200517_1542_50_952.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>PCA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS229_2.正规方程</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_2.%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_2.%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h1 id="CS229—正规方程"><a href="#CS229—正规方程" class="headerlink" title="CS229—正规方程"></a>CS229—正规方程</h1><p>1.定义</p><p>​    正规方程(Normal Equation)是通过对训练数据/训练答案/待求参数之间的矩阵变换得到答案</p><ul><li>正规方程求解参数矩阵的公式:<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1442_35_105.png" alt=""></li></ul><p>2.正规方程法和梯度下降法的对比</p><ul><li><p>梯度下降的特点</p><ul><li>要设置学习率α</li><li>多次迭代</li><li>工作地非常均衡</li><li>适合特征变量很多的时候用</li><li>要求参数归一化</li></ul></li><li><p>正规方程法</p><ul><li>不用设置学习率</li><li>不用迭代</li><li>需要计算</li><li>适合特征变量不超过1000时用</li><li>不要求参数归一化,如下图</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1442_51_944.png" alt=""></p><ul><li>有些较复杂问题不能使用正规方程法,只能使用梯度下降</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>正规方程</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS229_10.聚类Clustering</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_10.%E8%81%9A%E7%B1%BBClustering/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_10.%E8%81%9A%E7%B1%BBClustering/</url>
    
    <content type="html"><![CDATA[<h1 id="10-聚类Clustering"><a href="#10-聚类Clustering" class="headerlink" title="10.聚类Clustering"></a>10.聚类Clustering</h1><h2 id="10-1无监督学习简介"><a href="#10-1无监督学习简介" class="headerlink" title="10.1无监督学习简介"></a>10.1无监督学习简介</h2><p>无监督学习是让计算机学习无标签数据,而不是之前的标签数据</p><p>如图就是聚类问题<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1451_06_446.png" alt=""></p><p>左上第一张图是市场分割,就是根据数据库里沉淀的客户信息来对客户分类,做知识图谱和推荐系统</p><h2 id="10-2-K-mean-K均值算法"><a href="#10-2-K-mean-K均值算法" class="headerlink" title="10.2 K-mean (K均值算法)"></a>10.2 K-mean (K均值算法)</h2><p>K均值是最普及的一个聚类算法,算法接受一个数据集,将数据聚类为不同的组</p><ul><li><p>K-均值是一个迭代算法</p><ul><li><p>设想我们将数据聚集成n组,选择k个随机的点为聚类中心</p></li><li><p>对数据集中的每个数据,按照距离k个中心点的距离,将其与距离最近的点关联起来,与这个点聚为一类</p></li><li><p>然后开始迭代:</p><ul><li>每次计算一个组的平均值,将中心点移动到平均值的位置</li><li>如此反复几次中心点的位置就不再变化</li><li>如下图所示<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1451_27_402.png" alt=""></li></ul></li></ul></li></ul><h2 id="10-3优化目标"><a href="#10-3优化目标" class="headerlink" title="10.3优化目标"></a>10.3优化目标</h2><p>K-均值的优化目标就是最小化所有的数据点与其关联的聚类中心点之间的距离之和</p><p>因此K均值的代价函数为:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1451_42_387.png" alt=""></p><h2 id="10-4随机初始化"><a href="#10-4随机初始化" class="headerlink" title="10.4随机初始化"></a>10.4随机初始化</h2><p>在运行k均值算法之前,需要随机初始化所有的聚类中心点</p><p>1.选择K &lt; m ,即聚类中心点的个数要小于所有训练集实例的数量</p><p>2.随机选择K个训练实例,然后令K个聚类中心等于他们</p><p>K-均值算法的一个问题在于,他可能停留在某个局部最小值处,这取决于初始化的位置(类似于梯度下降算法的初始位置选取),为了解决此问题,可以多次运行K-均值算法,每次都随机初始化,然后比较各自结果,选取最小的值,当然如果K值较大结果不会有明显改善</p><h2 id="10-5选择聚类数"><a href="#10-5选择聚类数" class="headerlink" title="10.5选择聚类数"></a>10.5选择聚类数</h2><p>聚类的数量一般是人工选取,这时候需要思考运用K-均值算法聚类的动机是什么,最好能服务于该目标的聚类数</p><p>聚类数量选择的时候有个肘部法则,例如K值设置为1,2,3,4,5的时候发现3的效果最好,更多或者更少都不行,像人的肘关节那样有个拐点,通常这个肘部就是最优的点</p><p>比如T恤制造可以选择S,M,L三个尺码也可以选择 S,M,L,XL,XXL五个尺码一样,K选3还是5取决于具体的业务场景</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1451_56_417.png" alt=""></p><h2 id="10-6聚类的衡量指标"><a href="#10-6聚类的衡量指标" class="headerlink" title="10.6聚类的衡量指标"></a>10.6聚类的衡量指标</h2><ol><li><p>均一性:P</p><p>可以认为是正确率(每个聚簇中正确分类的样本数占该聚簇总样本数的比例和)</p></li><li><p>完整性:r</p><p>类似于召回率，同类别样本被归类到相同簇中，则满足完整性;每个聚簇中正确分类的样本数占该 类型的总样本数比例的和</p></li><li><p>V-measure:均一性和完整性的加权</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1452_07_748.png" alt=""></p></li></ol>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>聚类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS229_4.逻辑回归</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_4.%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_4.%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h1 id="4-逻辑回归-logistics-regression"><a href="#4-逻辑回归-logistics-regression" class="headerlink" title="4.逻辑回归(logistics regression)"></a>4.逻辑回归(logistics regression)</h1><h3 id="1-逻辑回归模型"><a href="#1-逻辑回归模型" class="headerlink" title="1.逻辑回归模型"></a>1.逻辑回归模型</h3><ul><li><p>分类问题输出一个结果”正确”或者”错误”</p></li><li><p>逻辑回归模型:</p></li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1443_44_904.png" alt=""></p><ul><li>当hθ(x) 在区间[0.5,1]时,预测是”正确”,越贴近1越确信</li><li>当hθ(x) 在区间[0,0.5)时,预测是”错误”,越贴近0越确信</li></ul><h3 id="2-逻辑回归的代价函数"><a href="#2-逻辑回归的代价函数" class="headerlink" title="2.逻辑回归的代价函数"></a>2.逻辑回归的代价函数</h3><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1443_56_769.png" alt=""></p><ul><li>简单点一句话讲:<ul><li>答案是1,你预测了1,无误差</li><li>答案是1,你预测的数字离0越近误差就越大,完全是0误差就是无限大(不可能情况)</li><li>答案是0的情况与答案是1的情况同理</li></ul></li></ul><h3 id="3-对逻辑回归的代价函数使用梯度下降算法-以得到最优参数-最优解"><a href="#3-对逻辑回归的代价函数使用梯度下降算法-以得到最优参数-最优解" class="headerlink" title="3.对逻辑回归的代价函数使用梯度下降算法,以得到最优参数/最优解"></a>3.对逻辑回归的代价函数使用梯度下降算法,以得到最优参数/最优解</h3><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1444_07_614.png" alt=""></p><h3 id="4-高级优化"><a href="#4-高级优化" class="headerlink" title="4.高级优化"></a>4.高级优化</h3><ul><li><p>一些建议</p><ul><li><p>不需要写代码实现代价函数的迭代</p><ul><li>从技术来讲,其实我们不需要自己手动写程序来计算刚才提到的梯度下降算法的代价函数以及其迭代过程,就像我们不需要自己手动写求平方根和创建数组一样,这些问题早都有非常成熟的库来调用</li></ul></li><li><p>更高级的算法</p><ul><li>除了梯度下降算法,还有<strong>共轭梯度法 BFGS</strong> (<strong>变尺度法</strong>) 和<strong>L-BFGS</strong> (<strong>限制变尺度法</strong>)这种更高级算法可调用</li><li>吴恩达本人已经使用如上提到的算法很多年了,也才是最近才搞清楚他们的内部实现细节</li></ul></li></ul></li></ul><h3 id="5-一对多分类问题"><a href="#5-一对多分类问题" class="headerlink" title="5.一对多分类问题"></a>5.一对多分类问题</h3><ul><li>逻辑回归是二选一分类问题,只用输出0或者1</li><li>还有一对多分类问题,输出结果可能是1,2,3,4,5<ul><li>对应的业务场景就是预测收到的邮件要自动打上什么样的标签?(重要,垃圾,家人,朋友,公司等)</li></ul></li></ul><ul><li>一对多分类问题的解决思想<ul><li>一对多假如有三个标签A,B,C,则采用一对余的思想来分类</li><li>具体就是把A打上”正确”标签,BC都是错误标签</li><li>B,C也是同理操作</li><li>然后就可以应用上文所说的二分法工具来做了(逻辑回归)</li></ul></li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1444_21_852.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>逻辑回归</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224n_9 - RNN,LSTM,GRU</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_9%20-%20RNN,LSTM,GRU/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_9%20-%20RNN,LSTM,GRU/</url>
    
    <content type="html"><![CDATA[<h4 id="助教分享：建立一个更好的语言模型"><a href="#助教分享：建立一个更好的语言模型" class="headerlink" title="助教分享：建立一个更好的语言模型"></a>助教分享：建立一个更好的语言模型</h4><p>语言模型是NLP中最经典的任务，这里有三种方式可以让其变得更好，第一是更好的输入表达，第二是更好的正则化或者预处理，第三是有更好的模型。</p><p>第一种是改进输入，例如Glove是一个词层面的表示，事实上可以把单词进一步编码为子词阶段，可以采用语素解码方式，可以采用BPE，最终也可以采用字母级的嵌入，他的作用是大大减少所用到的词汇量，让模型预测变得更简单，如下图展示的论文就是对输入端的改进。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191209_1558_58_194.png" alt=""></p><p>第二种办法是采用正则化技巧，来改善过拟合的问题，有一堆论文在讲如何做正则化，今天聚焦的点是在预处理阶段，通过和计算机视觉类似的替换技巧来构建更合理的语料库，比如把频繁和罕见的词语的评率都往平均值上拉一拉，就会使得曲线更显平滑，平滑的分布能让系统获得更好的语言模型和更好的实验结果。</p><p>第三是更好的模型，etc。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS229_1.绪论</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_1.%E7%BB%AA%E8%AE%BA/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_1.%E7%BB%AA%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="CS229–绪论"><a href="#CS229–绪论" class="headerlink" title="CS229–绪论"></a>CS229–绪论</h1><h3 id="1-监督学习和无监督学习"><a href="#1-监督学习和无监督学习" class="headerlink" title="1.监督学习和无监督学习"></a>1.监督学习和无监督学习</h3><ul><li><h3 id="监督学习定义"><a href="#监督学习定义" class="headerlink" title="监督学习定义"></a>监督学习定义</h3><ul><li>部分数据集已经有答案.比如房价信息集,知道了正确的房价,需要预测出更多的房价<ul><li>回归问题,预测连续值的输出(房价)</li><li>聚类问题,预测离散值的输出(肿瘤是良性还是恶性?)</li></ul></li></ul></li><li><p>无监督学习</p><ul><li>无监督学习没有答案,自动聚类:比如google搜索BP油井泄露,会把CNN,ABC,BBC新闻放一起</li></ul></li></ul><h3 id="2-线性回归算法"><a href="#2-线性回归算法" class="headerlink" title="2.线性回归算法"></a>2.线性回归算法</h3><ul><li><p>监督学习的典型过程</p><ul><li><p>Training set –[喂食给]—&gt;Learning Algorithm –[生成]–&gt;hypothesis(假设函数)</p></li><li><p>size of house–[输入]—-&gt;hypothesis(函数)—-[输出]—-&gt;Estimate price</p></li></ul></li></ul><ul><li><p>线性回归</p><ul><li>预测房价的假设函数:h(x) = θ0 + θ1x</li><li>根据训练集生成一元一次方程,输入房间大小得到房价预测</li></ul></li></ul><h3 id="3-代价函数"><a href="#3-代价函数" class="headerlink" title="3.代价函数"></a>3.代价函数</h3><ul><li><p>代价函数有利于弄清楚如何把最有可能的直线与我们的数据相拟合</p></li><li><p>最小化代价函数就可得到最好的预测函数</p></li><li><p>如何选择假设函数hypothesis “h(x) = θ0 + θ1x” 中的两个参数θ0和θ1,使预测结果与答案误差最小?</p><ul><li><p>用最小二乘法:找到预测直线中,y与实际的y0之差平方和最小的那个直线</p></li><li><p>代价函数:</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1442_21_679.png" alt=""></p></li><li><p>上图只是平方和代价函数,还有很多其他类型的代价函数.</p></li><li><p>代价函数是关于假设函数的参数 θ0/θ1的函数</p></li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224n_6 - 依存分析</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_6%20-%20%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_6%20-%20%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h1 id="Lecture6-依存分析"><a href="#Lecture6-依存分析" class="headerlink" title="Lecture6 依存分析"></a>Lecture6 依存分析</h1><p>本节课综述</p><ul><li>这节课主要讲句法 + 语法 + 依存分析</li><li>基于转移的依存关系分析语法</li><li>神经网络内容</li><li>TensorFlow + 实际的nlp内容例如构建神经依存关系分析器</li></ul><h4 id="1-依存分析的概念和一个例子"><a href="#1-依存分析的概念和一个例子" class="headerlink" title="1.依存分析的概念和一个例子"></a>1.依存分析的概念和一个例子</h4><p>句子或句子的一部分有一种结构，人们可以用特定的方式将他们组合起来，我们可以从非常简单的并不构成句子的东西入手。</p><ul><li>一个冠词 + 名字 通常被语言学家称为名词短语</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1421_08_337.png" alt=""></p><ul><li><p>有一些规则可以用来扩展他们：比如在冠词和名词之间加一个形容词</p><p>the large cat</p><p>a beautiful dog等</p></li><li><p>你也可以在名词后边添加介词短语</p><p>the large cat in a crate</p><p>the barking dog on the table等</p></li></ul><p>传统上，语言学家和自然语言处理器想做的是描述人类语言结构，人们过去有两个关键工具来做到这点，一种是上下文无关文法的方法，这种方法经常被语言学家引用为短语结构文法。我们现在要做的就是写出这些上下文无关文法的规则。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1421_18_978.png" alt=""></p><p>还有一种了解语言结构的不同角度，就是依存句法结构。它通过找到句子当中每一个词所依赖的部分来描述句子结构。如果一个词修饰另一个词，或者是另外一个词的论证，那么他就是那个词的依赖。例如在barking dog中barking就是dog的依赖，因为barking 修饰dog。在large barking dog中，large也是dog的依赖.同理， a large bakring dog by the door中，by the door也是dog的依赖。所以我们可以画出一个句子中的依存结构<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1421_37_671.png" alt=""></p><p>space可以修饰study，代表科学家从宇宙空间中学习鲨鱼的知识。 （科学家从宇宙空间中学习关于鲨鱼的知识）</p><p>space也可以修饰whale，代表鲨鱼是从宇宙空间中来的。(科学家研究从宇宙空间中来的鲨鱼）</p><h4 id="2-解析依存关系的方法"><a href="#2-解析依存关系的方法" class="headerlink" title="2.解析依存关系的方法"></a>2.解析依存关系的方法<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1421_47_879.png" alt=""></h4>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224n_7 - Tensorflow</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_7%20-%20Tensorflow/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_7%20-%20Tensorflow/</url>
    
    <content type="html"><![CDATA[<h1 id="Lecture7-TensorFlow"><a href="#Lecture7-TensorFlow" class="headerlink" title="Lecture7 TensorFlow"></a>Lecture7 TensorFlow</h1><p>为什么要使用深度学习框架？</p><ul><li>有助于扩展机器学习代码</li><li>自动计算梯度（通常梯度计算并不重要，这些框架可以自动处理梯度，使得我们把重点放在更高层次的数学上）</li><li>在很多领域使机器学习更标准化</li><li>这些框架提供了GPU接口</li></ul><h4 id="1-TensorFlow是什么？"><a href="#1-TensorFlow是什么？" class="headerlink" title="1.TensorFlow是什么？"></a>1.TensorFlow是什么？</h4><p>是一个谷歌开发的深度学习框架，使用流式图进行数值计算的开源软件库。TensorFlow是用于表示机器学习算法的接口，以及用于执行这种算法的实现。</p><p>TensorFlow的一个big idea就是数值计算表示为计算图来进行。 图的节点是操作，每个节点都有一个输入和输出，节点之间的边表示他们之间流动的张量，在实践中最好的方法就是认为，张量是一个n维数组。</p><p>使用流式图作为深度学习框架主干的优点在于它允许你使用小而简单的操作建立复杂的模型，当我们这样做的时候，会使得梯度计算变得极其简单。</p><p>此框架可以自动求导，而且图方法的每一个操作都是可以在其所在点被评估的一个函数。变量将成为输出其当前值的有状态节点。</p><p>占位符是那些在执行期间才会接收值的节点，如果在你的网络中有依赖外部数据的一些输入，你并不想在建立图时依赖任何实际的值。你并不想在建立图时依赖任何的实际的值，那么这些就是我们在训练时添加到计算中的充当占位符的值。对于占位符，我们不给定任何初始的值，仅仅分配一个数据类型，我们的图仍然知道该计算什么，尽管当前并没有存储的值。 </p><p>还有一种节点是数学操作节点，比如矩阵乘法，加法，激活函数、</p><h4 id="2-建立图和会话"><a href="#2-建立图和会话" class="headerlink" title="2.建立图和会话"></a>2.建立图和会话</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1422_00_819.png" alt=""></p><p>新建一个变量b，将其初始化大小是100维的“0”的向量。</p><p>新建一个W，服从[-1,1]均匀分布，shape是（784,100）的TensorFlow变量。</p><p>我们给输入创建一个占位符x，没有被初始化任何值。它仅仅接受32位浮点数的数据类型，接受一个（100,784）shape。</p><p>实际建立流式图，我们用h表示对x和W进行矩阵乘法并加上偏置b，然后进行Relu运行的结果。</p><p>h的数学表达式：</p><p><img src="http://q0u9fsub0.bkt.clouddn.com///20191204/4b2z8nKEsqKk.png?imageslim" alt="mark">)<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1422_10_895.png" alt=""></p><p>我们已经定义了图，下一个问题是我们如何执行它。在TensorFlow中运行图形的方式是将其部署在一个称为会话的地方（session）。</p><p>会话是特定的执行上下文（如CPU和GPU）的绑定。</p><p>我们将会建立一个会话对象，并且调用两个参数fetches和feeds，fetches是返回节点输出的图形节点列表，feeds是一个从图节点到我们想要在模型中运行的实际值的字典映射，所以这就是我们实际填写在前面提到的占位符的地方。</p><p> <img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1422_21_099.png" alt=""></p><p>代码增添了如图所示三行。</p><pre><code>sess = tf.Session()</code></pre><p>表示新建一个session，他会采取默认的环境，最有可能的是一个CPU，当然你也可以添加一个你想要的运行设备作为参数。 </p><pre><code class="python">sess.run(tf.initialize_all_variables())</code></pre><p>初始化所有的变量。</p><p>这是概念密集型流程，称为懒惰评估。这意味着你的图片的评估只会在运行时发生。我们可以在TensorFlow中给运行时间添加一个解释。这就是session（会话）。</p><p>一旦我们建立好了会话，我们就可以准备好实际调用。</p><pre><code class="python">sess.run(h,{x:np.random.random(100,784)})</code></pre><p>第三行：输入h中的x，x是随机初始化的矩阵</p><p>到目前为止，我们首先使用变量和占位符建立了一个图，然后我们将图部署到会话上，也就是我们的执行环境，接下来我们看看如何训练模型。</p><p>一个问题：变量和占位符的不同？为什么要使用占位符？</p><p>通常情况下 ，变量是我们感兴趣的参数，你几乎可以将他们视为直接对应关系。</p><p>上文代码中的x是数据，而不是我们感兴趣的需要调优的参数，对数据使用占位符。</p><p>此外，在刚开始的时候，对我们模型中的参数进行初始化试非常重要的。我们如果想要把我们的模型传给别人，他没有理由包含任何真实的数据，数据应该是任意的。模型中的参数才是你的模型的依据，这些参数是的你的模型更有趣，模型的计算实际上就是计算这些参数。</p><p>​        </p><h4 id="3-训练模型"><a href="#3-训练模型" class="headerlink" title="3.训练模型"></a>3.训练模型</h4><h4 id="3-1定义损失"><a href="#3-1定义损失" class="headerlink" title="3.1定义损失"></a>3.1定义损失</h4><p>对于优化而言，我们如何定义损失？我们将使用占位符作为标签，使用label和prediction建立一个损失节点。</p><p>如下代码表示我们在网路结束时的预测值，它是对神经网络顶层进行softmax函数计算，输出一个概率向量，这是一个回归过程。 </p><p>第一个标志是你的神经网络前馈阶段在哪里结束？他就是在网络预测出值的时候结束。</p><pre><code class="pyhton">prediction = tf.nn.softmax(...) </code></pre><p>如下代码表示创建一个label变量，它是我们真实标记的占位符，我们的模型根据他来训练。</p><pre><code class="python">label = tf.placeholder(tf.float32,[100,10])</code></pre><p>如下代码是创建交叉熵节点，它是真实标签乘以Tensorflow中对prediction取对数的乘积，然后再按列求和。也就是计算预测的结果准不准。</p><pre><code class="python">cross_entropy = -tf.reduce_sum(label * tf.log(prediction),axis = 1)</code></pre><p>TensorFlow会把我们写的加减乘除自动转换为TensorFlow中的操作。</p><h4 id="3-2计算梯度"><a href="#3-2计算梯度" class="headerlink" title="3.2计算梯度"></a>3.2计算梯度</h4><p>我们在计算梯度之前，首先要定义一个优化器对象。在tf中有一个通用的抽象列叫做优化器，在这个类中的每一个子类都是一个针对特定学习算法的优化器，我们目前使用的学习算法是梯度下降算法，</p><p>如下代码调用的.minimize方法，参数就是我们想要最小化的节点。</p><p>minimize方法其实做了两件事，第一是计算了交叉熵的梯度（他和我们在图中定义的所有的变量相关），第二是他会给这些变量根据梯度进行更新。</p><p>我们在底层实际是如何计算梯度的呢？每一个图节点都有一个附加的梯度操作，都有相对于输入预先构建的输出梯度，当我们在计算交叉熵相对于所有参数的梯度的时候，通过图使用链式法则利用反向传播计算是非常简单的。</p><p>这就是把机器学习框架表示为计算图表的主要优势。这样子程序也很容易回退，在图形中向后遍历，并且在每个节点上将误差信号乘以节点的预定义梯度。所有的这些都是自动实现的。</p><pre><code class="python">tf.train.GradientDescentOptimizer(lr).minimize(cross_entropy)</code></pre><h4 id="3-3学习计划"><a href="#3-3学习计划" class="headerlink" title="3.3学习计划"></a>3.3学习计划</h4><p>我们有了梯度，我们能对梯度进行更新，我们需要创建一个迭代的学习计划，比如这里我们需要迭代一千次，我们在数据集上使用next_batch方法。</p><pre><code class="python">for i in range(1000):    batch_x,batch_label = data.next_batch()</code></pre><p>我们将会得到我们输入的一个批次的数据，以及对应的标签值。</p><p>然后在train_step变量上调用session.run方法，当我们调用他的时候，已经对图中所有的变量应用了梯度。我们需要建立字典，将值传给我们之前定义好的两个占位符。这里的x和label是图中的节点，字典中的关键字是图中的节点，对应的项是numpy数据。</p><pre><code class="python">sess.run(train_step,feed_dict = {x:batch_x,label:batch_label}</code></pre><p>TensorFlow提供了很好的numpy接口，tf会把图中节点的数据转换为张量，所以我们可以把numpy数组插入到字典中，比如batch_x和batch_label，我们执行sess.run之后会得到输出，如果定义了变量比如output等于sess.run,那么他们也是相应节点的nump数组，也就是这些节点评估的值。通过train_step将会返回给你梯度的值。</p><h4 id="4-变量共享"><a href="#4-变量共享" class="headerlink" title="4.变量共享"></a>4.变量共享</h4><p>如果想要建立一个大的模型，经常需要共享大的变量，你可以能想在一个地方初始化他们，例如，我们可能想要多次实例化我的图，或者更有趣的是，我们想要在GPU集群上训练。也就是说通常情况下我们希望能在不同的设备上训练模型。</p><p>实现这种共享的简单思路是，在代码的顶端创建这个变量的字典。把一些字符串的字典放到他们所代表的变量中。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1422_33_648.png" alt=""></p><p>但是有一些原因证明这其实不是一个好的主意，这主要的原因是它破坏了封装性，在TensorFlow中构建图的代码应该始终具有相关使用节点和操作的所有有关信息。在代码文件中应该能找到神经元的名字，变量的形状。如果只是在代码的顶部有这些大的field，你其实会丢失这些信息。</p><p>TensorFlow设计了有关变量作用域的解决方案:variable_scope。</p><p>提供了一个简单的名称空间方案来避免冲突，另一个相关的函数是get_variable。get_variable会为你创建一个变量，如果具备特定名词的变量存在，他会访问该变量，让我们来看看它是如何起作用的。</p><pre><code class="pyhton">tf.variable_scope() #provides simple name_spacing to avoid clashes</code></pre><p>open一个foo的新变量作用域，调用get_variable方法，创建变量名，名字为v。 </p><pre><code class="python">with tf.varibale_scope(&quot;foo&quot;）:        v = tf.get_varibale(&quot;v&quot;,shape = [1])# v.name == &quot;foo/v:0&quot;</code></pre><p>这样当我们调用foo，就好像进入了一个目录，让我们关闭这个变量作用域并reopen，并使用参数reuse = True。现在如果调用get_varibale，name为v，我实际上得到的是同一个变量，我将会访问该变量，其实我访问的是和之前创建的对象相同的变量。</p><p> 如果我再一次关闭变量作用域，然后重新打开，使用参数reuse = False，如果我尝试再次运行，程序会崩溃。因为你已经设置他不复用任何变量，所以当它试图创建新的变量，但是已经存在一个同名变量了，在下个作业里，变量作用域的使用将会是明显的。</p><pre><code class="python">小结;1.建立一个    1.1前馈+预测    1.2优化器（梯度+训练步骤设置）2.初始化一个会话3.训练session.run（train_step,feed_dict）</code></pre><h4 id="live-demo环节"><a href="#live-demo环节" class="headerlink" title="live demo环节"></a>live demo环节</h4><p>直接在anaconda中做。</p><h4 id="助教论文分享：Visual-Dialog（可视对话）"><a href="#助教论文分享：Visual-Dialog（可视对话）" class="headerlink" title="助教论文分享：Visual Dialog（可视对话）"></a>助教论文分享：Visual Dialog（可视对话）</h4><p>许多人相信下一代人工智能系统将会能够根据视觉内容以自然语言与人类进行有意义的对话。在看这篇论文之前，我们先看看相关工作。</p><p>许多人对自然语言处理和计算机视觉融合方向做了许多工作。比如给图像加字幕或者描述，如下图片。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1422_44_145.png" alt=""></p><p>还有一类工作是视觉-语义对齐，这个模型并不是给出每张图片的描述，而是给出图片中每个独立组件的描述。在搜集图片中的组件时，需要给图片画出许多边界框，并单独描述每一个独立的组件。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1422_53_931.png" alt=""></p><p>还有一类工作与我们今天分享的论文很相关，就是视觉问答，模型基于视觉内容回答给定的问题。在此种情况下，如图所示，回答是二分类的，yes or no或者非常简洁的其他二分类回答（数字，圆或者方之类的）。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1423_01_032.png" alt=""></p><p>我们要介绍的论文，《visual dialog》中，模型可以基于视觉内容，和人进行有意义的对话。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1423_10_721.png" alt=""></p><p>如下图所示，这个模型使用卷积神经网络将图片进行编码，同时使用两个循环神经网络将问题和对话历史进行编码，然后将三个编码合并为一个向量。为他接上一层全连接层和一个解码器，用来生成基于向量表示的答案。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1423_39_943.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224n_8 - RNN和语言模式</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_8%20-%20RNN%E5%92%8C%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%BC%8F/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_8%20-%20RNN%E5%92%8C%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<h1 id="Lecture8-RNN和语言模式"><a href="#Lecture8-RNN和语言模式" class="headerlink" title="Lecture8 RNN和语言模式"></a>Lecture8 RNN和语言模式</h1><p>本节讲简单的循环神经网络模型，这个模型家族是大多数人现在在实际训练环境中使用的。</p><p> 概览：</p><p>1.传统的语言模型</p><p>2.RNNs</p><p>3.语言模型建模来驱动循环神经网络（RNN）</p><p>4.重要的训练时的问题和技巧</p><p>​    梯度消失问题</p><p>​    梯度爆炸问题</p><p>5.用于队列处理的RNN</p><p>6.双向深度RNN</p><h4 id="1-传统的语言模型和现在的词向量模型对比"><a href="#1-传统的语言模型和现在的词向量模型对比" class="headerlink" title="1. 传统的语言模型和现在的词向量模型对比"></a>1. 传统的语言模型和现在的词向量模型对比</h4><p>传统语言模型中，理想情况下预测一个语序是根据前n-1个词出现的条件概率下，第n个词出现的概率。实际中这么做不可行，一个是语序有无限多个，而且计算每一个w(n)都要把之前所有的词都遍历一遍这个成本太大了。<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1423_54_634.png" alt=""></p><p>如上条件概率公式也就是表示在词语w1出现的条件下，w2出现的概率是由在整个语料库中的count(w1,w2)/count(w1)决定的。</p><p>如果我想提高精度，将预测w1的工作加上了w2和w3.那么我就需要统计出语料库中，所有三元组三三组合出现的概率，假设物料库有10万个词，那么这个统计和存储的工作量就是10万的三次方。要求有140G的内存仅仅用来存放1260亿的记号语料库计算所有的计数。所以从内存的角度看，这种方法非常低效。</p><p>我们发明了很多种不同的途径来规避这些问题。例如由于4元统计模型出现频率过低根本不使用4-gram。那么我会尝试使用3元统计模型，或者再没有充足的计数，则会再次退而求其次，在上下文大小范围内使用较少的字组预估概率。但总的来说没如果你希望至少获取存储的3元或者4元统计模型的话，相关的内存需求会非常非常大。</p><p>通过深度学习模型和传统nlp模型进行大量比较，自然会注意到上述现象。而这些模型大都围绕着特定类别的字组进行计数。通常模型越强大，内存的需求会增长地更快。</p><h4 id="2-RNN"><a href="#2-RNN" class="headerlink" title="2.RNN"></a>2.RNN</h4><p>我们目前回避这种内存需求爆炸的问题的方法是，RNN，循环神经网络。</p><p>循环神经网络在不同时间步长之间绑定相应的权重，在你通过这条路径时，其实你在使用同一个线性和非线性的层，至少从理论层面上来说，我们可以根据所有前序字组来进行预测。如此以来，内存需求只会取决于字词的数量规模，而不受我们想依据的序列长度的影响。也就是十万就是十万，没有十万的平方或者立方。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191208_0918_21_529.png" alt=""></p><p>假设我们有一个词汇向量表x,假设这些词汇向量均是固定的，我们在后续环节中放宽此假设并将其去除。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191208_0927_58_435.png" alt=""></p><p>紧接着，计算每个时间步长的隐状态，这个步骤中我们只有两个线性层和矩阵向量结果：W（hh）和W（hx）。我们会把这个两个矩阵相加。</p><p>其中x[t]是在第t个时间步长中出现的字组向量。 </p><p>如上这个公式将时间步长t-1和t处的词汇向量串联起来。同时也将这两个矩阵串联起来。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191208_0928_11_626.png" alt=""></p><p>我们可以将h(t)用作特征向量或者作为softmax函数分类层的输入，来获取概率输出（比如所有字组的概率）。</p><p>几个矩阵W的含义：</p><p>我们通常使用上标来识别现存的矩阵，W（hh）表示本W是输入h（t-1）后计算出的隐层h；W（hx）表示刚计算的h层有一个h_x,h_x将x映射到已有的相同向量空间。 </p><p>x[t]是第t个时间步长出现的字组向量。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191208_0933_51_156.png" alt=""></p><p>如下就是在特异指标j的约束下，下一个特定字组的概率。所有前序字组本质上都是该大规模输出向量的第j个元素。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191208_0934_34_205.png" alt=""></p><h4 id="3-探究梯度消失-爆炸问题"><a href="#3-探究梯度消失-爆炸问题" class="headerlink" title="3.探究梯度消失/爆炸问题"></a>3.探究梯度消失/爆炸问题</h4><p>训练上文讲的循环神经网络事实上是一件非常困难的事情，这也是后边的课程我们会引入更优质的神经网络的原因。</p><p>下文是对困难的描述</p><p>基本上我们是让每个时间步长上相同的矩阵相乘，这可以看作是一个在所有时间步长上反复方法某些特定模式的过程。在理想情况下，我们很乐意看到多个时间步长中的输入值可以在后续的时间步长中继续修正我们一直努力预测的目标字组。尝试对W求导，如果你只有两三个字组序列的话，这将会是一项非常行之有效的训练，在正向传播过程中，我们想成了每个时间步长上相同的矩阵，而在反向传播期间我们也同样要这样做，我们必须时刻记住增量、空中讯号、以及各种梯度方向的全局元素。本质上讲，每个时间步长上的全局元素都会通过这个网络向后流动。所以当我们处理交叉熵损失函数时 ，通常会利用导数，我们需要回过头重新传播增量。第一个接近输出值的时间步长就会有效执行更新操作。并且字组向量也有可能得到合理的更新。</p><p>随着时间的推移，你的讯号有可能变得过于强烈或者过于微弱。这就是所谓的梯度消失问题。随着时间的推移，你尝试在时间步长t处发送空中讯号，但由于此前已经存在很多时间步长，这样你就会遭遇梯度消失问题。</p><ul><li><p>简化的RNN模型来研究爆炸/消失问题。h(t)剥离西格玛，yt剥离softmax</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191208_1019_48_100.png" alt=""></p></li></ul><ul><li><p>所有时间步长中的误差W是分步误差的加和，如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191208_1028_39_394.png" alt=""></p></li></ul><ul><li><p>接下来我们要核算的是总误差的时间标记t中的元素，我们只需要核算单个时间步长以及单个时间步长上的单向误差。而现在即使计算也需要我们有一个非常大的链式求导法则。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191208_1031_02_913.png" alt=""></p><ul><li><p>主要分析这个求导法则中的第三项</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191208_1859_39_756.png" alt=""></p></li><li><p>记住ht的定义</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191208_1900_20_207.png" alt=""></p></li><li><p>基本上每个方向的前序时间步长的ht的偏导数都已经给出，为了计算ht的所有偏导数，又要使用链式求导法则。如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191208_1905_20_004.png" alt=""></p></li><li><p>这就意味着向量相对于另一个向量的偏导数是存在的。其实假如我们可以灵活运用反向传播的定义，那就没必要再进行实际计算。实际上，在将计算与流程图以及增量信息有效结合之前，我们并不需要计算清楚这些雅克比矩阵，但是为了分析其中的数学问题，我们还是需要计算所有的导数。</p><p>每一个方向的ht的偏导数都是一个雅克比矩阵</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191208_1907_42_256.png" alt=""></p></li></ul></li></ul><p>  这么多雅克比行列式最终得到的结果也相当大，为了简化它，我们可以设定一个上限。相对于hj我们还要设定h的导数。事实上，在这一系列简单定义的基础上，所有的h都可以通过这种方式来计算。我们还可以设定矩阵的范数上限，这个上限就是这些方程的乘积。也就是Wt，我们可以将其算作一个已知的对角线，而非元素级结果。一般情况下，我们会将元素归置到一个较大矩阵的对角线上，在0路径中一切都是非对角线分布的。接下来我们将两个范数相乘，并将beta_W和beta_h定义为上限.那么每一个数值和单个标量，都能达到自身的最大值。</p><p>  <img src="http://bqlab-pic.test.upcdn.net/pic/20191208_1918_58_863.png" alt=""></p><p>  已知W我们可轻松算出W的任意一种范数。我们只需要一个矩阵就可以计算出一个矩阵范数，当把这些范数全部列出来后，我们就发现这些雅克比行列式的上限实际上被用作这些元素的乘积。就其上限beta而言，假如我们定义了所有元素，那就等于拥有了计算t-k幂的乘积beta。所以序列越长t值越大。它真的取决于测试的t值究竟是变大还是变小。例如当前矩阵范数就是上述范数，那就意味着你已经控制了该范数。在开始训练之前，你将等待矩阵W,初始化为一些小的随机值，如果将其初始化为一个范数大于1的矩阵，那么在反向传播过程中，时间序列就会变得更长，由于你采用了大于1的值，最终你会得到一个爆发的梯度。例如数值为100，但现有的范数只有两个，那么在该梯度的上限就在这两个范数到100之间。反之亦然，假如在开始的时候，你将矩阵W初始化为一群小的随机值，那么W的范数就会小于1，那么最终ht发送到hk的最终梯度可能会缩小为一个十分小的数值，差不多是第100个范数的一半。总体来讲，这一过程不会有任何误差。同时这一过程也不会有误差讯号，随着时间推移，梯度消失或者爆炸也会越来越严重。</p><p>  假如现有的梯度面临激增或者突然减少，是否意味着较远的数组最较近的数组有较大影响呢？答案是，当梯度激增或者突减时，在短时间内你便无法获得数字。这些数字并非字面意义上的数字，因为他们的数值实在太大了，我们根本无法计算。因此上述问题并非实际问题，我们必须设法回归。事实证明，梯度爆炸问题的确包含一些很强势的非法入侵，相比这些梯度消失问题，解决这些非法入侵问题其实要简单的多。</p><p>  <img src="http://bqlab-pic.test.upcdn.net/pic/20191208_1931_18_003.png" alt=""></p><p>  在语言模型或者问答模型的情况下，一些时间序列间隔较大的语句，通常在预测下一个词是什么的时候不被考虑进去，这是梯度消失问题的一种。我们通过如下一个语言建模的例子来了解梯度消失问题。</p><p>  Jane walk into the room,John walked in too,It was late in the day,Jane said hi to ___.</p><p>  人类会觉得几乎概率是100%的下一个单词应该是John，但是现在每个单词都有词向量，你需要将他们全部输入到隐状态上，然后进行计算。接下来会希望模型可以筛选出一个特定的模型，在该模式中，一个人遇见另一个人，然后两个人互相问好，然后介绍姓名等等等等。但是实际上你无法命令模型将中间这个错误讯号收回，你需要手动修改隐状态，将John这个答案设置为较高的概率。中间的这个it was late in the day是一个打乱了时间序列的句子。</p><h4 id="4-python笔记本演示简单的梯度消失问题"><a href="#4-python笔记本演示简单的梯度消失问题" class="headerlink" title="4.python笔记本演示简单的梯度消失问题"></a>4.python笔记本演示简单的梯度消失问题</h4><p>展示的时候用的是一个简单的两层神经网络，并没有用完整的周期性实体网络。在这个网络中会看到从顶部开始的误差，会随着你在网络中的搜索，梯度范数已经会变得越来越小了。如下的方程能解释这个现象，这两个方程在代码中也得到体现。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191209_1311_03_191.png" alt=""></p><p>代码对数学公式的实现格式非常规整，值得好好学习。</p><pre><code>小结:1.传统语言模型是使用条件概率公式，遍历前n-1个词汇，预测第n个词汇出现的概率。这种方式cost太大2.后来改进，根据1-gram，2-gram，3-gram模型预测下一个词出现概率。本质还是使用条件概率公式，这种方式内存开销仍然太大。3.为了解决开销问题，引入RNN网络，RNN定义由上文方程式和图片清晰定义。4.RNN又有新的麻烦，就是梯度爆炸or消失问题。因为RNN基本上就是在时间序列上连续传导特定的模式，随着时间的积累这种模式一定会变得过于强烈或者过于微弱。</code></pre>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224n_2,- 注释之softmax</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_2,-%20%E6%B3%A8%E9%87%8A%E4%B9%8Bsoftmax/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_2,-%20%E6%B3%A8%E9%87%8A%E4%B9%8Bsoftmax/</url>
    
    <content type="html"><![CDATA[<h1 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h1><p><strong>定义：</strong></p><p>假如有一个数组V，Vi表示V中的第i个元素，这个元素的softmax值如下，也就是该元素的指数值和所有元素指数值之和的比值。softmax通常希望特征对概率的影响是乘性的。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191215_1105_33_496.png" alt=""></p><p><strong>softmax VS k个二元分类器</strong>：<br>如果你在开发一个音乐分类的应用，需要对k种类型的音乐进行识别，那么是选择使用 softmax 分类器呢，还是使用 logistic 回归算法建立 k 个独立的二元分类器呢？<br>这一选择取决于你的类别之间是否互斥，例如，如果你有四个类别的音乐，分别为：古典音乐、乡村音乐、摇滚乐和爵士乐，那么你可以假设每个训练样本只会被打上一个标签（即：一首歌只能属于这四种音乐类型的其中一种），此时你应该使用类别数 k = 4 的softmax回归。（如果在你的数据集中，有的歌曲不属于以上四类的其中任何一类，那么你可以添加一个“其他类”，并将类别数 k 设为5。）<br>如果你的四个类别如下：人声音乐、舞曲、影视原声、流行歌曲，那么这些类别之间并不是互斥的。例如：一首歌曲可以来源于影视原声，同时也包含人声 。这种情况下，使用4个二分类的 logistic 回归分类器更为合适。这样，对于每个新的音乐作品 ，我们的算法可以分别判断它是否属于各个类别。<br>现在我们来看一个计算视觉领域的例子，你的任务是将图像分到三个不同类别中。(i) 假设这三个类别分别是：室内场景、户外城区场景、户外荒野场景。你会使用sofmax回归还是 3个logistic 回归分类器呢？ (ii) 现在假设这三个类别分别是室内场景、黑白图片、包含人物的图片，你又会选择 softmax 回归还是多个 logistic 回归分类器呢？<br>在第一个例子中，三个类别是互斥的，因此更适于选择softmax回归分类器 。而在第二个例子中，建立三个独立的 logistic回归分类器更加合适。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224n_5 - 项目建议</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_5%20-%20%E9%A1%B9%E7%9B%AE%E5%BB%BA%E8%AE%AE/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_5%20-%20%E9%A1%B9%E7%9B%AE%E5%BB%BA%E8%AE%AE/</url>
    
    <content type="html"><![CDATA[<h1 id="Lecture5-项目建议"><a href="#Lecture5-项目建议" class="headerlink" title="Lecture5 项目建议"></a>Lecture5 项目建议</h1><h4 id="1-一层神经网络过渡到多层神经网络"><a href="#1-一层神经网络过渡到多层神经网络" class="headerlink" title="1.一层神经网络过渡到多层神经网络"></a>1.一层神经网络过渡到多层神经网络</h4><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1420_08_008.png" alt=""></p><h4 id="2-项目建议"><a href="#2-项目建议" class="headerlink" title="2.项目建议"></a>2.项目建议</h4><p>老师推荐的文章和会议</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1420_24_988.png" alt=""></p><p>1.定义你的任务：</p><p>例如：summarization</p><p>2.定义数据集：</p><p>最好使用现成的数据集，因为他们已经有baselines</p><p>3.建立baseline</p><p>他可以是一个非常简单的一元线性回归，然后在你的训练数据集上计算你的评价标准，看看模型是过拟合还是欠拟合</p><h4 id="4-选做题：自己发明新的模型"><a href="#4-选做题：自己发明新的模型" class="headerlink" title="4.选做题：自己发明新的模型"></a>4.选做题：自己发明新的模型</h4><ol><li><p>首先，你需要做好以上说的几个步骤.</p></li><li><p>然后你需要知道已经存在的模型上有哪些问题。然后你就可以设计出自己的模型。如果你想要这样做的话，<strong>你真的需要和你的导师和其他研究者保持沟通，除非你自己就是研究者并且已经获得了博士文凭</strong>。</p></li><li><p>你需要实现你的模型，然后根据你的新点子去对它快速迭代。（也许在某个位置新加一层？然后看看他起不起作用？）</p></li><li><p>那么在迭代的过程中，拥有足够多的的软件工程技能来配置高效的实验框架，从而能收集到这些结果就很重要。</p></li><li><p>建议从一个和你的真实想法比起来相对容易很多的模型做起。先把简单模型建立起来。然后逐步尝试更复杂的模型。</p></li><li><p>对于终极任务：summarization任务。</p><ol><li>一开始你可能尝试一些非常简单的模型。比如对自然段中的所有单词向量求平均，然后用贪心搜索一个接一个地生成单词，或者用贪心搜索对维基百科中的现有文章寻找一些片段，然后把合适的片段复制过去。</li><li>然后升级你的目标，尝试某些真正让你生产整段总结的方法。</li></ol></li></ol><p>一些项目的idea（包括了在kaggle上打比赛！）</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1420_37_463.png" alt=""></p><p>下次课程会学一些有趣的基础语言学任务，比如句法分析；然后会学习TensorFlow的应用。</p><h4 id="论文分享："><a href="#论文分享：" class="headerlink" title="论文分享："></a>论文分享：</h4><p>词袋模型</p><p>文本分类是nlp领域一个非常重要的话题， 给定一串文本，我们就可能问，它带有的是积极还是消极情绪。是垃圾邮件还是正常邮件。如图就是一个情感分析的例子。</p><p>取得一个句子的特征的一种简单的办法就是对句子中所有的单词向量做平均。这篇论文中的模型也是类似这样的方式。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1420_49_271.png" alt=""></p><p>对他们求平均，但是会丢失他们之间的顺序。然后得到的是这个低纬度的文本向量，代表这个句子，这就揉碎了的低纬度的文本向量就是词袋（word  bag）</p><h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>反向传播和词袋模型在其他文档中有总结。</p><p>本节课最有含金量的就是老师对想要发文章的同学的做项目的建议。</p><p>首先，纯理论突破现在不可能。所以必须得要做项目，项目的选题要与老师或者其他研究者讨论，避免过大或者过小，除非你自己是一个phd和独立研究者。但是现在我没有人去讨论，但是回避六个月白忙的风险的动作是要做的，解决思路有两个</p><ul><li>发邮件、线下拜访中国的nlp领域的老师，求指点</li><li>直接打kaggle。问题具体，选题明确。</li></ul><p>做能发文章的项目要先确定baseline和数据集。然后遍历先有的模型，然后提出一些修改意见（idea），然后在数据集上跑，看看是不是确实有提高，如果有，就完事儿了嗷。做出修改意见并且修改别人的模型的前提是得有充足的工程能力，这个工程能力我现在他妈绝对不具备。难顶，只能硬上。</p><p>另一个要注意的事情是老师反复<strong>强调迭代 + 先从容易的模型开始实现</strong>。我认为这是把大问题难问题拆分成小问题容易问题的思路。这种思路非常关键，小熊也提过很多次。我需要做计划，把大问题拆分成小问题。当然具体是一个多大的问题，发论文一共有多少步，哪些步骤最难大概耗时多久如何拆分成容易问题都是还需要探索的地方。</p><p>所以在做总计划的时候也要秉承把困难计划先简答实现然后再根据新发现的信息对其迅速迭代的思路。</p><p>之所以这样做因为what else？</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224n_4 - wordWindow和神经网络</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_4%20-%20wordWindow%E5%92%8C%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_4%20-%20wordWindow%E5%92%8C%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="Lecture4-wordWindow和神经网络"><a href="#Lecture4-wordWindow和神经网络" class="headerlink" title="Lecture4  wordWindow和神经网络"></a>Lecture4  wordWindow和神经网络</h1><h4 id="1-分类背景知识"><a href="#1-分类背景知识" class="headerlink" title="1.分类背景知识"></a>1.分类背景知识</h4><p>对分类的直觉感受是什么？在机器学习领域，在还没有达到深度学习领域的情况下，我们通常将分类理解为简单的逻辑回归，也就是定义一个简单的决策边界。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1417_17_117.png" alt=""></p><h4 id="2-窗口分类"><a href="#2-窗口分类" class="headerlink" title="2.窗口分类"></a>2.窗口分类</h4><p>在一般的机器学习中，我们假设输入是固定的。输入X都是固定的，我们只训练<strong>参数</strong>W，也就是softmax的权值，然后计算给定输入X时输出Y的概率。<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1417_27_826.png" alt=""></p><h4 id="3-交叉熵"><a href="#3-交叉熵" class="headerlink" title="3.交叉熵"></a>3.交叉熵</h4><p>我们假设正确类别的概率为1，其余的概率为0.举个例子，假设共有5个类别，正确类别是中间的第三个，那么第三个的概率为1，其他都是0.我们把理想的概率定义为p，softmax计算的概率为q，这里给出了交叉熵的定义，就是对所有类别的求和<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1417_43_174.png" alt=""></p><p>正则化项：里边包含的参数θ如果是标准逻辑回归中的矩阵W，实际上目标函数加入这个正则化项的目的就是是为了鼓励模型中的所有权值尽可能地小。</p><p>可以假设你想要一个贝叶斯模型，你可以有一个先验的高斯分布，理想情况下这些参数的值都很小，但是如果没有这个正则化项，通常情况下你得到的模型参数会爆炸，他会越来越过拟合。如果没有正则化项，我们会专注于如何拟合我们的模型。</p><p>通常的机器学习优化就是只优化模型的参数W，也就是softmax分类器的参数，因为参数众多，所以每次的更新和梯度会特别小。通常我们只有两三个类别，但是词向量的维度是上百。例如我们有三个类别，和100维的词向量，这样我们就有300个参数。在深度学习中，我们有相当惊人的词向量，而且实际上我们不仅仅是学习softmax分类器，还有词向量，我们可以反向传播到词向量。我们每次更新词向量的时候，必须意识到词向量通常都巨大。例如你的词有300个维度，你的词典中有10000个单词，突然你有如此巨大的参数集，在这种情况下，你很有可能会过拟合。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1417_53_374.png" alt=""></p><p><strong>小数据集训练容易过拟合</strong></p><p>我们想对单个词做积极还是消极的分类，我们的训练测试集中包含了单词TV，telly，television。假设这三个单词是在一个评价刚上映电影的语境中（说一个上线的电影适合在电视上放），那么这种评价通常是消极的。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1418_09_632.png" alt=""></p><p>在开始的时候，这三个词汇在向量空间中很接近，我们用word2vec或者glove向量学习，我们在非常大的语料库上训练这些词向量，他学习到这三个单词经常出现在相似的上下文中，所以他们在向量空间中很接近，现在我们要在更小的情感数据集中训练，只包含了TV和telly，不包括television，当我们重新训练这两个向量的时候会发生什么？</p><p>结果是他们发生了变化，把情感分类的结果显示出来，你会看到telly和tv移动到向量空间的其他位置，而television还停留在原来的位置。这就是向量在小数据集上的过拟合问题。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1418_22_424.png" alt=""></p><p>我们实际上会把这个词汇错误分类，因为他们三个词汇其实是一个意思。</p><p>what does it mean？</p><p>这里的关键信息是，如果你只有小训练数据集，不要训练词向量。（深入模型在小数据上容易过拟合）</p><h4 id="4-窗口分类的例子：命名实体识别4分类问题"><a href="#4-窗口分类的例子：命名实体识别4分类问题" class="headerlink" title="4.窗口分类的例子：命名实体识别4分类问题"></a>4.窗口分类的例子：命名实体识别4分类问题</h4><p>问题：将语料库中的单词分为如下四类：地点，人名，组织，其他</p><p>思路：训练一个softmax模型，然后用一个窗口把他们前后的单词链接起来</p><p>目的：给中心词Paris分类</p><p>中心词巴黎周围的步长是2，这就构成了一个五维的窗口向量，每一维都是一个单独的词向量，比如【I love Paris very much】。我们能想到的最简单的窗口分类器就是将这五个词向量拼接放入softmax分类器中，五个词构成了softmax分类器的分母，分子是一个单词paris。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1418_34_641.png" alt=""></p><p>参数解释：</p><p>x是窗口，W矩阵是参数。</p><p>代价函数J(θ)中，F（yi）是参数W和X的交叉熵。</p><p>如何更新词向量X？X是五个词构成的窗口，X深入在softmax中。直接回答就是我们可以多次求导。</p><p>y尖 为softmax概率向量，也就是对所有四个分类的归一化的得分或概率，比如（0.9,0.05,0.05,0）.那么分类结果就是第一个（比如是地点）</p><p>其实我们是想更新pairs这个词的向量，而不是整个窗口，窗口只是一个中间步骤，所以其实我们做的是更新和计算关于词向量的每个元素的导数，最终他会变得很简单（类似于梯度下降）。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1418_42_262.png" alt=""></p><p>softmax只会给我们线性决策边界，如左图。而神经网络可以给我们复杂的决策边界，通常会work的更好，如右图。</p><pre><code>定义了窗口分类和交叉熵，用一个实例实现了窗口分类，其实就是对softmax的应用。窗口分类只能给出直线边界，没有神经网络的复杂边界好用，本节课还提到了神经网络的基本概念，为了解耦把神经网络的概念剪切到CS229的相关文章里边去了。</code></pre>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_词向量_Word2vec</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_%E8%AF%8D%E5%90%91%E9%87%8F_Word2vec/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_%E8%AF%8D%E5%90%91%E9%87%8F_Word2vec/</url>
    
    <content type="html"><![CDATA[<h1 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h1><p>from笔记ofCS224N</p><p>这节课深入语言的底层，做一些向量和计算，这节课提到的数学是后边内容的基础。这节课将用很慢的速度来仔细讲解一些基础，以便大家可以使用神经网络来学习词语表征这样的简单任务。</p><h3 id="1-语言学和NLP对词语释义的不同做法"><a href="#1-语言学和NLP对词语释义的不同做法" class="headerlink" title="1.语言学和NLP对词语释义的不同做法"></a>1.语言学和NLP对词语释义的不同做法</h3><h4 id="语言学"><a href="#语言学" class="headerlink" title="语言学"></a>语言学</h4><p>用如下词典释义来解释“meaning”这个词的意思</p><ul><li>the idea that is represented by a word,phrase,etc</li><li>the idea that i person wants to express by using words,signs,etc</li><li>the idea that is expressed in a work of writing,art,etc</li></ul><h4 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h4><p>同义词描述词汇</p><p>如下图左侧，用一段代码演示nltk如何抓取wordnet中”panda“这个词的分类信息，panda是一种肉食动物，一种有胎盘的哺乳动物，再往上上溯可抽象为动物，物体，物理实体等等。</p><p>如下图右侧，抓取“good“的同义词，这里显示的就是wordnet的答案。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1319_14_209.png" alt=""></p><p>同义词描述词汇本义毕竟是离散的方法，会有如下的问题：</p><ul><li>同义词的描述资源很多，但分类关系会遗漏大量的细微差别。例如I am good at NL和I am expert of NL虽然是同义词但是还是有不少的区别。</li><li>语言是动态变化的，不同圈子对同样的词汇的语义指代也不同，wordnet不可能跟上这种变化。</li><li>难以获得某个词汇的精确含义</li></ul><h3 id="2-独热编码的问题与分布式表示的提出"><a href="#2-独热编码的问题与分布式表示的提出" class="headerlink" title="2.独热编码的问题与分布式表示的提出"></a>2.独热编码的问题与分布式表示的提出</h3><p>独热码是由稀疏向量构成的，所以是离散的。独热码的问题有两个：并不能显示近义词的<strong>相似性</strong>。会造成<strong>维灾难</strong>。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1319_22_169.png" alt=""></p><p>遂提出<strong>分布式表示</strong>（也就是非稀疏向量表示），两个非稀疏向量点积后能算出<strong>相似度</strong>，而且无<strong>维灾难</strong>。</p><p>思路是通过训练，将每个词都映射到一个较短的向量上来，到底有多短一般我们自己指定。比如”Royalty”,”Masculinity”, “Femininity”和”Age”4个维度来表示，King这个词对应的词向量可能是(0.99,0.99,0.05,0.7)。这样的非稀疏向量表示每个维度是有具体 意义的。把king这个词从的热码的高维度降到四维空间时，这个过程也称为<strong>词嵌入（word embedding）</strong>，词嵌入其实也是一种投影，甚至可以把词汇从4维进一步降低到2维。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191215_1003_25_105.png" alt=""></p><p>分布式表示理论基础是<strong>分布相似性</strong>的存在。一句关于分布相似性的名言是你可以根据一个词周围的词语来了解本词的意思，也就是说<strong>一个词汇他身边的词汇总是相对固定的</strong>，也就是说<strong>一个词汇可以用他身边这些相对固定的词汇作为向量的维度被表示出来</strong>。以banking为例，把语料库中所有包含banking的句子都抽出来，可以看到围绕着banking的词的词无外乎就是贸易，银行，欠债，还债，柜台等等。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200514_1242_36_405.png" alt=""></p><p>​    </p><h3 id="3-Word2Vec的骨架"><a href="#3-Word2Vec的骨架" class="headerlink" title="3.Word2Vec的骨架"></a>3.Word2Vec的骨架</h3><p>Word2vec的引出：我们定义了一个<strong>预测模型</strong>，来根据中心词汇预测它的上下文词汇。公式表示：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200514_1324_33_619.png" alt=""></p><p>word2vec预测模型的损失函数：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20200514_1325_38_553.png" alt=""></p><h4 id="两个生成词向量的算法-两个训练方法"><a href="#两个生成词向量的算法-两个训练方法" class="headerlink" title="两个生成词向量的算法 + 两个训练方法"></a>两个生成词向量的算法 + 两个训练方法</h4><p>Word2vec尝试去做的事情是利用语言的意义理论，来预测每个单词和他的上下文词汇。</p><p>两个生成词汇向量的算法：</p><ul><li><p><strong>Continuous Bag of Word (CBOW)</strong></p><p>输入特征词的上下文向量，输出这个特征词的向量。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191215_1011_39_444.png" alt=""></p></li><li><p><strong>Skip - grams</strong></p><p>反过来，输入一个特征词向量，输出上下文向量</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191215_1011_15_137.png" alt=""></p></li></ul><p>  两个训练效率尚可的方法：</p><ul><li>Negative sampling</li><li>Hierarchical softmax</li></ul><h3 id="为解耦，Skip-gram和词袋在“NLP概念”文件夹中另起文件描述。"><a href="#为解耦，Skip-gram和词袋在“NLP概念”文件夹中另起文件描述。" class="headerlink" title="为解耦，Skip-gram和词袋在“NLP概念”文件夹中另起文件描述。"></a><strong>为解耦，Skip-gram和词袋在“NLP概念”文件夹中另起文件描述。</strong></h3><h3 id="4-总结word2vec"><a href="#4-总结word2vec" class="headerlink" title="4.总结word2vec"></a><strong>4.总结word2vec</strong></h3><ul><li>会遍历语料库中的每个词</li><li>预测每个词的周围词</li><li>一次只捕获一个单词的重复出现</li><li>为什么不直接捕获所有词的同现计数？</li></ul><p>基本上我们会遍历语料库中的每个单词，观察窗口周围的单词， 预测周围的词，我们在努力抓取词的共同点，这个词和其他词共同出现的频率是多少？每出现一次进行一次计数，就像看到deep和learning同时出现， 就对着两个向量做一次梯度更新，然后再次浏览语料库，你很有可能发现deep和learning又共同出现了，然后对其再做一次梯度更新，你可能会觉得这样做很没有效率，为什么现在仅对整个语料库计算一次deep和learning这两个词共同出现的频率？我们能捕获整个计数而不是一个样本。实际上我们可以那样做，这也是出现word2vec之前的做法，这里有可供我们选择的不同做法。</p><p>最简单的类似于word2vec的方法是在每个单词周围使用了一个窗口，这基本上上相当于遍历了整个语料库，我们不对任何部分进行梯度更新，不是用SGD，首先对计数结果进行采集，一旦获得了计数结果，就对矩阵进行操作，窗口的长度可能会是2词的句法信息， 也可能是5（一些单词周围的小窗口），我们要抓取的不仅仅是语义，还有每个词的句法信息，即它是哪一种词性标签，所以动词之间会更接近，例如，动词变为名词，另一方面，观测共同出现的计数结果不仅在窗口周围，而是整个文档的，那我们不能仅仅只观测每个窗口。但是我们认为这个词和其他这些词出现在所有维基百科文章中或者整个词文档中，然后你会捕捉到更多的话题，这通常被称作潜在语义分析，这是以前流行的一个模型，and忽略掉词性和句法信息，基本上你会得到的仅仅是游泳，船，水，天气，太阳等等，他们都是在这个话题上出现的，在这个文档中一起出现。我们不会太过深入这个话题，因为这涉及到其他的下游任务，比如机器翻译等，但是掌握使用这些窗口的知识是很有用的。</p><h4 id="一个例子：基于窗口的共现矩阵"><a href="#一个例子：基于窗口的共现矩阵" class="headerlink" title="一个例子：基于窗口的共现矩阵"></a>一个例子：基于窗口的共现矩阵</h4><ul><li><p>窗口长度：1( 通常更普遍的是5 -10)</p></li><li><p>对称性：中心词汇的左右词汇一视同仁</p></li><li><p>简化的语料库由如下三句话构成</p><ul><li>I like deep learning</li><li>I like NLP</li><li>I enjoy flying</li></ul><p>上图的解读：</p></li></ul><p>第一行是第一个词I的周围词汇统计：like在右侧出现两次，enjoy在右侧出现一次。</p><p>第二行同理：I出现两次，deep出现一次，flying出现一次。</p><p>这个矩阵的行或者列已经是向量了，但是从几个原因来说，它并不是非常理想的向量：</p><ol><li>随着字典加入新的单词，所有向量的维度都会发生变化，因此，如果有一些下游的机器学习模型来接受这个向量的输入，他们总会需要改变而且有一些参数的缺失。</li><li>这个向量的维度会非常高，因此在训练机器学习模型时会存在稀疏性问题，在此之后会训练出一个不那么健壮的模型。</li></ol><p>共现矩阵如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191215_1507_38_708.png" alt=""></p><h4 id="共现矩阵的奇异值分解方法"><a href="#共现矩阵的奇异值分解方法" class="headerlink" title="共现矩阵的奇异值分解方法"></a>共现矩阵的奇异值分解方法</h4><p>解决这个问题的办法就是参考word2vec的思路，并不只是存储所有共同出现的次数、每个单独的数字，还存储重要的信息，维度的固定小数，与word2vec的相似度。这将会使向量的维度在25维到1000维之间。所以现在的问题如何进行降维?</p><p>在实际的设置中，会有20000*20000或者百万乘百万的数量级，降维的思路是简单的奇异值分解。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1415_25_219.png" alt=""></p><p>奇异值分解的具体做法见线性代数的文档</p><p>奇异值分解后就实现了矩阵的压缩功能，然后取出U的头两列，在二维图上画出他们。奇异值分解的降维思路就是，你要降低到几维，就取U的前几列。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1415_35_511.png" alt=""></p><p>分解之后可得，I和like是频率比较高的词，enjoy和learning和flying都是动词距离就比较近</p><p>奇异值分解是个不错的办法，妥善处理停止词能提高性能，例如the,he,has太多太频繁了，他们又没有给我们太多有用的信息，只是不停地出现。而文本量越大这些越经常出现的词就会出现地更频繁，出现频率很少的词反而带有大量的语义信息。</p><ul><li><p>一种解决的办法是遮住他们出现的次数，限定在比如一百次。</p></li><li><p>或者不对所有的词汇平等计数，比如中心词的邻居可计数为1，五步外的词汇计数为0,5</p></li></ul><p><strong>几种模型的对比</strong></p><p>虽然奇异值分解很简单，只需要一行python代码，但是从计算方面来说效果不是很好，尤其是当处理越来越巨大的矩阵时，因此需要将二次损失函数定在较小的维度上。</p><p>两种基于奇异值分解的共现矩阵的语义预测方法的对比</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1415_44_077.png" alt=""></p><ul><li><p>Count based</p><p>PCA的优点是相对来说训练速度更快，通常会非常有效地使用我们拥有的统计数据。我们只需要搜集一次统计信息，理论上只需要把语料库放一边，然后对共现计数进行一系列的操作，但它主要是捕获大多数词的相似性，没有word2vec模型抓取的其他模式。</p></li><li><p>Skip-gram模型的缺点是他会缩放语料库的尺寸，必须遍历每一个单一窗口，这非常低效，因此不能有效地利用数据集的统计信息，实际上在下游任务很多的情况下，他能获得更好的性能。但是还是不知道下游具体的任务。但是对于不同的问题，比如命名实体识别或者词性标注等，这都是在作业集中需要实现的部分，事实证明，这种任务skip - gram模型的性能会好一点。我们还能捕获很多复杂的模式，其中一些非常令人震惊。</p></li></ul><p>  <strong>集成以上两种模型优点的模型：GloVe</strong></p><p>  global vector模型，θ在这里表示所有的参数，在这个情况下，已有U和V向量，我们基本上会遍历所有可能共现的词对，P是非常巨大的共现矩阵，对于语料库中的每一对词，我们希望最小化内积距离和两个词计数的对数。代价函数如下。与之对比的skip-gram模型的代价函数的目的是要最大化预测词和中心词的条件概率总加和，这个GloVe的代价函数目的是根据UV的交叉熵来动态调整Pij这个共现矩阵来达到J(θ)的最值。</p><p>  <img src="http://bqlab-pic.test.upcdn.net/pic/20191215_1507_22_743.png" alt=""></p><p>  <strong>内部评价与外部评价</strong></p><p>  内在评价通常是针对特定的或者中间环节的子任务进行的，可以观察这些向量的差异性或者相似性，以及内部产品对人类相似性的判断有多大的相关性。</p><p>  内部评估的优点是计算很快，已有向量的情况下，可以通过这个快速相似相关的研究评估他们，然后得到数值结果，然后可以修改结果，尝试50000种的旋钮组合的快速调整。有时他们会帮助大家非常快速地理解系统的工作原理。例如什么样的超参数实际上会对这种相似性指标产生影响。</p><p>  最近发表的word2vec论文中，其中一个很受欢迎<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1416_00_202.png" alt=""></p><p>例如，男人之于女人，所对应的的国王对于什么？（王后）。</p><p>这是一个简单的类比。如上图所示，向量（女人 - 男人 + 国王）与哪个向量i的余弦距离最近？最近的那个向量i就是答案，也就是queen。</p><p> 这个计算类比词的余弦向量距离方法的效果是相当不错的，如下图所示</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1416_13_585.png" alt=""></p><p>在简单的欧几里得加减法中，这些余弦距离很好地捕捉到了他们。下图是另一个例子，公司名称和ceo名称之间也有类似的相同向量距离的关系。<img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1416_30_754.png" alt=""></p><p>下图是一个更有趣的例子，多项式加减法。例如寿司 - 日本 + 德国 = 德式小香肠等。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1416_46_933.png" alt=""></p><ul><li>外在评价基本上是对习一个任务的真正评价，对于新的单词向量，取皮尔森相关性，而不是核心矩阵的原始计数。我现在要评估这个词向量是否对机器翻译有帮助。外部评价通常需要消耗大量时间，而且是各个子系统耦合在一起，最后只能得出一个总体结果。</li></ul><p>​    </p><h4 id="CS224N-Research-Highlight-陈丹琦的热门论文分享"><a href="#CS224N-Research-Highlight-陈丹琦的热门论文分享" class="headerlink" title="CS224N Research Highlight(陈丹琦的热门论文分享)"></a>CS224N Research Highlight(陈丹琦的热门论文分享)</h4><p>标题：A simple but Tough-to-beat Baseline for Sentence Embeddings,简单但是难以超越的基线句向量表示.</p><p>今天我们正在学习单词向量表示，所以我们希望这些向量可以对词汇意思进行编码，但是在nlp问题中，核心是我们如何才能定义可编码句子含义的向量表示（    how we could have the vector representations that encode the meaning of sentences）.</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1414_21_171.png" alt="">我们还可以将这个句子表示作为一个特征去处理一些句子分类任务，例如情感分析问题。我们给定一个句子，例如自然语言处理很有趣， 我们可以对向量表示运用分类器，来判断这句话的情感是不是正面的。</p><p> 有很多方法来做词汇向量表示，最简单的方法是使用词袋，词袋的最简单用法如下图所示，一个句子由每个单词均权重构成。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1414_28_979.png" alt=""></p><pre><code>小结：1.单词有如下三种表达方式。    指称，直接释义词是什么意思。    独热编码，也就是稀疏，有维灾难    词向量表示，在向量空间中，语义近似的词汇距离近。2.本文终点就是word2vec，掌握如下两种生成词向量的方法。    词袋：输入上下文向量，得到中心词向量。输入独热码，动态调整W矩阵，输出前过一道softmax，终点labelled          独热码在那等着，根据是否正确的结果来调整W，不断循环完成训练。    skip-gram:输入特征值，得到上下文向量，代价函数时一个条件概率的二重累加，注意数学细节。也是动态调整                  矩阵W。3.基于窗口的共现矩阵可以用SVD方法压缩。奇异值分解的数学方法作为另一篇文档存在线性代数文件夹里。4.GloVe方法是继承了PCA和Skip-gram优点的方法，代价函数大同小异。5.一个优秀的word embedding可视化网站：https://ronxin.github.io/wevi/</code></pre>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224n_16 - 深度学习在NLP中的限制</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_16%20-%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9C%A8NLP%E4%B8%AD%E7%9A%84%E9%99%90%E5%88%B6/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_16%20-%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9C%A8NLP%E4%B8%AD%E7%9A%84%E9%99%90%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="Lecture16-深度学习在NLP中的限制"><a href="#Lecture16-深度学习在NLP中的限制" class="headerlink" title="Lecture16 - 深度学习在NLP中的限制"></a>Lecture16 - 深度学习在NLP中的限制</h1><p>对于语言，如果你想要一个合适的语言系统，你可能需要一些对于输入的情感理解，在逻辑上推理某些事实，在数据库中检索一些事实，或者基于数据库中的逻辑原因，再做一些内存检索。还有一些时候，你需要对谈论的内容做一些奖励。在现实世界中这些过程是由很多不同的成分组成的。我们想要一个更好的理解语言的系统，理想情况下这个系统应该包括很多内容，以一种更科学的方式呈现。</p><p>现在联合多任务学习仍然非常困难，迄今为止，人们谈论多任务学习时，他们假设有一个源任务和一个目标任务，他们希望神经网络在源任务的预训练可以加强另一个目标任务的表现。就我而言，理想的情况是让他们共同训练，而不是分开训练不同的解码器。例如不同语言中，不同的分类问题。理想情况下，我们只有一个很大的不同种类的数据集，我们想根据输入来预测，他们有完全一样的解码器，但是很多时候人们做多任务学习的时候他们只是共享低层的参数，共同训练他们，但是不能共享高层的参数。</p><p>也就是说，在自然语言处理的过程中，我们通常只是共享了词向量，我们没有共享其他高层的比如LSTM层，这种层能解决更多的任务，其实<strong>计算机视觉在这方面有挺大的进步，他们有预训练的卷积神经网络，在Imagenet这样大的数据集上训练的网络</strong>，可以运用在许多其他的任务中，并且有很好的表现。计算机视觉中，只用改变深度卷积神经网络的前几层，就可以得到很好的准确率。并且在不同的视觉任务中迁移学习。在NLP中我们还不能做到这种令人信服的事情。</p><p>很多时候，你只能看到一些多任务学习的例子，这些任务有一些关联，所有互相有帮助，比如词性标注有助于句法分析，但是你很难看到彼此任务不相关联，任务对彼此无帮助但是对互相的结果有影响的论文。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_2111_11_257.png" alt=""></p><p>如下的论文试图结果这个多任务处理艰难的问题，他的模型像个怪物。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_2123_46_118.png" alt=""></p><p>从下往上看，使用了n-gram代替了普通的词向量表示，这些词向量又被输入到一系列的LSTM中，上边的大块都是用了LSTM表示，一个LSTM的输出作为下一个LSTM的输入被传递出去。第一个LSTM被用作词性标注，下一个LSTM用作分块工作，对每一块的起始和结束做分类，再下一个LSTM会做依存语法分析，然后把词性标注等等底层链接作为每个高层任务的输入。</p><p>有时候会要求同时理解两个句子，在两个冷却塔尖的中间使用了一个池化的结构。池化结构和卷积神经网络中讲到的类似，池化操作后我们可以对句子的关联性和蕴含性做分类，最后我们可以对整个的联合结构用一个损失函数训练。</p><p>第一层用于词性标注，在每一个时刻，有一个单层的LSTM,把它输入到softmax中，在此同时生成一个向量标签，这个操作可以让我们考虑到词性标注的一些不确定性。你可以把这里当做是一个层，这个层接受softmax的输出，凸组合公式如下图右下角所示，在这里每一个标签都与一个向量组合，把所有向量的加权相加，权重取决于模型在某一时刻得到的那个标签的确定性。<br>举个例子，如果有三个不同的标签（动词名词形容词），这三个标签每一个都有一个向量，是一个50维的随机向量，模型可以把它训练的很好。这时比如动词的可能性是0.9，名词和形容词都是0.05。把这三个数都乘以相应的向量，这就得到最后的标签向量。这就是词性标注LSTM的输出。</p><p>下一层分块模型，会接收和前面一样的输入，也就是词向量X，词性标注LSTM的隐藏状态h，以及标签向量y，这些都是输入，把他们拼接起来，再输入到另一个LSTM中，接下来会做类似的事情，输出一个隐藏状态，将隐藏状态输入到softmax中，得到一个分块的标签向量。理论上来说，你可以把这个操作做很多次。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_2132_26_804.png" alt=""></p><p>根据历史经验，词性标注，分块，和依存句法分析，这三个功能让他们分别有自己的LSTM层对性能优化最有利。</p><p>下一层，依存句法分析<br>我们想要获得一个树结构，因此依存句法分析，在很多情况下要求集束搜索，但这个模型十分简单，使用一个双向的LSTM,有四个输入，词向量，分块的隐藏状态，词性标注和分块的标签向量。每一时刻都有这四个输入，这里是双向的LSTM的结构，我们把它当成一个分类问题。运行一个 二次分类问题，判断某个词语是依存这个词语还是依存那个词语，所有通通运行一遍，取每一个的最大值，这就生成了我们希望得到的树。虽然有概率出现回环生成不合法的树，但是现实情况是99%的情况都生成的是合法的树，加上一些简单的剪枝规则，删除一些边，能让这1%的不合法的树也变得合法。</p><p>奇怪的是，不再要求使用束搜索，只是前馈计算，就能在大多数情况下获得很优秀的树。（可以做很多优化，可以加入集束搜索，通过几个节点或者类似的东西，也可以做适当的SQL，因为通常有连续的向量）</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_2145_54_515.png" alt=""></p><p>最后一层可以训练多个句子，处理语义相关性等任务，这里所做的就是，用一个简单的时间最大池化，这个LSTM的最后一个隐藏状态，会在每一个时刻生成一个特征向量，在所有时刻隐藏状态的特征的每一个维度中，找到最大的值并选择它，这就是我们把它叫做最大池化的原因。这些看起来都很复杂，但是都是之前仔细讲过的内容。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_2152_02_006.png" alt=""></p><p>这里有一个额外的想法可以用，叫做连续正则化，在每个mini-batch，先让模型专注于不同的任务，当层次越来越高的时候，对低层的权重加入正则化项，使他们不要有太多的改变，是用这里的正则化项delta实现的。这在训练中是一个新方法，可以使鲁棒性更强，最终的系统在五个任务中的四个都达到了当前最好的结果，直觉上讲，在第一个mini-batch结束的时候，模型关注的只是词性标注，得到权重theta，定义了标签向量，LSTM权重等等,然后训练下一个，高层次任务也就是分块的时候，不要让他们改变太多，这些权重在词性标注模块已经被调节的很好了，在向更高层训练时，基本上是同样的想法， 不要对底层权重改变太多，但如果高层任务非常想改变权重，也是可以做的。有人将SNLI和蕴含性分类作为预训练步骤，应用在问答系统中，这些都是可以尝试的方法。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_2247_36_017.png" alt=""></p><p>另一个NLP领域的障碍，就是在使用词向量的时候有许多冗余，我们使用word2vec和Glove作为预训练词向量，如果我们训练出一个输出，比如机器翻译或者语言模型，在softmax中其实是另外一组权重，在softmax的输出中，每个单词都用一个向量表示，意思就是，在上面这里有一个很大的softmax，是词汇表的大小乘以LSTM的隐藏状态的维度，在输入时，每个单词也有对应的词向量，也是同样的大小的v乘以词向量的维度。这里有一篇很赞的论文，实际上是上这门课的两个同学的论文，也就是把这两组向量联系起来，也就是将这两组向量变成一样的向量，每个向量的softmax权重和输入的词向量是完全相等的。经过共同训练，只需要反向传播，使用同样的导数，但是他们现在是相等的，如果不自己求导，是很容易实现的。使用tensorflow等工具就可以了，否则会有一些困难。他们也有一些关于softmax的很好的理论，例如热度，看了论文就知道了。这是一个很简单的思想，但是又很大的帮助。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_2300_18_923.png" alt=""></p><h3 id="读这篇论文"><a href="#读这篇论文" class="headerlink" title="读这篇论文"></a>读这篇论文</h3><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_2304_28_736.png" alt=""></p><p>以前我们花大量时间做特诊工程，现在可以自动化了，我们又花大量时间设计深度学习神经网络，如下这篇论文提出了我们可以使用AI模型去做架构，代替人类创造与设计网络架构。controller是一个循环神经网络，循环神经网络输出的是网络架构的超参数。应该由多少隐藏层，在每个时刻，循环神经网络会输出这些特征。每当他输出时，他会尝试训练一个新的简单的网络。用这个架构在一个简单任务上可以得到一个准确率。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_2313_25_221.png" alt=""></p><p>【CS224N完结撒花】</p><h1 id="记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记别跟论坛项目一样做完了之后就把半成品笔记往那一扔很多概念在热乎的时候不搞清楚可能以后就更懒得对着乱乱的笔记搞清楚了。笔记应该是愿意反复看，反复修改调整结构迭代版本的东西"><a href="#记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记别跟论坛项目一样做完了之后就把半成品笔记往那一扔很多概念在热乎的时候不搞清楚可能以后就更懒得对着乱乱的笔记搞清楚了。笔记应该是愿意反复看，反复修改调整结构迭代版本的东西" class="headerlink" title="记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记别跟论坛项目一样做完了之后就把半成品笔记往那一扔很多概念在热乎的时候不搞清楚可能以后就更懒得对着乱乱的笔记搞清楚了。笔记应该是愿意反复看，反复修改调整结构迭代版本的东西"></a>记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记记得整理笔记别跟论坛项目一样做完了之后就把半成品笔记往那一扔很多概念在热乎的时候不搞清楚可能以后就更懒得对着乱乱的笔记搞清楚了。笔记应该是愿意反复看，反复修改调整结构迭代版本的东西</h1>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224n_15 - NLP的问题和可能性架构</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_15%20-%20NLP%E7%9A%84%E9%97%AE%E9%A2%98%E5%92%8C%E5%8F%AF%E8%83%BD%E6%80%A7%E6%9E%B6%E6%9E%84/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_15%20-%20NLP%E7%9A%84%E9%97%AE%E9%A2%98%E5%92%8C%E5%8F%AF%E8%83%BD%E6%80%A7%E6%9E%B6%E6%9E%84/</url>
    
    <content type="html"><![CDATA[<h1 id="Lecture15-NLP的问题和可能性架构"><a href="#Lecture15-NLP的问题和可能性架构" class="headerlink" title="Lecture15 - NLP的问题和可能性架构"></a>Lecture15 - NLP的问题和可能性架构</h1><h3 id="1-NLP问题"><a href="#1-NLP问题" class="headerlink" title="1.NLP问题"></a>1.NLP问题</h3><p>视觉研究以前在深度学习中占主导地位，但是三巨头Yann LeCun,Geoff Hinton,Yoshua Bengio都把他们的研究方向转向NLP(也就是说深度学习三巨头的研究方向有风向标的意义)</p><p>Bengio在采访中表示，实验论证深度学习技术应用与新应用的可能性，这些新应用包括了计算机视觉，对话系统，虚拟助手，语音识别，NLP,机器翻译以及其他应用。</p><p>这世界上有太多不同的语言现象和相对应的任务要完成，所以解决NLP问题不能像解决计算机视觉那样，某个人构建出一个足够复杂的深度神经网络就vans了.NLP问题们并不同源，不能说解决了其中一个，就可以宣布全部解决。</p><p>在传统NLP问题中丢失了的东西：</p><p>将现在和过去对比是一件很有趣的事情，在70和80年代的那批从事NLP研究的人员里，他们有着非常崇高的目标，他们想要达到人类级别的语言理解能力，但是他们最终达到的是非常骨感的现实。</p><p>不论上述对比的结果如何，我们现在所处的环境，我们能比他们当时更好地实现目标，但是，达到目标的途径却开始变得不那么明朗。作为实践，我们可以在这个语料数据上运行一个LSTM,然后将这个模型取得很好的性能表现，然后感觉这似乎就已经是极限了。</p><p>老师采用Peter Norvig的博士论文作为例子，Norvig是谷歌的研究主管，距离他发表这篇博士论文到现在已经过去32年了，他的论文的题目叫做，一种关于文本理解的推论的统一理论。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_1955_43_230.png" alt=""></p><p>他的论文实际上在讲NLP和语言理解的内容，但是在他的论文里，只有很少的篇幅在分析自然语言，他举得例子都是很小儿科的语句，比如A给了B一个自行车，C想要它，于是B又给了C都是这样的话。</p><p>80年代的人处理自然语言的观点和现在截然相反，当时的人们都认为唯一的方法是，拥有可用于推理的知识库，他在论文的第四页写到，一个合适的知识库，是做出正确推理的先决条件，这不过是说，我们需要在事实之间建立推理关系，以此来理解较复杂的指代问题。当时他非常依赖知识库，但是后来20年里大家发现其实有很多自然语言是可以仅仅通过文本的组成而不需要任何知识库就可以进行处理的。</p><p>在他的论文里，他列举了6种推理形式试图去建立一个包含这些种类的自然语言理解理论。然后能实现在一篇简单的童话故事中推理。</p><p>第一种推理形式是阐述，就是处理如何连接各个单词实体，这里的例子是john拿出一个存钱罐，因为他需要去买礼物，也就是需要对这些单词实体做上下文的阐述。</p><p>第二是指代消解，也就是处理好指针和内存的关系，这种方法之前已经讲过，这个现在已经能被很好地处理。</p><p>第三种是解释语言中的隐喻和抽象，比如Laker队kill了Yeekoins，这里的kill并不是杀死，而是把yeekinos队打败了。Norvig另有一篇论文去讲述人们如何去衍生这些抽象解释。第三种我们一直以来没有考虑太多，但实际上这是很重要的一部分，具体化是可以让你解决如何将一个大致上的描述，转变为更具体的形态。比如有人驾车出游，那么在自动驾驶技术成熟之前，你一定能推测出有人在开车。这就是具体化，能够让你在有一定的常识的基础上，做出更加合理的推论。</p><p>在Norvig的论文中，他们当年没有一个良好的语法分析器供他们使用，他们当时手动把句子拆开，我们现在的parser发展已经很不错了，这就是progress。</p><p>在自然语言处理的方面还有很多事情需要完成，一方面，在过去的几年中，NLP领域和其他各种领域一样，都度过了一段激动人心的时期，很多系统取得了非常好的效果，值得注意的是双向LSTM正在占领自然语言处理领域，因为你可以在任何任务上尝试他们。你可以把双向LSTM用作注意力机制，他也可以成为你的天然语言分析器。</p><p>另一个令人激动的事情是这些神经方法，赋予了语言生成这一新生任务，生成对话，机翻，等等，这些任务在2000-2010之间还停滞不前，但是现在，我们有了这些令人惊叹的语言模型，我们有了各种的方式去配置他们，我们可以进行很棒的语言生成，这些领域最近都活跃了起来。</p><p>从科学上来讲，这是很有趣的一段时间，因为大多数的NLP任务都可以假设我们需要建立独有的语言表示体系并且来使用他们进行进一步分析，所以我们需要语法表示，我们想要使用语义框架去表示语言中的事件与关联，同时还需要建立详尽的有地方特色的知识表示方式。</p><p>当下的工作能显示出很多现在的情况，我们能建立端到端的深度学习系统，但是对于那些地方语言特色的表示，这些系统运行的并不好。另一方面，我们在很多事情上并没有触及到问题的本质，其中之一就是我们现在还在停留在使用很原始的方法来构建和得到记忆或者知识，的确现在已经有了很多关于LSTM的工作，但其实这里边的记忆都是非常短时的记忆，它并不是我们人类在脑子里的模型，能够存储生活中很多年积累的经验那种模型，我们人类还能在需要的时候将这些经验灵活地调动和组织起来，而LSTM能做的就只是线性扫描：最后100个单词发生了什么事诸如此类。</p><p>另外还显得不足的方面是，我们并没有什么模型能够使我们让目标或者计划公式化，比如在一组人类的有意义的对话里边，你有一些计划和目标需要达成，但是在自然语言处理中，还不能对这个目标或者计划公式化（我理解为不能建模）。</p><p>还有一个有待提高的领域是语际关系，一旦我们需要处理单个的句子，句子的结构通常很清晰，我们可以直接加工，但是我们一旦尝试解释句子或者从句之间的关系，或者说出他们之间这样的原因，我们就不能很好地处理。</p><p>我们想要实现借助常识实现阐述，以目前的能力，是没有办法造出这样的深度学习系统的。</p><h3 id="2-树形结构"><a href="#2-树形结构" class="headerlink" title="2.树形结构"></a>2.树形结构</h3><p>如下，通过树形结构把文章拆分为小块，红色为保守派观点，灰色是中性表达，蓝色是自由派观点，通过各种组合最后得到的还是自由派的观点。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_2041_50_770.png" alt=""></p><p>另一个例子也是使用树形结构去分析文章的意识形态是偏保守还是自由</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_2042_35_003.png" alt=""></p><p>如下是树形循环神经网络，它在理论上很吸引人，特别是如果你没有一亿个单词作为训练集加以训练，从经验上来看，他是很有竞争力的。但是他的缺陷就是非常慢。大多数这种网络都需要外部分析器。</p><p>树形结构的运算表现非常之差的原因是他们没有适应GPU上的批处理运算，而批处理运算恰恰是让深度学习模型能够高效训练的核心原因。如果你有一个序列模型，一个序列模型只能有一种结构，假如你的结构就是从左往右这样，你开始循环计算每一个单词的隐藏属性，这种结构你能在序列模型中使用一堆句子（最好长度相仿），然后在序列模型中运行并锁定步长，这样做效率真的很高。但是如果你在TreesRNN网络中训练，又使用这种特定的结构，他每一句的结构都不同，这就削弱了你进行批处理计算的能力。因为你每句句子都要构造出不同的结构单元。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224n_14 - 问答系统</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_14%20-%20%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_14%20-%20%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="Lecture14-问答系统"><a href="#Lecture14-问答系统" class="headerlink" title="Lecture14 - 问答系统"></a>Lecture14 - 问答系统</h1><h3 id="1-实现问答系统的联合模型：动态记忆网络"><a href="#1-实现问答系统的联合模型：动态记忆网络" class="headerlink" title="1.实现问答系统的联合模型：动态记忆网络"></a>1.实现问答系统的联合模型：动态记忆网络</h3><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_1049_47_393.png" alt=""></p><p>从下边开始看，输入都是词向量，比如word2vec或者glove之类。基本会有一个循环的神经序列模型，比如GRU,他为每个输入单词计算一个隐藏层。</p><p>在右下角的问题模块中也有处理问题的GRU,用于计算输入的问题的向量。</p><p>以右下角输入的问题为例，“足球在哪里？”我们会认为足球被提及的事实的这个问题储存在上一次GRU隐藏层的某个地方，我们称之为Q，我们使用Q从本质引发所有潜在输入的注意力机制，每当有一个特定的句子被强烈关注，我们将把这个句子作为另一个GRU的输入，那就是情节记忆模块。基本上只要有上图蓝色的线存在，那基本就是一种循环的神经网络序列模型。所以基本上，一个问题促发一个注意力机制。遍历所有输入的所有隐藏层，现在基本上说这个事实似乎与这个问题有关。基本上说，一个问题引发一个注意力机制，然后遍历所有输入的所有隐藏层，也就是说在输入模块，我们认为最后一次提到“足球”肯定和现在问的问题是相关的，于是GRU的隐藏层捕捉到了这一点，然后经过上边的记忆模块中的GRU（fliter）过滤，现在只聚合与当前问题相关的句子。 图上方记忆模块的第三条线，这个是GRU的最后隐藏层。对于下一次迭代，发现“joho put down the football”，然后不知道john在哪，所以我们现在已经存储在这个向量m中，为了回答现在的问题，他似乎是相关的，模型会再一次自己学习所有这些东西，然后会注意到所有提及john或者足球的每一个事实。在这种情况下，这些都是现实的数字，然后很容易发现“john去了走廊”和“john去了卧室“的句子，然后提到这两个句子的注意力值，然后在右上角的回答模块输出回答。 </p><p>问题：上图上方的情节记忆模块的两个不同轨道是什么？实际上，这些轨道是输入的完全镜像。因此在情节记忆模块中总是有很多时间步骤，因为输入中有时间步骤，但是模型可以用分类器来决定或者只是靠固定次数的输入。介于2到5之间，每次它结束时，他都会注意输入中的各种句子，取决于问题是什么以及它迄今为止在前一时间步骤或者一个情节中的相关事实方面聚集在一起。所以在这里，我们再次回顾一下第一次的输入，储存关于john和足球的情节，那么第二次，我们现在也要关注john的事实，所有再由直觉，第一句，我问足球在哪里？在这种情况下，john移动了卧室这一句似乎与提问并没有任何关系，所以你必须做传递推理，这种传递推理能力，只有你在信息中有很多次传递时才能得到。</p><h3 id="2-组件剖析"><a href="#2-组件剖析" class="headerlink" title="2.组件剖析"></a>2.组件剖析</h3><p>为了定义模块，我们得将整个体系结构进行端到端的约束。我们基本上只有一个标准GRU,你只需要给它一个输入序列，一个问题和一个答案，现在从这个答案开始我们基本上会在一开始就会犯错，然后就会有很高的交叉熵误差，那么在这里我们基本上可以通过所有这些向量反向传播，通过所有的方式进入单词向量，我们可以训练整个系统，这真的是深度学习的力量。</p><p><strong>input模块：</strong></p><p>输入模块是标准的GRU，我们实际上会使每个句子的最后一个隐藏状态明确地被访问，我们已经在第二次迭代中做了一个改进，把它换成了双向GRU。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_1223_27_097.png" alt=""></p><p><strong>question模块：</strong></p><p>问题模块同样是一个标准GRU,这里我们有称之为v_t的单词向量，q_t是GTU模型接受vt和上一个q_(t-1)之后的输出。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_1225_26_291.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_1225_48_410.png" alt=""></p><p><strong>情节记忆模块的第一条线</strong></p><p>这个模块带有注意力机制，示意图如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_1226_48_307.png" alt=""></p><p>隐藏状态如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_1227_07_348.png" alt=""></p><p>首先定义g的关注机制，g只是一个标量数字（代表我是否应该注意第i句话？）这里h的上标表示迭代次数。这里的主要想法是我们在标准GRU之上有一个全局gate，“g”。这个全局gate会说整句话都非常重要或者整句话都不重要，当然并不是所有的隐藏状态都有自己的大门。也就是说，g（i，t）代表的第i个句子和第t个情节，通过整个输入是0，我们要做的就是基本上完全复制h向量，没有计算和更新。比如上文举得例子，比如找的是足球，足球和john有关，其他人比如mary就和足球无关。然后和mary有联系的话语就被标记成0.</p><p>那么如何计算g？</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_1244_54_642.png" alt=""></p><p>其实非常简单直接，首先用句子部分之间的一些简单相似性来计算这个向量z，这又是每句话结束时候的隐藏层，计算组z之后进行简单装配和e和sum就OK了。</p><p>关于计算z的详解：</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_1245_37_692.png" alt=""></p><p>第一项si * q是如何标识产出</p><p>第二项si * m(t - 1)是 在记忆中的句子、问题与记忆状态下的每个句子之间的乘法交互作用。</p><p>第三和第四项是简单的向量相似度量度。</p><p>举例：我在si处输入，在这里捕捉关于john的fact。但是其实我们问题是关于足球，所以在第一次迭代中，m（0）也刚刚初始化为这个问题，不会添加任何东西吗，因为我们还没有完成整个输入。基本上在隐藏状态里，我们希望能捕捉到一些约翰走到走廊的一些事实，所以|si - q|这一项，也就是si和q之间的相似性不会太高，然后我们把|si - m|这个长特征向量插入这g（it）个双层神经网络，所以g(it)的值会非常小，但是在第二次迭代中，基本上链接了john，足球，和走廊。现在在m隐藏层下有一些关于john和足球的fact，现在这种相似性可以通过注意力机制来获得，然后下次给这个句子分配多一些注意力。</p><p><strong>情节记忆模块的第二和第三条线</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191214_1250_01_915.png" alt=""></p><p>第二第三条线也是GRU,但是是一个不会有很多时间步骤的GRU，基本是一个从一个记忆状态到下一个记忆状态，并在最右侧聚集的GRU。</p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224n_12 - 语音处理的端到端模型</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_12%20-%20%E8%AF%AD%E9%9F%B3%E5%A4%84%E7%90%86%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E6%A8%A1%E5%9E%8B/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_12%20-%20%E8%AF%AD%E9%9F%B3%E5%A4%84%E7%90%86%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="Lecture12-语音处理的端到端模型"><a href="#Lecture12-语音处理的端到端模型" class="headerlink" title="Lecture12 - 语音处理的端到端模型"></a>Lecture12 - 语音处理的端到端模型</h1><ol><li>介绍传统的语音识别系统</li><li>引出语音识别的端到端模型，并给出描述（CTC,LAS）</li><li>端到端模型的改进版</li><li>语言模型如何影响语音识别的</li><li>语音识别模型中的解码工作的改进</li></ol><h3 id="1-传统语音识别系统"><a href="#1-传统语音识别系统" class="headerlink" title="1.传统语音识别系统"></a>1.传统语音识别系统</h3><p>自动语音识别的基本定义是把语音信号自动转换为文字呈现，语音识别系统的经典实现方法是使用生成模型（generate model），后来被简单的神经网络模型取代。</p><p>如下图最右，你从语言模型（n-gram）中生成了一个由单词组成的特定序列。然后到右二，每个单词都有一个读音模型，也就是每个单词都有自己的指定发音方法（音标），读音模型可以将文本序列转换为读音token序列，然后将这些模型传递给给右三声学模型（acoustic models），声学模型基本上给出一个token听起来是什么样，一般由高斯混合模型来构建，然后这个声学模型会输出一组一组的声音features，一般这些features是由信号处理专家定义的，就像是被捕捉到的声音的评率成分的特征一样（又被称为频谱图或者钟形滤波器组频图）。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1349_12_368.png" alt=""></p><p><strong>如下图，应用神经网络，同架构，performance提升较大</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1353_03_768.png" alt=""></p><h3 id="2-端到端模型"><a href="#2-端到端模型" class="headerlink" title="2.端到端模型"></a>2.端到端模型</h3><p>虽然每个组件都应用了神经网络导致性能有提升，但是这里有个问题，就是虽然每个组件的误差都不同，所以组合起来效果会不太好。一个解决的思路是把整体系统当成一个大模型来统一训练。</p><p>引出端到端模型：直接从语音输入得到文字输出</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1402_53_072.png" alt=""></p><p><strong>端到端模型实例1：CTC模型（Connectionist temporal classification）</strong></p><p>在这个实例中，X是频谱图，Y是相应的输出文本。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1405_50_030.png" alt=""></p><p>下图是CTC模型核心架构（双向的RNN），图中箭头指的地方是基于输入数据的任意时间节点。输入的每个时间帧上都有一个这个模型。softmax模型作用在你比较感兴趣的词汇表上。被箭头指着的上边竖着的条条，是预测值。每一个预测值都会产生一个不同token类在这个时间上的对数概率。这个对数概率我们称它为score，在这里，得分s(k,t)就是log(Pr(k,t|X))，也就是数据X在时间为t的类别k上的对数概率。</p><p>对竖着的条条(预测值)的进一步阐述：<br>仅看softmax函数，假如第一个脚标是关于字符a的概率，第二个脚标是关于字符b的概率，第27个角标是关于1的概率，最后一个角标是空字符的概率。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1406_28_030.png" alt=""></p><p>CTC做的就是如果你仅仅看由RNN在每个时间节点上产生的softmax函数的话，慢慢的你会对根据单个softmax函数找到文本概率更感兴趣。我们可以选择一条通过整个softmax空间的路径，然后找到对应每个时间节点的字符。如下图，cc aa t，或者 cccccc aaaaa t预测同一个词可以有不同的路径和重复率，但是每个token都用空格符&lt;b&gt;隔开。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1418_58_941.png" alt=""></p><p><strong>端到端模型实例2：seq2seq with attention for speech</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1425_05_004.png" alt=""></p><p>给出了输入数据x（语音频率）和前i个输出y，预测y（i+1）的概率。</p><p>如上图，中间白色块是这个模型的解码器，也就是接受下部输入数据作为编码。他能馈入你生成的字符路径，因为他是RNN,你可以持续馈入字符并且字符长度不是问题。所以你在RNN的字符路径上馈入一个字符，然后你就能生成下一个token并且输出。 </p><p>需要实现的是让模型具有读取在时序空间上的不同部分数据的能力，因为输入数据真的很长，如果你看翻译结果的话，你会发现翻译的句子越长翻译效果越差，因为模型在定位需要翻译的内容的时候会有困难，尤其是输入的数据时音频数据流时。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1430_46_313.png" alt=""></p><p>音频数据比文本数据大多了，可能每秒有100组数据，比如你想存储10秒的音频输入，大概就有1000个token。所以这是个很恼人的问题，想要让模型工作的好的话一定要引入注意向量（attention）。</p><p>注意向量如下所示</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1434_08_958.png" alt=""></p><p><strong>端到端模型实例3:LAS模型（listen attend and spell）</strong></p><p>首先，下图是个LAS模型的编码器，图中ht和ht+1就是时间为t和时间为t+1时候生成的向量，在每一个时间节点上都使用解码器生成下一个字母，所以你做的就是提取解码器的状态向量，在RNN底层是解码器。现在来比较一下状态向量，每一个时间节点上的隐藏向量，从语义上讲，有这样一个查询，也就是状态s，将正在看到的ht放到了一个他可能出现的地方，你使用这个请求并将比较每个ht，你可以做一些简单的事情比如说点乘，（向量不必维数相等）。你也可以做一些更复杂的事情，比如将想要比较的隐藏向量和查询的向量串联成同一个向量，然后将他们输入到神经网络中生成一个数值。这里有一个et函数，<strong>et  = f([ht,s])</strong>,函数f将时间为t时的隐藏状态向量ht和RNN中的状态向量级联到一起作为一个解码状态，然后生成数字et。现在每一个时间节点上你都对编码器这么做，然后你就在时间上在编码空间上有了这么一个趋势，得到这个趋势et后（一串数字），然后把这一串数字丢给softmax，然后就可以得到一个注意力向量。如下的图片也清楚地展示了随着时间的变化，注意力向量变化的一个趋势。注意力向量告诉你在该时间节点上你看的位置应该是哪。然后接着向下一个时间节点，移动计算出新的注意力向量，依次计算。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1501_28_029.png" alt=""></p><p>现在已经有了注意力向量，现在就可以使用在时间节点上的概率来将隐藏状态混合到一起。然后得到文本值表示。在这里，你会取得所有与注意力向量的值有关的隐藏状态，将他们乘起来再加起来。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1507_03_732.png" alt=""></p><p>然后就会得到一个内容向量。这个内容向量就是一个会引导你做出预测的向量。有了它就可以把r和n的状态链接到一起，然后再传入到一个神经网络中就可以得到该时间点上的预测。输出的Y就是下一个token的概率</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191212_1506_45_828.png" alt="">    </p>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0.概念/NLP_注意力机制Attenion</title>
    <link href="undefined2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6Attenion/"/>
    <url>2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6Attenion/</url>
    
    <content type="html"><![CDATA[<h1 id="Attention注意力机制"><a href="#Attention注意力机制" class="headerlink" title="Attention注意力机制"></a>Attention注意力机制</h1><h4 id="什么是attention？"><a href="#什么是attention？" class="headerlink" title="什么是attention？"></a>什么是attention？</h4><p>根据数据对当前任务的重要性维持的权重数组。</p><p>CV举例，如下图所示，要预测的对象是黄框 - 狗子的左耳朵，那么对人类来说，预测重要的信息就是红框中的狗子的右耳朵和狗眼睛还有狗鼻子，这些东西可以判断出这是一只狗子，通过右耳朵和眼睛可以对左耳朵进行定位。<br>attention机制模仿人类的认知框架，着重注意红框中的有用信息，忽略例如背景等不重要的信息。<br><img src="http://bqlab-pic.test.upcdn.net/pic/20200507_1053_55_520.png" alt=""></p><p>NLP举例，如下图，对词和词之间的关系，分配不同的注意力等级。green修饰apple，eat和apple只见也构成谓宾关系，于是他们都被分类了强注意力，而eat和green没有只见关系，于是只被分配了弱注意力(数组中的权重值低)。<br><img src="http://bqlab-pic.test.upcdn.net/pic/20200507_1040_45_963.png" alt=""></p><p>进一步讲，注意力机制其实就是表示重要性的<strong>权重向量</strong>，比如上图识别狗子特征，发现嘴巴和鼻子这一块特别重要，于是就提高了权重值全部设置为1，可以全部采集，而其他不重要的地方比如背景之类就设置为0或者很小的数字，这样就降低了采集的强度。<br>为了预测图像中的元素或者句子中的单词，我们使用注意力权重来估计其他元素与其相关的强度。并将注意力权重加权的值的总和作为最终预测的目标的一个特征。</p><h4 id="attention计算步骤："><a href="#attention计算步骤：" class="headerlink" title="attention计算步骤："></a>attention计算步骤：</h4><ul><li>计算其他元素与待预测元素的相关性权重（狗鼻子和狗耳朵，背景和狗耳朵之间的权重），这一步拿到的是一批浮点数。</li><li>根据相关性权重对其他元素进行加权求和。也就是狗鼻子乘以0.9,背景乘以0.1，然后做加权求和。 </li></ul><h4 id="seq2seq问题"><a href="#seq2seq问题" class="headerlink" title="seq2seq问题"></a>seq2seq问题</h4><p>就是对于一个输入序列，我们需要将其转化为一个新的输出序列，其中输入输出序列可以是任何长度。典型的seq2seq问题包括了机器翻译，问答系统，语音识别等问题。<br>经典的seq2seq模型：Encoder-Decoder框架，这个框架的好处就是并没有把所有的输入都一视同仁的处理，而是按照词语和词语之间的关系分配不同的注意力。<br><img src="http://bqlab-pic.test.upcdn.net/pic/20200507_1040_59_649.png" alt=""></p><p> Encoder，decoder，上下文向量之间的关系， 注意力机制的好处：<br> 如下图，是一个机器翻译的任务场景，英语输入，汉语输出。she和她相关，吃和eat相关，苹果和apple相关，理想的情况是刚才罗列的三个词组对分别都用定制的上下文向量，而没有注意力机制的时候这一段话中每一个相关的词语对都公用一套上下文向量，效果可想而后。引入注意力机制后就可以定制每一个中文词语和分别每一个英语词组的注意力高低值。<br><img src="http://bqlab-pic.test.upcdn.net/pic/20200507_1041_30_521.png" alt=""></p><p>如下是seq2seq的另一个框架（Bahdanau 2015），输入source，输出target，在souce和target之间是Attention层，source和target与attention层通信的介质是Encoder和decoder。Encoder是一个双向RNN，decoder是一个动态的上下文向量+带状态的RNN。<br><img src="http://bqlab-pic.test.upcdn.net/pic/20200507_1041_15_437.png" alt=""><br>之前没有注意力机制的时候，模型和上图也类似，无外乎是拿出Encoder里边的双向RNN的正向最后一个向量和反向第一个向量拼接，然后做一个maxpooling。</p><p>有了注意力机制后，如下图，2015模型中的注意力层：每一个a(t,n)都是一个输出值对应全部输入值的权重组。比如她和she的atn值是0.8，她和is的atn值就是0.03。而且这个注意力层的参数的初始化和确定都是通过简单的前馈神经网络。<br><img src="http://bqlab-pic.test.upcdn.net/pic/20200507_1041_42_379.png" alt=""></p><p>另一个seq2seq中应用注意力机制的图片实例：在cs224n上有提到过，其实就是源头语言和目标语言之间的每一个单词互相建立二维数组，然后给出他们之间的相关性。如图，1992和1992强相关，和其他任何单词都不相关，was signed这个过去时态的sign也和法语中的相应过去时态的sign强相关，和其他词不相关。</p><h4 id="注意力机制的来源（改进机翻）"><a href="#注意力机制的来源（改进机翻）" class="headerlink" title="注意力机制的来源（改进机翻）"></a>注意力机制的来源（改进机翻）</h4><p>如果我们使用一个固定维数的向量Y来表示输入，也就是编码器的最后一个输入状态，而整个解码过程我们都依赖这个Y在，这样做的话翻译的效果仅在短句上表现的好，长句子表现较差。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1145_18_548.png" alt=""></p><p>所以人们想到了一种注意力机制的解决方案，这种方案最早在计算机视觉方向被提出：</p><p>与其使用编码器的最后一个隐状态Y生成目标语句，不如直接使用编码过程中所有的隐层编码向量，也就是所有源对应的隐层状态来进行翻译工作。这样以来这些源状态成为了类似于随机存储器的东西，后边的神经网络可以根据翻译的需要访问他们。如下图所示，这样以来我们除了有了LSTM的增强记忆，同时还有了按需访问的记忆长度长的多的架构。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1150_06_551.png" alt=""></p><p>如下是注意力模型示意图，注意力机制就是手动维持了目标语言和源头语言中的每一个单词的两两相关关系。相关性强的在完成目标任务时会被分配更多资源。<br><img src="http://bqlab-pic.test.upcdn.net/pic/20200507_1042_09_370.png" alt=""></p><p>下面要预测的单词应该去访问之前状态层（蓝色）的哪个内存单元？</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1201_08_059.png" alt=""></p><p>现行做法是使用上一层的状态来作为注意力机制的判别标准。具体做法是使用结合上一层的状态和当前隐层的元素来生成一个分数(后边会详细解释),把得到的分数丢给softmax函数，返回一个概率分布权重,根据这个概率分布来决定注意力的分配。softmax函数示意图如下</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1209_30_771.png" alt=""></p><p>在此之后，我们将编码器所有的隐层状态根据注意力机制给出的权重将其组合起来，具体来说，拿到编码器的每一个隐层的状态（蓝色块），和注意力机制给出的权重，然后求它的加权和，这个加权和就上下文向量。这样我们子在预测下一个词的时候就可以不是简单地使用最后一个隐层状态作为预测的标准，而是用到了编码器的全部隐层状态信息。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1213_09_050.png" alt=""></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1213_22_190.png" alt=""></p><p>具体如何得到这个分数（被喂给softmax的那个分数）？</p><p>如下图所示，使用单层神经网络，就像LSTM内部那些普通神经网络一样，我们这里使用了两个隐层状态的拼接，做一次矩阵乘法后再通过一次tan函数变换。其中第二个双线性注意力函数相当成功，在两个点乘之间插入了一个矩阵W，这种矩阵实际上学习到了如何将不同权重分配到点乘的不同部分。这是实现注意力机制的想法之一，这样可以使ht和hs之间产生某种交互。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1221_22_000.png" alt=""></p><p>对于简单的注意力模型，有一个注意力函数覆盖了整个源编码，并且将此函数的输出作为权重，这是一个简单的函数容易学习，也是一个连续可微的模型，但是如果序列过长，其计算的复杂度就令人头疼，因为如果你从反向传播算法的角度来考虑，你需要每时每刻将误差传递到所有位置。因此人们开始寻求局部注意力模型，也就是每次只将注意力放到一部分状态的模型，这更像是从记忆里检索出某些事物的概念。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1226_17_470.png" alt=""></p><p> 如下红线演示了不带注意力的模型，在短句子上表现不错，长句子down很快。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1230_53_670.png" alt=""></p><p><strong>注意力机制在计算机视觉中的idea</strong></p><p>如下图演示，注意力在不同的地方，得到的输出不同，比如鸟儿，数次，背景等等。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191210_1234_55_097.png" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>概念</category>
      
    </categories>
    
    
    <tags>
      
      <tag>注意力机制</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>3.课程/CS224n_1 - 导论</title>
    <link href="undefined2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_1%20-%20%E5%AF%BC%E8%AE%BA/"/>
    <url>2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_1%20-%20%E5%AF%BC%E8%AE%BA/</url>
    
    <content type="html"><![CDATA[<h1 id="Lecture1-导论"><a href="#Lecture1-导论" class="headerlink" title="Lecture1-导论"></a>Lecture1-导论</h1><h4 id="1-本课程研究对象"><a href="#1-本课程研究对象" class="headerlink" title="1.本课程研究对象"></a>1.本课程研究对象</h4><p>NLP是计算机科学和语言学的交叉学科。</p><p>语言是人类特有的属性，人类通过语言思考、交流、行动。</p><p>nlp要解决的核心问题是让计算机了解人类语言，然后完成有意义的任务（知识图谱，语义识别等）</p><p>nlp的步骤</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1315_43_291.png" alt=""></p><ul><li>预处理：首先speech和text都会被拆分</li><li>构词法分析：Morphological analysis，拆开ing和ex等前缀后缀，还原不同时态的词根。</li><li>句子结构分析：Syntactic analysis，比如主语是我，宾语是你，谓语是艹。</li><li>语义解释：Semantic interpretation，得出句子的含义。仅知道每个单词的含义是不够的，许多单词在一块构成了上下文，许多含义是依靠上下文传递的。这就引出了语用学和语篇处理等研究领域。</li></ul><p>cs224n这门课主要讨论<strong>句子结构分析</strong>和<strong>语义解释</strong></p><h4 id="2-人类语言的特殊之处"><a href="#2-人类语言的特殊之处" class="headerlink" title="2.人类语言的特殊之处"></a>2.人类语言的特殊之处</h4><p>做信号处理，数据挖掘之类的事情时人类往往只用视觉系统识别数据，然后处理数据。</p><p>但几乎所有的人类语言都是某人想要传递某个信息，他为此构建了一条信息，并试图传递给另一个人。语言有明确的指向性。</p><p>人类的语言系统是一套离散的符号分类信号系统，如下图</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1316_33_231.png" alt=""></p><p>人类语言在一些富有表现力的场景下会有一些小区别</p><pre><code>I loooooooooove it.            Whoooooooooooooooha</code></pre><p> 据推测，这些特征都是基于EE信息理论，因为有了符号，所以能可靠地进行远距离信息传输。</p><p>语言就是符号，符号不基于逻辑。</p><p>人类在交流时想表达的内容包括离散的符号，但具体沟通方式是使用了一种连续的载体。而且人类可以用不同的连续载体来传递完全相同的信息。比如声音/手势/文章。这就很有趣了，因为表达载体是连续的，而语言这一套符号系统是离散的，而大脑处理这些符号又是连续的。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1316_40_260.png" alt=""></p><p>哲学史和科学史上的主导观点是，将语言的符号体系映射到我们大脑中去，大脑可抽象为一组符号处理器。深度学习领域产生的一个有意思的想法是应该把大脑想象成具有连续的激活模式，上图显示的就是我们从连续到离散再到连续。语言通常都有巨大的词汇库（几万词），像英语还有巨大的科学词汇库（十几万词），在加上无穷无尽的构词法，可以说有些语言的词汇是无限的。</p><h4 id="3-深度学习介绍"><a href="#3-深度学习介绍" class="headerlink" title="3.深度学习介绍"></a>3.深度学习介绍</h4><p>首先深度学习是机器学习的一个分支，基本思想是怎样让电脑自动学习，而不是传统那样人工写代码，明确告诉计算机你想做什么。对于大多数传统的机器学习，他们都围绕着决策树和逻辑回归，朴素贝叶斯，SVM等，这些本质都是人类仔细审视一个特定的问题，找出这个问题的关键要素，设计出该问题相关的重要特征要素，然后手工写代码，识别这些特征，这些特征可以是单词是否大小写，单词是否有连字符等，这个系统最终可能会有数百万的人工设定的特征，其实2015年之前谷歌搜索就是这么做的，他们的具体做法就是工程师通过实验演示，如果添加了这些额外的特征，谷歌搜索效果就会好一点点，如果大家一致同意就加入这个特征。这种方式当时被称为机器学习，但是机器到底学习了什么？</p><p>机器只是在做数值优化，当收集好了特征，就会建立一个线性分类器，给每个特征量前加一个参数权重，而机器学习系统的工作就是调校这些数字来优化表现。电脑非常擅长做数值优化，如果有100个特征，每个特征之前要放一个实数，使得表现最优，人类对此只有一个模糊的概念，但是数值优化是否是计算机在机器学习中唯一该做的事情呢？</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1316_47_910.png" alt=""></p><p> 如图，前边大部分工作都是人类主导的特征工程，后边的数值优化算法交给计算机。特征工程是决胜的关键，数值优化算法大家用的都差不多。</p><p>深度学习是表征学习的一个分支，表征学习的理念是，我们可以只向电脑提供来自世界的原始信号（视觉/语言）电脑就可以得到好的中介表征，来很好的完成任务。某种意义上来讲，他是自己在定义特征，和过去人类自己定义特征一样。深度学习的真实含义是指，你将得到多层的习得表征。<strong>机器学习自己要定义特征，表征学习只把表面信息丢进去，特征由模型自己定义</strong>。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1316_58_968.png" alt=""></p><p>神经网络其实是<strong>栈逻辑回归</strong>，或者更广义的堆栈线性模型。</p><p>为什么深度学习如此激动人心？因为一般我们手动设计的特征往往太过具体，且不完整，需要花很长时间来验证和设计，而最终只能达到一定的表现水平。</p><p>但习得的特征适应性强，训练得快。他们可以不断学习，不断提升表现水平，比之前的作品要好。深度学习提供了非常灵活通用的学习架构，可以表示各种类型的信息。</p><p>从2010年开始，深度学习的表现就优于传统机器学习，最近六七年发展突飞猛进。从某种意义来说，之说以发展如此之快，因为现实使用的很多深度学习关键技术，其实是在8090年代发明的，只是当年他们并没有真正发挥作用，现在我们有了海量数据，计算机也有了足够的算力，这两个条件8090年代是不具备的。因此经过了几十年的计算能力的提升，现在可以搭建有效的系统了。</p><p>深度学习也被证明非常适合并行向量处理，可以在GPU中以非常廉价的方式实现（tensorflow，pytorch）。</p><h4 id="4-本课程教学目标"><a href="#4-本课程教学目标" class="headerlink" title="4.本课程教学目标"></a>4.本课程教学目标</h4><p>首先是理解并有能力运用有效的现代方法进行深度学习，我们将涵盖所有的基础知识，特别强调的是NLP中使用的主要方法，例如RNN循环神经网络，注意力机制等。</p><p>对人类语言要有总体的了解，以及目前人们理解和产生语言中遇到的困难。</p><p>第三点是这两个内容的交集，具有为重要NLP问题构建系统架构的能力。（会在作业中遇见这类问题）</p><h4 id="5-为什么NLP很难"><a href="#5-为什么NLP很难" class="headerlink" title="5.为什么NLP很难"></a>5.为什么NLP很难</h4><p>自然语言是模糊的，而编程语言在设计之初就是非常明确的。比如自然语言else在翻译中只是和看上去最合理的那个if合成一对，而编程语言中if和else严格按照最近的为一对。</p><p>自然语言作为一种高效的沟通工具，其中高效的关键就是大量使用省略。写程序会明确写出需要它运行的一切，但自然语言就像Stack Overflow上的代码段一样，听众需要自己去补全缺失的上下文代码。一句话来讲，自然语言需要“大家都懂的”。</p><p>自然语言通信靠的是用嘴说用耳朵听，这和5G通信信道比起来是非常慢的信道，但是它能有效运作的关键是沟通者只用说出最少量的信息，听众会根据自身的世界性常识和交流情境来自动补全剩下的部分。</p><p>所以说切换语言和国家之后，感到沟通困难的原因就是共识变少，要我们自己靠常识补全的部分非常不一样，<strong>“说话是冰山一角，没说的常识是冰山”</strong>。英语世界不会明白“天行健，君子以自强不息”，同样，只会说two car pengpeng one car die的老移民必然不明白boundary是什么。</p><p><strong>一个引申</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1317_26_340.png" alt=""></p><p>我们就是如此不可思议的复杂动物，漂浮在虚空之中，想要链接他人而总不得不盲目地向黑暗中蹦出词语，    每一种解读，拼读，语音语调，以及说话的时机都蕴含了丰富的信息，语境和言外之意。而每一个听众都会    以自己的方式来解读这些信号。自然语言并不是一种语言的正式系统，而是一种华丽的混沌。你永远不知道    某个词对某个人意味着什么，你所能做的，就是试图更好地猜测你的词语如何影响别人。你还是有机会找到    那些合适的词语，让对方按照你的意图来理解它的含义。而其他的一切毫无意义。</p><h4 id="6-Deep-NLP-深度学习-NLP"><a href="#6-Deep-NLP-深度学习-NLP" class="headerlink" title="6.Deep NLP = 深度学习 + NLP"></a>6.Deep NLP = 深度学习 + NLP</h4><p>深度学习近些年在如下三个方面有巨大突破：</p><ul><li><p>Level：speech,word,syntax,semantics</p><p>词汇学，语义学，句法学等</p></li><li><p>tools:  part-of-speech,entities,parsing</p><p>比如为标注词性，命名实体识别，parsing</p></li><li><p>applications:   machine translation,sentiment analysis,dialogue agent,question answering</p><p>applications:   machine translation,sentiment analysis,dialogue agent,question answering</p><p>机翻，情感分析，聊天助手等</p></li></ul><p>有意思的是，深度学习运用一套模式，解决各个领域的问题。<br>深度学习和语言学都是以词义为起点，这是一个非常核心的想法，具体做法是词语向量表示。一般会用最少25维的向量，通常是300维的向量。有了高维向量，说明我们将单词放在了高维向量空间中，这些空间就变成了非常棒的语义空间，具有相似含义的词汇将在矢量空间中形成聚集块，不止如此，我们会发现，向量空间中存在方向，它们会披露关于成分和意思的信息。人类的问题是不擅长解读高维空间，人类总是不得不将高维映射成二维或者三维才能理解。如下图所示，看到的词云其实是从高维映射下来的，相似的名词比如国家就在相近的地方。</p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1317_35_665.png" alt=""></p><h4 id="7-从语言学的几个层面看DL与传统方法的异同"><a href="#7-从语言学的几个层面看DL与传统方法的异同" class="headerlink" title="7.从语言学的几个层面看DL与传统方法的异同"></a>7.从语言学的几个层面看DL与传统方法的异同</h4><ul><li>传统语言学和DL在处理构词法上的区别：</li></ul><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1317_44_699.png" alt=""></p><p>传统语言处理句子就是分析构词法。DL是通过把单词向量表示，在处理构词法的时候采用了树结构去拼装。</p><p><strong>助教分享</strong></p><p><img src="http://bqlab-pic.test.upcdn.net/pic/20191207_1318_13_449.png" alt=""></p><p>她强调向量的重要性，所有这些神奇的事情背后都运用的是向量，我们用这个来表示所有的语言形式，无论是声音，单词，句子，他们都变成了含有真实值的向量。其实向量要比人意识到的还要微妙许多。你可能觉得这么庞大的数字向量里没有结构，我们也可以将这个向量变形，变成一个矩阵或者更高阶的阵列（张量），其实向量是一个非常灵活的数据结构，具有强大的表达能力，这也是深度学习的真正强项。</p><p>第一课陈丹琦的分享ppt中其实涵盖了很多后边才讲的内容，比如词性标注，比如词语的向量表示，比如输入层–隐藏层–softmax输出层等等。</p><pre><code>小结：NLP是语言学和计算机科学的交叉学科，既然是交叉学科，我现在假设各位DL大佬对DL的了解要远远多于对语言学的了解，而语言学排除了可能存在的迂腐研究方法之后仍然是一门有悠久历史和大量沉淀的值得尊重的学科，如果能发掘出这个学科内部不足为外人道的真知灼见，也许有助于在NLP上做出突破。找准自己的研究方向超重要，nlp作为一个新兴领域刚出现时肯定无人即精通语言学又精通DL，那个时候如果自己有深厚的积淀并选择了正确的导师果断进入行业内展开研究，多少能搭上一点历史进程的便车。现在nlp问题也火了好几年，语音处理已经做得非常不错了，机器翻译也有长足的进步，各分支领域大牛已经出现。自己不是天才，到处都是天资更高基础更扎实的业内人士，不选准方向就很难进入学界，就算进入了也可能是炮灰毕不了业，或者勉强毕业但方向大错误，一辈子也只是个平庸之人。想想自己的经历，若能找一个力学/土木/采购+计算机交叉的方向来做，这是沉没成本最低的路径，当然这个路径不一定存在或者非常坑。若经过【调研】发现上文说的交叉方向不靠谱，那就想办法选一个风口，跟上时代的大势去做研究。**如何摸到时代的脉搏？一个思路是对当前cs界的所有知名学者的个人页面爬数据，把他们的当前研究兴趣这一栏做聚类分析**，这个工作做完应该能对当下技术的走向有些眉目，当然，这个过程中也得有自己的独立思考。【2019.12.15批注】：上边这一段是11.20刚开始看NLP这门课的时候写的，我艹，这门课居然看了25天，进度太鸡儿慢了，做事一定要有可行的计划。CS224N第一次出现在计划里的时候是还在武大实验室的时候，我还记得当时的计划是一个星期看完这门课，外加很多其他的算法+后端的任务。结果老是完不成。其实就好好做做可行的计划，可能看完NLP也不需要花25天这么久。【2019.12.15批注】：上边这一段要寻找力学/土木/采购和计算机交叉学科的想法非常错误，因为我本身在力学和另外这两个领域都学的一塌糊涂，不要欺骗自己。数据结构的基本类型我能说的头头是道因为我真的好好学了，剪力图和弯矩图现在还是一塌糊涂，本科怎么混毕业的不用多说了8，心里千万不可没b数。NLP问题解决好的话就能对人类千万年历史积淀下来的浩如烟海的文字记录中提取出大量的宝贵知识，在此问题解决之前不会有任何一个人有精力或技术去了解古今中外所有典籍中的内容并做出提炼归纳总结。这是非常honor的工作。“本课程的教学目标是能让你使用现代方法进行深度学习，同时对人类语言也要有所了解。在这两者都具备的情况下，要能为nlp问题构建一个系统（炸翻世界的牛逼系统），这个项目不必写项目计划书【调研项目计划书】，因为导师已经帮大家设计好了整个项目&quot;。斯坦福的大作业居然是要写一个能用的nlp处理框架……这里大胆猜测，李沐的parameter server，mxnet等系统，最初的灵感和自信就是来源于这种课程的大作业吧【李沐的ps系统以及2+1篇论文调研，看看他到底怎么就两个月发文的】。斯坦福的大作业让我想到冷奕在面试亚麻的时候也把自己计算机课上的大作业拿出来说事儿，这个大作业是真的知识输出，我是不是也应该在kaggle上整个比赛，不说拿名次发论文了，可以当成我的NLP截课大作业啊，毕竟知识输出和输入一样重要，只输出文档还是不行。斯坦福课程的学习强度和创造性level很高，如果没有良好的积极性肯定不能完成学业。要发掘自己学习的热情和积极性，否则就算申上phd也难以胜任科研工作。学术是一种志业，科研是一种lifestyle，lifestyle是对生活的理解和自己想活成的样子，这种lifestyle应该是愉快和始终如一的。</code></pre>]]></content>
    
    
    <categories>
      
      <category>3.课程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
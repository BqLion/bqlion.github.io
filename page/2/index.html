<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <link rel="icon" type="image/png" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="description" content="desc">
  <meta name="author" content="LiuBingqian">
  <meta name="keywords" content="">
  <title>刘秉乾的技术博客 :)</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"  >
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css"  >
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css"  >
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css"  >

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css"  >

<link rel="stylesheet" href="/css/main.css"  >


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css"  >


<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header style="height: 100vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>LiuBingqian`s Blog</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">主页</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">分类</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">所有文章</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">标签</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">关于</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>


</nav>

    <div class="view intro-2" id="background"
         style="background: url('http://bqlab-pic.test.upcdn.net/index.jpg')no-repeat center center;
           background-size: cover;
           background-attachment: fixed;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
          </div>

          
            <div class="scroll-down-bar">
              <i class="fas fa-angle-down scroll-down-arrow"></i>
            </div>
          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      <div class="container nopadding-md">
        <div class="py-5 z-depth-3" id="board">
          
          <div class="container">
            <div class="row">
              <div class="col-12 col-md-10 m-auto">
                


  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/15/0.%E6%A6%82%E5%BF%B5/0.%E6%96%87%E7%AB%A0/NLP_The%20Annotated%20Transformer/">
        <p class="h4 index-header">0.概念/0.文章/NLP_The Annotated Transformer</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">The Annotated Transformerharvard nlp 对Transformer论文的逐行注解,原文：http://nlp.seas.harvard.edu/2018/04/03/attention.html
本文在github和谷歌drive上的网址：
https://github.com/harvardnlp/annotated-transformer
https://drive.google.com/file/d/1xQXSv6mtAOLXxEMi8RvaW8TW-7bvYBDF/view
Import lib:import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import math, copy, time
from torch.autograd import Variable
import matplotlib.pyplot as plt
import seaborn
seaborn.set_context(context=&quot;ta</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-15&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/15/0.%E6%A6%82%E5%BF%B5/NLP_%E8%AF%8D%E5%90%91%E9%87%8F_WordEmbedding%E8%AF%A6%E8%A7%A3/">
        <p class="h4 index-header">0.概念/NLP_词向量_WordEmbedding详解</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Word_embedding独热码的缺点：
它只考虑了单个单词在向量中的作用，而没有考虑词语与词语之间的关系。
如下，有一个训练好的语言模型，要对“I want a glass of orange  ____”进行识别，并预测接下来的词，那么独热码就不能知道apple 和 orange之间的词义相近性，于是不能在算法习得了I want a glass of orange juice之后，泛化地预测 I want a glass of apple juice，这是因为orange和apple在独热码中的编码是完全随机的，并没有因为词性相近就结构也相近，456和6257的位置并没有章法（随机或者按照词频分布），并且由于独热码的结构特殊性，任何两个独热向量相乘的结果都是0。
特征化表示：
所以自然地，我们使用这些词的特征化表示，比如把性别作为一个维度来度量这些词，男的是-1，女的是1，然后king就可以是-0.95,queen就是0.97，橘子和苹果就是基本性别中性的词。另一个维度是有多高贵，明显king和queen是高贵的词而apple和orange的高贵属性为接近0的数字。

如下是对词</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-15&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/14/0.%E6%A6%82%E5%BF%B5/NLP_%E8%AF%8D%E5%90%91%E9%87%8F_SkipGram/">
        <p class="h4 index-header">0.概念/NLP_词向量_SkipGram</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">skip-gram算法详解Skip-gram算法是word2vec模型中两大生成词向量算法中的一个。
生成词汇向量的skip-gram算法的中心思想：给定中心词汇，预测单词们出现在它周围的概率，最终确定的这些单词们，要使概率分布值最大化。
如下，中心词是banking，Skip - gram会尝试去预测它一定范围内（m window）的上下文词汇。

Skip-gram的损失函数损失函数J`(θ)表示的是：一串很长的文本比如整个维基百科（which has足够长的词汇序列和足够多的词汇和真正的行文）然后遍历文本中的所有位置，对于每个位置，我们都会定义一个围绕输入词汇的大小为2m的窗口，窗口包括前后各m个单词，把每个词汇和周围的窗口的p(周围每一个词|中心词)都连乘起来。
这就是一个有监督学习，目标是使损失函数最小化，手段是通过对训练集的training动态调整模型的超参数们(窗口大小，容差系数等).

损失函数plus : 计算机中大量连乘容易误差累计和误差消失，给上边的损失函数整体套上log，将连乘转换为带log的连加。如下
要不是高中学的明白根本肝不到这里……answer the </div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-14&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/13/0.%E6%A6%82%E5%BF%B5/0.%E6%96%87%E7%AB%A0/NLP_word2vec%E5%BA%94%E7%94%A8/">
        <p class="h4 index-header">0.概念/0.文章/NLP_word2vec应用</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">word2vec应用读取数据import pandas as pd

# Read data from files 
train = pd.read_csv( &quot;labeledTrainData.tsv&quot;, header=0, 
 delimiter=&quot;\t&quot;, quoting=3 )
test = pd.read_csv( &quot;testData.tsv&quot;, header=0, delimiter=&quot;\t&quot;, quoting=3 )
unlabeled_train = pd.read_csv( &quot;unlabeledTrainData.tsv&quot;, header=0, 
 delimiter=&quot;\t&quot;, quoting=3 )

# Verify the number of reviews that were read (100,000 in total)
print (&quot;Read %d labeled train reviews, %d labeled test</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-13&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/12/0.%E6%A6%82%E5%BF%B5/NLP_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/">
        <p class="h4 index-header">0.概念/NLP_神经网络和反向传播算法</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">神经网络神经网络深入学习的网站：neuralnetworksanddeeplearning.com 
神经网络就是一个函数，本例中有764个输入，有10个输出 
总览图:
第一层(764)：
764个输入源于28 * 28的正方形格子，每个小格子都是0到1之间的值，越接近1就越白，越接近0就越黑。
隐藏层
隐藏层是干什么的？ 大体上说，隐藏层用于发掘模式。如上所示，隐藏层一识别小线段，隐藏层二识别小线段构成层的圈或者大线段
从这里看出深度神经网络其实就是模式识别的高级应用，所以要恶补pattern recognition。
如何训练神经网络？ 训练神经网络就是把训练集中的数据和label在神经网络中的(784 * 16 + 16 * 16 + 16 * 10) + (16+16+10) =13002    个参数通过动态调整来拟合。
从数学上来说，动态调整13002个参数以获得min(代价函数)的做法，本质上就是寻找函数的最小值，如下

w1和a1： 
a1就是输入的784个点中第一个点的亮暗情况
w1就是接受这第一个点a1刺激的第一层的第一个神经元对第二层的如图所示的神经元的兴奋程度</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-12&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/12/0.%E6%A6%82%E5%BF%B5/NLP_%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/">
        <p class="h4 index-header">0.概念/NLP_随机森林</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">随机森林随机森林就是用随机的方法建立一堆决策树。每棵树之间是没有关联的，当一个待分类样本输入森林中时，将此样本输入进每一个决策树然后看哪种分类结果最多，就最终输出哪种分类结果。
随机森林的构造过程：

假设有N个样本，有放回的随机选择N个样本，然后按照这N个样本训练一个决策树，这N个样本就是决策树根节点的样本
当每个样本有M个属性时，就需要在决策树中分裂节点以做进一步的判断。于是从M中随机选出m个(m &lt;&lt; M)个属性，然后从m个属性中采取某种策略（比如min信息熵）来选择一个属性来作为该节点的分裂属性。
然后就是重复上边的步骤。当下一次分裂选出来的属性还是父节点的属性时，说明已经穷尽属性，到达叶子节点。

按照如上三个步骤，可以构造大量决策树，例如movie sentiment tutor step1中的随机森林就是构造了100颗决策树。
注意：随机森林中的决策树并没有为了回避过拟合问题而剪枝，因为随机采样已经避免了过拟合。
此算法优点：

数据上表现好，两个随机性的引入避免了过拟合问题
抗噪声
能处理高维度数据，而且不用做特征选择。
对数据集的适应能力强，既能处理连续性</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-12&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/12/0.%E6%A6%82%E5%BF%B5/NLP_%E5%86%B3%E7%AD%96%E6%A0%91/">
        <p class="h4 index-header">0.概念/NLP_决策树</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">决策树决策树是一种树形结构分类器。每个节点都有一个表示属性的判断，每个分支都代表了一个判断结果的输出。
常见的决策树生成算法有ID3,C4.5,C5.0,CART等。
示例：如下图片是正确的分类集（train set）

如下是分类树的判断过程（演示了分类树其实可以有不同构型）


如上分类树的根节点都是判断分数是否大于A（90），如果是就是好学生。然后第二个节点就出现了不同，分别是判断分数是否小于70 OR 出勤率是否大于0.8。
这个分类器节点的构造其实很灵活，怎样构造最好的分类器呢？ID3,ID4.5算法都是倾向于迭代选择熵值最小化的那个分类器。但是ID3,ID4.5算法都有严重的过拟合问题。
CART算法也使用了类似熵的指数–GINI指数，CART算法迭代的目标也是向着GINI指数最小化的方向进行，不过CART算法为了解决过拟合的问题，使用了剪枝的策略，也就是将树中过长的枝叶剪去。
</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-12&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/12/0.%E6%A6%82%E5%BF%B5/NLP_%E8%AF%8D%E5%90%91%E9%87%8F_%E8%AF%8D%E8%A2%8BBOW/">
        <p class="h4 index-header">0.概念/NLP_词向量_词袋BOW</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">词袋算法详解词袋算法是word2vec中构建两大构建词向量算法中的一个。
kaggle tutor实战解读：具体操作如下：

清洗整个语料库by美丽汤
去掉停止词和非英语单词，去掉数组，大小写统一by正则表达式
将剩下的单词组成词典
词典按照词频降序排列
然后指定词袋的feature个数，以五千为例就是前5000个常用单词作为每一个词语/句子的向量表示。
每个向量都是5000维，第一维就是最常用的单词，第二维就是第二常用的单词，第一维是1说明被本向量表示的句子中最常用的单词出现了一次，第一维是2说明被表示的句子中最常用的单词出现了两次。


然后可以将例如25000条语料依次分别按照5000维的向量表示，集合到shape(25000 * 5000)的矩阵中

然后此矩阵带上label就可以用来训练model了
CS224N解读词袋的训练模型如下。

输入层：上下文的独热码，假设向量空间V维，上下文单词个数是C。

所有onehot分别乘以共享的W（V*N）初始化权重矩阵，V是向量维度，N自己设置。

因为是onehot向量，只有一个值为1，其他都是0，所以乘以权重矩阵后仍然是向量，然后</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-12&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/08/0.%E6%A6%82%E5%BF%B5/0.%E6%96%87%E7%AB%A0/NLP_%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8/">
        <p class="h4 index-header">0.概念/0.文章/NLP_词袋模型应用</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">词袋模型的应用1.引入训练数据
import pandas as pd
train = pd.read_csv(&quot;labeledTrainData.tsv&quot;, header=0,delimiter=&quot;\t&quot;, quoting=3)
2.确定引入数据是否正确 – 看一下shape和列标签
&gt;&gt;&gt; train.shape
(25000, 3)

&gt;&gt;&gt; train.columns.values
array([id, sentiment, review], dtype=object)
3.glipise
print train[&quot;review&quot;][0]
4.用beautiful soap清洗掉HTML标签
# Import BeautifulSoup into your workspace
from bs4 import BeautifulSoup             

# Initialize the BeautifulSoup object on a single movie revie</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-08&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/07/0.%E6%A6%82%E5%BF%B5/0.%E6%96%87%E7%AB%A0/NLP_%E4%BB%8EWordEmbedding%E5%88%B0Bert/">
        <p class="h4 index-header">0.概念/0.文章/NLP_从WordEmbedding到Bert</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">从Word embedding到Bert：A brief history of nlp pre-train modelWord embeddingLM做word embedding –&gt; 使用的工具是word2vec –&gt; word2vec的训练方法：CBOW 和 SKIP - GRAM
word embedding的问题：无法解决一词多义。
​                                                两种不同的上下文信息经过word2vec都会预测相同的单词bank，因为同一个单词占                                                用的是同一行的参数空间。所以WE无法区分一词多义。
ELMO为解决WE一词多义问题，提出ELMO，其思路是事先用LM学好 一个单词的word embedding，后边根据上下文动态调整。WE是静态，ELMO是动态，所以ELMO可以解决一词多义问题。
ELMO工作方式分为两段，分别是利用语言模型进行预训练和做下游任务时将预训练网络中提取对应单词的网络的各层的wor</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-07&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/07/2.%E6%AF%94%E8%B5%9B/Toxic_%E6%80%9D%E8%B7%AF/">
        <p class="h4 index-header">2.比赛/Toxic_思路</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">思路ToDoList







bpe
跨语言编码


XLM-R + ERNIE
kaggle最热跨语言模型 + 注入特定知识toxic


transformer
特征抽取器(模型)要向匹配问题领域的特点去修改


lstm
transformer组件










选择正确的特征抽取器解决情感分类问题，从模型角度讲，最重要的是特征抽取器的能力。以前是研发人员设计抽取哪些特征，现在都是端到端的抽取，也就是特征抽取器Transformer自动抽取。根据今日阅读知乎和github上的观点，不要使用RNN和CNN，应该采用更加先进的Transformer。
</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-07&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/07/3.%E8%AF%BE%E7%A8%8B/CS224n_9%20-%20%E6%9C%BA%E7%BF%BB/">
        <p class="h4 index-header">3.课程/CS224n_9 - 机翻</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">机翻对于机器翻译这种任务，你可以抛弃那些考虑规则的想法。情感分析类任务也许可以建立词汇的正面或者负面表格，然后根据这些词汇的词性进一步延伸出其他规则或系统，但是翻译类问题baseline一般都是基于统计的，因为没有人可以遍历一门语言翻译为另一门语言时所有的规则和异常语言。
我们总是倾向于使用相当庞大的语料库，我们通常称为平行语料库。在平行语料库中，很多相同的段落平行地以多种语言表达。最经典的平行语料库应该就是圣经了。
基于统计的机翻模型介绍
源语言是法语，目标语言是英语，利用贝叶斯准则的概率公式,也就是后验概率 = 显先验概率*似然，然后除以边缘概率。这里的边缘概率可以是源语言。其中p(f|e)是翻译模型，p(e)是语言模型，语言模型也就是我们试图计算更长序列概率的那个模型。

如下是机器翻译模型的工作流程：首先翻译模型会把一个法语句子基于统计列出一系列可能的英语选项。然后使用语言模型，在这些选项里找一个简单流畅的句子。

其中的一个模块，翻译模型中，第一件要做的事情是匹配两种语言的单词，匹配所面临的困难使用法语英语互译来展示。其实英法是相似度很高的语言，换成其他相似度低的语言则难度会</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-07&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/04/28/3.%E8%AF%BE%E7%A8%8B/CS224u_Sentiment_Analysis_%E6%9C%89%E7%9B%91%E7%9D%A3%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90.md/">
        <p class="h4 index-header">3.课程/CS224u_Sentiment_Analysis_有监督的情感分析.md</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Sentiment_Analysis_有监督的情感分析.md
如上所示，phi函数就是上篇介绍的feature function，提供的功能就是把tree转换成字典。

如上所示，fit model函数提供将数据x和y fit进指定模型（逻辑回归）中的功能。

然后使用上节介绍的sst.experiment函数（三个参数）分别是路径，feature函数和fit函数。
这样设计的好处就是实现了类的分离，如果需要改写feature函数或者fit函数，直接在本体里边修改就好。
超参数的搜索与确定超参数的基本原理：超参数是模型的优化过程之外的“settings”。有如下几个例子：

超参数搜索实例

如上，In[4] 的 fit_softmax_with_crossvalidation函数，就是搜索超参数的函数。basemod是逻辑回归，给他加上了搜索空间：c,罚函数，等，然后调用系统工具箱里的utils.fit_classifier_with_crossvalidation就可以得到搜索结果。当然这个搜索的过程耗时较长。
然后把得到的函数放进experiment函数中开始实验。因为得到的函数</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-04-28&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/04/28/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/Chrome%20OS%E5%88%9D%E5%A7%8B%E5%8C%96%E6%93%8D%E4%BD%9C/">
        <p class="h4 index-header">4.安装调试记录/Chrome OS初始化操作</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Chrome OS初始化操作0.进入开发者模式，esc + f３ + 电源，then ctrl + D 
1.参见“共享代理文章”，用手机和电脑开启代理共享，chromebook连上wifi，登陆谷歌账号，不断重试，连接google play（这一步非常耗时）。
2.安装包管理工具crew  Ctrl + Alt +t  curl -Ls https://raw.github.com/skycocker/chromebrew/master/install.sh | bash
3.安装ssr  在主流的apk网站上下载并安装ssr，登陆amytelecom，将信息复制到ssr中，删除无关的免费节点。
4.安装马克飞象，印象笔记
5.setting中安装linux，打开linux命令行输入sudo apt install vim-gtk3安装gvim
6.用gvim打开rsa_pub，在linux中连接github，将post项目拉下来
</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-04-28&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/04/23/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/chromebook%E4%BB%A3%E7%90%86%E5%85%B1%E4%BA%AB/">
        <p class="h4 index-header">4.安装调试记录/chromebook代理共享</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">1.下载ssr
2.google免费ssr节点，复制粘贴。连接ssr，右键ssr图标-选项设置-允许来自局域网的连接。
3.电脑连接手机热点，电脑打                          开热点共享，chromebook连接电脑发射的热点
4.电脑win+R,cmd,ipconfig,得到如下所示ip信息

5.chromebook点击所连接wifi热点，点击热点详情-连接类型-手动配置代理-输入上图ip，端口号是1080（详见第2条中允许来自局域网连接的按钮周围的端口信息）。
6.登录，done。
</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-04-23&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/04/21/3.%E8%AF%BE%E7%A8%8B/CS224u_Sentiment_Analysis_SST/">
        <p class="h4 index-header">3.课程/CS224u_Sentiment_Analysis_SST</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Sentiment_Analysis SSTSST项目基本信息
SST斯坦福链接
SST工作展示蓝色是正面情绪，黄色是负面，灰色是中性情绪。叶子节点的merge原理暂时未知。They said it would be great这句话下边都是正面情绪，直到跟节点的they才恢复为中性情绪，因为they said其实不能反映出作者的 bias。
而后边接上的they were wrong明显是负面，然后merge后总体就是负面情绪了。这个总体的判断要比直接把一句话拆开按照各个单词的正负性简单sum要靠谱很多。

verse vesa

Readers讲解
In [5]、In [6]中的train_reader参数详解




meaning



class_func=sst.binary_class_func
二分类问题


class_func=sst.ternary_class_func
三分类问题


class_func=None
五分类问题


核心类 ： nltk.tree.Tree输入一个tree：

循环打印各个subtree：

输出tree的lable和subtre</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-04-21&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/04/19/0.%E6%A6%82%E5%BF%B5/0.%E6%96%87%E7%AB%A0/python_env_kag1_yml/">
        <p class="h4 index-header">0.概念/0.文章/python_env_kag1_yml</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">environment_ymlname: kag1
channels:
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch
  - https://mirrors.ustc.edu.cn/anaconda/pkgs/main
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge
  - defaults
dependencies:
  - absl-py=0.9.0=py37_0
  - astor=0.7.1=py_0
  - attrs=19.3.0=py_0
  - boto=2.49.0=py_0
  - boto3=1.12.11=py_0
  - botocore=1.15.11</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-04-19&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/04/19/0.%E6%A6%82%E5%BF%B5/0.%E6%96%87%E7%AB%A0/python_%E7%8E%AF%E5%A2%83%E8%BF%81%E7%A7%BB/">
        <p class="h4 index-header">0.概念/0.文章/python_环境迁移</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">python环境迁移copy from：WenShan`s blog
Conda 是著名的包管理器和虚拟环境管理器。
在配置完项目环境，并编写和测试代码后，您可能希望将其移至另一台计算机。
Conda 提供了多种保存和移动环境的方法。
Clone在本地，conda 可以方便地创建环境的快照或者备份：
1 conda create --name snapshot --clone myenv Spec List如果需要在具有 相同操作系统 的计算机之间复制环境，则可以生成 spec list。
生成 spec list 文件：
1 conda list --explicit &gt; spec-list.txt 重现环境：
1 conda create  --name python-course --file spec-list.txt Environment.yml也可以使用 -export 选项生成一个 environment.yml 文件，以在 不同的平台和操作系统之间 复现项目环境。 spec list 文件和 environment.yml 文件之间的区别在于： environm</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-04-19&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/04/18/3.%E8%AF%BE%E7%A8%8B/CS224u_Sentiment_Analysis_%E6%9D%90%E6%96%99%E5%92%8C%E9%A2%84%E5%A4%84%E7%90%86/">
        <p class="h4 index-header">3.课程/CS224u_Sentiment_Analysis_材料和预处理</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Sentiment Analysis阅读材料和数据预处理核心阅读材料

情感分析各子领域合适的切入点。他们入选的原因是位置合适以及有公开数据集

常用情感分析数据库

lexica

preprocessingpreprocessing ： tokenizing

preprocessing : 词干化stemming

porter stemming演示

兰开斯特stemming演示

wordNet stemming演示

preprocessing:  POS tagging
有些英文单词的sentiment只能通过pos来确定。

需要注意的是即使有些单词的pos tagging一模一样仍然会有相反的sentiment

preprocessing : 简单情感标记和推荐阅读

简单负面情绪标记示例

推荐阅读

负面情绪标记带来的性能提升

</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-04-18&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/04/08/0.%E6%A6%82%E5%BF%B5/NLP_batchsize/">
        <p class="h4 index-header">0.概念/NLP_batchsize</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">BatchSize机器学习训练中，把数据集分割成若干个batch，每个batch的大小叫做batchsize。
对batchsize、epoch、iteration的举例：
mnist 数据集有张图片作为训练数据，张图片作为测试数据。假设现在选择 Batch_Size = 对模型进行训练。迭代次。

每个 Epoch 要训练的图片数量：(训练集上的所有图像)
训练集具有的 Batch 个数： 
每个 Epoch 需要完成的 Batch 个数：
每个 Epoch 具有的 Iteration 个数：（完成一个Batch训练，相当于参数迭代一次）
每个 Epoch 中发生模型权重更新的次数：
训练 10 个Epoch后，模型权重更新的次数： 
不同Epoch的训练，其实用的是同一个训练集的数据。第1个Epoch和第10个Epoch虽然用的都是训练集的图片，但是对模型的权重更新值却是完全不同的。因为不同Epoch的模型处于代价函数空间上的不同位置，模型的训练代越靠后，越接近谷底，其代价越小。
总共完成30000次迭代，相当于完成了个Epoch
</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-04-08&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/04/08/0.%E6%A6%82%E5%BF%B5/NLP_epoch/">
        <p class="h4 index-header">0.概念/NLP_epoch</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Epoch定义训练集的所有数据对模型进行了一次完整训练，称为“一代训练”（epoch）。
也就是说所有训练样本在神经网络中都进行了一次正向传播和一次反向传播。
为什么要进行多于一次的epoch？神经网络中传递完整的数据集一次是不够的，而且我们需要将完整的数据集在同样的神经网络中传递多次。
但是因为使用的是有限的数据集，并且使用一个迭代过程也就是梯度下降来优化学习过程。如下图所示，随着epoch的增加，神经网络的权重更新次数也在增加，曲线从欠拟合变成了过拟合。所以需要选择合适的epoch。

</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-04-08&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/04/08/0.%E6%A6%82%E5%BF%B5/NLP_%E6%B1%A0%E5%8C%96/">
        <p class="h4 index-header">0.概念/NLP_池化</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">池化池化是什么卷积之后往往是池化操作，which可以降低卷积输出的特征向量，避免过拟合。
以最大池化max pooling为例：整个图像被分为不重叠的若干小块，每个小块内取最大的数字，然后保持原平面结构，得出output。

​    
池化的作用感受到不变性，增大感受野。
减小下一层的输入大小，减小计算量和参数个数，降维。
如上图，第一行是卷积，第二行是池化。经过这两行操作，“横折”这个点，不论做平移旋转或者伸缩，都有相同的输出。于是
</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-04-08&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/03/31/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/U%E7%9B%98%E5%AE%89%E8%A3%85Ubuntu/">
        <p class="h4 index-header">4.安装调试记录/U盘安装Ubuntu</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">U盘安装Ubuntuwindows下 - 磁盘管理 - 压缩卷，然后删除卷。
找一个大于4G的u盘
http://mirrors.ustc.edu.cn/ubuntu-releases/16.04/ 下载ubuntu-16.04.6-desktop-amd64.iso    
https://cn.ultraiso.net/xiazai.html 下载ultralSo，安装，运行
左上角 文件 - 打开iso文件；左下角本地目录 选中u盘，启动 - 写入硬盘映像 - 写入
插入U盘，重启，F1,选择直接进入U盘。
选择安装Ubuntu - 其他选项（根据需求调整分区）-  点击左下方的加号添加分区，分区方案参考如下

现在安装 - 等待  - 拔U盘 - 重启 - 进
</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-03-31&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/03/28/0.%E6%A6%82%E5%BF%B5/NLP_tokenize%E5%92%8C%E9%A2%84%E5%A4%84%E7%90%86/">
        <p class="h4 index-header">0.概念/NLP_tokenize和预处理</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Tokenize把长句子拆分成有意义的小句子。
import nltk
sentence = &quot;hello,world&quot;
tokens = nltk.word_tokenize(sentence)
token = 
[&#39;hello&#39;, &#39;,&#39; , &#39;world&#39;]
有时候tokenize没那么简单，比如如下的例子是黄晓明在推特上@安吉拉卑鄙秀恩爱的语料。
在这个场景下，大家会用很多# @ 表情符和超链接等，自然语言呈现出与书本不同形态，如果还沿用老的tokenize策略，就会分出不少乱七八糟的符号，并且拆散了有意义的本符号比如“ :D ” 。

所以需要根据不同的业务场景开发定制化的tokenize策略，根据上文的分析，将本场景下最容易出现的专有token全部用正则表达式重写，试图完整表达。

通过re.compile定义一个小方法token_re,返回和regex匹配的模式。

词形的变化词有两种变化，分别是
Inflection变化: walk -&gt; walking -&gt;walked不影响词性
deri</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-03-28&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/03/27/5.%E6%9D%82%E8%B0%88/%E6%82%89%E8%BE%BE%E5%A4%9A%E7%9A%84%E7%8E%8B%E5%AD%90/">
        <p class="h4 index-header">5.杂谈/悉达多的王子</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">悉达多1.爱与知识并不能来带peace
悉达多感到，父母朋友之爱，长者传授的知识不能让他感到满足，不能使他的内心安宁。悉达多开始怀疑，沐浴、祭祀等仪式是否有必要？
爱是大脑的内分泌短暂失调+欲求不满的表现，知识的传授其实可以在极短的时间内完成（剩余时间是磨洋工），沐浴祭祀等繁文缛节是keep everybody busy的手段，换成现代概念几乎可以等同于各种繁复的人事缓冲池——若不是要严控失业率，从基层到决策层之间90%的传递信息者都可被优化掉。
2.众生皆苦，逃避无用
悉达多说，冥想，对肉体的决弃，斋戒和调息都是在逃避自我，是对自我所受的苦难的短暂的逃避，这种逃避和牧牛人在酒馆里喝几碗米酒是同样的。在这种短暂的麻醉下，他们不再感受到自我，不再感觉到生命的苦难。那几碗米酒让牧牛人浑然入睡，他同样找到了悉达多和乔文达在长时间的修行中逃离肉体并宅于非我之境所找到的感觉。
悉达多说的这种逃避痛苦的方式，暗示了痛苦的根源就在与“我”，感受到我的存在，便能感受到痛苦。喝了酒，进入了禅定，做爱，入睡了，在别的国家旅游用英语讲话逃离中文世界，weed等都可短暂的忘我。但是在短暂的忘我结束后，一切一如</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-03-27&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/03/26/0.%E6%A6%82%E5%BF%B5/%E6%8A%80%E6%9C%AF_%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/">
        <p class="h4 index-header">0.概念/技术_正则表达式</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">正则表达式re.compile()compile() 的定义：
compile(pattern,flags=0)
# compile a regular expression pattern,returning a pattern object
从原文档的定义可知，compile函数返回的是一个匹配对象，一般和findall(),search(),match()函数搭配使用。返回的是一个列表。
搭配使用的例子：
import re

def main():
    content = &#39;Hello, I am Jerry,from Chongqing, a montain city,nice to meet you&#39;
    regex = re.compile(&#39;\W*o\w*&#39;)
    x = regex.findall(content)
    print(x)

if __name__ == &#39;__main__&#39;：
    main()
# [&#39;Hello&#39;,&#39;from&#39;,&#39;Chongqi</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-03-26&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/03/25/0.%E6%A6%82%E5%BF%B5/NLP_CNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">
        <p class="h4 index-header">0.概念/NLP_CNN卷积神经网络</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">CNN卷积神经网络卷积神经网络擅长处理网格结构的数据。
顾名思义，卷积神经网络是利用了数学上的卷积操作的神经网络。和之前的前馈神经网络比起来，卷积神经网络不过是将其中的某几层的矩阵乘法运算替换为卷积运算。其他的比如最大似然法则、反向传播算法等都不变。
什么是卷积运算回顾一下矩阵乘法C = AB的表示：

卷积的定义：

其中I是输入矩阵，K是kernels核矩阵，S就是输出的特征图，m和m是K矩阵的大小
如下图，卷积就是kernel核矩阵扫过输入矩阵产生输出矩阵的过程。


卷积神经网络的两大优势卷积神经网络擅长处理网格结构的数据，比如一维的时间序列，二维的图像像素，三维的医学CT图。之所以擅长是因为卷积神经网络的两个优势：

稀疏连接
传统的神经网络主要操作是矩阵乘法，每个输入输出元之间都需要一个独立的参数表示，这也代表着两层之间每个输出元和输入元都有连接。于是就需要很大的存储空间来存储这些参数。对于CNN来说，核矩阵的大小远小于输入的大小，对于图像数据来说，输入通常有成千上万的数据，但是检测图像中边edge的结构的核矩阵可能只有数十最多数百个像素。这极大的减小了存储所需空间和计算量</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-03-25&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/03/13/0.%E6%A6%82%E5%BF%B5/NLP_gensim/">
        <p class="h4 index-header">0.概念/NLP_gensim</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Gensimword2vec
word2vec的使用就是gensim中models/word2vec.py文件里的word2vec类中，共有如下24个参数。



参数名称
默认值
用途



sentences
None
训练的语料，一个可迭代对象。对于从磁盘加载的大型语料最好用gensim.models.word2vec.BrownCorpus，gensim.models.word2vec.Text8Corpus ，gensim.models.word2vec.LineSentence 去生成sentences


size
100
生成词向量的维度


alpha
0.025
初始学习率


window
5
句子中当前和预测单词之间的最大距离，取词窗口大小


min_count
5
文档中总频率低于此值的单词忽略


max_vocab_size
None
构建词汇表最大数，词汇大于这个数按照频率排序，去除频率低的词汇


sample
1.00E-03
高频词进行随机下采样的阈值，范围是(0, 1e-5)


seed
1
向量初始化的随机数种子


workers
3
几</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-03-13&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/03/10/0.%E6%A6%82%E5%BF%B5/Python_numpy_seed/">
        <p class="h4 index-header">0.概念/Python_numpy_seed</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Numpynp.random.seed()函数np.random.seed()函数用于生成指定的随机数，seed ()被设定后可以按照顺序产生一组固定的数组，如果使用相同的seed（）值，每次产生的随机数都相同。如果不设置这个值，则每次产生的随机数都
np.random.seed()函数这个seed（）函数可以说是随机数的编号。比如seed（1） 产生的数组是[1,2,3,4,2,2,1]，那么以后调用seed（1）出来的数组都是[1,2,3,4,2,2,1].
不使用seed（）就得不到相同的随机数：
import numpy as np
np.random.seed(1)

L1 = np.random.randn(3, 3)
L2 = np.random.randn(3, 3)
print(L1)
print(L2)
out：
[[ 1.62434536 -0.61175641 -0.52817175]
 [-1.07296862  0.86540763 -2.3015387 ]
 [ 1.74481176 -0.7612069   0.3190391 ]]

[[-0.2493</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-03-10&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/03/08/0.%E6%A6%82%E5%BF%B5/Python_Pandas_apply_series/">
        <p class="h4 index-header">0.概念/Python_Pandas_apply_series</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Pandas_SeriesSeries数据包括：一列数据和一列index组成。和字典非常相似
创建空的series
import numpy as np
import pandas as pd
S1 = pd.Series()
S1    
Series([],dtype:float64)
指定value和index的值
S2=pd.Series([1,3,5,7,9],index=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;])
S2
a    1
b    3
c    5
d    7
e    9
dtype: int64
S2.values
array([1, 3, 5, 7, 9])
S2.index
Index([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;], dtype=&#39;object&#39;)
Pandas处理series的方法:map,apply数据由如下代码模拟生成
boolean=[True,F</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-03-08&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/03/07/0.%E6%A6%82%E5%BF%B5/%E6%8A%80%E6%9C%AF_Lamada%E8%A1%A8%E8%BE%BE%E5%BC%8F/">
        <p class="h4 index-header">0.概念/技术_Lamada表达式</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Python中lamada函数的概念lamada函数就是个匿名函数，即定义即用，省去了起名字等等环节，使用方便快捷。
不用lamada函数的例子
def f(x):
return x**2
print f(4)
使用lamada函数的例子：
g = lambda x : x**2
print g(4)
lamada用例
# lamada语句中，冒号前是参数，可以有多个，用逗号隔开。冒号后是返回值，lamada语句构建的其实是一个函数对象

&gt;&gt;&gt; foo = [2, 18, 9, 22, 17, 24, 8, 12, 27]
&gt;&gt;&gt; print filter(lambda x: x % 3 == 0, foo)
[18, 9, 24, 12, 27]
&gt;&gt;&gt; print map(lambda x: x * 2 + 10, foo)
[14, 46, 28, 54, 44, 58, 26, 34, 64]
&gt;&gt;&gt; print reduce(lambda x, y: x + y, foo)
139

</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-03-07&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/03/05/2.%E6%AF%94%E8%B5%9B/Google_QA_%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E9%83%A8%E5%88%86/">
        <p class="h4 index-header">2.比赛/Google_QA_数据处理部分</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">数据处理部分总体代码如下，各个程序块在pycharm中分开运行，用#%%分割。
上来是import部分，然后是数据清洗和构建embedding矩阵等。
import numpy as np
#numpy是主要用于数组计算，线性代数，傅里叶变换等。
import pandas as pd
#pandas基于numpy，可以处理高纬数据
from sklearn.manifold import TSNE
# sklearn是机器学习中常用的第三方模块，对常见的机器学习算法进行了封装，包括回归、降维、分类、聚类,sklearn.manifold是流形学习，非
# 线性降维的手段。最简单的降维手段是随机投影，但是会导致结构丢失,manifold learning是一种类似主成分分析(PCA)的线性框架，不会错失数据结构中的非线性项  ，TSNE提供了一种画图方式，让高维的数据降低为二维画出来
import seaborn as sns
# 基于matplotlib的画图工具
import glob
# glob是查找模块。支持空格 ，问号？，方括号[]这三个通配符。空格代表0个或者多个字符，问</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-03-05&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/03/04/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E6%89%B9%E9%87%8F%E4%BF%AE%E6%94%B9%E6%96%87%E4%BB%B6%E5%90%8E%E7%BC%80%E5%90%8D%E7%9A%84%E6%96%B9%E5%BC%8F/">
        <p class="h4 index-header">4.安装调试记录/批量修改文件后缀名的方式</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">新建一个.txt文档，输入：
ren *.java *.md
保存
将文件后缀名改成.bat,双击运行
</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-03-04&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/03/02/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E4%BF%AE%E6%94%B9pip%E6%BA%90/">
        <p class="h4 index-header">4.安装调试记录/修改pip源</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">
将pip切换回国内源pip国内的一些镜像
  阿里云 http://mirrors.aliyun.com/pypi/simple/  中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/  豆瓣(douban) http://pypi.douban.com/simple/  清华大学 https://pypi.tuna.tsinghua.edu.cn/simple/  中国科学技术大学 http://pypi.mirrors.ustc.edu.cn/simple/
修改源方法：
临时使用：可以在使用pip的时候在后面加上-i参数，指定pip源eg: pip install scrapy -i https://pypi.tuna.tsinghua.edu.cn/simple   –trusted-host  pypi.tuna.tsinghua.edu.cn
永久修改：linux:修改 ~/.pip/pip.conf (没有就创建一个)， 内容如下：
[global]
index-url = https://pypi.tuna.tsinghua</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-03-02&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_Transformer/">
        <p class="h4 index-header">0.概念/NLP_Transformer</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Transformer总览图：

Transformer由编码器和解码器构成，如下：

1.编码器详解编码器有两层（前馈神经网络层，自注意力层）。
像大部分NLP应用一样，Transformer首先将每个输入单词通过词嵌入算法转换为词向量。每个单词都被嵌入为512维的向量，下图使用方框表示这些向量。

词嵌入只发生在最下面那个编码器中，然后这个编码器会输出一个向量列表，列表中的每个向量大小都是512维。向量列表的大小是可以调节的超参数–一般被设置为最长句子的长度。每个编码器的输入输出格式都相同。

开始编码的时候，将列表中的向量传递到自注意力层进行处理，然后传到到前馈神经网络层，然后将输出结果传递到下一层。
2.自注意力机制详解(其实就是维持了一个关于句子本身的二维数组)宏观角度看，比如翻译一个句子，“The animal didn`t cross the street because it was too tired”，it指代什么？对于人类来说，指代的是animal不是street，很容易但是对于计算机来说这就是一个复杂的问题。
当模型处理“it”这个单词的时候，自注意力机制会允许</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_BERT/">
        <p class="h4 index-header">0.概念/NLP_BERT</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Bert 概念1.全景介绍BERT是一种深度学习模型，他是Transformer的双向编码器表示，在维基百科和Books Corpus上预训练过，运用于特定任务时只需要微调即可。
他的效果很好，在很多NLP任务中都有最近进展，包括问答系统（Squad）和自然语言推理（MNLI）任务。
BERT改变了NLP的格局。跑一个在大量未标记数据集上训练的模型，在11个单独的NLP任务中仅仅通过不同的微调，就可以分别得到11个最新的结果，这种表现只有BERT可以做到。
BERT还启发了TransformerXL,GPT-2,XLNet,ERNIE2.0,RoBERTa等等。
2.什么是BERT?1.基本上是堆叠在一起的一堆Transformer encoders，注意仅仅是Transformer encoders不是整个Transformer架构。双向性的概念十分重要，是BERT和其前身OpenAL GPT的关键区别，BERT是双向的因为他在自我注意层的两个方向上都执行自我注意。
2.要注意的是BERT在维基百科（25亿词）和Books Corpus（8亿词）上的预训练非常重要，因为模型在一个大的</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BD%91%E7%BB%9C_2-%E7%89%A9%E7%90%86%E5%B1%82/">
        <p class="h4 index-header">3.课程/网络_2-物理层</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">2.物理层2.1物理层的基本概念物理层解决如何在链接各种计算机的传输媒体上传输数据比特流。
物理层的主要任务描述：确定传输媒体的接口的一些特性。例如：

机械特性：接口形状，大小，引线数目


电器特性：电压范围

功能特性，过程特性等


2.2数据通信的基本模型
相关术语：

数据：运送消息的实体
信号：数据的电器或电磁表现
模拟信号：消息的参数取值是连续的
数字信号：消息的参数取值是离散的



信道信道一般表示向一个方向传送信息的媒体，我们常说的信道往往包含了一条发送信息的信道和一条接受信息的信道
单向信道：信息只能单向流动
双向交替信道：信息可双向流动，但不能同时流动
双向同时信道：信息科双向，同时流动
基带信号和带通信号
基带信号：来自信源的信号，像计算机输出的各种代表文字和图像的信号都属于基带信号，我们说话的声波也是基带信号

基带数字信号的几种调制方法




带通信号：把基带信号进行载波调制后，把信号的频率范围搬到较高的频段以便在信道中传输

在传输范围较大的时候，计算机网络必须通过带通信号传输



曼彻斯特编码：由低到高是0，高到低是1

差分曼彻斯特编码


2</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BD%91%E7%BB%9C_1-%E6%A6%82%E8%BF%B0/">
        <p class="h4 index-header">3.课程/网络_1-概述</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">1.计算机网络概述1.1局域网​        覆盖范围小（１００ｍ）,自己花钱买设备,贷款固定
​        接入层的交换机们星形连接到汇聚层交换机,这样分散IO
​        接入层交换机们不该串联,不然最后一台交换机那里IO压力太大
1.2Internet和广域网​    Internet ISP :有自己的机房,对网民提供internet访问
　广域网：距离较远，花钱租带宽，
1.3 数据包和数据帧在一个计算机想把数据发给另一个计算机时,需要在发送信息里携带ip地址和mac地址,其中ip地址是最终目地,mac地址是下一跳的目的地,ip地址不变,mac地址每次经过路由器都刷新.下一跳的目的地由迪杰斯特拉或其他寻路算法确定.
1.4 OSI参考模型 应用层:所有能产生网络流量的程序
表示层:在传输之前是否进行加密或压缩处理
会话层:查木马,netstat -n
传输层:可靠传输,流量控制,不可靠传输
网络层:负责选择最佳路径,规划ip地址
数据链路层:帧的开始和结束,透明传输,差错校验
物理层:接口标准,电器标准,如何在物理链路层上传输的更快

osi参考模型对网络排错指导</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BD%91%E7%BB%9C_3-%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/">
        <p class="h4 index-header">3.课程/网络_3-数据链路层</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">3.数据链路层3.0概述数据链路层使用的信道主要有以下两类

点对点信道
这种信道使用一对一的点对点通信方式


广播信道
这种信道使用一对多的广播通信方式，因此过程较为复杂，广播信道上的链接主机很多，因此必须使用专门的共享信道协议来协调这些数据的转发



3.1 数据链路和帧链路是一条无源的点到点的物理线路，中间没有任何的其他交换节点。数据链路是除了物理线路之外，还得有通信协议来控制这些数据的传输，若把实现了这些协议的软硬件加到链路上，就构成了数据链路。
现在最常用的办法是使用适配器（网卡）来实现这些协议的硬件和软件，一般的适配器都包括了数据链路层和物理层这两层的功能。
数据链路层传输的是帧。

三个基本问题

封装成帧
透明传输
差错控制


封装成帧：

在一段数据的前后分别打上首部和尾部，首部尾部可以确定帧的界限。


透明传输问题：

类似于我想在markdown文件中打出[]()字符缺被误以为是超连接一样，需要在[]()字符之前加上转义字符\。


CRC差错检测
传输过程中可能出现比特差错，1变成0或者0变成1。一段时间内传输错误的比特与总比特比值称谓误码率。为保证数据</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/3.%E8%AF%BE%E7%A8%8B/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95_1_%E6%A6%82%E8%AE%BA/">
        <p class="h4 index-header">3.课程/统计学习方法_1_概论</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">1.概论介绍基本概念，是对全书的概括。
首先叙述统计学习的定义、研究对象和方法，然后叙述监督学习。
然后提出统计学习方法的三要素：模型，策略和算法。
介绍模型选择，包括正则化、交叉验证和学习的泛化能力。
介绍生成模型和判别模型
介绍监督学习方法的应用： 分类问题，标注问题，回归问题。
1.1统计学习定义
统计学习是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测和分析的学科。
统计学习是概率论，统计学，信息论，计算理论，最优化理论，计算机科学等领域的交叉学科。在发展中逐步有了自己的理论体系和方法论。
目的
统计学习用于对未知数据预测和分析。
对数据的预测可以使计算机更加智能化/计算机的性能提升。
对数据的分析可以使人们获取新的知识。
方法
统计学习的方法是基于数据构建统计模型进而对数据进行预测和分析，统计学习由监督学习，半监督学习，无监督学习和强化学习等组成。
本书主要讨论的是监督学习，这种情况统计学习的方法可以概括如下：
从给定的有限的训练数据集出发（training set）,假设数据是独立同分布产生的；
假设要学习的模型属于某个函数的集合，称为假设空间。
把模型应用于某</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>



  <nav aria-label="index posts navigation">
    <span class="pagination pg-blue justify-content-center mt-5" id="pagination">
      <a class="extend prev" rel="prev" href="/"><i class="fas fa-angle-double-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/3/"><i class="fas fa-angle-double-right"></i></a>
    </span>
  </nav>
  
  <script>
    for (ele of document.getElementById("pagination").getElementsByClassName("page-number")) {
      ele.href += '#board';
    }
  </script>



              </div>
            </div>
          </div>
        </div>
      </div>
    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>

    
  
    <!-- 不蒜子统计PV -->
    
    &nbsp;<span id="busuanzi_container_site_pv">总访问量 
          <span id="busuanzi_value_site_pv"></span> 次</span>&nbsp;
  
  
    <!-- 不蒜子统计UV -->
    
    &nbsp;<span id="busuanzi_container_site_uv">总访客数 
            <span id="busuanzi_value_site_uv"></span> 人</span>&nbsp;
  
  <br>



    

  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js" ></script>
<script src="/lib/popper/popper.min.js" ></script>
<script src="/lib/bootstrap/js/bootstrap.min.js" ></script>
<script src="/lib/mdbootstrap/js/mdb.min.js" ></script>
<script src="/js/main.js" ></script>






  <script src="/lib/smooth-scroll/smooth-scroll.min.js" ></script>



  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>


<!-- Plugins -->


  

  

  

  

  




  <script src="/lib/prettify/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "暗影疾行&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 120,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="/lib/anchor/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "false",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>







</body>
</html>

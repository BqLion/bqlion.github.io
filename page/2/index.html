<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <link rel="icon" type="image/png" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="description" content="desc">
  <meta name="author" content="LiuBingqian">
  <meta name="keywords" content="">
  <title>刘秉乾的技术博客 :)</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"  >
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css"  >
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css"  >
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css"  >

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css"  >

<link rel="stylesheet" href="/css/main.css"  >


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css"  >


<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header style="height: 100vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>LiuBingqian`s Blog</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">主页</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">分类</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">所有文章</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">标签</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">关于</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>


</nav>

    <div class="view intro-2" id="background"
         style="background: url('http://bqlab-pic.test.upcdn.net/index.jpg')no-repeat center center;
           background-size: cover;
           background-attachment: fixed;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
          </div>

          
            <div class="scroll-down-bar">
              <i class="fas fa-angle-down scroll-down-arrow"></i>
            </div>
          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      <div class="container nopadding-md">
        <div class="py-5 z-depth-3" id="board">
          
          <div class="container">
            <div class="row">
              <div class="col-12 col-md-10 m-auto">
                


  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/08/15/test/">
        <p class="h4 index-header">test</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">1.熟悉课题组最近毕业的学生的发文情况（知网，web of science）
2.磨样，基础课补课
3.写专利
4.水果摊小程序
魏学长方差数据
5.805,5.792,5.807,5.808,5.801,5.796,5.802,5.797,5.804,5.811
0.00003

5.788,5.805,5.782,5.811,5.806,5.795,5.812,5.807,5.809,5.799
0.00009

5.814,5.807,5.789,5.796,5.807,5.811,5.813,5.809,5.804,5.813
0.00006

5.788,5.796,5.804,5.809,5.799,5.805,5.809,5.811,5.787,5.803
0.00007

5.811,5.817,5.812,5.807,5.801,5.806,5.795,5.794,5.806,5.808
0.00005

5.807,5.798,5.792,5.788,5.786,5.809,5.810,5.808,5.805,5.802
0.00007

5.811,5.815,5</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-08-15&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/08/08/6.%E8%8B%B1%E8%AF%AD/GER_%E5%86%99%E4%BD%9C/">
        <p class="h4 index-header">6.英语/GER_写作</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">写作是输出。
GRE写作考察的是批判性思维和议论文写作的能力。
</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-08-08&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/07/30/6.%E8%8B%B1%E8%AF%AD/GRE_%E5%8F%A5%E5%86%85%E5%8F%A5%E9%97%B4%E5%85%B3%E7%B3%BB/">
        <p class="h4 index-header">6.英语/GRE_句内句间关系</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">GRE考试，填空主要考的是句内关系。
阅读主要考的是句间关系。
如果做不完，或者分数在140-152之间晃荡，肯定是因为没有明确GRE在考什么。
句内关系如下所示，句内关系或者取同或者取反。

尽管你是个好人，但是你干了坏事。这就是取反。
因为你是个好人，所以你干了好事。这就是取同。
句间关系句间关系也是不是取同就是取反。
如果出现But,Yet,However,Nevertheless这四个词，那么句间关系一定是前后取反。
如果没有出现这四个转折词，那么一定是前后取同。
如果前后取同，那么后边句子主体就是同。如果后面是从句，那么主句要和前句保持一致取同，从句出现了Although这个句内取反词语，那么从句必然和前句不同。就是个子集背反。

GRE模考软件 pp2
ets官网里面
</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-07-30&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/07/30/%E5%81%A5%E8%BA%AB%E8%AE%A1%E5%88%92/">
        <p class="h4 index-header">健身计划</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">营养学​    
宏观总热量摄入：3247大卡每天
碳水 = 45% * 总量 / 4 = 365g（碳水的摄入应该紧跟身体的活跃度需要）
蛋白质 = 30% * 总量 / 4 = 244g（蛋白质应该随着身体的适应逐渐提高，最多可到400g每天）
脂肪 = 25% * 总量 / 9 = 91g（脂肪任何时候不能超过100g每天）
三天循环总碳水 = 365 * 3 = 1095g












三头，胸，前束
二头，背，后束
休息
中束，前束，斜方
手臂
休息


425
450
220
450
425
220


腿
胸，三头
休息
背，二头
肩膀
休息


450
425
220
450
425
220


















蛋白质摄入方法论
蛋白质是建造身体的原材料，碳水和脂肪是建造身体时用的能量。
普通人正常蛋白质摄入量应该在每磅体重1g每天，外胚型和健美运动员应该提高蛋白摄入，在1.5-2g每天。我200磅，疑似外胚形，想增肌，应该摄入300g每天，但是考虑到黄种人饮食习惯和历史遗留问题，暂定每天摄入244g是没有什么问题的。
但是蛋白质应该在每天的</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-07-30&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/07/15/3.%E8%AF%BE%E7%A8%8B/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB_5_%E7%9B%B8%E4%BC%BC%E6%80%A7%E6%B5%8B%E5%BA%A6/">
        <p class="h4 index-header">3.课程/模式识别_5_相似性测度</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">#
5.相似性测度相似性的含义特征向量是n维向量，它相当于特征空间中的一个点。
在特征空间中，点之间的距离函数作为模式相似性的测量。点之间距离越近，相似性越高。
如何测量点之间的距离？
欧式距离
就是x1中的数字剪去x2中对应的数字，然后平方相加再开根号（二维空间中就是两点之间的几何距离）

欧式距离的缺点：
收到量纲的影响，如下所示，如果都使用mm，距离就是图一的样子。如果两个坐标有一个使用cm，则距离有变化。

解决办法：将特征数据标准化（归一化），使其与数据的单位无关。
马氏距离
</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-07-15&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/07/14/3.%E8%AF%BE%E7%A8%8B/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB_4_%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E7%9A%84%E4%B8%80%E8%88%AC%E8%BF%87%E7%A8%8B/">
        <p class="h4 index-header">3.课程/模式识别_4_模式识别的一般过程</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">#
4.模式识别的一般过程无论采用哪种方法，模式识别一般都要包括如下两个基本过程：
分别是学习过程和识别过程。

如下是细分步骤示意图，细分步骤分别是收集数据-选择特征-选择模型-训练分类器-评价分类器。
其中选择特征和选择模型这两项是需要用到“prior knowledge”的。

</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-07-14&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/07/14/3.%E8%AF%BE%E7%A8%8B/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB_3_%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/">
        <p class="h4 index-header">3.课程/模式识别_3_设计模式识别系统</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">#
3.设计模式识别系统——以鱼罐头分类为例总体流程图如下：

第一步：收集data（训练数据）收集训练数据，手工分类

第二步：预处理训练集如下，把鱼从背景中提取出来

第三步：提取特征如下，可以对处理过的训练集中提取出例如（长度，重量，光泽度，宽度，鱼鳍的数量）等特征。
第四步：设计分类器
发现鲈鱼的长度要短于金枪鱼，于是把鱼的长度作为一个分类特征

如下所示，第一行是“如果以此数字为分类界限”，第二行和第三行是分类的结果 

如下是上边数据的折线图
​    
如下所示，如果以长度“9”作为判别标准，误分类的结果如下图所示

可以看到，当特征和特征的边界选择的好的话，可以有效降低分类的错误率。
系统设计阶段小结
如果只使用长度，是一个不够用的特征
尝试加入其它特征（因为鲈鱼光泽度高，所以再加入“光泽度”这个特征）

如下是光泽度特征的数据和表格

综上，同时使用上边提到的“长度”，“亮度”两项，来判别鱼的种类。两个特征作出二维图后，可见有一条直线能基本将鱼类完全正确分开。 

</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-07-14&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/07/03/6.%E8%8B%B1%E8%AF%AD/%E6%89%98%E7%A6%8F_%E5%90%AC%E5%8A%9B/">
        <p class="h4 index-header">6.英语/托福_听力</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">听力讲座泛结构：

</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-07-03&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/06/22/3.%E8%AF%BE%E7%A8%8B/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB_2_%E8%81%9A%E7%B1%BB%E6%A6%82%E8%AE%BA/">
        <p class="h4 index-header">3.课程/模式识别_2_聚类概论</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">#
2.聚类分析聚类分析方法的有效性
选取不当，则分类无效，如下（比如测量是否有肺炎，参数却选取身高体重而非体温肺片）



特征选取不足，如下只选择X2作为特征，只能切分出w1，w2和w3无法分出



特征选择过多可能无益反而有害，增加分析负担并且让分析效果变差
    


聚类应用的四个基本方向
减少数据
数据量过大，使得数据处理十分费力，可以采用聚类分析的方法将数据分成几组可判断的聚类来处理，然后被分开的每一个类被看成是独立的实体来对待。从这个角度看，数据被压缩了。例如我在做toxic比赛的时候，各种毒性评论分别来自于不同的网站，如果嫌数据量过大，就可以把数据按照不同的网站来源做聚类分离，以减小数据的处理压力.

假说生成
在这种情况下，为了推导出数据性质的一些假说，对数据集进行聚类分析。
因此m，这里使用聚类作为建立假说的方法，然后使用其他数据集验证这些假说。

假说检验
利用聚类分析来验证指定假说的有效性。
例如，大公司往往在海外投资这个假说需要被验证是否正确，就要对大公司按照规模、海外活跃度、成功完成项目的能力等进行聚类分析。从而来支持这个假说。

基于分组的预测
对现</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-06-22&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/06/20/3.%E8%AF%BE%E7%A8%8B/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB_1_%E6%A6%82%E8%AE%BA/">
        <p class="h4 index-header">3.课程/模式识别_1_概论</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">1.模式识别概论本节内容概览：

什么是模式识别
模式识别的应用
模式识别系统的结构

什么是模式识别定义：利用计算机模仿人脑对世界各种事物进行描述、分类、判断、识别的过程    
目标：用计算机实现具有感知、识别、理解、自学习和自适应能力的灵活和智能的机器
模式识别的应用如下图，要分辨图中人物的性别

医学影像分类：

语音识别系统中波形转换为音素：

是否决定贷款的二分类问题（其实就是机器学习中的分类问题）

</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-06-20&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/06/16/1.%E7%A7%91%E7%A0%94/%E9%98%85%E8%AF%BB%E5%AD%A6%E8%80%85%E4%B8%AA%E4%BA%BA%E5%85%A8%E9%83%A8%E8%AE%BA%E6%96%87%E7%9A%84%E6%96%B9%E6%B3%95/">
        <p class="h4 index-header">1.科研/阅读学者个人全部论文的方法</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">阅读学者个人全部论文的方法读人为什么要读人？零散的阅读文献，不能把握一个领域整体发展的脉络。很多时候一个领域的发展是由为数不多的几个科学家或者几个实验室推动的。如果对这些实验室的文章进行系统性的梳理，那么就会在相关领域的发展脉络上有更加深刻的理解。而且通过对这个实验室未来发展方向的预测，这对于我们布局以后的科研方向有很好的促进作用。
以生物界David R Liu为例，演示如何系统性读人，大体分为三个方面：

系统性搜寻文章
文章的阅读与归类
文章的分析和思考

1.1系统性搜寻文章首先找到这个学者的谷歌学术个人首页。个人首页上的文章有两种排序方式，分别是按照引用量排序和按照哦时间序列排序。本文作者推荐使用时间倒序查阅，从当下一直阅读到第一篇博士文章的发表。如下：

谷歌学术镜像站：https://ac.scmor.com/
然后将文章全部下载下来，可以通过endnote，readcube，mendeley等软件实现对文献的管理，作者平常用的是readcube，which will自动标注文章的相关信息。
如下是readcube的示意图：

但是readcube不是免费的，遂下载men</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-06-16&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/06/11/0.%E6%A6%82%E5%BF%B5/NLP_Bert%E7%A0%94%E7%A9%B6_WordEmbedding/">
        <p class="h4 index-header">0.概念/NLP_Bert研究_WordEmbedding</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">BERT研究_WordEmbedding本session目的：如下，省略掉Transformer以下所有的模型的冗长发展史直接从Transformer学起。毕竟LSTM也是从更简单的模型发展来的，无限dive一下就到牛三去了。有所取舍cannot agree more。
RNN 和 LSTM都是原始模型，在他们的基础之上改进除了双向LSTM，后边的Attention可以称之为带有注意力的LSTM,其实也就是LSTM的爆改模型。而Transformer则是在Attention之上的更进一步的改装模型。
作者提出观点：希望能够直接认识Transformer和之上的bert，省略掉下面冗长的历史（我同意，毕竟LSTM和RNN也是从别的更原始的数学模型改装来的，无限往下细化就没有尽头了。）

ChrisMcCormickAI认为了解新算法的一般方法
直接找教程、博客文章、原始paper来看。不要硬刚code那样可读性太差。
直接看原始paper，不要无限延伸到其他基础模型。（作者直接调出了bert的paper）


BERT - word embeddingBERT是一个pre-trained</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-06-11&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/06/10/0.%E6%A6%82%E5%BF%B5/NLP_GRU_%E4%BB%A3%E7%A0%81_toxic_kaggle/">
        <p class="h4 index-header">0.概念/NLP_GRU_代码_toxic_kaggle</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1"></div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-06-10&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/06/09/0.%E6%A6%82%E5%BF%B5/NLP_LSTM_kaggle_toxic%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/">
        <p class="h4 index-header">0.概念/NLP_LSTM_kaggle_toxic代码实现</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">LSTM模型在kaggle_toxic比赛中的代码实现embedding_matrix = np.zeros((len(word_index) + 1, 300))
for word, i in tqdm(word_index.items()):
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector
%%time
with strategy.scope():

    # A simple LSTM with glove embeddings and one dense layer
    model = Sequential()
    model.add(Embedding(len(word_index) + 1,
                     300,
                     weights=[embedding_matrix],
    </div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-06-09&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/06/07/0.%E6%96%87%E7%AB%A0/%E9%9D%92%E5%B7%A5%E5%A7%94%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB%E7%9B%B4%E6%92%AD%E7%AC%94%E8%AE%B0/">
        <p class="h4 index-header">0.文章/青工委论文分享直播笔记</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">青工委论文分享直播笔记1.青工委是国内很强的NLP的学术团体，有刘洋刘知远这种大牛成员。
2.青工委隶属于CIPSC，中国中文信息学会网站：http://www.cipsc.org.cn/
3.认可度高的会议：ACL,IJCAI,SIGIR,AAAI,EMNLP&amp;CIKM:（中了自然有组织找上门）
4.青工委2020学术活动list：

5.青工委学术活跃委员与合作单位list


6.研究前沿：


对话系统和信息抽取是热点研究方向
情感分析都是sentence-level和基于文本推理的

7.机器学习的新的轮子

8.多语言模型是热点（落地意义大）

mBERT Probing
9.多语言论文有29篇：

10.多语言学习论文选读
从结果上来说，有监督学习的效果比无监督学习的效果好。有监督和无监督的2020代表性论文分别是：

无监督学习：
UniTrans
Unsupverised Domain Adaptation


有监督学习：
Bilingual Paraphrase Generation
Multilingual Word Sense Disambiguati</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-06-07&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/06/06/0.%E6%A6%82%E5%BF%B5/NLP_%E8%AF%8D%E5%90%91%E9%87%8F_WordEmbedding%E4%BB%A3%E7%A0%81/">
        <p class="h4 index-header">0.概念/NLP_词向量_WordEmbedding代码</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Word_embedding 代码自己定义各个单词的维度是一个工作量巨大、需要深入理解语言内核的工作，而且本工作有很强的复用性。于是不必自己在每一个project中定义各个单词的维度然后算出整个语料库的向量表示，完全可以使用前人算好的结果，比如Glove vector：http://www-nlp.stanford.edu/data/glove.840B.300d.zip 。
其他的选项还有word2vec或者fasttext。
kaggle multi-toxic比赛中用到了glove-840b-300d的word embedding人，如下，读入840-300d的模型，遍历所有的line，输出embedding向量的个数。
# load the GloVe vectors in a dictionary:

embeddings_index = {}
f = open(&#39;/kaggle/input/glove840b300dtxt/glove.840B.300d.txt&#39;,&#39;r&#39;,encoding=&#39;utf-8&#39;)
for line </div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-06-06&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/06/06/0.%E6%A6%82%E5%BF%B5/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E8%AE%AD%E7%BB%83%E9%9B%86&%E6%B5%8B%E8%AF%95%E9%9B%86&%E9%AA%8C%E8%AF%81%E9%9B%86&%E6%97%A0%E5%81%8F%E4%BC%B0%E8%AE%A1/">
        <p class="h4 index-header">0.概念/机器学习_训练集&amp;测试集&amp;验证集&amp;无偏估计</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">训练集&amp;测试集&amp;验证集
训练集trian set： 是用来拟合模型的数据集，因为是整体样本数据的一部分，所以虽然他规模最大但是仍然存在有偏估计的问题。
测试集test set：最终模型训练好后，用来提供相对于trian+valid set
validation set：提供相对于训练集的无偏估计的数据集

无偏估计：
当样本分布的平均值等于总体样本的平均值的时候这个估计就是无偏估计。
比如全市十万名小学生的考试成绩的期望（平均值）是60分，那么部分差的小学的平均值就是45分，这个估计就是有偏估计；对全市所有的小学分别随机抽取出100个样本，这个样本的平均值就是60分，和总体十万个样本的平均值相同，这个估计就是无偏估计。
train set 毕竟是整体样本的一部分，不是严格的分层取样所以必然存在有偏估计，validation set就是在train和test上都平均取样的set，解决了有偏估计的问题。
</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-06-06&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/06/06/0.%E6%A6%82%E5%BF%B5/NLP_LSTM_input&outputShape/">
        <p class="h4 index-header">0.概念/NLP_LSTM_input&amp;outputShape</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">LSTM （RNN）的输入输出形状辨析
在Keras 的LSTM模型中，LSTM的input形状总是一个三维的向量(input_dim,batch_size,time_steps)，如下所示。
建立模型的代码：
model = keras.models.Sequential()
model.add(keras.layer.LSTM(units=3,input_shape=(2,10)))
关于模型summary输出的代码:
model = keras.models.Sequential()
model.add(keras.layers.LSTM(units=3,input_shape(2,10),return_sequences=False))
model.summary()

因为不知道batch_size,所以output shape第一个参数为none，其实应该是(batch_size,3)
总结
LSTM的输入总是一个3D矩阵（batch_size,dim,seq_len）
LSTM的输出是一个2D或者3D矩阵（根据return_sequences不同而有所不同）
If ret</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-06-06&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/06/05/0.%E6%A6%82%E5%BF%B5/NLP_CNN_padding%E5%A1%AB%E5%85%85/">
        <p class="h4 index-header">0.概念/NLP_CNN_padding填充</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">padding填充padding的CV解释当卷积操作用于图像处理中时，因为卷积窗口总是从图片的左上角滑到右下角，在这个过程中，图片的四条边界区域总是不能位于卷积窗口的中心，于是产生了信息损失，于是将图片的四周填充上空白，这样卷积窗口滑动的时候就能收集到边界的完整信息，如下图所示：

padding的nlp解释通常在业务场景中，CNN网络处理语言文字的时候并不是一次只处理一句话（一条向量），而是成批量的处理例如64句话（64*向量维）的矩阵。因为向量有长有短，矩阵又必须是长方形，于是在缺口的地方都填充上0。
tips：如果将向量的长度进行排序后再批量放到不同的矩阵中，这样不同的矩阵就有不同的大小，减少了对0的处理（本质是浪费）可以提高性能。
</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-06-05&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/06/05/0.%E6%A6%82%E5%BF%B5/NLP_RNN_%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/">
        <p class="h4 index-header">0.概念/NLP_RNN_代码实现</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">RNN代码实现与注释RNN - recurrent neural network - 循环神经网络，如概念文章所描述，是一个擅长处理序列化数据的网络。
在这个网络中，上一个步骤的输出是下一个步骤的输入，在传统的神经网络中，所有的输入-输出都是彼此的依赖项。
但是在预测下一个词的业务场景中，预测单词需要记住之前的所有词，所以RNN就很适用于这种情况with a help of hidden layer。
这篇代码记录源于kaggle toxic比赛的notebook，代码是有上下文的，代码的上文就是对比赛的train和validate和test数据进行了必要的处理，切分等操作，如下
xtrain, xvalid, ytrain, yvalid = train_test_split(train.comment_text.values, train.toxic.values, 
                                                  stratify=train.toxic.values, 
                            </div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-06-05&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/06/04/0.%E6%A6%82%E5%BF%B5/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_ConfusedMatrix_%E5%87%86%E7%A1%AE%E7%8E%87&%E5%8F%AC%E5%9B%9E%E7%8E%87&ROC&AUC/">
        <p class="h4 index-header">0.概念/机器学习_ConfusedMatrix_准确率&amp;召回率&amp;ROC&amp;AUC</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Confused Matrix如何衡量一个机器学习模型的效果？不能只看准确率，因为准确率这个评价标准不足以全面评价模型。
先看两个失败的分类模型：
假如意大利展开了新冠肺炎排查行动，暂不管具体是如何排查，输出结果的数学模型是:

把所有人的检测结果都输出为“没有肺炎”，这个时候假设10000个样本，只有2个人得了，准确率高达99.98%，但是这个模型就是个废物，把目的完全丢弃了。
把所有人的检测结果都输出为“有肺炎”，这样能确保阳性样本检出率为100%，但是对所有人都采取隔离措施浪费巨大。这种检测手段也是废物。

几个定义：
上述检测肺炎的例子中，携带病毒为阳性（有敌机，有怀孕），不携带病毒为阴性（气球，无敌机，没怀孕）。把confused matrix画出来如下所示：

更生动的例子：

召回率和准确率一般来说，阳性的例子是检测目标。如果FN假阴性太多，就说这个模型的召回率很低，自然风险控制能力很差（放走了病毒携带者/敌机）。

如果FP假阳性太多，我们就说这种方法的准确率很低，把大量正常人隔离，对假目标比如气球发射导弹。自然这种方法很浪费。

如下所示，召回率（recall）是左侧</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-06-04&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/06/04/0.%E6%A6%82%E5%BF%B5/Python_keras_layer%E6%A6%82%E5%86%B5/">
        <p class="h4 index-header">0.概念/Python_keras_layer概况</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Keras_Layerkeras的层包括：
常用层，卷积层，池化层，局部连接层，递归层，嵌入层，高级激活层，规范层，噪声层，包装层，and，也可以编写自己的层。
对于层的通用操作
layer.get_weights()  #返回该层的权重
layer.set_weigths(weights) #将权重加到该层
config = layer.get_config() #保存该层的配置
layer =  layer_from_config(config) # 加载一个配置到该层

#如果该层不是共享层而是一个计算节点，那么可以通过如下方法获得输入、输出张量。输入和输出张量的形状
layer.input
layer.output
layer.input_shape
layer.output_shape

#如果该层有多个计算节点。可以使用下面的方法
layer.get_input_at(node_index)
layer.get_output_at(node_index)
layer.get_input_shape_at(node_index)
layer.get_output_shape_</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-06-04&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/06/01/0.%E6%A6%82%E5%BF%B5/Python_Keras_Dense_activation_dropout/">
        <p class="h4 index-header">0.概念/Python_Keras_Dense_activation_dropout</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Keras_常用层常用层对应于core模块，core内部定义了一系列的常用的网络层，包括全连接层和激活层等等。
Dense层keras.layers.core.Dense(units, activation=None, use_bias=True, kernel_initializer=&#39;glorot_uniform&#39;, bias_initializer=&#39;zeros&#39;, kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)
Dense层就是常用的全连接层，实现的运算是output = activation(dot(input,kernel)+bias)其中activation是逐元素计算的激活函数，kernel是本层的权值矩阵，bias是偏置向量，只有当use_bias=True才会添加。
如果本层的输入的数据的维度大于2，则会先被压缩成与kernel相匹配的大小。
</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-06-01&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/06/01/0.%E6%A6%82%E5%BF%B5/Python_Keras_Sequential/">
        <p class="h4 index-header">0.概念/Python_Keras_Sequential</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Keras_Sequential开始使用模型顺序模型是多个网络层的线性堆叠。
用户可以通过网络层的实例的列表传递给Sequential的构造器，来创建一个Sequential模型。
from keras.model import Sequential
from keras.layers import Dense,Activation

model = Sequential([
    Dense(32,input_Shape=(784,)),
    Activation(&#39;relu&#39;),
    Dense(10),
    Activation(&#39;softmax&#39;),
])
也可以使用简单的.add()方法将各层添加到模型中
model = Sequential()
model.add(Dense(32,input_dim=784))
model.add(Activation(&#39;relu&#39;))
指定输入数据的尺寸模型需要知道输入尺寸，于是模型的第一层需要明确地被输入数据尺寸（其他层不需要因为可以推测出数据尺寸）。输出尺寸的工作有以下几</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-06-01&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/31/0.%E6%96%87%E7%AB%A0/Kaggk_toxic_%E4%BB%8ERNN%E5%88%B0Transformer%E5%92%8CBert%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/">
        <p class="h4 index-header">0.文章/Kaggk_toxic_从RNN到Transformer和Bert的代码实现</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">从0构建：RNN - WE - LSTM&amp;GRU - SEQ2SEQ - Attention - Transformer - BERTTOXIC比赛阐述目的：
测试集来源于wiki讨论区（多语言），输出测试集中每一条评论是否为有毒评论（有毒输1，没毒输0）。
训练集：
前两次比赛的英文数据。
注意点：

有毒是情感分析中专门的一类，要查阅文献了解研究前沿
测试集是跨语言的，怎么处理跨语言。是五种语言分别建模还是都翻译成英语？
Jigsaw的API Perspective可以使用越来越多的语言为有毒评论分类提供服务。据他们自己所说，去年一年该领域从最新的模型创新中获得了令人印象深刻的多语言功能。包括few- and zero-shot learning。

Mr_KnowNothing的Notebook笔记本文如标题所示，包括如下几个内容：

Simple RNN
Word Embedding
LSTM
GRU
Bi - Directional RNN
Encoder - Decoder Model (Seq2Seq)
Attention Models
Transformers </div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-31&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/30/0.%E6%A6%82%E5%BF%B5/NLP_%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6Attenion%E8%AF%A6%E7%BB%86(3)%E5%BA%94%E7%94%A8%E4%B8%8E%E6%94%B9%E8%BF%9B/">
        <p class="h4 index-header">0.概念/NLP_注意力机制Attenion详细(3)应用与改进</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Attention注意力机制详解（应用）Attention详解分三篇文章：

Seq2seq问题中RNN与Attention的结合
抛弃RNN的self-Attention模型和Transformer架构
Attention和Transformer在NLP和CV问题上的应用

主要参考资料是Yoshua Bengio组的论文、谷歌研究组的论文、Tensor2Tensor的文档、斯坦福NLP讲义
NLP:Universal Transformer：此模型结合了transformer结构和RNN中循环归纳的特点，使得transformer结构能够适应更多的NLU问题。改进的结构如下图，主要是引入了Transition Function，我们可以对attention进行多次循环，这一机制被应用到QA，主语推测谓语，缺失单词填充，数字字符串处理等问题中。

BERT:双向Transformer结构，下图是bert与单向Transformer结构的OpenAI GPT、双向独立LSTM的ElMo对比。

CV : 从略</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-30&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/30/0.%E6%A6%82%E5%BF%B5/NLP_%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6Attenion%E8%AF%A6%E7%BB%86(2)%E8%87%AA%E6%B3%A8%E6%84%8F%E4%B8%8ETransformer/">
        <p class="h4 index-header">0.概念/NLP_注意力机制Attenion详细(2)自注意与Transformer</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Attention注意力机制详解Attention详解分三篇文章：

Seq2seq问题中RNN与Attention的结合
抛弃RNN的self-Attention模型和Transformer架构
Attention和Transformer在NLP和CV问题上的应用

主要参考资料是Yoshua Bengio组的论文、谷歌研究组的论文、Tensor2Tensor的文档、斯坦福NLP讲义
第二部分：自注意力和transformer上文讲解了早期注意力机制与RNN结合，在机器翻译中的效果。RNN由于其顺序结构训练速度常常受到限制。既然注意力机制模型本身可以看到全局信息，那么一个自然的思路是我们能不能去掉RNN，仅仅依赖Attention注意力模型呢？这样训练既可以并行化，同时也拥有全局信息。
本文就讲解Transformer模型，which仅仅依赖于Attention架构。
代码部分的讲解是Tensor2Tensor的源代码
整体模型架构和工作原理如下例句是“I arrived at the bank after crossing the river”.这里的bank指的是河岸还是银行呢，</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-30&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/28/0.%E6%A6%82%E5%BF%B5/NLP_%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6Attenion%E8%AF%A6%E7%BB%86(1)%E6%B3%A8%E6%84%8F%E5%8A%9B%E4%B8%8ERNN/">
        <p class="h4 index-header">0.概念/NLP_注意力机制Attenion详细(1)注意力与RNN</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Attention注意力机制详解Attention详解分三篇文章：

Seq2seq问题中RNN与Attention的结合
抛弃RNN的self-Attention模型和Transformer架构
Attention和Transformer在NLP和CV问题上的应用

主要参考资料是Yoshua Bengio组的论文、谷歌研究组的论文、Tensor2Tensor的文档、斯坦福NLP讲义
第一部分 ：Seq2seq问题中RNN与Attention的结合RNN结构的局限在机翻任务场景中，之前的NMT模型（neural machine translation）中，通常的配置是encoder - decoder结构。即encoder读取输入的句子将其转换成定长向量，然后decoder再将这个向量翻译成相应的目标语言的文字。
通常encoder和decoder都采用RNN结构（例如 gated RNN : LSTM &amp;&amp; GRU)。如下图所示，我们采用encoder RNN将输入语句信息总结到最后一个隐向量中，并将其作为decoder的初始向量。然后decoder会将其解码为目标</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-28&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/27/3.%E8%AF%BE%E7%A8%8B/%E6%B8%85%E5%8D%8E%E8%AF%BE%E8%A1%A8/">
        <p class="h4 index-header">3.课程/清华课表</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">计算机科学与技术系 00240013 计算机辅助设计技术基础 3 学分 48 学时 Fundamentals of Computer Aided Design 本课程坚持基础知识与实践相结合的指导思想，在学习计算机辅助设计的基础知识的同时,学习三维动画软 件的操作与使用。基础知识包括：图形变换、自由曲线曲面造型技术、三维几何造型、真实感图形。应用 软件：3DSMAX 2013 软件的操作。这两部分内容是有机联系的，基础知识对理解 3DSMAX 软件中的基本概 念提供帮助；通过 3DSMAX 软件的操作与实践，能够更好地理解有关基本概念并学习到一门很实用的技术 （三维造型与动画）。本课程的授课特点：1）基础知识课在教室；三维动画课在机房，主要采用案例的教 学方法，边学边练。2） 基础知识与软件学习交错安排。在一学期 16 周中，三维动画课 8 周，基础知识课 6 周，优秀动画作业课堂交流 1 周，节假日 1 周。课程成绩 100 分，由平时成绩（大约 40 分）和大实验作 业（大约 60 分）组成。平时作业主要由课后作业及出勤情况确定；大实验作业题目自选，鼓励多人合作。 提供三维动画软件</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-27&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/27/4.%E5%AE%89%E8%A3%85%E8%B0%83%E8%AF%95%E8%AE%B0%E5%BD%95/%E5%8D%9A%E5%AE%A2%E5%BB%BA%E7%AB%99%E8%AE%B0%E5%BD%95_hiker/">
        <p class="h4 index-header">4.安装调试记录/博客建站记录_hiker</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">windows环境下使用hexo搭建hiker博客记录搭建步骤
购买域名

github创建个人仓库

安装node.js

安装hexo

推送网站

域名解析

更换主题

更改配置

发布文章

使用图床


1.域名购买godaddy.com: 域名很全但价格较高，首年年费均价￥60左右,往后每年续费￥120左右。
阿里云：价格适中但个性化后缀较少,cc后缀域名无法备案
namesilo.com: 域名较全,价格最低,bqlab.cc十年￥280,最终在此购买。 
在namesilo.com中的Domain manager中将域名DNS服务器修改为：

ns1.alidns.com
ns2.alidns.com
2.github创建仓库仓库名命名为fuzzypw.github.io，亲测改成其他域名网站不能正常访问，
settings，设置custom domain,输入fuzzy.pw
3.安装node.js下载地址：https://nodejs.org/en/download/
安装完成后检测是否成功：node -v
检测npm是否一起安装成功: npm - v
4.安装h</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-27&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/25/2.%E6%AF%94%E8%B5%9B/%E6%8C%87%E5%AF%BC_%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87&%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9&%E8%B6%85%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE/">
        <p class="h4 index-header">2.比赛/指导_性能指标&amp;模型选择&amp;超参数设置</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">性能指标 &amp; 模型选择 &amp; 超参数设置已经知道了常用的模型和训练方法。本文主要记录实际应用中对应特定场景选择哪种模型、如何通过收集模型的反馈结果改善模型。
有时候并不需要采用最新最复杂的模型，而是深入理解一些通用的模型，并更好的实际应用到我们处理的具体问题上去。
（和健身同理，卧推后加夹胸超级组、肩推往头顶夹等高端技巧，但训练效果不好；基础动作（屈伸）练扎实，改善基本问题（脊椎旋转、肩胛骨不稳）后，做简单的推肩10kg和侧平举2kg都觉得酸的要死）。
XLM-R就是fancy的训练技巧，基本的LSTM等理解透彻就是基本功扎实后的简单动作。
孰优孰劣pretty obvious
性能指标准确率与召回率
例如罕见病识别问题，我们只需要设计一个模型将所有的样本的结果都判定为“否”，就可以得到99.99%的准确率，但是无意义。于是在不同的场景下对模型的性能指标有不同定义。
准确率与召回率是两个重要的性能指标。
其中准确率 真正例 / (真正例 + 假正例)是模型作出正例判断时的正确率
召回率是 真正例 / (真正例 + 假反例) 是对于所有实际上正确的例，模型检测到的比率
通常</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-25&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/25/2.%E6%AF%94%E8%B5%9B/%E6%8C%87%E5%AF%BC_%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B8%8E%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/">
        <p class="h4 index-header">2.比赛/指导_特征提取与特征选择</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">特征提取与特征选择这两者的效果是一样的，都是试图去减少特征数据集中的属性（特征）的数目，但是两者采用的方法不同，
特征提取的方法是通过属性之间的关系，通过组合不同的属性得到新的属性，改变了特征空间
特征选择的方法是选择出特征集的子集，是一种包含的关系，没有改变特征空间
1.2特征提取的主要方法PCA,LDA,SVD等
1.3特征选择的主要方法1.fliter方法
对每个特征打分，即给出每一维的特征赋予权重，该权重代表了特征的重要性，然后根据权重排序。
主要的方法：卡方，信息增益，相关系数
2.Wrapper方法
主要思想：将子集的选择看成一个搜索寻优问题，生成不同的组合，对组合进行评论
可看做一个优化问题，有很多优化算法可供使用
尤其是启发式算法GA，PSO,DE,ABC等等
主要方法：递归特征消除方法
3.Embedded方法
主要思想：在模型既定的情况下，学习出对提高模型准确性最好的属性，即在确定模型的过程中，挑选出哪些对模型训练有重要意义的属性
主要方法：正则化，如岭回归就是在回归的基础上加上了正则化项
1.4总结1.特征提取是从杂乱无章的世界中，去到更高的世界俯瞰原有世界，这时</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-25&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/25/0.%E6%A6%82%E5%BF%B5/%E6%8A%80%E6%9C%AF_Storm/">
        <p class="h4 index-header">0.概念/技术_Storm</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Stormstorm是一个Apache顶级开源项目,用于流计算
流式计算

实时获取/传输/计算/展示数据
代表技术:Flume实时获取,KAFKA实时存储,Storm实时计算,Redis实时缓存,mysql实时存储

批计算

批量获取/传输数据
代表技术:sqoop导入数据,HDFS存储数据,MapReduce计算数据,Hive计算数据

storm和hadoop的区别:

storm用于流计算,数据用网络传进来,处理过的数据放在内存中
hadoop用于批计算,处理的数据保存在文件存放系统中

storm和hadoop的相同点:

编程模型类似
storm                     hadoop
Nimbus                JobTracker
Supervisor         TaskTracker
Worker                  child
topology              job
spout/bolt           mapper/reducer



</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-25&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/19/0.%E6%A6%82%E5%BF%B5/NLP_GRU/">
        <p class="h4 index-header">0.概念/NLP_GRU</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Gated RNN : GRULSTM和GRU这两个重要的深度学习模型，几乎所有能看到的所有深度学习论文都是以这两个模型为基础。
GRU模型GRU模型的主要思想是，我们希望能够保存一些记忆，能够捕获一些较长的依赖，需要有一个模型去学习何时以及如何去做，同时还得允许错误消息流转。基于输入，这些流转会向不同的方向以不同的强度传递。
对GRU的介绍从标准的RNN开始
循环神经网络RNN基本上直接计算下一步的隐层，同样的，之前的隐状态会回归给向量，我们不是去计算GRU数值，而是先要计算其中各个门，这些门和h(t)一样，就是一些连续的隐状态长度一致的向量。这些门包括更新门和重置门。
计算更新门时主要是基于当前输入的词向量和隐藏状态

计算重置门方法类似但是使用的是不同的权重

通过一个直观例子来认识重置门的作用：

详细分析上图的公式和重置门的作用：假如这是一个语义的情感分析任务，前边都是对一个电影长篇累牍的剧情讨论，最后一句话是it`s a really boring movie.那么其实只要最后这个boring就OK了，前边的都可以舍弃。所以这里重置门就可以置为0，把计算累计的东西都清理掉，</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-19&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/19/0.%E6%A6%82%E5%BF%B5/NLP_LSTM/">
        <p class="h4 index-header">0.概念/NLP_LSTM</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">LSTMLSTM是什么？GRU可以在序列中学习较长的序列连接，另一种允许你很好的处理长距离序列连接的模型就是LSTM—长短时记忆单元，这个模型比GRU更加强大。
LSTM原始论文中采取了大量的篇幅来分析梯度消失的原因，吴恩达不建议读原始论文来了解LSTM的细节。
控制LSTM行为的核心等式

ct，at是维持的状态值（时时更新），u，f，o分别是更新门，遗忘门，输出门。
LSTM的内部结构图：

Runtime LSTM结构图：

如上的红线表示了通过适当地设置了更新们和遗忘门后，一些数据可以长效地被记住的过程。（从左到右一直被维持着。）
朴素RNN与展开是一种处理序列数据的神经网络，某单词根据上下文变化而有不同含义这种任务就适合交给RNN来做。

RNN在时序上展开

LSTM与RNN结构对比长短期记忆（Long short-term memory, LSTM）是一种特殊的RNN，主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。简单来说，就是相比普通的RNN，LSTM能够在更长的序列中有更好的表现。
LSTM结构和普通RNN的对比图如下，LSTM有两个传输状态，分别是cell</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-19&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/18/0.%E6%A6%82%E5%BF%B5/NLP_RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">
        <p class="h4 index-header">0.概念/NLP_RNN循环神经网络</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">循环神经网络RNN循环神经网络擅长处理序列化的数据，比如文字序列或者时间序列。
循环神经网络比基本的前馈神经网络强在它可以跨时间序列地共享不同输入序列之间的特征。
之前提到的CNN卷积神经网络也可参数共享，而RNN循环神经网络与之不同的是每一点的输出依赖于之前的结果。
状态机近似表示RNN名字中的recurrent得名于如下图所示的t时刻的状态 依赖于 t - 1时刻的状态 + 输入x。

​              ↑总图                   ↑拆分                                 ↑ t - 1时刻            ↑ t时刻               ↑ t + 1时刻
上图维持的状态h(t)可以看作是之前的向量序列【x1，x2，x3，x4，……，xn】的全体有损表示（损度可控，某些语言模型只有附近的文字重要就可以加大损度）。
RNN基本结构图
如上，x是时时输入的序列，h是维持的状态，o是状态给出的输出，y是训练集中的答案，L是答案和输出之间的损失函数（如交叉熵）。
因为上一个状态h(t-1)和下一个状态ht之间有循环连</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-18&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/17/3.%E8%AF%BE%E7%A8%8B/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0_%E6%AD%A3%E4%BA%A4%E5%BD%92%E4%B8%80%E6%80%A7/">
        <p class="h4 index-header">3.课程/线性代数_正交归一性</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">正交归一性如果内积空间的两个向量是互相正交的，并且两个向量的范数都是1，则称这两个向量互相具有正交归一性/正交规范性。
</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-17&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/17/3.%E8%AF%BE%E7%A8%8B/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0_%E5%BD%92%E4%B8%80%E5%8C%96/">
        <p class="h4 index-header">3.课程/线性代数_归一化</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">归一化归一化消除了量纲，是处理数据的一种手段。通过归一化最优解的寻找过程会变得平缓，更容易正确的收敛到最优解。
下图的例子是房价预测模型，横坐标是房间数量（0 - 10），纵坐标是面积大小（0 - 1000），预测结果是等高线上的没画出的第三维。
图一未归一化，图像是长椭圆，图二归一化后是正圆（拍扁了数据），可见归一化后收敛过程更平缓。


</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-17&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/15/0.%E6%96%87%E7%AB%A0/NLP_The%20Annotated%20Transformer/">
        <p class="h4 index-header">0.文章/NLP_The Annotated Transformer</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">The Annotated Transformerharvard nlp 对Transformer论文的逐行注解,原文：http://nlp.seas.harvard.edu/2018/04/03/attention.html
本文在github和谷歌drive上的网址：
https://github.com/harvardnlp/annotated-transformer
https://drive.google.com/file/d/1xQXSv6mtAOLXxEMi8RvaW8TW-7bvYBDF/view
Import lib:import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import math, copy, time
from torch.autograd import Variable
import matplotlib.pyplot as plt
import seaborn
seaborn.set_context(context=&quot;ta</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-15&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/05/15/0.%E6%A6%82%E5%BF%B5/NLP_%E8%AF%8D%E5%90%91%E9%87%8F_WordEmbedding%E8%AF%A6%E8%A7%A3/">
        <p class="h4 index-header">0.概念/NLP_词向量_WordEmbedding详解</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Word_embedding独热码的缺点：
它只考虑了单个单词在向量中的作用，而没有考虑词语与词语之间的关系。
如下，有一个训练好的语言模型，要对“I want a glass of orange  ____”进行识别，并预测接下来的词，那么独热码就不能知道apple 和 orange之间的词义相近性，于是不能在算法习得了I want a glass of orange juice之后，泛化地预测 I want a glass of apple juice，这是因为orange和apple在独热码中的编码是完全随机的，并没有因为词性相近就结构也相近，456和6257的位置并没有章法（随机或者按照词频分布），并且由于独热码的结构特殊性，任何两个独热向量相乘的结果都是0。
特征化表示：
所以自然地，我们使用这些词的特征化表示，比如把性别作为一个维度来度量这些词，男的是-1，女的是1，然后king就可以是-0.95,queen就是0.97，橘子和苹果就是基本性别中性的词。另一个维度是有多高贵，明显king和queen是高贵的词而apple和orange的高贵属性为接近0的数字。

如下是对词</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-05-15&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>



  <nav aria-label="index posts navigation">
    <span class="pagination pg-blue justify-content-center mt-5" id="pagination">
      <a class="extend prev" rel="prev" href="/"><i class="fas fa-angle-double-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/3/"><i class="fas fa-angle-double-right"></i></a>
    </span>
  </nav>
  
  <script>
    for (ele of document.getElementById("pagination").getElementsByClassName("page-number")) {
      ele.href += '#board';
    }
  </script>



              </div>
            </div>
          </div>
        </div>
      </div>
    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>

    
  
    <!-- 不蒜子统计PV -->
    
    &nbsp;<span id="busuanzi_container_site_pv">总访问量 
          <span id="busuanzi_value_site_pv"></span> 次</span>&nbsp;
  
  
    <!-- 不蒜子统计UV -->
    
    &nbsp;<span id="busuanzi_container_site_uv">总访客数 
            <span id="busuanzi_value_site_uv"></span> 人</span>&nbsp;
  
  <br>



    

  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js" ></script>
<script src="/lib/popper/popper.min.js" ></script>
<script src="/lib/bootstrap/js/bootstrap.min.js" ></script>
<script src="/lib/mdbootstrap/js/mdb.min.js" ></script>
<script src="/js/main.js" ></script>






  <script src="/lib/smooth-scroll/smooth-scroll.min.js" ></script>



  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>


<!-- Plugins -->


  

  

  

  

  




  <script src="/lib/prettify/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "暗影疾行&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 120,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="/lib/anchor/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "false",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>







</body>
</html>

<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <link rel="icon" type="image/png" href="http://bqlab-pic.test.upcdn.net/myicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="description" content="desc">
  <meta name="author" content="LiuBingqian">
  <meta name="keywords" content="">
  <title>刘秉乾的技术博客 :)</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"  >
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css"  >
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css"  >
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css"  >

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css"  >

<link rel="stylesheet" href="/css/main.css"  >


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css"  >


<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header style="height: 100vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>LiuBingqian`s Blog</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">主页</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">分类</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">所有文章</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">标签</a>
          </li>
        
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">关于</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>


</nav>

    <div class="view intro-2" id="background"
         style="background: url('http://bqlab-pic.test.upcdn.net/index.jpg')no-repeat center center;
           background-size: cover;
           background-attachment: fixed;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
          </div>

          
            <div class="scroll-down-bar">
              <i class="fas fa-angle-down scroll-down-arrow"></i>
            </div>
          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      <div class="container nopadding-md">
        <div class="py-5 z-depth-3" id="board">
          
          <div class="container">
            <div class="row">
              <div class="col-12 col-md-10 m-auto">
                


  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_CRF%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/">
        <p class="h4 index-header">0.概念/NLP_CRF条件随机场</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">条件随机场
概率统计图概览


马尔科夫假设/马尔科夫性

马尔科夫假设
马尔科夫链  里的  总是只受  一个人的影响。马尔科夫假设这里相当于就是个2-gram。
马尔科夫过程
在一个过程中，每个状态的转移只依赖于前n个状态




马尔科夫性
马尔科夫性是保证或者判断概率图是否为概率无向图的条件
成对性
局部性
全局性






条件随机场定义

条件随机场是在给定的随机变量 （具体，对应观测序列  ）条件下，随机变量  （具体，对应隐状态序列  的马尔科夫随机场。
广义的CRF的定义是： 满足  的马尔科夫随机场叫做条件随机场（CRF）
条件随机场是一种特殊的马尔科夫随机场
马尔科夫随机场
首先我们有无向图G=(V,E)， 图G中每个节点v上都有一个随机变量y，这样所有的节点上的随机变量就构成一组随机变量Y，图G上有联合概率分布P(Y)。边e表示相邻节点的变量存在某种神秘的联系。
图G上的随机变量Y满足马尔科夫性，即两个不相邻的节点上的随机变量yi，yj条件独立。这就是马尔科夫随机场。




CRF建模公式


CRF特征函数











</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_%E8%AF%8D%E5%90%91%E9%87%8F_n-gram/">
        <p class="h4 index-header">0.概念/NLP_词向量_n-gram</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">n-gram语言模型1.Statistical Language Model
在自然语言处理中的一个基本问题,如何计算一段文本序列在某某种语言下出现的概率?
例子

我经常会去图书馆＿＿＿？
预测该句后面的词，我们通常会根据已有的语料的上下文，来统计预测这句话可以填某个词汇的概率，将最大的概率作为结果返回


机器翻译中,I like  Tomc so much,将单词逐个翻译—-{我,喜欢,汤姆,非常},这个集合中的字词排列组合成句子,然后用语言模型去计算组成句子概率的大小,概率越大越流畅

2.n-gram语言模型
理解:
n-gram语言模型的思想,可以追溯到香农的问题:给定一串字母,比如”for ex”,下一个最可能出现的字母是什么?从训练语料中,我们可以通过极大似然估计的方法,得到N个概率分布,是”a”的概率是0.4,是”b”的概率是0.0001,是c的概率是….,and 别忘记约束条件:所有N个概率的分布总和为1
如下图,运用条件概率和乘法公式推倒:
直接这么计算比较困难,需要引入马尔科夫假设,即,一个item的出现,只与前m个items有关,m = 0时,就是unigra</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/2.%E6%AF%94%E8%B5%9B/%E6%8C%87%E5%AF%BC_%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2&%E9%A2%84%E5%A4%84%E7%90%86_CrowdFlower/">
        <p class="h4 index-header">2.比赛/指导_数据探索&amp;预处理_CrowdFlower</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">以CrowdFlower比赛为例讲解数据探索与预处理比赛目标​    衡量搜索结果的相关性

比赛数据集
CrowdFlower平台丰富的查询结果配对创建的
为了评估搜索相关性,CrowdFlower已经将261个搜索词与产品列表放在一起,要求人群对每个搜索结果评分,1,2,3,4分别表示搜索结果从完全不相关到完全相关



数据集
train.csv训练集数据
id 产品id
query 搜索词语
product_title 产品标题
product_description 产品描述文本
median_relevance 三位评分员的相关性评分中位数
relevance_variance 评分员的相关性评分方差


test.csv
id 产品id
query 搜索词语
product_description 产品描述文本


目标变量
median_relevance



数据预处理

首先本数据以文字为主,文字只能输入进分类树模型,所以首先要把文字转换成数字
Dropping HTML标签
Word Replacement:然后要把拼写错误的单词替换掉
stemming:词干化</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/2.%E6%AF%94%E8%B5%9B/%E6%8C%87%E5%AF%BC_%E4%B8%89%E4%B8%AA%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/">
        <p class="h4 index-header">2.比赛/指导_三个集成学习模型</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">三个常用的集成学习模型常见集成学习模型一览图
集成学习的优点采用多个分类器对数据集预测,提高整体分类器的泛化能力
三种常见的集成学框架
bagging 装袋
boosting 提升
stacking 堆栈

bagging–装袋
子训练集一般是各不相同的
基模型一般采用SVM或者朴素贝叶斯(大家一般采用同一种模型)
测试集扔给基模型们,然后各个基模型投票表决,简单多数为最终结果

Boosting提升第一次训练得到返回结果,然后给每一个结果分配权值,分类正确的权值降低,错误的权值上升
分类错误权值升高,在第二次训练时被重点关照
测试–测试集扔给各个样本,最后根据投票权值分配投票权,最终得到分配结果
Stacking–堆叠

训练集分出n个基模型
集成方法:
基础模型比如有100个,每个输出三维向量,一共就输出300维的向量
这个向量在堆叠模型那里训练
测试集也有三百维,最后生成测试数据让模型训练,得到最终结果



集成模型的偏差与方差

</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/2.%E6%AF%94%E8%B5%9B/%E6%8C%87%E5%AF%BC_%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95%E4%B8%8E%E5%B7%A5%E5%85%B7/">
        <p class="h4 index-header">2.比赛/指导_常见算法与工具</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">比赛常用算法与工具1.1 提纲
机器学习应用领域
机器学习常见算法
常用工具
建模与问题解决流程
数据处理
特征工程
模型选择
寻找最佳超参数:交叉验证
模型分析与模型融合


kaggle wiki
简单案例讲解

1.2机器学习常见算法
1.3机器学习常见工具

scikit - learn :速度不快,但是全面,封装的好,只需要造出来基本参数就可以自动去跑
gensim - 自然语言处理会用
NUmPy - 科学计算(封装到其他工具里了)
matplotlib - 绘图
pandas - 数据清洗,产出特征,缺省值,填充等
xgboost - 基于boost的库,分类和回归都可以完成
Natural Language Toolkit多用于英文的自然语言处理,中文用的很少
Jieba - 多用于中文语言处理
TensorFlow - 深度学习库,对显存的占用较高,速度不算太快
Caffe -深度学习库, 图像用的很多
Keras - 深度学习库,接口简单,本视频deep learning部分用Keras

1.4解决问题流程
了解场景和目标
了解评估准则
认识数据
数据预处理(清洗</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/">
        <p class="h4 index-header">0.概念/NLP_极大似然估计</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">极大似然估计极大似然估计是确定机器学习模型的参数的一种办法。确定参数值的过程，是找到能最大化“模型产生真实观察数据可能性“的那一组参数，略抽象，如下是一个例子：
从某过程观察了如下十个数据点，每个数据点代表了学生回答问题使用的秒数。

这些数据的生成过程可以使用高斯分布（正态分布）进行充分描述。高斯分布有两个参数，西格玛和μ，如何确定参数？如下示意图表示了使用不同参数的不同高斯分布（方差大的中心函数更扁平）。
注：蓝色曲线是正确曲线N(10,2.25)

OK，如何反编译确定正确参数？我们把这个例子再次简化，同样的情境，这次只存在三个数据点：9,9.5,11
如何使用最大似然估计确定这个高斯分布的参数？
高斯分布中，单个数据点x的边缘概率如下

同时观察到上边所提三个点（9,9.5,11）的联合概率是带入上边三个数据的连乘积：

我们只要能找到最大化上述连乘积的参数μ和西格玛就ok了。也就是说，最大似然估计是一个通过确定参数得到函数最大值的优化问题。
那么，如何求出上述函数的最大值？
easy，二元函数求偏导标准步骤，加以两边套上对数等数学小技巧就完事儿了。
SOP:


</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_%E6%A2%AF%E6%AE%B5%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/">
        <p class="h4 index-header">0.概念/NLP_梯段下降算法</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">梯度下降算法对于优化问题，机器学习的目标是使得某个损失函数最小。也就是找到一个x = min f(x)。但并不是所有的问题都能找到解析解，部分问题只能通过数值计算的方法逼近最优解 —— 一阶导数的梯度下降算法和二阶导数的牛顿方法。

问题描述:有一个代价函数，它有两个参数，想让这个代价函数的值最小化。
做法：持续把这个两个参数向着梯度下降最快的方向迭代。



梯度下降算法的学习率α设置:
α过小,收敛太慢
α过大,在最小值附近震荡


梯度下降缺点:可能求的是局部最优解，解决办法是多次随机初始化起点。

1.多特征值的回归问题

单特征回归:只有房子面积一个特征,求预测房价

单特征回归的假设函数:h(x) = θ0 + θ1x


多特征回归:有房子面积,卧室数量,几层高,使用年限四个特征,求预测房价

四特征回归的假设函数:h(x) = θ0 + θ1x1+θ2x2+θ3x3+θ4x4    —-&gt;   缩写h = θ(XT) :向量θ乘以向量X的转置
只考虑最简单的一次线性多项式


多特征回归的梯度下降算法:

每次对一个参数求偏导,并对其迭代.
所有参数都迭代这么一圈</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/">
        <p class="h4 index-header">0.概念/NLP_支持向量机</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">支持向量机SVM9.1SVM的优化目标从逻辑回归展示如何一点点修改得到本质上的支持向量机

逻辑回归的假设函数




9.2核函数对支持向量机算法做一些修改,以构造复杂的非线性分类器
我们用”核函数”来达到此目的
问题的提出:
使用高级数的多项式模型来解决无法使用直线进行分割的分类问题,如何确定模式中的每一项的参数?
支持向量机的假设函数和代价函数

9.3 SVM的使用不建议自己写代码求解参数θ,就像没有人会写代码自己去求解平方根一样,可以直接调用现有的库
除了高斯核函数之外还有其他核函数可以用:
多项式核函数（Polynomial Kernel）
字符串核函数（String kernel）
卡方核函数（ chi-square kernel）
直方图交集核函数（histogram intersection kernel）
等等
SVM模型 和 逻辑回归模型之间的取舍:

</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_%E5%88%A4%E5%AE%9A%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/">
        <p class="h4 index-header">0.概念/NLP_判定模型和生成模型</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">判定模型和生成模型的区别
机器学习的任务是从属性X预测标记Y，即是求概率P（Y|X）

判别式模型是上图左边示例，有个明显的边界，新来一个值需要判断他属于哪一类的时候直接算出他的score，当score大于threshold时为正类，反之为负类。线性回归，SVM模型都是典型的判别式模型
生成式模型是上图右边示例，无明显边界，新来一个值要判断他是哪一类的时候，首先求该值与两个不同标记的不同联合概率分布，然后大的获胜。朴素贝叶斯模型，HMM模型都是生成式模型。

一个生动的例子说明两者的区别:

判别式模型：要确定一个羊是山羊还是绵羊，用判别式的方法是从历史数据中学到模型（运行同一个模型得到确定的结果），然后通过提取这只羊的特征来预测出羊的类型。
生成式模型：根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模型。然后提取这只待判定羊的特征，放到山羊模型中看看概率是多少，再放到绵羊模型中看看概率是多少。哪个大就是哪个。（两个模型，两个结果，最后比比数值大小得出结论）

</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_8.%E5%BA%94%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BB%BA%E8%AE%AE/">
        <p class="h4 index-header">3.课程/CS229_8.应用机器学习的建议</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">8.应用机器学习的建议8.1如何提高一个算法/机器学习模型的性能问题:预测房价的机器学习模型性能遇见瓶颈,如何提升性能?

很多人是凭感觉去解决问题,他们有如下猜想并随便选一个去做

随便某个猜想都是耗时耗力巨大的项目,人们常常选择的是一条不归路

如何尽量排除无效的道路?



获得更多的训练样本——解决高方差   
尝试减少特征的数量——解决高方差
尝试获得更多的特征——解决高偏差
尝试增加多项式特征——解决高偏差
尝试减少正则化程度λ——解决高偏差
尝试增加正则化程度λ——解决高方差

8.2如何评估一个模型表现
模型的代价函数非常小未必是好事,因为可能存在过拟合的现象
为了检验模型是否过拟合,可以采用交叉验证的方法
可以把数据集分为训练集和测试集,然后重复洗牌交叉验证

高偏差与高方差

高偏差是欠拟合

高方差是过拟合




增加模型的多项式次数导致的过/欠拟合问题

训练集上:
增加模型多项式的次数会导致代价函数单调递减


交叉验证集上:
增加模型多项式的次数会导致代价函数先减后增
先减是改善了欠拟合的问题
后增就是过拟合越来越严重


结论:
交叉验证是评估过/欠拟合</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_5.%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AD%A3%E5%88%99%E5%8C%96/">
        <p class="h4 index-header">3.课程/CS229_5.过拟合和正则化</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">1.过拟合和欠拟合问题两张插图说明白


解决办法:
过拟合就丢弃不能正确预测的特征,欠拟合就加特征
2.代价函数的惩罚项由下图可以看出,过拟合由于高维项造成



给高维项设置惩罚项

代价函数里给三次项和四次项增加一些重量,梯度下降的时候系统就会更急切想把三次项和四次项降下来

增加了惩罚项的代价函数





</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_6.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(%E8%A1%A8%E8%BF%B0)/">
        <p class="h4 index-header">3.课程/CS229_6.神经网络(表述)</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">#
6.神经网络(表述)6.1问题的由来
特征爆炸无法处理
例如,智能识别一个图片是不是汽车的问题
假设使用50*50小黑白照片
那么就有2500个像素灰度作为特征
2500个特征两两组合就是3000000项,这的多项式处理不了
这时候就需要神经网络



6.2直观理解和逻辑运算符的构建
直观理解

神经网络能够通过自身学习得到一些列特征,这些特征是使用数据的原始特征经过拆分组合和一系列逻辑运算得到的
这样的一系列特征比普通的逻辑回归的表层特征要深刻的多
最后做决定的神经元依据上一层传进来的特征,这些特征已经经过多层传导,内化为自己的特征了


神经元可以通过增加一个固定权重神经元构建逻辑运算符(与,非,或,抑或等)

如下图




6.3神经网络的形态
6.4神经网络基本概念的定义from CS224N如图，神经网络有输入，有偏置单元，有激活函数，以及对后续神经网络的输出。
输入x，w是乘以输入的权重，b是偏置值，f是sigmod函数。
输入乘以权重加上偏置值后，进入sigmod函数，输出一个分类值。
有了神经网络，让我们再回到之前的单窗口分类器。代替之前直接将softmax应用</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_7.%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(%E5%AD%A6%E4%B9%A0)/">
        <p class="h4 index-header">3.课程/CS229_7.神经网络(学习)</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">7.神经网络(学习)7.1神经网络的代价函数

第一项是在K个输出项上(比如四分类就是K=4),累加逻辑回归的代价函数
第二项就是更高维的罚函数

代价函数的目的:找到使得J(θ)最小化的θ
7.2单个训练数据如何通过神经网络    

z(2) = θ(1)a(1) 

z(2)是第二层神经元的输入项,共五项,是a(1)的三项经过θ(1)矩阵拆分后的结果


a(2) = g[z(2)]

a(2)是第二层神经元的输出项,共五项,是z(2)五项经过第二层神经元的sigmod函数变换后的结果



后边的几层同理
</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_PCA%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/">
        <p class="h4 index-header">0.概念/NLP_PCA主成分分析</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">PCA主成分分析问题定义机器学习有一类问题是有损压缩，即我们对原数据进行一些压缩以减少存储空间，这个过程不可避免的会损失一些原有的信息，希望能尽量减少信息丢失。
假设有三个三维空间的点x1，x2，x3，希望找到降维后二维空间的点t1，t2，t3。
将三个点从三维空间映射到二维空间的是压缩函数f（x）= t
将三个点从二维空间映射到回三维空间的是解压函数g（t）= x，对应的，若使用矩阵乘法来解压缩，则g(t) = Dt = x
其中D就是解压缩矩阵。
例如                           
x1   [1,2,3                t1  [1,2
x2    1,2,3     =   D  *   t2   1,2
x3    1,2,3]               t3   1,2]​              ↑压缩前数据   ↑解压缩矩阵    ↑压缩后数据
​           【  ↑ X      】      【               g(t)               】              
为了简化问题，PCA规</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_2.%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B/">
        <p class="h4 index-header">3.课程/CS229_2.正规方程</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">CS229—正规方程1.定义
​    正规方程(Normal Equation)是通过对训练数据/训练答案/待求参数之间的矩阵变换得到答案

正规方程求解参数矩阵的公式:

2.正规方程法和梯度下降法的对比

梯度下降的特点

要设置学习率α
多次迭代
工作地非常均衡
适合特征变量很多的时候用
要求参数归一化


正规方程法

不用设置学习率
不用迭代
需要计算
适合特征变量不超过1000时用
不要求参数归一化,如下图



有些较复杂问题不能使用正规方程法,只能使用梯度下降



</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_4.%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/">
        <p class="h4 index-header">3.课程/CS229_4.逻辑回归</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">4.逻辑回归(logistics regression)1.逻辑回归模型
分类问题输出一个结果”正确”或者”错误”

逻辑回归模型:




当hθ(x) 在区间[0.5,1]时,预测是”正确”,越贴近1越确信
当hθ(x) 在区间[0,0.5)时,预测是”错误”,越贴近0越确信

2.逻辑回归的代价函数

简单点一句话讲:
答案是1,你预测了1,无误差
答案是1,你预测的数字离0越近误差就越大,完全是0误差就是无限大(不可能情况)
答案是0的情况与答案是1的情况同理



3.对逻辑回归的代价函数使用梯度下降算法,以得到最优参数/最优解
4.高级优化
一些建议

不需要写代码实现代价函数的迭代

从技术来讲,其实我们不需要自己手动写程序来计算刚才提到的梯度下降算法的代价函数以及其迭代过程,就像我们不需要自己手动写求平方根和创建数组一样,这些问题早都有非常成熟的库来调用


更高级的算法

除了梯度下降算法,还有共轭梯度法 BFGS (变尺度法) 和L-BFGS (限制变尺度法)这种更高级算法可调用
吴恩达本人已经使用如上提到的算法很多年了,也才是最近才搞清楚他们的内部实现细节



</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_10.%E8%81%9A%E7%B1%BBClustering/">
        <p class="h4 index-header">3.课程/CS229_10.聚类Clustering</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">10.聚类Clustering10.1无监督学习简介无监督学习是让计算机学习无标签数据,而不是之前的标签数据
如图就是聚类问题
左上第一张图是市场分割,就是根据数据库里沉淀的客户信息来对客户分类,做知识图谱和推荐系统
10.2 K-mean (K均值算法)K均值是最普及的一个聚类算法,算法接受一个数据集,将数据聚类为不同的组

K-均值是一个迭代算法

设想我们将数据聚集成n组,选择k个随机的点为聚类中心

对数据集中的每个数据,按照距离k个中心点的距离,将其与距离最近的点关联起来,与这个点聚为一类

然后开始迭代:

每次计算一个组的平均值,将中心点移动到平均值的位置
如此反复几次中心点的位置就不再变化
如下图所示





10.3优化目标K-均值的优化目标就是最小化所有的数据点与其关联的聚类中心点之间的距离之和
因此K均值的代价函数为:

10.4随机初始化在运行k均值算法之前,需要随机初始化所有的聚类中心点
1.选择K &lt; m ,即聚类中心点的个数要小于所有训练集实例的数量
2.随机选择K个训练实例,然后令K个聚类中心等于他们
K-均值算法的一个问题在于,他可能停留在某个</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_9%20-%20RNN,LSTM,GRU/">
        <p class="h4 index-header">3.课程/CS224n_9 - RNN,LSTM,GRU</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">助教分享：建立一个更好的语言模型语言模型是NLP中最经典的任务，这里有三种方式可以让其变得更好，第一是更好的输入表达，第二是更好的正则化或者预处理，第三是有更好的模型。
第一种是改进输入，例如Glove是一个词层面的表示，事实上可以把单词进一步编码为子词阶段，可以采用语素解码方式，可以采用BPE，最终也可以采用字母级的嵌入，他的作用是大大减少所用到的词汇量，让模型预测变得更简单，如下图展示的论文就是对输入端的改进。

第二种办法是采用正则化技巧，来改善过拟合的问题，有一堆论文在讲如何做正则化，今天聚焦的点是在预处理阶段，通过和计算机视觉类似的替换技巧来构建更合理的语料库，比如把频繁和罕见的词语的评率都往平均值上拉一拉，就会使得曲线更显平滑，平滑的分布能让系统获得更好的语言模型和更好的实验结果。
第三是更好的模型，etc。
</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS229_1.%E7%BB%AA%E8%AE%BA/">
        <p class="h4 index-header">3.课程/CS229_1.绪论</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">CS229–绪论1.监督学习和无监督学习
监督学习定义
部分数据集已经有答案.比如房价信息集,知道了正确的房价,需要预测出更多的房价
回归问题,预测连续值的输出(房价)
聚类问题,预测离散值的输出(肿瘤是良性还是恶性?)




无监督学习

无监督学习没有答案,自动聚类:比如google搜索BP油井泄露,会把CNN,ABC,BBC新闻放一起



2.线性回归算法
监督学习的典型过程

Training set –[喂食给]—&gt;Learning Algorithm –[生成]–&gt;hypothesis(假设函数)

size of house–[输入]—-&gt;hypothesis(函数)—-[输出]—-&gt;Estimate price





线性回归

预测房价的假设函数:h(x) = θ0 + θ1x
根据训练集生成一元一次方程,输入房间大小得到房价预测



3.代价函数
代价函数有利于弄清楚如何把最有可能的直线与我们的数据相拟合

最小化代价函数就可得到最好的预测函数

如何选择假设函数hypothesis “h(x) = θ0 + θ1x” 中的两个参数</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_8%20-%20RNN%E5%92%8C%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%BC%8F/">
        <p class="h4 index-header">3.课程/CS224n_8 - RNN和语言模式</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Lecture8 RNN和语言模式本节讲简单的循环神经网络模型，这个模型家族是大多数人现在在实际训练环境中使用的。
 概览：
1.传统的语言模型
2.RNNs
3.语言模型建模来驱动循环神经网络（RNN）
4.重要的训练时的问题和技巧
​    梯度消失问题
​    梯度爆炸问题
5.用于队列处理的RNN
6.双向深度RNN
1. 传统的语言模型和现在的词向量模型对比传统语言模型中，理想情况下预测一个语序是根据前n-1个词出现的条件概率下，第n个词出现的概率。实际中这么做不可行，一个是语序有无限多个，而且计算每一个w(n)都要把之前所有的词都遍历一遍这个成本太大了。
如上条件概率公式也就是表示在词语w1出现的条件下，w2出现的概率是由在整个语料库中的count(w1,w2)/count(w1)决定的。
如果我想提高精度，将预测w1的工作加上了w2和w3.那么我就需要统计出语料库中，所有三元组三三组合出现的概率，假设物料库有10万个词，那么这个统计和存储的工作量就是10万的三次方。要求有140G的内存仅仅用来存放1260亿的记号语料库计算所有的计数。所以从内存的角度看，这种方法非常低效</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_6%20-%20%E4%BE%9D%E5%AD%98%E5%88%86%E6%9E%90/">
        <p class="h4 index-header">3.课程/CS224n_6 - 依存分析</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Lecture6 依存分析本节课综述

这节课主要讲句法 + 语法 + 依存分析
基于转移的依存关系分析语法
神经网络内容
TensorFlow + 实际的nlp内容例如构建神经依存关系分析器

1.依存分析的概念和一个例子句子或句子的一部分有一种结构，人们可以用特定的方式将他们组合起来，我们可以从非常简单的并不构成句子的东西入手。

一个冠词 + 名字 通常被语言学家称为名词短语



有一些规则可以用来扩展他们：比如在冠词和名词之间加一个形容词
the large cat
a beautiful dog等

你也可以在名词后边添加介词短语
the large cat in a crate
the barking dog on the table等


传统上，语言学家和自然语言处理器想做的是描述人类语言结构，人们过去有两个关键工具来做到这点，一种是上下文无关文法的方法，这种方法经常被语言学家引用为短语结构文法。我们现在要做的就是写出这些上下文无关文法的规则。

还有一种了解语言结构的不同角度，就是依存句法结构。它通过找到句子当中每一个词所依赖的部分来描述句子结构。如果一个词修饰另一</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_7%20-%20Tensorflow/">
        <p class="h4 index-header">3.课程/CS224n_7 - Tensorflow</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Lecture7 TensorFlow为什么要使用深度学习框架？

有助于扩展机器学习代码
自动计算梯度（通常梯度计算并不重要，这些框架可以自动处理梯度，使得我们把重点放在更高层次的数学上）
在很多领域使机器学习更标准化
这些框架提供了GPU接口

1.TensorFlow是什么？是一个谷歌开发的深度学习框架，使用流式图进行数值计算的开源软件库。TensorFlow是用于表示机器学习算法的接口，以及用于执行这种算法的实现。
TensorFlow的一个big idea就是数值计算表示为计算图来进行。 图的节点是操作，每个节点都有一个输入和输出，节点之间的边表示他们之间流动的张量，在实践中最好的方法就是认为，张量是一个n维数组。
使用流式图作为深度学习框架主干的优点在于它允许你使用小而简单的操作建立复杂的模型，当我们这样做的时候，会使得梯度计算变得极其简单。
此框架可以自动求导，而且图方法的每一个操作都是可以在其所在点被评估的一个函数。变量将成为输出其当前值的有状态节点。
占位符是那些在执行期间才会接收值的节点，如果在你的网络中有依赖外部数据的一些输入，你并不想在建立图时依赖任何实际的值。</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_2,-%20%E6%B3%A8%E9%87%8A%E4%B9%8Bsoftmax/">
        <p class="h4 index-header">3.课程/CS224n_2,- 注释之softmax</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">softmax定义：
假如有一个数组V，Vi表示V中的第i个元素，这个元素的softmax值如下，也就是该元素的指数值和所有元素指数值之和的比值。softmax通常希望特征对概率的影响是乘性的。

softmax VS k个二元分类器：如果你在开发一个音乐分类的应用，需要对k种类型的音乐进行识别，那么是选择使用 softmax 分类器呢，还是使用 logistic 回归算法建立 k 个独立的二元分类器呢？这一选择取决于你的类别之间是否互斥，例如，如果你有四个类别的音乐，分别为：古典音乐、乡村音乐、摇滚乐和爵士乐，那么你可以假设每个训练样本只会被打上一个标签（即：一首歌只能属于这四种音乐类型的其中一种），此时你应该使用类别数 k = 4 的softmax回归。（如果在你的数据集中，有的歌曲不属于以上四类的其中任何一类，那么你可以添加一个“其他类”，并将类别数 k 设为5。）如果你的四个类别如下：人声音乐、舞曲、影视原声、流行歌曲，那么这些类别之间并不是互斥的。例如：一首歌曲可以来源于影视原声，同时也包含人声 。这种情况下，使用4个二分类的 logistic 回归分类器更为合适。这样，对于</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_4%20-%20wordWindow%E5%92%8C%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">
        <p class="h4 index-header">3.课程/CS224n_4 - wordWindow和神经网络</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Lecture4  wordWindow和神经网络1.分类背景知识对分类的直觉感受是什么？在机器学习领域，在还没有达到深度学习领域的情况下，我们通常将分类理解为简单的逻辑回归，也就是定义一个简单的决策边界。

2.窗口分类在一般的机器学习中，我们假设输入是固定的。输入X都是固定的，我们只训练参数W，也就是softmax的权值，然后计算给定输入X时输出Y的概率。
3.交叉熵我们假设正确类别的概率为1，其余的概率为0.举个例子，假设共有5个类别，正确类别是中间的第三个，那么第三个的概率为1，其他都是0.我们把理想的概率定义为p，softmax计算的概率为q，这里给出了交叉熵的定义，就是对所有类别的求和
正则化项：里边包含的参数θ如果是标准逻辑回归中的矩阵W，实际上目标函数加入这个正则化项的目的就是是为了鼓励模型中的所有权值尽可能地小。
可以假设你想要一个贝叶斯模型，你可以有一个先验的高斯分布，理想情况下这些参数的值都很小，但是如果没有这个正则化项，通常情况下你得到的模型参数会爆炸，他会越来越过拟合。如果没有正则化项，我们会专注于如何拟合我们的模型。
通常的机器学习优化就是只优化模型的参数W</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_5%20-%20%E9%A1%B9%E7%9B%AE%E5%BB%BA%E8%AE%AE/">
        <p class="h4 index-header">3.课程/CS224n_5 - 项目建议</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Lecture5 项目建议1.一层神经网络过渡到多层神经网络
2.项目建议老师推荐的文章和会议

1.定义你的任务：
例如：summarization
2.定义数据集：
最好使用现成的数据集，因为他们已经有baselines
3.建立baseline
他可以是一个非常简单的一元线性回归，然后在你的训练数据集上计算你的评价标准，看看模型是过拟合还是欠拟合
4.选做题：自己发明新的模型
首先，你需要做好以上说的几个步骤.

然后你需要知道已经存在的模型上有哪些问题。然后你就可以设计出自己的模型。如果你想要这样做的话，你真的需要和你的导师和其他研究者保持沟通，除非你自己就是研究者并且已经获得了博士文凭。

你需要实现你的模型，然后根据你的新点子去对它快速迭代。（也许在某个位置新加一层？然后看看他起不起作用？）

那么在迭代的过程中，拥有足够多的的软件工程技能来配置高效的实验框架，从而能收集到这些结果就很重要。

建议从一个和你的真实想法比起来相对容易很多的模型做起。先把简单模型建立起来。然后逐步尝试更复杂的模型。

对于终极任务：summarization任务。

一开始你可能尝试一些非常简</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_%E8%AF%8D%E5%90%91%E9%87%8F_Word2vec/">
        <p class="h4 index-header">0.概念/NLP_词向量_Word2vec</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Word2Vecfrom笔记ofCS224N
这节课深入语言的底层，做一些向量和计算，这节课提到的数学是后边内容的基础。这节课将用很慢的速度来仔细讲解一些基础，以便大家可以使用神经网络来学习词语表征这样的简单任务。
1.语言学和NLP对词语释义的不同做法语言学用如下词典释义来解释“meaning”这个词的意思

the idea that is represented by a word,phrase,etc
the idea that i person wants to express by using words,signs,etc
the idea that is expressed in a work of writing,art,etc

NLP同义词描述词汇
如下图左侧，用一段代码演示nltk如何抓取wordnet中”panda“这个词的分类信息，panda是一种肉食动物，一种有胎盘的哺乳动物，再往上上溯可抽象为动物，物体，物理实体等等。
如下图右侧，抓取“good“的同义词，这里显示的就是wordnet的答案。

同义词描述词汇本义毕竟是离散的方法，会有如下的问题：

同</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_16%20-%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9C%A8NLP%E4%B8%AD%E7%9A%84%E9%99%90%E5%88%B6/">
        <p class="h4 index-header">3.课程/CS224n_16 - 深度学习在NLP中的限制</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Lecture16 - 深度学习在NLP中的限制对于语言，如果你想要一个合适的语言系统，你可能需要一些对于输入的情感理解，在逻辑上推理某些事实，在数据库中检索一些事实，或者基于数据库中的逻辑原因，再做一些内存检索。还有一些时候，你需要对谈论的内容做一些奖励。在现实世界中这些过程是由很多不同的成分组成的。我们想要一个更好的理解语言的系统，理想情况下这个系统应该包括很多内容，以一种更科学的方式呈现。
现在联合多任务学习仍然非常困难，迄今为止，人们谈论多任务学习时，他们假设有一个源任务和一个目标任务，他们希望神经网络在源任务的预训练可以加强另一个目标任务的表现。就我而言，理想的情况是让他们共同训练，而不是分开训练不同的解码器。例如不同语言中，不同的分类问题。理想情况下，我们只有一个很大的不同种类的数据集，我们想根据输入来预测，他们有完全一样的解码器，但是很多时候人们做多任务学习的时候他们只是共享低层的参数，共同训练他们，但是不能共享高层的参数。
也就是说，在自然语言处理的过程中，我们通常只是共享了词向量，我们没有共享其他高层的比如LSTM层，这种层能解决更多的任务，其实计算机视觉在这方面有挺</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_14%20-%20%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/">
        <p class="h4 index-header">3.课程/CS224n_14 - 问答系统</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Lecture14 - 问答系统1.实现问答系统的联合模型：动态记忆网络
从下边开始看，输入都是词向量，比如word2vec或者glove之类。基本会有一个循环的神经序列模型，比如GRU,他为每个输入单词计算一个隐藏层。
在右下角的问题模块中也有处理问题的GRU,用于计算输入的问题的向量。
以右下角输入的问题为例，“足球在哪里？”我们会认为足球被提及的事实的这个问题储存在上一次GRU隐藏层的某个地方，我们称之为Q，我们使用Q从本质引发所有潜在输入的注意力机制，每当有一个特定的句子被强烈关注，我们将把这个句子作为另一个GRU的输入，那就是情节记忆模块。基本上只要有上图蓝色的线存在，那基本就是一种循环的神经网络序列模型。所以基本上，一个问题促发一个注意力机制。遍历所有输入的所有隐藏层，现在基本上说这个事实似乎与这个问题有关。基本上说，一个问题引发一个注意力机制，然后遍历所有输入的所有隐藏层，也就是说在输入模块，我们认为最后一次提到“足球”肯定和现在问的问题是相关的，于是GRU的隐藏层捕捉到了这一点，然后经过上边的记忆模块中的GRU（fliter）过滤，现在只聚合与当前问题相关的句子。 图上</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_15%20-%20NLP%E7%9A%84%E9%97%AE%E9%A2%98%E5%92%8C%E5%8F%AF%E8%83%BD%E6%80%A7%E6%9E%B6%E6%9E%84/">
        <p class="h4 index-header">3.课程/CS224n_15 - NLP的问题和可能性架构</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Lecture15 - NLP的问题和可能性架构1.NLP问题视觉研究以前在深度学习中占主导地位，但是三巨头Yann LeCun,Geoff Hinton,Yoshua Bengio都把他们的研究方向转向NLP(也就是说深度学习三巨头的研究方向有风向标的意义)
Bengio在采访中表示，实验论证深度学习技术应用与新应用的可能性，这些新应用包括了计算机视觉，对话系统，虚拟助手，语音识别，NLP,机器翻译以及其他应用。
这世界上有太多不同的语言现象和相对应的任务要完成，所以解决NLP问题不能像解决计算机视觉那样，某个人构建出一个足够复杂的深度神经网络就vans了.NLP问题们并不同源，不能说解决了其中一个，就可以宣布全部解决。
在传统NLP问题中丢失了的东西：
将现在和过去对比是一件很有趣的事情，在70和80年代的那批从事NLP研究的人员里，他们有着非常崇高的目标，他们想要达到人类级别的语言理解能力，但是他们最终达到的是非常骨感的现实。
不论上述对比的结果如何，我们现在所处的环境，我们能比他们当时更好地实现目标，但是，达到目标的途径却开始变得不那么明朗。作为实践，我们可以在这个语料数据上运</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_12%20-%20%E8%AF%AD%E9%9F%B3%E5%A4%84%E7%90%86%E7%9A%84%E7%AB%AF%E5%88%B0%E7%AB%AF%E6%A8%A1%E5%9E%8B/">
        <p class="h4 index-header">3.课程/CS224n_12 - 语音处理的端到端模型</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Lecture12 - 语音处理的端到端模型
介绍传统的语音识别系统
引出语音识别的端到端模型，并给出描述（CTC,LAS）
端到端模型的改进版
语言模型如何影响语音识别的
语音识别模型中的解码工作的改进

1.传统语音识别系统自动语音识别的基本定义是把语音信号自动转换为文字呈现，语音识别系统的经典实现方法是使用生成模型（generate model），后来被简单的神经网络模型取代。
如下图最右，你从语言模型（n-gram）中生成了一个由单词组成的特定序列。然后到右二，每个单词都有一个读音模型，也就是每个单词都有自己的指定发音方法（音标），读音模型可以将文本序列转换为读音token序列，然后将这些模型传递给给右三声学模型（acoustic models），声学模型基本上给出一个token听起来是什么样，一般由高斯混合模型来构建，然后这个声学模型会输出一组一组的声音features，一般这些features是由信号处理专家定义的，就像是被捕捉到的声音的评率成分的特征一样（又被称为频谱图或者钟形滤波器组频图）。

如下图，应用神经网络，同架构，performance提升较大

2.端到端模</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/0.%E6%A6%82%E5%BF%B5/NLP_%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6Attenion%E6%A6%82%E8%A7%88/">
        <p class="h4 index-header">0.概念/NLP_注意力机制Attenion概览</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Attention注意力机制什么是attention？根据数据对当前任务的重要性维持的权重数组。
CV举例，如下图所示，要预测的对象是黄框 - 狗子的左耳朵，那么对人类来说，预测重要的信息就是红框中的狗子的右耳朵和狗眼睛还有狗鼻子，这些东西可以判断出这是一只狗子，通过右耳朵和眼睛可以对左耳朵进行定位。attention机制模仿人类的认知框架，着重注意红框中的有用信息，忽略例如背景等不重要的信息。
NLP举例，如下图，对词和词之间的关系，分配不同的注意力等级。green修饰apple，eat和apple只见也构成谓宾关系，于是他们都被分类了强注意力，而eat和green没有只见关系，于是只被分配了弱注意力(数组中的权重值低)。
进一步讲，注意力机制其实就是表示重要性的权重向量，比如上图识别狗子特征，发现嘴巴和鼻子这一块特别重要，于是就提高了权重值全部设置为1，可以全部采集，而其他不重要的地方比如背景之类就设置为0或者很小的数字，这样就降低了采集的强度。为了预测图像中的元素或者句子中的单词，我们使用注意力权重来估计其他元素与其相关的强度。并将注意力权重加权的值的总和作为最终预测的目标的一个</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>

  <div class="row mb-4 mx-auto">
    
    
    <div class="col-12 col-md-8 m-auto">
      <a href="/2020/02/24/3.%E8%AF%BE%E7%A8%8B/CS224n_1%20-%20%E5%AF%BC%E8%AE%BA/">
        <p class="h4 index-header">3.课程/CS224n_1 - 导论</p>
        
        
          
        
        <div class="index-excerpt" >
          <div class="index-text mb-1">Lecture1-导论1.本课程研究对象NLP是计算机科学和语言学的交叉学科。
语言是人类特有的属性，人类通过语言思考、交流、行动。
nlp要解决的核心问题是让计算机了解人类语言，然后完成有意义的任务（知识图谱，语义识别等）
nlp的步骤


预处理：首先speech和text都会被拆分
构词法分析：Morphological analysis，拆开ing和ex等前缀后缀，还原不同时态的词根。
句子结构分析：Syntactic analysis，比如主语是我，宾语是你，谓语是艹。
语义解释：Semantic interpretation，得出句子的含义。仅知道每个单词的含义是不够的，许多单词在一块构成了上下文，许多含义是依靠上下文传递的。这就引出了语用学和语篇处理等研究领域。

cs224n这门课主要讨论句子结构分析和语义解释
2.人类语言的特殊之处做信号处理，数据挖掘之类的事情时人类往往只用视觉系统识别数据，然后处理数据。
但几乎所有的人类语言都是某人想要传递某个信息，他为此构建了一条信息，并试图传递给另一个人。语言有明确的指向性。
人类的语言系统是一套离散的符号分类信号系统，如下图</div>
        </div>
      </a>

      <div>
        
          <i class="iconfont icon-riqi2"></i>&nbsp;2020-02-24&nbsp;&nbsp;
        
        
        
      </div>
    </div>
  </div>



  <nav aria-label="index posts navigation">
    <span class="pagination pg-blue justify-content-center mt-5" id="pagination">
      <a class="extend prev" rel="prev" href="/page/4/"><i class="fas fa-angle-double-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span>
    </span>
  </nav>
  
  <script>
    for (ele of document.getElementById("pagination").getElementsByClassName("page-number")) {
      ele.href += '#board';
    }
  </script>



              </div>
            </div>
          </div>
        </div>
      </div>
    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>

    
  
    <!-- 不蒜子统计PV -->
    
    &nbsp;<span id="busuanzi_container_site_pv">总访问量 
          <span id="busuanzi_value_site_pv"></span> 次</span>&nbsp;
  
  
    <!-- 不蒜子统计UV -->
    
    &nbsp;<span id="busuanzi_container_site_uv">总访客数 
            <span id="busuanzi_value_site_uv"></span> 人</span>&nbsp;
  
  <br>



    

  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js" ></script>
<script src="/lib/popper/popper.min.js" ></script>
<script src="/lib/bootstrap/js/bootstrap.min.js" ></script>
<script src="/lib/mdbootstrap/js/mdb.min.js" ></script>
<script src="/js/main.js" ></script>






  <script src="/lib/smooth-scroll/smooth-scroll.min.js" ></script>



  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>


<!-- Plugins -->


  

  

  

  

  




  <script src="/lib/prettify/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "暗影疾行&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 120,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="/lib/anchor/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "false",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>







</body>
</html>
